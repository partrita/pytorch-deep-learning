<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; 03 - PyTorch 컴퓨터 비전 – 파이토치 딥러닝 입문</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04_pytorch_custom_datasets.html" rel="next">
<link href="./02_pytorch_classification.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-803ae4f7a8810f475153517dc3c4ebae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03_pytorch_computer_vision.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">파이토치 딥러닝 입문</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">딥러닝을 위한 PyTorch 배우기</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_pytorch_fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">00 - PyTorch 기초</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_pytorch_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">01 - PyTorch 워크플로우</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_pytorch_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">02 - PyTorch 신경망 분류</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_pytorch_computer_vision.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_pytorch_custom_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">04 - PyTorch 사용자 정의 데이터셋</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_pytorch_going_modular.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">05 - PyTorch 모듈화</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_pytorch_transfer_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">06 - PyTorch 전이 학습</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_pytorch_experiment_tracking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">07 - PyTorch 실험 추적</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_pytorch_paper_replicating.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">08 - PyTorch 논문 복제</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_pytorch_model_deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">09 - PyTorch 모델 배포</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#where-does-computer-vision-get-used" id="toc-where-does-computer-vision-get-used" class="nav-link active" data-scroll-target="#where-does-computer-vision-get-used"><span class="header-section-number">5.1</span> Where does computer vision get used?</a></li>
  <li><a href="#what-were-going-to-cover" id="toc-what-were-going-to-cover" class="nav-link" data-scroll-target="#what-were-going-to-cover"><span class="header-section-number">5.2</span> What we’re going to cover</a></li>
  <li><a href="#where-can-can-you-get-help" id="toc-where-can-can-you-get-help" class="nav-link" data-scroll-target="#where-can-can-you-get-help"><span class="header-section-number">5.3</span> Where can can you get help?</a></li>
  <li><a href="#computer-vision-libraries-in-pytorch" id="toc-computer-vision-libraries-in-pytorch" class="nav-link" data-scroll-target="#computer-vision-libraries-in-pytorch"><span class="header-section-number">5.4</span> 0. Computer vision libraries in PyTorch</a></li>
  <li><a href="#getting-a-dataset" id="toc-getting-a-dataset" class="nav-link" data-scroll-target="#getting-a-dataset"><span class="header-section-number">5.5</span> 1. Getting a dataset</a>
  <ul class="collapse">
  <li><a href="#input-and-output-shapes-of-a-computer-vision-model" id="toc-input-and-output-shapes-of-a-computer-vision-model" class="nav-link" data-scroll-target="#input-and-output-shapes-of-a-computer-vision-model"><span class="header-section-number">5.5.1</span> 1.1 Input and output shapes of a computer vision model</a></li>
  <li><a href="#visualizing-our-data" id="toc-visualizing-our-data" class="nav-link" data-scroll-target="#visualizing-our-data"><span class="header-section-number">5.5.2</span> 1.2 Visualizing our data</a></li>
  </ul></li>
  <li><a href="#prepare-dataloader" id="toc-prepare-dataloader" class="nav-link" data-scroll-target="#prepare-dataloader"><span class="header-section-number">5.6</span> 2. Prepare DataLoader</a></li>
  <li><a href="#model-0-build-a-baseline-model" id="toc-model-0-build-a-baseline-model" class="nav-link" data-scroll-target="#model-0-build-a-baseline-model"><span class="header-section-number">5.7</span> 3. Model 0: Build a baseline model</a>
  <ul class="collapse">
  <li><a href="#setup-loss-optimizer-and-evaluation-metrics" id="toc-setup-loss-optimizer-and-evaluation-metrics" class="nav-link" data-scroll-target="#setup-loss-optimizer-and-evaluation-metrics"><span class="header-section-number">5.7.1</span> 3.1 Setup loss, optimizer and evaluation metrics</a></li>
  <li><a href="#creating-a-function-to-time-our-experiments" id="toc-creating-a-function-to-time-our-experiments" class="nav-link" data-scroll-target="#creating-a-function-to-time-our-experiments"><span class="header-section-number">5.7.2</span> 3.2 Creating a function to time our experiments</a></li>
  <li><a href="#creating-a-training-loop-and-training-a-model-on-batches-of-data" id="toc-creating-a-training-loop-and-training-a-model-on-batches-of-data" class="nav-link" data-scroll-target="#creating-a-training-loop-and-training-a-model-on-batches-of-data"><span class="header-section-number">5.7.3</span> 3.3 Creating a training loop and training a model on batches of data</a></li>
  </ul></li>
  <li><a href="#make-predictions-and-get-model-0-results" id="toc-make-predictions-and-get-model-0-results" class="nav-link" data-scroll-target="#make-predictions-and-get-model-0-results"><span class="header-section-number">5.8</span> 4. Make predictions and get Model 0 results</a></li>
  <li><a href="#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one" id="toc-setup-device-agnostic-code-for-using-a-gpu-if-there-is-one" class="nav-link" data-scroll-target="#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one"><span class="header-section-number">5.9</span> 5. Setup device agnostic-code (for using a GPU if there is one)</a></li>
  <li><a href="#model-1-building-a-better-model-with-non-linearity" id="toc-model-1-building-a-better-model-with-non-linearity" class="nav-link" data-scroll-target="#model-1-building-a-better-model-with-non-linearity"><span class="header-section-number">5.10</span> 6. Model 1: Building a better model with non-linearity</a>
  <ul class="collapse">
  <li><a href="#setup-loss-optimizer-and-evaluation-metrics-1" id="toc-setup-loss-optimizer-and-evaluation-metrics-1" class="nav-link" data-scroll-target="#setup-loss-optimizer-and-evaluation-metrics-1"><span class="header-section-number">5.10.1</span> 6.1 Setup loss, optimizer and evaluation metrics</a></li>
  <li><a href="#functionizing-training-and-test-loops" id="toc-functionizing-training-and-test-loops" class="nav-link" data-scroll-target="#functionizing-training-and-test-loops"><span class="header-section-number">5.10.2</span> 6.2 Functionizing training and test loops</a></li>
  </ul></li>
  <li><a href="#model-2-building-a-convolutional-neural-network-cnn" id="toc-model-2-building-a-convolutional-neural-network-cnn" class="nav-link" data-scroll-target="#model-2-building-a-convolutional-neural-network-cnn"><span class="header-section-number">5.11</span> 7. Model 2: Building a Convolutional Neural Network (CNN)</a>
  <ul class="collapse">
  <li><a href="#what-model-should-i-use" id="toc-what-model-should-i-use" class="nav-link" data-scroll-target="#what-model-should-i-use"><span class="header-section-number">5.11.1</span> What model should I use?</a></li>
  <li><a href="#stepping-through-nn.conv2d" id="toc-stepping-through-nn.conv2d" class="nav-link" data-scroll-target="#stepping-through-nn.conv2d"><span class="header-section-number">5.11.2</span> 7.1 Stepping through <code>nn.Conv2d()</code></a></li>
  <li><a href="#stepping-through-nn.maxpool2d" id="toc-stepping-through-nn.maxpool2d" class="nav-link" data-scroll-target="#stepping-through-nn.maxpool2d"><span class="header-section-number">5.11.3</span> 7.2 Stepping through <code>nn.MaxPool2d()</code></a></li>
  <li><a href="#setup-a-loss-function-and-optimizer-for-model_2" id="toc-setup-a-loss-function-and-optimizer-for-model_2" class="nav-link" data-scroll-target="#setup-a-loss-function-and-optimizer-for-model_2"><span class="header-section-number">5.11.4</span> 7.3 Setup a loss function and optimizer for <code>model_2</code></a></li>
  <li><a href="#training-and-testing-model_2-using-our-training-and-test-functions" id="toc-training-and-testing-model_2-using-our-training-and-test-functions" class="nav-link" data-scroll-target="#training-and-testing-model_2-using-our-training-and-test-functions"><span class="header-section-number">5.11.5</span> 7.4 Training and testing <code>model_2</code> using our training and test functions</a></li>
  </ul></li>
  <li><a href="#compare-model-results-and-training-time" id="toc-compare-model-results-and-training-time" class="nav-link" data-scroll-target="#compare-model-results-and-training-time"><span class="header-section-number">5.12</span> 8. Compare model results and training time</a></li>
  <li><a href="#make-and-evaluate-random-predictions-with-best-model" id="toc-make-and-evaluate-random-predictions-with-best-model" class="nav-link" data-scroll-target="#make-and-evaluate-random-predictions-with-best-model"><span class="header-section-number">5.13</span> 9. Make and evaluate random predictions with best model</a></li>
  <li><a href="#making-a-confusion-matrix-for-further-prediction-evaluation" id="toc-making-a-confusion-matrix-for-further-prediction-evaluation" class="nav-link" data-scroll-target="#making-a-confusion-matrix-for-further-prediction-evaluation"><span class="header-section-number">5.14</span> 10. Making a confusion matrix for further prediction evaluation</a></li>
  <li><a href="#save-and-load-best-performing-model" id="toc-save-and-load-best-performing-model" class="nav-link" data-scroll-target="#save-and-load-best-performing-model"><span class="header-section-number">5.15</span> 11. Save and load best performing model</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.16</span> Exercises</a></li>
  <li><a href="#extra-curriculum" id="toc-extra-curriculum" class="nav-link" data-scroll-target="#extra-curriculum"><span class="header-section-number">5.17</span> Extra-curriculum</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<p><a href="https://en.wikipedia.org/wiki/Computer_vision">Computer vision</a> is the art of teaching a computer to see.</p>
<p>For example, it could involve building a model to classify whether a photo is of a cat or a dog (<a href="https://developers.google.com/machine-learning/glossary#binary-classification">binary classification</a>).</p>
<p>Or whether a photo is of a cat, dog or chicken (<a href="https://developers.google.com/machine-learning/glossary#multi-class-classification">multi-class classification</a>).</p>
<p>Or identifying where a car appears in a video frame (<a href="https://en.wikipedia.org/wiki/Object_detection">object detection</a>).</p>
<p>Or figuring out where different objects in an image can be separated (<a href="https://arxiv.org/abs/1801.00868">panoptic segmentation</a>).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png" class="img-fluid" alt="example computer vision problems"> <em>Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.</em></p>
<section id="where-does-computer-vision-get-used" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="where-does-computer-vision-get-used"><span class="header-section-number">5.1</span> Where does computer vision get used?</h2>
<p>If you use a smartphone, you’ve already used computer vision.</p>
<p>Camera and photo apps use <a href="https://machinelearning.apple.com/research/panoptic-segmentation">computer vision to enhance</a> and sort images.</p>
<p>Modern cars use <a href="https://youtu.be/j0z4FweCy4M?t=2989">computer vision</a> to avoid other cars and stay within lane lines.</p>
<p>Manufacturers use computer vision to identify defects in various products.</p>
<p>Security cameras use computer vision to detect potential intruders.</p>
<p>In essence, anything that can described in a visual sense can be a potential computer vision problem.</p>
</section>
<section id="what-were-going-to-cover" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="what-were-going-to-cover"><span class="header-section-number">5.2</span> What we’re going to cover</h2>
<p>We’re going to apply the PyTorch Workflow we’ve been learning in the past couple of sections to computer vision.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png" class="img-fluid figure-img"></p>
<figcaption>a PyTorch workflow with a computer vision focus</figcaption>
</figure>
</div>
<p>Specifically, we’re going to cover:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Topic</strong></th>
<th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>0. Computer vision libraries in PyTorch</strong></td>
<td>PyTorch has a bunch of built-in helpful computer vision libraries, let’s check them out.</td>
</tr>
<tr class="even">
<td><strong>1. Load data</strong></td>
<td>To practice computer vision, we’ll start with some images of different pieces of clothing from <a href="https://github.com/zalandoresearch/fashion-mnist">FashionMNIST</a>.</td>
</tr>
<tr class="odd">
<td><strong>2. Prepare data</strong></td>
<td>We’ve got some images, let’s load them in with a <a href="https://pytorch.org/docs/stable/data.html">PyTorch <code>DataLoader</code></a> so we can use them with our training loop.</td>
</tr>
<tr class="even">
<td><strong>3. Model 0: Building a baseline model</strong></td>
<td>Here we’ll create a multi-class classification model to learn patterns in the data, we’ll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong>.</td>
</tr>
<tr class="odd">
<td><strong>4. Making predictions and evaluting model 0</strong></td>
<td>Let’s make some predictions with our baseline model and evaluate them.</td>
</tr>
<tr class="even">
<td><strong>5. Setup device agnostic code for future models</strong></td>
<td>It’s best practice to write device-agnostic code, so let’s set it up.</td>
</tr>
<tr class="odd">
<td><strong>6. Model 1: Adding non-linearity</strong></td>
<td>Experimenting is a large part of machine learning, let’s try and improve upon our baseline model by adding non-linear layers.</td>
</tr>
<tr class="even">
<td><strong>7. Model 2: Convolutional Neural Network (CNN)</strong></td>
<td>Time to get computer vision specific and introduce the powerful convolutional neural network architecture.</td>
</tr>
<tr class="odd">
<td><strong>8. Comparing our models</strong></td>
<td>We’ve built three different models, let’s compare them.</td>
</tr>
<tr class="even">
<td><strong>9. Evaluating our best model</strong></td>
<td>Let’s make some predictons on random images and evaluate our best model.</td>
</tr>
<tr class="odd">
<td><strong>10. Making a confusion matrix</strong></td>
<td>A confusion matrix is a great way to evaluate a classification model, let’s see how we can make one.</td>
</tr>
<tr class="even">
<td><strong>11. Saving and loading the best performing model</strong></td>
<td>Since we might want to use our model for later, let’s save it and make sure it loads back in correctly.</td>
</tr>
</tbody>
</table>
</section>
<section id="where-can-can-you-get-help" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="where-can-can-you-get-help"><span class="header-section-number">5.3</span> Where can can you get help?</h2>
<p>All of the materials for this course <a href="https://github.com/mrdbourke/pytorch-deep-learning">live on GitHub</a>.</p>
<p>If you run into trouble, you can ask a question on the course <a href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">GitHub Discussions page</a> there too.</p>
<p>And of course, there’s the <a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> and <a href="https://discuss.pytorch.org/">PyTorch developer forums</a>, a very helpful place for all things PyTorch.</p>
</section>
<section id="computer-vision-libraries-in-pytorch" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="computer-vision-libraries-in-pytorch"><span class="header-section-number">5.4</span> 0. Computer vision libraries in PyTorch</h2>
<p>Before we get started writing code, let’s talk about some PyTorch computer vision libraries you should be aware of.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>PyTorch module</th>
<th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://pytorch.org/vision/stable/index.html"><code>torchvision</code></a></td>
<td>Contains datasets, model architectures and image transformations often used for computer vision problems.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/vision/stable/datasets.html"><code>torchvision.datasets</code></a></td>
<td>Here you’ll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains <a href="https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets">a series of base classes for making custom datasets</a>.</td>
</tr>
<tr class="odd">
<td><a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a></td>
<td>This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/vision/stable/transforms.html"><code>torchvision.transforms</code></a></td>
<td>Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.</td>
</tr>
<tr class="odd">
<td><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.Dataset</code></a></td>
<td>Base dataset class for PyTorch.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/docs/stable/data.html#module-torch.utils.data"><code>torch.utils.data.DataLoader</code></a></td>
<td>Creates a Python iterable over a dataset (created with <code>torch.utils.data.Dataset</code>).</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Note:</strong> The <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> classes aren’t only for computer vision in PyTorch, they are capable of dealing with many different types of data.</p>
</blockquote>
<p>Now we’ve covered some of the most important PyTorch computer vision libraries, let’s import the relevant dependencies.</p>
<div id="c263a60d-d788-482f-b9e7-9cab4f6b1f72" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import PyTorch</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Import torchvision </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib for visualization</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Check versions</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ch">\n</span><span class="ss">torchvision version: </span><span class="sc">{</span>torchvision<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 1.11.0
torchvision version: 0.12.0</code></pre>
</div>
</div>
</section>
<section id="getting-a-dataset" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="getting-a-dataset"><span class="header-section-number">5.5</span> 1. Getting a dataset</h2>
<p>To begin working on a computer vision problem, let’s get a computer vision dataset.</p>
<p>We’re going to start with FashionMNIST.</p>
<p>MNIST stands for Modified National Institute of Standards and Technology.</p>
<p>The <a href="https://en.wikipedia.org/wiki/MNIST_database">original MNIST dataset</a> contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.</p>
<p><a href="https://github.com/zalandoresearch/fashion-mnist">FashionMNIST</a>, made by Zalando Research, is a similar setup.</p>
<p>Except it contains grayscale images of 10 different kinds of clothing.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png" class="img-fluid" alt="example image of FashionMNIST"> <em><code>torchvision.datasets</code> contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it’s a multi-class classification problem.</em></p>
<p>Later, we’ll be building a computer vision neural network to identify the different styles of clothing in these images.</p>
<p>PyTorch has a bunch of common computer vision datasets stored in <code>torchvision.datasets</code>.</p>
<p>Including FashionMNIST in <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html"><code>torchvision.datasets.FashionMNIST()</code></a>.</p>
<p>To download it, we provide the following parameters: * <code>root: str</code> - which folder do you want to download the data to? * <code>train: Bool</code> - do you want the training or test split? * <code>download: Bool</code> - should the data be downloaded? * <code>transform: torchvision.transforms</code> - what transformations would you like to do on the data? * <code>target_transform</code> - you can transform the targets (labels) if you like too.</p>
<p>Many other datasets in <code>torchvision</code> have these parameter options.</p>
<div id="486f8377-6810-4367-859d-69dccc7aef95" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup training data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, <span class="co"># where to download data to?</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>, <span class="co"># get training data</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>, <span class="co"># download data if it doesn't exist on disk</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor(), <span class="co"># images come as PIL format, we want to turn into Torch tensors</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    target_transform<span class="op">=</span><span class="va">None</span> <span class="co"># you can transform labels as well</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup testing data</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>, <span class="co"># get test data</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s check out the first sample of the training data.</p>
<div id="43bfd3d9-a132-41e8-8ccd-5ae25a7da59a" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See first training sample</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>image, label <span class="op">=</span> train_data[<span class="dv">0</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>image, label</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,
           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0039, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,
           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,
           0.0157, 0.0000, 0.0000, 0.0118],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,
           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0471, 0.0392, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,
           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,
           0.3020, 0.5098, 0.2824, 0.0588],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,
           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,
           0.5529, 0.3451, 0.6745, 0.2588],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,
           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,
           0.4824, 0.7686, 0.8980, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,
           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,
           0.8745, 0.9608, 0.6784, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,
           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,
           0.8627, 0.9529, 0.7922, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,
           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,
           0.8863, 0.7725, 0.8196, 0.2039],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,
           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,
           0.9608, 0.4667, 0.6549, 0.2196],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,
           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,
           0.8510, 0.8196, 0.3608, 0.0000],
          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,
           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,
           0.8549, 1.0000, 0.3020, 0.0000],
          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,
           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,
           0.8784, 0.9569, 0.6235, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,
           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,
           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,
           0.9137, 0.9333, 0.8431, 0.0000],
          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,
           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,
           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,
           0.8627, 0.9098, 0.9647, 0.0000],
          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,
           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,
           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,
           0.8706, 0.8941, 0.8824, 0.0000],
          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,
           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,
           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,
           0.8745, 0.8784, 0.8980, 0.1137],
          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,
           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,
           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,
           0.8627, 0.8667, 0.9020, 0.2627],
          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,
           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,
           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,
           0.7098, 0.8039, 0.8078, 0.4510],
          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,
           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,
           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,
           0.6549, 0.6941, 0.8235, 0.3608],
          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,
           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,
           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,
           0.7529, 0.8471, 0.6667, 0.0000],
          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,
           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,
           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,
           0.3882, 0.2275, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,
           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000]]]),
 9)</code></pre>
</div>
</div>
<section id="input-and-output-shapes-of-a-computer-vision-model" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="input-and-output-shapes-of-a-computer-vision-model"><span class="header-section-number">5.5.1</span> 1.1 Input and output shapes of a computer vision model</h3>
<p>We’ve got a big tensor of values (the image) leading to a single value for the target (the label).</p>
<p>Let’s see the image shape.</p>
<div id="c2997d9f-b574-4d23-aa34-1a4df1751226" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What's the shape of the image?</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>torch.Size([1, 28, 28])</code></pre>
</div>
</div>
<p>The shape of the image tensor is <code>[1, 28, 28]</code> or more specifically:</p>
<pre><code>[color_channels=1, height=28, width=28]</code></pre>
<p>Having <code>color_channels=1</code> means the image is grayscale.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png" class="img-fluid" alt="example input and output shapes of the fashionMNIST problem"> <em>Various problems will have various input and output shapes. But the premise reamins: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.</em></p>
<p>If <code>color_channels=3</code>, the image comes in pixel values for red, green and blue (this is also known a the <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB color model</a>).</p>
<p>The order of our current tensor is often referred to as <code>CHW</code> (Color Channels, Height, Width).</p>
<p>There’s debate on whether images should be represented as <code>CHW</code> (color channels first) or <code>HWC</code> (color channels last).</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> You’ll also see <code>NCHW</code> and <code>NHWC</code> formats where <code>N</code> stands for <em>number of images</em>. For example if you have a <code>batch_size=32</code>, your tensor shape may be <code>[32, 1, 28, 28]</code>. We’ll cover batch sizes later.</p>
</blockquote>
<p>PyTorch generally accepts <code>NCHW</code> (channels first) as the default for many operators.</p>
<p>However, PyTorch also explains that <code>NHWC</code> (channels last) performs better and is <a href="https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice">considered best practice</a>.</p>
<p>For now, since our dataset and models are relatively small, this won’t make too much of a difference.</p>
<p>But keep it in mind for when you’re working on larger image datasets and using convolutional neural networks (we’ll see these later).</p>
<p>Let’s check out more shapes of our data.</p>
<div id="fc4f768c-c3f6-454d-a633-673ad1d6eca0" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># How many samples are there? </span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train_data.data), <span class="bu">len</span>(train_data.targets), <span class="bu">len</span>(test_data.data), <span class="bu">len</span>(test_data.targets)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(60000, 60000, 10000, 10000)</code></pre>
</div>
</div>
<p>So we’ve got 60,000 training samples and 10,000 testing samples.</p>
<p>What classes are there?</p>
<p>We can find these via the <code>.classes</code> attribute.</p>
<div id="e22849c6-d93f-4b38-8403-5ebf0deaf008" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See classes</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> train_data.classes</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>class_names</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>['T-shirt/top',
 'Trouser',
 'Pullover',
 'Dress',
 'Coat',
 'Sandal',
 'Shirt',
 'Sneaker',
 'Bag',
 'Ankle boot']</code></pre>
</div>
</div>
<p>Sweet! It looks like we’re dealing with 10 different kinds of clothes.</p>
<p>Because we’re working with 10 different classes, it means our problem is <strong>multi-class classification</strong>.</p>
<p>Let’s get visual.</p>
</section>
<section id="visualizing-our-data" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="visualizing-our-data"><span class="header-section-number">5.5.2</span> 1.2 Visualizing our data</h3>
<div id="b1df1f2c-28c9-43bf-aaef-cf996c9ae1c5" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>image, label <span class="op">=</span> train_data[<span class="dv">0</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image shape: </span><span class="sc">{</span>image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.squeeze()) <span class="co"># image shape is [1, 28, 28] (colour channels, height, width)</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.title(label)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image shape: torch.Size([1, 28, 28])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can turn the image into grayscale using the <code>cmap</code> parameter of <code>plt.imshow()</code>.</p>
<div id="92f09917-88f7-4446-b65f-baae586914c9" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p>
<p>Let’s view a few more.</p>
<div id="7188ed7a-5959-48c4-ac7f-19129a2adc83" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot more images</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>rows, cols <span class="op">=</span> <span class="dv">4</span>, <span class="dv">4</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, rows <span class="op">*</span> cols <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    random_idx <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(train_data), size<span class="op">=</span>[<span class="dv">1</span>]).item()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    img, label <span class="op">=</span> train_data[random_idx]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    fig.add_subplot(rows, cols, i)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    plt.imshow(img.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    plt.title(class_names[label])</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Hmmm, this dataset doesn’t look too aesthetic.</p>
<p>But the principles we’re going to learn on how to build a model for it will be similar across a wide range of computer vision problems.</p>
<p>In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.</p>
<p>Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?</p>
<p>You probably could.</p>
<p>But I think coding a model in PyTorch would be faster.</p>
<blockquote class="blockquote">
<p><strong>Question:</strong> Do you think the above data can be model with only straight (linear) lines? Or do you think you’d also need non-straight (non-linear) lines?</p>
</blockquote>
</section>
</section>
<section id="prepare-dataloader" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="prepare-dataloader"><span class="header-section-number">5.6</span> 2. Prepare DataLoader</h2>
<p>Now we’ve got a dataset ready to go.</p>
<p>The next step is to prepare it with a <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.DataLoader</code></a> or <code>DataLoader</code> for short.</p>
<p>The <code>DataLoader</code> does what you think it might do.</p>
<p>It helps load data into a model.</p>
<p>For training and for inference.</p>
<p>It turns a large <code>Dataset</code> into a Python iterable of smaller chunks.</p>
<p>These smaller chunks are called <strong>batches</strong> or <strong>mini-batches</strong> and can be set by the <code>batch_size</code> parameter.</p>
<p>Why do this?</p>
<p>Because it’s more computationally efficient.</p>
<p>In an ideal world you could do the forward pass and backward pass across all of your data at once.</p>
<p>But once you start using really large datasets, unless you’ve got infinite computing power, it’s easier to break them up into batches.</p>
<p>It also gives your model more opportunities to improve.</p>
<p>With <strong>mini-batches</strong> (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).</p>
<p>What’s a good batch size?</p>
<p><a href="https://twitter.com/ylecun/status/989610208497360896?s=20&amp;t=N96J_jotN--PYuJk2WcjMw">32 is a good place to start</a> for a fair amount of problems.</p>
<p>But since this is a value you can set (a <strong>hyperparameter</strong>) you can try all different kinds of values, though generally powers of 2 are used most often (e.g.&nbsp;32, 64, 128, 256, 512).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png" class="img-fluid" alt="an example of what a batched dataset looks like"> <em>Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.</em></p>
<p>Let’s create <code>DataLoader</code>’s for our training and test sets.</p>
<div id="bb2dbf90-a326-43cb-b25b-71af142fafeb" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the batch size hyperparameter</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn datasets into iterables (batches)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_data, <span class="co"># dataset to turn into iterable</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE, <span class="co"># how many samples per batch? </span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span> <span class="co"># shuffle data every epoch?</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_data,</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span> <span class="co"># don't necessarily have to shuffle the testing data</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's check out what we've created</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataloaders: </span><span class="sc">{</span>train_dataloader<span class="sc">,</span> test_dataloader<span class="sc">}</span><span class="ss">"</span>) </span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Length of train dataloader: </span><span class="sc">{</span><span class="bu">len</span>(train_dataloader)<span class="sc">}</span><span class="ss"> batches of </span><span class="sc">{</span>BATCH_SIZE<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Length of test dataloader: </span><span class="sc">{</span><span class="bu">len</span>(test_dataloader)<span class="sc">}</span><span class="ss"> batches of </span><span class="sc">{</span>BATCH_SIZE<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193a8a90&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193b0700&gt;)
Length of train dataloader: 1875 batches of 32
Length of test dataloader: 313 batches of 32</code></pre>
</div>
</div>
<div id="7a925ee7-484b-4149-be8f-3ad790172a5f" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out what's inside the training dataloader</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>train_features_batch, train_labels_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>train_features_batch.shape, train_labels_batch.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</code></pre>
</div>
</div>
<p>And we can see that the data remains unchanged by checking a single sample.</p>
<div id="c863d66a-49be-43be-84dc-372a5d6fc2c2" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Show a sample</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(train_features_batch), size<span class="op">=</span>[<span class="dv">1</span>]).item()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>img, label <span class="op">=</span> train_features_batch[random_idx], train_labels_batch[random_idx]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(img.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"Off"</span>)<span class="op">;</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image size: </span><span class="sc">{</span>img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, label size: </span><span class="sc">{</span>label<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image size: torch.Size([1, 28, 28])
Label: 6, label size: torch.Size([])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="model-0-build-a-baseline-model" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="model-0-build-a-baseline-model"><span class="header-section-number">5.7</span> 3. Model 0: Build a baseline model</h2>
<p>Data loaded and prepared!</p>
<p>Time to build a <strong>baseline model</strong> by subclassing <code>nn.Module</code>.</p>
<p>A <strong>baseline model</strong> is one of the simplest models you can imagine.</p>
<p>You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.</p>
<p>Our baseline will consist of two <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code>nn.Linear()</code></a> layers.</p>
<p>We’ve done this in a previous section but there’s going to one slight difference.</p>
<p>Because we’re working with image data, we’re going to use a different layer to start things off.</p>
<p>And that’s the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code>nn.Flatten()</code></a> layer.</p>
<p><code>nn.Flatten()</code> compresses the dimensions of a tensor into a single vector.</p>
<p>This is easier to understand when you see it.</p>
<div id="405319f1-f242-4bd9-90f5-3abdc50782ac" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a flatten layer</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>flatten_model <span class="op">=</span> nn.Flatten() <span class="co"># all nn modules function as a model (can do a forward pass)</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a single sample</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> train_features_batch[<span class="dv">0</span>]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the sample</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> flatten_model(x) <span class="co"># perform forward pass</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out what happened</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape before flattening: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [color_channels, height, width]"</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after flattening: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [color_channels, height*width]"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Try uncommenting below and see what happens</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">#print(x)</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co">#print(output)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]
Shape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]</code></pre>
</div>
</div>
<p>The <code>nn.Flatten()</code> layer took our shape from <code>[color_channels, height, width]</code> to <code>[color_channels, height*width]</code>.</p>
<p>Why do this?</p>
<p>Because we’ve now turned our pixel data from height and width dimensions into one long <strong>feature vector</strong>.</p>
<p>And <code>nn.Linear()</code> layers like their inputs to be in the form of feature vectors.</p>
<p>Let’s create our first model using <code>nn.Flatten()</code> as the first layer.</p>
<div id="1449f427-6859-41ae-8133-50b58ffbce72" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FashionMNISTModelV0(nn.Module):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), <span class="co"># neural networks like their inputs in vector form</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>input_shape, out_features<span class="op">=</span>hidden_units), <span class="co"># in_features = number of features in a data sample (784 pixels)</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units, out_features<span class="op">=</span>output_shape)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_stack(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Wonderful!</p>
<p>We’ve got a baseline model class we can use, now let’s instantiate a model.</p>
<p>We’ll need to set the following parameters: * <code>input_shape=784</code> - this is how many features you’ve got going in the model, in our case, it’s one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features). * <code>hidden_units=10</code> - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we’ll start with <code>10</code>. * <code>output_shape=len(class_names)</code> - since we’re working with a multi-class classification problem, we need an output neuron per class in our dataset.</p>
<p>Let’s create an instance of our model and send to the CPU for now (we’ll run a small test for running <code>model_0</code> on CPU vs.&nbsp;a similar model on GPU soon).</p>
<div id="dd18384a-76f9-4b5a-a013-fda077f16865" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Need to setup model with input parameters</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>model_0 <span class="op">=</span> FashionMNISTModelV0(input_shape<span class="op">=</span><span class="dv">784</span>, <span class="co"># one for every pixel (28x28)</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    hidden_units<span class="op">=</span><span class="dv">10</span>, <span class="co"># how many units in the hidden layer</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    output_shape<span class="op">=</span><span class="bu">len</span>(class_names) <span class="co"># one for every class</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>model_0.to(<span class="st">"cpu"</span>) <span class="co"># keep model on CPU to begin with </span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>FashionMNISTModelV0(
  (layer_stack): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=10, bias=True)
    (2): Linear(in_features=10, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<section id="setup-loss-optimizer-and-evaluation-metrics" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="setup-loss-optimizer-and-evaluation-metrics"><span class="header-section-number">5.7.1</span> 3.1 Setup loss, optimizer and evaluation metrics</h3>
<p>Since we’re working on a classification problem, let’s bring in our <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"><code>helper_functions.py</code> script</a> and subsequently the <code>accuracy_fn()</code> we defined in <a href="https://www.learnpytorch.io/02_pytorch_classification/">notebook 02</a>.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the <a href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics package</a>.</p>
</blockquote>
<div id="31c91f17-d810-46a4-97c3-c734f93430b1" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Download helper functions from Learn PyTorch repo (if not already downloaded)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> Path(<span class="st">"helper_functions.py"</span>).is_file():</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"helper_functions.py already exists, skipping download"</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"Downloading helper_functions.py"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Note: you need the "raw" GitHub URL for this to work</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>  request <span class="op">=</span> requests.get(<span class="st">"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py"</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"helper_functions.py"</span>, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    f.write(request.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>helper_functions.py already exists, skipping download</code></pre>
</div>
</div>
<div id="ce3d13b8-f018-4b44-8bba-375074dc4c5f" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import accuracy metric</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> accuracy_fn <span class="co"># Note: could also use torchmetrics.Accuracy()</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup loss function and optimizer</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss() <span class="co"># this is also called "criterion"/"cost function" in some places</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_0.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="creating-a-function-to-time-our-experiments" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="creating-a-function-to-time-our-experiments"><span class="header-section-number">5.7.2</span> 3.2 Creating a function to time our experiments</h3>
<p>Loss function and optimizer ready!</p>
<p>It’s time to start training a model.</p>
<p>But how about we do a little experiment while we train.</p>
<p>I mean, let’s make a timing function to measure the time it takes our model to train on CPU versus using a GPU.</p>
<p>We’ll train this model on the CPU but the next one on the GPU and see what happens.</p>
<p>Our timing function will import the <a href="https://docs.python.org/3/library/timeit.html#timeit.default_timer"><code>timeit.default_timer()</code> function</a> from the Python <a href="https://docs.python.org/3/library/timeit.html"><code>timeit</code> module</a>.</p>
<div id="31adc3fe-ce90-4b4e-b0d4-3613abae5714" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_train_time(start: <span class="bu">float</span>, end: <span class="bu">float</span>, device: torch.device <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Prints difference between start and end time.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">        start (float): Start time of computation (preferred in timeit format). </span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">        end (float): End time of computation.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">        device ([type], optional): Device that compute is running on. Defaults to None.</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">        float: time between start and end in seconds (higher is longer).</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    total_time <span class="op">=</span> end <span class="op">-</span> start</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Train time on </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>total_time<span class="sc">:.3f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_time</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="creating-a-training-loop-and-training-a-model-on-batches-of-data" class="level3" data-number="5.7.3">
<h3 data-number="5.7.3" class="anchored" data-anchor-id="creating-a-training-loop-and-training-a-model-on-batches-of-data"><span class="header-section-number">5.7.3</span> 3.3 Creating a training loop and training a model on batches of data</h3>
<p>Beautiful!</p>
<p>Looks like we’ve got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.</p>
<p>Let’s now create a training loop and a testing loop to train and evaluate our model.</p>
<p>We’ll be using the same steps as the previous notebook(s), though since our data is now in batch form, we’ll add another loop to loop through our data batches.</p>
<p>Our data batches are contained within our <code>DataLoader</code>s, <code>train_dataloader</code> and <code>test_dataloader</code> for the training and test data splits respectively.</p>
<p>A batch is <code>BATCH_SIZE</code> samples of <code>X</code> (features) and <code>y</code> (labels), since we’re using <code>BATCH_SIZE=32</code>, our batches have 32 samples of images and targets.</p>
<p>And since we’re computing on batches of data, our loss and evaluation metrics will be calculated <strong>per batch</strong> rather than across the whole dataset.</p>
<p>This means we’ll have to divide our loss and accuracy values by the number of batches in each dataset’s respective dataloader.</p>
<p>Let’s step through it: 1. Loop through epochs. 2. Loop through training batches, perform training steps, calculate the train loss <em>per batch</em>. 3. Loop through testing batches, perform testing steps, calculate the test loss <em>per batch</em>. 4. Print out what’s happening. 5. Time it all (for fun).</p>
<p>A fair few steps but…</p>
<p>…if in doubt, code it out.</p>
<div id="c07bbf10-81e3-47f0-990d-9a4a838276ab" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import tqdm for progress bar</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the seed and start the timer</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>train_time_start_on_cpu <span class="op">=</span> timer()</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of epochs (we'll keep this small for faster training times)</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create training and testing loop</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ch">\n</span><span class="ss">-------"</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add a loop to loop through training batches</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        model_0.train() </span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Forward pass</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model_0(X)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Calculate loss (per batch)</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss <span class="co"># accumulatively add up the loss per epoch </span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Loss backward</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. Optimizer step</span></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Print out how many samples have been seen</span></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">400</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Looked at </span><span class="sc">{</span>batch <span class="op">*</span> <span class="bu">len</span>(X)<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(train_dataloader.dataset)<span class="sc">}</span><span class="ss"> samples"</span>)</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Divide total train loss by length of train dataloader (average loss per batch per epoch)</span></span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">/=</span> <span class="bu">len</span>(train_dataloader)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Testing</span></span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup variables for accumulatively adding up loss and accuracy </span></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>    test_loss, test_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>    model_0.<span class="bu">eval</span>()</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> test_dataloader:</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 1. Forward pass</span></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>            test_pred <span class="op">=</span> model_0(X)</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 2. Calculate loss (accumatively)</span></span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(test_pred, y) <span class="co"># accumulatively add up the loss per epoch</span></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 3. Calculate accuracy (preds need to be same as y_true)</span></span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>            test_acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y, y_pred<span class="op">=</span>test_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculations on test metrics need to happen inside torch.inference_mode()</span></span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Divide total test loss by length of test dataloader (per batch)</span></span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">/=</span> <span class="bu">len</span>(test_dataloader)</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Divide total accuracy by length of test dataloader (per batch)</span></span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">/=</span> <span class="bu">len</span>(test_dataloader)</span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Print out what's happening</span></span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Train loss: </span><span class="sc">{</span>train_loss<span class="sc">:.5f}</span><span class="ss"> | Test loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">, Test acc: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate training time      </span></span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>train_time_end_on_cpu <span class="op">=</span> timer()</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>total_train_time_model_0 <span class="op">=</span> print_train_time(start<span class="op">=</span>train_time_start_on_cpu, </span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>                                           end<span class="op">=</span>train_time_end_on_cpu,</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>                                           device<span class="op">=</span><span class="bu">str</span>(<span class="bu">next</span>(model_0.parameters()).device))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0132aa6e08d747e7af202440f9615a86","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%

Epoch: 1
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%

Epoch: 2
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%

Train time on cpu: 14.975 seconds</code></pre>
</div>
</div>
<p>Nice! Looks like our baseline model did fairly well.</p>
<p>It didn’t take too long to train either, even just on the CPU, I wonder if it’ll speed up on the GPU?</p>
<p>Let’s write some code to evaluate our model.</p>
</section>
</section>
<section id="make-predictions-and-get-model-0-results" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="make-predictions-and-get-model-0-results"><span class="header-section-number">5.8</span> 4. Make predictions and get Model 0 results</h2>
<p>Since we’re going to be building a few models, it’s a good idea to write some code to evaluate them all in similar ways.</p>
<p>Namely, let’s create a function that takes in a trained model, a <code>DataLoader</code>, a loss function and an accuracy function.</p>
<p>The function will use the model to make predictions on the data in the <code>DataLoader</code> and then we can evaluate those predictions using the loss function and accuracy function.</p>
<div id="8317dd04-9de2-4fd7-97bd-1e202621397d" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model: torch.nn.Module, </span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>               data_loader: torch.utils.data.DataLoader, </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>               loss_fn: torch.nn.Module, </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>               accuracy_fn):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns a dictionary containing the results of model predicting on data_loader.</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_fn (torch.nn.Module): The loss function of model.</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">        (dict): Results of model making predictions on data_loader.</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> data_loader:</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Make predictions with the model</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(X)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accumulate the loss and accuracy values per batch</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> loss_fn(y_pred, y)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y, </span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>                                y_pred<span class="op">=</span>y_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>)) <span class="co"># For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale loss and acc to find the average loss/acc per batch</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"model_name"</span>: model.<span class="va">__class__</span>.<span class="va">__name__</span>, <span class="co"># only works when model was created with a class</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_loss"</span>: loss.item(),</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_acc"</span>: acc}</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate model 0 results on test dataset</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>model_0_results <span class="op">=</span> eval_model(model<span class="op">=</span>model_0, data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>model_0_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>{'model_name': 'FashionMNISTModelV0',
 'model_loss': 0.47663894295692444,
 'model_acc': 83.42651757188499}</code></pre>
</div>
</div>
<p>Looking good!</p>
<p>We can use this dictionary to compare the baseline model results to other models later on.</p>
</section>
<section id="setup-device-agnostic-code-for-using-a-gpu-if-there-is-one" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="setup-device-agnostic-code-for-using-a-gpu-if-there-is-one"><span class="header-section-number">5.9</span> 5. Setup device agnostic-code (for using a GPU if there is one)</h2>
<p>We’ve seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.</p>
</blockquote>
<p>Now let’s setup some <a href="https://pytorch.org/docs/stable/notes/cuda.html#best-practices">device-agnostic code</a> for our models and data to run on GPU if it’s available.</p>
<p>If you’re running this notebook on Google Colab, and you don’t a GPU turned on yet, it’s now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you’ll have to run all of the cells above by going <code>Runtime -&gt; Run before</code>.</p>
<div id="17b69fe9-f974-4538-922c-20c5cc8220cc" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup device agnostic code</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>'cuda'</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>Let’s build another model.</p>
</section>
<section id="model-1-building-a-better-model-with-non-linearity" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="model-1-building-a-better-model-with-non-linearity"><span class="header-section-number">5.10</span> 6. Model 1: Building a better model with non-linearity</h2>
<p>We learned about <a href="https://www.learnpytorch.io/02_pytorch_classification/#6-the-missing-piece-non-linearity">the power of non-linearity in notebook 02</a>.</p>
<p>Seeing the data we’ve been working with, do you think it needs non-linear functions?</p>
<p>And remember, linear means straight and non-linear means non-straight.</p>
<p>Let’s find out.</p>
<p>We’ll do so by recreating a similar model to before, except this time we’ll put non-linear functions (<code>nn.ReLU()</code>) in between each linear layer.</p>
<div id="2ccce5f2-b1e5-47a6-a7f3-6bc096b35ffb" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a model with non-linear and linear layers</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FashionMNISTModelV1(nn.Module):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), <span class="co"># flatten inputs into single vector</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>input_shape, out_features<span class="op">=</span>hidden_units),</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units, out_features<span class="op">=</span>output_shape),</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_stack(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>That looks good.</p>
<p>Now let’s instantiate it with the same settings we used before.</p>
<p>We’ll need <code>input_shape=784</code> (equal to the number of features of our image data), <code>hidden_units=10</code> (starting small and the same as our baseline model) and <code>output_shape=len(class_names)</code> (one output unit per class).</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again.</p>
</blockquote>
<div id="907091ec-7e46-470b-a305-788a3009b837" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>model_1 <span class="op">=</span> FashionMNISTModelV1(input_shape<span class="op">=</span><span class="dv">784</span>, <span class="co"># number of input features</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    hidden_units<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    output_shape<span class="op">=</span><span class="bu">len</span>(class_names) <span class="co"># number of output classes desired</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>).to(device) <span class="co"># send model to GPU if it's available</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="bu">next</span>(model_1.parameters()).device <span class="co"># check model device</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>device(type='cuda', index=0)</code></pre>
</div>
</div>
<section id="setup-loss-optimizer-and-evaluation-metrics-1" class="level3" data-number="5.10.1">
<h3 data-number="5.10.1" class="anchored" data-anchor-id="setup-loss-optimizer-and-evaluation-metrics-1"><span class="header-section-number">5.10.1</span> 6.1 Setup loss, optimizer and evaluation metrics</h3>
<p>As usual, we’ll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we’ll stick with accuracy for now).</p>
<div id="fe7e463b-d46c-4f00-853c-fdf0a28d74c8" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> accuracy_fn</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_1.parameters(), </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                            lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="functionizing-training-and-test-loops" class="level3" data-number="5.10.2">
<h3 data-number="5.10.2" class="anchored" data-anchor-id="functionizing-training-and-test-loops"><span class="header-section-number">5.10.2</span> 6.2 Functionizing training and test loops</h3>
<p>So far we’ve been writing train and test loops over and over.</p>
<p>Let’s write them again but this time we’ll put them in functions so they can be called again and again.</p>
<p>And because we’re using device-agnostic code now, we’ll be sure to call <code>.to(device)</code> on our feature (<code>X</code>) and target (<code>y</code>) tensors.</p>
<p>For the training loop we’ll create a function called <code>train_step()</code> which takes in a model, a <code>DataLoader</code> a loss function and an optimizer.</p>
<p>The testing loop will be similar but it’ll be called <code>test_step()</code> and it’ll take in a model, a <code>DataLoader</code>, a loss function and an evaluation function.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Since these are functions, you can customize them in any way you like. What we’re making here can be considered barebones training and testing functions for our specific classification use case.</p>
</blockquote>
<div id="3d239ed2-4028-4603-8db3-ffca2b727819" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(model: torch.nn.Module,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>               data_loader: torch.utils.data.DataLoader,</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>               loss_fn: torch.nn.Module,</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>               optimizer: torch.optim.Optimizer,</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>               accuracy_fn,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>               device: torch.device <span class="op">=</span> device):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    train_loss, train_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(data_loader):</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Send data to GPU</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Forward pass</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Calculate loss</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y,</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>                                 y_pred<span class="op">=</span>y_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>)) <span class="co"># Go from logits -&gt; pred labels</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Loss backward</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. Optimizer step</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate loss and accuracy per epoch and print out what's happening</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Train loss: </span><span class="sc">{</span>train_loss<span class="sc">:.5f}</span><span class="ss"> | Train accuracy: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_step(data_loader: torch.utils.data.DataLoader,</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>              model: torch.nn.Module,</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>              loss_fn: torch.nn.Module,</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>              accuracy_fn,</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>              device: torch.device <span class="op">=</span> device):</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>    test_loss, test_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="co"># put model in eval mode</span></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn on inference context manager</span></span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode(): </span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> data_loader:</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Send data to GPU</span></span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>            X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 1. Forward pass</span></span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>            test_pred <span class="op">=</span> model(X)</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 2. Calculate loss and accuracy</span></span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(test_pred, y)</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>            test_acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y,</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>                y_pred<span class="op">=</span>test_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Go from logits -&gt; pred labels</span></span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust metrics and print out</span></span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Test loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss"> | Test accuracy: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Woohoo!</p>
<p>Now we’ve got some functions for training and testing our model, let’s run them.</p>
<p>We’ll do so inside another loop for each epoch.</p>
<p>That way for each epoch we’re going a training and a testing step.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.</p>
</blockquote>
<p>Let’s also time things to see how long our code takes to run on the GPU.</p>
<div id="2bb8094b-01a0-4b84-9526-ba8888d04901" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure time</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>train_time_start_on_gpu <span class="op">=</span> timer()</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ch">\n</span><span class="ss">---------"</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    train_step(data_loader<span class="op">=</span>train_dataloader, </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_1, </span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    test_step(data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_1,</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>train_time_end_on_gpu <span class="op">=</span> timer()</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>total_train_time_model_1 <span class="op">=</span> print_train_time(start<span class="op">=</span>train_time_start_on_gpu,</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>                                            end<span class="op">=</span>train_time_end_on_gpu,</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>                                            device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"03eed0dd4e134aea983b4c5383502cf0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0
---------
Train loss: 1.09199 | Train accuracy: 61.34%
Test loss: 0.95636 | Test accuracy: 65.00%

Epoch: 1
---------
Train loss: 0.78101 | Train accuracy: 71.93%
Test loss: 0.72227 | Test accuracy: 73.91%

Epoch: 2
---------
Train loss: 0.67027 | Train accuracy: 75.94%
Test loss: 0.68500 | Test accuracy: 75.02%

Train time on cuda: 16.943 seconds</code></pre>
</div>
</div>
<p>Excellent!</p>
<p>Our model trained but the training time took longer?</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you’re using. Read on for a more explained answer.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Question:</strong> “I used a a GPU but my model didn’t train faster, why might that be?”</p>
<p><strong>Answer:</strong> Well, one reason could be because your dataset and model are both so small (like the dataset and model we’re working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.</p>
<p>There’s a small bottleneck between copying data from the CPU memory (default) to the GPU memory.</p>
<p>So for smaller models and datasets, the CPU might actually be the optimal place to compute on.</p>
<p>But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.</p>
<p>However, this is largely dependant on the hardware you’re using. With practice, you will get used to where the best place to train your models is.</p>
</blockquote>
<p>Let’s evaluate our trained <code>model_1</code> using our <code>eval_model()</code> function and see how it went.</p>
<div id="32a544e3-9dbe-4aa1-b074-22e28b8f2f2a" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: This will error due to `eval_model()` not using device agnostic code </span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>model_1_results <span class="op">=</span> eval_model(model<span class="op">=</span>model_1, </span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, </span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    accuracy_fn<span class="op">=</span>accuracy_fn) </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>model_1_results </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">RuntimeError</span>                              Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_1084458/2906876561.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg ansi-bold">      2</span> 
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-red-fg"># Note: This will error due to `eval_model()` not using device agnostic code</span>
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> model_1_results = eval_model(model=model_1, 
</span><span class="ansi-green-fg ansi-bold">      5</span>     data_loader<span class="ansi-blue-fg">=</span>test_dataloader<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg ansi-bold">      6</span>     loss_fn<span class="ansi-blue-fg">=</span>loss_fn<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">/tmp/ipykernel_1084458/2300884397.py</span> in <span class="ansi-cyan-fg">eval_model</span><span class="ansi-blue-fg">(model, data_loader, loss_fn, accuracy_fn)</span>
<span class="ansi-green-fg ansi-bold">     20</span>         <span class="ansi-green-fg">for</span> X<span class="ansi-blue-fg">,</span> y <span class="ansi-green-fg">in</span> data_loader<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg ansi-bold">     21</span>             <span class="ansi-red-fg"># Make predictions with the model</span>
<span class="ansi-green-fg">---&gt; 22</span><span class="ansi-red-fg">             </span>y_pred <span class="ansi-blue-fg">=</span> model<span class="ansi-blue-fg">(</span>X<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">     23</span> 
<span class="ansi-green-fg ansi-bold">     24</span>             <span class="ansi-red-fg"># Accumulate the loss and accuracy values per batch</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1108</span>         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1109</span>                 or _global_forward_hooks or _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1110</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> forward_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">   1111</span>         <span class="ansi-red-fg"># Do not call functions when jit is used</span>
<span class="ansi-green-fg ansi-bold">   1112</span>         full_backward_hooks<span class="ansi-blue-fg">,</span> non_full_backward_hooks <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/tmp/ipykernel_1084458/3744982926.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, x)</span>
<span class="ansi-green-fg ansi-bold">     12</span> 
<span class="ansi-green-fg ansi-bold">     13</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">:</span> torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 14</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>layer_stack<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1108</span>         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1109</span>                 or _global_forward_hooks or _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1110</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> forward_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">   1111</span>         <span class="ansi-red-fg"># Do not call functions when jit is used</span>
<span class="ansi-green-fg ansi-bold">   1112</span>         full_backward_hooks<span class="ansi-blue-fg">,</span> non_full_backward_hooks <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/container.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, input)</span>
<span class="ansi-green-fg ansi-bold">    139</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> input<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg ansi-bold">    140</span>         <span class="ansi-green-fg">for</span> module <span class="ansi-green-fg">in</span> self<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 141</span><span class="ansi-red-fg">             </span>input <span class="ansi-blue-fg">=</span> module<span class="ansi-blue-fg">(</span>input<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">    142</span>         <span class="ansi-green-fg">return</span> input
<span class="ansi-green-fg ansi-bold">    143</span> 

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1108</span>         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1109</span>                 or _global_forward_hooks or _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1110</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> forward_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">   1111</span>         <span class="ansi-red-fg"># Do not call functions when jit is used</span>
<span class="ansi-green-fg ansi-bold">   1112</span>         full_backward_hooks<span class="ansi-blue-fg">,</span> non_full_backward_hooks <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/linear.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, input)</span>
<span class="ansi-green-fg ansi-bold">    101</span> 
<span class="ansi-green-fg ansi-bold">    102</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> input<span class="ansi-blue-fg">:</span> Tensor<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> Tensor<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 103</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> F<span class="ansi-blue-fg">.</span>linear<span class="ansi-blue-fg">(</span>input<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>weight<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>bias<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">    104</span> 
<span class="ansi-green-fg ansi-bold">    105</span>     <span class="ansi-green-fg">def</span> extra_repr<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> str<span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">RuntimeError</span>: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre>
</div>
</div>
</div>
<p>Oh no!</p>
<p>It looks like our <code>eval_model()</code> function errors out with:</p>
<blockquote class="blockquote">
<p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code></p>
</blockquote>
<p>It’s because we’ve setup our data and model to use device-agnostic code but not our evaluation function.</p>
<p>How about we fix that by passing a target <code>device</code> parameter to our <code>eval_model()</code> function?</p>
<p>Then we’ll try calculating the results again.</p>
<div id="f3665d99-1adc-4d9f-bfc6-e5601a80691c" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Move values to device</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model: torch.nn.Module, </span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>               data_loader: torch.utils.data.DataLoader, </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>               loss_fn: torch.nn.Module, </span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>               accuracy_fn, </span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>               device: torch.device <span class="op">=</span> device):</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluates a given model on a given dataset.</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_fn (torch.nn.Module): The loss function of model.</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co">        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="co">        device (str, optional): Target device to compute on. Defaults to device.</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">        (dict): Results of model making predictions on data_loader.</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> data_loader:</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Send data to the target device</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>            X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(X)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> loss_fn(y_pred, y)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y, y_pred<span class="op">=</span>y_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale loss and acc</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"model_name"</span>: model.<span class="va">__class__</span>.<span class="va">__name__</span>, <span class="co"># only works when model was created with a class</span></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_loss"</span>: loss.item(),</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_acc"</span>: acc}</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate model 1 results with device-agnostic code </span></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>model_1_results <span class="op">=</span> eval_model(model<span class="op">=</span>model_1, data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, accuracy_fn<span class="op">=</span>accuracy_fn,</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>model_1_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>{'model_name': 'FashionMNISTModelV1',
 'model_loss': 0.6850008964538574,
 'model_acc': 75.01996805111821}</code></pre>
</div>
</div>
<div id="a9e916cf-f873-4481-a983-bac26ce4cac2" class="cell" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check baseline results</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>model_0_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>{'model_name': 'FashionMNISTModelV0',
 'model_loss': 0.47663894295692444,
 'model_acc': 83.42651757188499}</code></pre>
</div>
</div>
<p>Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.</p>
<p>That’s a thing to note in machine learning, sometimes the thing you thought should work doesn’t.</p>
<p>And then the thing you thought might not work does.</p>
<p>It’s part science, part art.</p>
<p>From the looks of things, it seems like our model is <strong>overfitting</strong> on the training data.</p>
<p>Overfitting means our model is learning the training data well but those patterns aren’t generalizing to the testing data.</p>
<p>Two of the main to fix overfitting include: 1. Using a smaller or different model (some models fit certain kinds of data better than others). 2. Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).</p>
<p>There are more, but I’m going to leave that as a challenge for you to explore.</p>
<p>Try searching online, “ways to prevent overfitting in machine learning” and see what comes up.</p>
<p>In the meantime, let’s take a look at number 1: using a different model.</p>
</section>
</section>
<section id="model-2-building-a-convolutional-neural-network-cnn" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="model-2-building-a-convolutional-neural-network-cnn"><span class="header-section-number">5.11</span> 7. Model 2: Building a Convolutional Neural Network (CNN)</h2>
<p>Alright, time to step things up a notch.</p>
<p>It’s time to create a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network</a> (CNN or ConvNet).</p>
<p>CNN’s are known for their capabilities to find patterns in visual data.</p>
<p>And since we’re dealing with visual data, let’s see if using a CNN model can improve upon our baseline.</p>
<p>The CNN model we’re going to be using is known as TinyVGG from the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> website.</p>
<p>It follows the typical structure of a convolutional neural network:</p>
<p><code>Input layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer</code></p>
<p>Where the contents of <code>[Convolutional layer -&gt; activation layer -&gt; pooling layer]</code> can be upscaled and repeated multiple times, depending on requirements.</p>
<section id="what-model-should-i-use" class="level3" data-number="5.11.1">
<h3 data-number="5.11.1" class="anchored" data-anchor-id="what-model-should-i-use"><span class="header-section-number">5.11.1</span> What model should I use?</h3>
<blockquote class="blockquote">
<p><strong>Question:</strong> Wait, you say CNN’s are good for images, are there any other model types I should be aware of?</p>
</blockquote>
<p>Good question.</p>
<p>This table is a good general guide for which model to use (though there are exceptions).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Problem type</strong></th>
<th><strong>Model to use (generally)</strong></th>
<th><strong>Code example</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Structured data (Excel spreadsheets, row and column data)</td>
<td>Gradient boosted models, Random Forests, XGBoost</td>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"><code>sklearn.ensemble</code></a>, <a href="https://xgboost.readthedocs.io/en/stable/">XGBoost library</a></td>
</tr>
<tr class="even">
<td>Unstructured data (images, audio, language)</td>
<td>Convolutional Neural Networks, Transformers</td>
<td><a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a>, <a href="https://huggingface.co/docs/transformers/index">HuggingFace Transformers</a></td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Note:</strong> The table above is only for reference, the model you end up using will be highly dependant on the problem you’re working on and the constraints you have (amount of data, latency requirements).</p>
</blockquote>
<p>Enough talking about models, let’s now build a CNN that replicates the model on the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer website</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-cnn-explainer-model.png" class="img-fluid figure-img"></p>
<figcaption>TinyVGG architecture, as setup by CNN explainer website</figcaption>
</figure>
</div>
<p>To do so, we’ll leverage the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>nn.Conv2d()</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"><code>nn.MaxPool2d()</code></a> layers from <code>torch.nn</code>.</p>
<div id="dce60214-63fd-46e2-89ba-125445ac76b7" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a convolutional neural network </span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FashionMNISTModelV2(nn.Module):</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Model architecture copying TinyVGG from: </span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co">    https://poloclub.github.io/cnn-explainer/</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span>input_shape, </span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>                      out_channels<span class="op">=</span>hidden_units, </span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span><span class="dv">3</span>, <span class="co"># how big is the square that's going over the image?</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>                      stride<span class="op">=</span><span class="dv">1</span>, <span class="co"># default</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>                      padding<span class="op">=</span><span class="dv">1</span>),<span class="co"># options = "valid" (no padding) or "same" (output has same shape as input) or int for specific number </span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span>hidden_units, </span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>                      out_channels<span class="op">=</span>hidden_units,</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>                      stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>                      padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>                         stride<span class="op">=</span><span class="dv">2</span>) <span class="co"># default stride value is same as kernel_size</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(hidden_units, hidden_units, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(hidden_units, hidden_units, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">2</span>)</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Where did this in_features shape come from? </span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># It's because each layer of our network compresses and changes the shape of our inputs data.</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units<span class="op">*</span><span class="dv">7</span><span class="op">*</span><span class="dv">7</span>, </span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>                      out_features<span class="op">=</span>output_shape)</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block_1(x)</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(x.shape)</span></span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block_2(x)</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(x.shape)</span></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(x.shape)</span></span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>model_2 <span class="op">=</span> FashionMNISTModelV2(input_shape<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>    hidden_units<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>    output_shape<span class="op">=</span><span class="bu">len</span>(class_names)).to(device)</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>model_2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>FashionMNISTModelV2(
  (block_1): Sequential(
    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block_2): Sequential(
    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=490, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<p>Nice!</p>
<p>Our biggest model yet!</p>
<p>What we’ve done is a common practice in machine learning.</p>
<p>Find a model architecture somewhere and replicate it with code.</p>
</section>
<section id="stepping-through-nn.conv2d" class="level3" data-number="5.11.2">
<h3 data-number="5.11.2" class="anchored" data-anchor-id="stepping-through-nn.conv2d"><span class="header-section-number">5.11.2</span> 7.1 Stepping through <code>nn.Conv2d()</code></h3>
<p>We could start using our model above and see what happens but let’s first step through the two new layers we’ve added: * <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>nn.Conv2d()</code></a>, also known as a convolutional layer. * <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"><code>nn.MaxPool2d()</code></a>, also known as a max pooling layer.</p>
<blockquote class="blockquote">
<p><strong>Question:</strong> What does the “2d” in <code>nn.Conv2d()</code> stand for?</p>
<p>The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there’s color channel dimension but each of the color channel dimensions have two dimensions too: height and width.</p>
<p>For other dimensional data (such as 1D for text or 3D for 3D objects) there’s also <code>nn.Conv1d()</code> and <code>nn.Conv3d()</code>.</p>
</blockquote>
<p>To test the layers out, let’s create some toy data just like the data used on CNN Explainer.</p>
<div id="058b01ac-3f6a-4472-bcbf-3377974e3254" class="cell" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample batch of random numbers with same size as image batch</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> torch.randn(size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">64</span>)) <span class="co"># [batch_size, color_channels, height, width]</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>test_image <span class="op">=</span> images[<span class="dv">0</span>] <span class="co"># get a single image for testing</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image batch shape: </span><span class="sc">{</span>images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, color_channels, height, width]"</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Single image shape: </span><span class="sc">{</span>test_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [color_channels, height, width]"</span>) </span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Single image pixel values:</span><span class="ch">\n</span><span class="sc">{</span>test_image<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]
Single image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]
Single image pixel values:
tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],
         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],
         ...,
         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],
         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],
         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],

        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],
         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],
         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],
         ...,
         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],
         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],
         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],

        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],
         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],
         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],
         ...,
         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],
         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],
         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])</code></pre>
</div>
</div>
<p>Let’s create an example <code>nn.Conv2d()</code> with various parameters: * <code>in_channels</code> (int) - Number of channels in the input image. * <code>out_channels</code> (int) - Number of channels produced by the convolution. * <code>kernel_size</code> (int or tuple) - Size of the convolving kernel/filter. * <code>stride</code> (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1. * <code>padding</code> (int, tuple, str) - Padding added to all four sides of input. Default: 0.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/03-conv2d-layer.png" class="img-fluid figure-img"></p>
<figcaption>example of going through the different parameters of a Conv2d layer</figcaption>
</figure>
</div>
<p><em>Example of what happens when you change the hyperparameters of a <code>nn.Conv2d()</code> layer.</em></p>
<div id="ebd39562-1dad-40e3-90f5-750a5dac24e2" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a convolutional layer with same dimensions as TinyVGG </span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># (try changing any of the parameters and see what happens)</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>conv_layer <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>                       out_channels<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>                       kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>                       stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="dv">0</span>) <span class="co"># also try using "valid" or "same" here </span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the data through the convolutional layer</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>conv_layer(test_image) <span class="co"># Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) </span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],
         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],
         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],
         ...,
         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],
         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],
         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],

        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],
         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],
         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],
         ...,
         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],
         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],
         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],

        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],
         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],
         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],
         ...,
         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],
         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],
         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],

        ...,

        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],
         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],
         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],
         ...,
         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],
         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],
         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],

        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],
         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],
         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],
         ...,
         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],
         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],
         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],

        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],
         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],
         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],
         ...,
         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],
         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],
         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],
       grad_fn=&lt;SqueezeBackward1&gt;)</code></pre>
</div>
</div>
<p>If we try to pass a single image in, we get a shape mismatch error:</p>
<blockquote class="blockquote">
<p><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead</code></p>
<p><strong>Note:</strong> If you’re running PyTorch 1.11.0+, this error won’t occur.</p>
</blockquote>
<p>This is because our <code>nn.Conv2d()</code> layer expects a 4-dimensional tensor as input with size <code>(N, C, H, W)</code> or <code>[batch_size, color_channels, height, width]</code>.</p>
<p>Right now our single image <code>test_image</code> only has a shape of <code>[color_channels, height, width]</code> or <code>[3, 64, 64]</code>.</p>
<p>We can fix this for a single image using <code>test_image.unsqueeze(dim=0)</code> to add an extra dimension for <code>N</code>.</p>
<div id="abba741d-a1ed-44ed-ba53-41d589433a2c" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add extra dimension to test image</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>torch.Size([1, 3, 64, 64])</code></pre>
</div>
</div>
<div id="c7280a49-4ee0-452b-a514-61115b6a444c" class="cell" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass test image with extra dimension through conv_layer</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>conv_layer(test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>torch.Size([1, 10, 62, 62])</code></pre>
</div>
</div>
<p>Hmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a>), we get different channel sizes as well as different pixel sizes.</p>
<p>What if we changed the values of <code>conv_layer</code>?</p>
<div id="04445d45-cf2f-4c1d-b215-bc50865a207a" class="cell" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new conv_layer with different values (try setting these to whatever you like)</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>conv_layer_2 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, <span class="co"># same number of color channels as our input image</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>                         out_channels<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>                         kernel_size<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>), <span class="co"># kernel is usually a square so a tuple also works</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>                         stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>                         padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>conv_layer_2(test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>torch.Size([1, 10, 30, 30])</code></pre>
</div>
</div>
<p>Woah, we get another shape change.</p>
<p>Now our image is of shape <code>[1, 10, 30, 30]</code> (it will be different if you use different values) or <code>[batch_size=1, color_channels=10, height=30, width=30]</code>.</p>
<p>What’s going on here?</p>
<p>Behind the scenes, our <code>nn.Conv2d()</code> is compressing the information stored in the image.</p>
<p>It does this by performing operations on the input (our test image) against its internal parameters.</p>
<p>The goal of this is similar to all of the other neural networks we’ve been building.</p>
<p>Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.</p>
<p>The only difference is <em>how</em> the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer <code>forward()</code> method.</p>
<p>If we check out our <code>conv_layer_2.state_dict()</code> we’ll find a similar weight and bias setup as we’ve seen before.</p>
<div id="46027ed1-c3a7-46bd-bab7-17f8c20e354b" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out the conv_layer_2 internal parameters</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conv_layer_2.state_dict())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],
          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],
          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],
          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],
          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],

         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],
          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],
          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],
          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],
          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],

         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],
          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],
          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],
          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],
          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],


        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],
          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],
          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],
          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],
          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],

         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],
          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],
          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],
          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],
          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],

         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],
          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],
          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],
          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],
          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],


        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],
          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],
          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],
          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],
          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],

         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],
          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],
          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],
          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],
          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],

         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],
          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],
          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],
          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],
          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],


        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],
          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],
          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],
          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],
          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],

         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],
          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],
          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],
          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],
          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],

         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],
          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],
          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],
          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],
          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],


        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],
          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],
          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],
          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],
          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],

         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],
          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],
          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],
          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],
          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],

         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],
          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],
          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],
          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],
          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],


        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],
          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],
          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],
          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],
          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],

         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],
          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],
          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],
          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],
          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],

         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],
          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],
          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],
          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],
          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],


        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],
          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],
          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],
          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],
          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],

         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],
          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],
          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],
          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],
          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],

         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],
          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],
          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],
          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],
          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],


        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],
          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],
          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],
          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],
          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],

         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],
          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],
          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],
          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],
          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],

         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],
          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],
          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],
          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],
          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],


        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],
          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],
          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],
          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],
          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],

         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],
          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],
          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],
          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],
          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],

         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],
          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],
          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],
          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],
          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],


        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],
          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],
          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],
          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],
          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],

         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],
          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],
          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],
          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],
          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],

         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],
          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],
          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],
          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],
          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,
        -0.0797,  0.1121]))])</code></pre>
</div>
</div>
<p>Look at that! A bunch of random numbers for a weight and bias tensor.</p>
<p>The shapes of these are manipulated by the inputs we passed to <code>nn.Conv2d()</code> when we set it up.</p>
<p>Let’s check them out.</p>
<div id="e5518d61-c0b7-4351-b5ea-4d6b6144291a" class="cell" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get shapes of weight and bias tensors within conv_layer_2</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"conv_layer_2 weight shape: </span><span class="ch">\n</span><span class="sc">{</span>conv_layer_2<span class="sc">.</span>weight<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]"</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">conv_layer_2 bias shape: </span><span class="ch">\n</span><span class="sc">{</span>conv_layer_2<span class="sc">.</span>bias<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [out_channels=10]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>conv_layer_2 weight shape: 
torch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]

conv_layer_2 bias shape: 
torch.Size([10]) -&gt; [out_channels=10]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Question:</strong> What should we set the parameters of our <code>nn.Conv2d()</code> layers?</p>
<p>That’s a good one. But similar to many other things in machine learning, the values of these aren’t set in stone (and recall, because these values are ones we can set ourselves, they’re referred to as “<strong>hyperparameters</strong>”).</p>
<p>The best way to find out is to try out different values and see how they effect your model’s performance.</p>
<p>Or even better, find a working example on a problem similar to yours (like we’ve done with TinyVGG) and copy it.</p>
</blockquote>
<p>We’re working with a different of layer here to what we’ve seen before.</p>
<p>But the premise remains the same: start with random numbers and update them to better represent the data.</p>
</section>
<section id="stepping-through-nn.maxpool2d" class="level3" data-number="5.11.3">
<h3 data-number="5.11.3" class="anchored" data-anchor-id="stepping-through-nn.maxpool2d"><span class="header-section-number">5.11.3</span> 7.2 Stepping through <code>nn.MaxPool2d()</code></h3>
<p>Now let’s check out what happens when we move data through <code>nn.MaxPool2d()</code>.</p>
<div id="1164c753-19d9-43b7-a04f-017d0f7188c3" class="cell" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out original image shape without and with unsqueezed dimension</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test image original shape: </span><span class="sc">{</span>test_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test image with unsqueezed dimension: </span><span class="sc">{</span>test_image<span class="sc">.</span>unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a sample nn.MaxPoo2d() layer</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>max_pool_layer <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass data through just the conv_layer</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>test_image_through_conv <span class="op">=</span> conv_layer(test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after going through conv_layer(): </span><span class="sc">{</span>test_image_through_conv<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass data through the max pool layer</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>test_image_through_conv_and_max_pool <span class="op">=</span> max_pool_layer(test_image_through_conv)</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after going through conv_layer() and max_pool_layer(): </span><span class="sc">{</span>test_image_through_conv_and_max_pool<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test image original shape: torch.Size([3, 64, 64])
Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])
Shape after going through conv_layer(): torch.Size([1, 10, 62, 62])
Shape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])</code></pre>
</div>
</div>
<p>Notice the change in the shapes of what’s happening in and out of a <code>nn.MaxPool2d()</code> layer.</p>
<p>The <code>kernel_size</code> of the <code>nn.MaxPool2d()</code> layer will effects the size of the output shape.</p>
<p>In our case, the shape halves from a <code>62x62</code> image to <code>31x31</code> image.</p>
<p>Let’s see this work with a smaller tensor.</p>
<div id="e6a2b196-4845-4b40-9212-e75406e88875" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a random tensor with a similiar number of dimensions to our images</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>random_tensor <span class="op">=</span> torch.randn(size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor:</span><span class="ch">\n</span><span class="sc">{</span>random_tensor<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor shape: </span><span class="sc">{</span>random_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a max pool layer</span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>max_pool_layer <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>) <span class="co"># see what happens when you change the kernel_size value </span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the random tensor through the max pool layer</span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>max_pool_tensor <span class="op">=</span> max_pool_layer(random_tensor)</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Max pool tensor:</span><span class="ch">\n</span><span class="sc">{</span>max_pool_tensor<span class="sc">}</span><span class="ss"> &lt;- this is the maximum value from random_tensor"</span>)</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Max pool tensor shape: </span><span class="sc">{</span>max_pool_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random tensor:
tensor([[[[0.3367, 0.1288],
          [0.2345, 0.2303]]]])
Random tensor shape: torch.Size([1, 1, 2, 2])

Max pool tensor:
tensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor
Max pool tensor shape: torch.Size([1, 1, 1, 1])</code></pre>
</div>
</div>
<p>Notice the final two dimensions between <code>random_tensor</code> and <code>max_pool_tensor</code>, they go from <code>[2, 2]</code> to <code>[1, 1]</code>.</p>
<p>In essence, they get halved.</p>
<p>And the change would be different for different values of <code>kernel_size</code> for <code>nn.MaxPool2d()</code>.</p>
<p>Also notice the value leftover in <code>max_pool_tensor</code> is the <strong>maximum</strong> value from <code>random_tensor</code>.</p>
<p>What’s happening here?</p>
<p>This is another important piece of the puzzle of neural networks.</p>
<p>Essentially, <strong>every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space</strong>.</p>
<p>In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being <em>smaller</em> in size than the original values.</p>
<p>From an artificial intelligence perspective, you could consider the whole goal of a neural network to <em>compress</em> information.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png" class="img-fluid figure-img"></p>
<figcaption>each layer of a neural network compresses the original input data into a smaller representation that is (hopefully) capable of making predictions on future input data</figcaption>
</figure>
</div>
<p>This means, that from the point of view of a neural network, intelligence is compression.</p>
<p>This is the idea of the use of a <code>nn.MaxPool2d()</code> layer: take the maximum value from a portion of a tensor and disregard the rest.</p>
<p>In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.</p>
<p>It is the same story for a <code>nn.Conv2d()</code> layer.</p>
<p>Except instead of just taking the maximum, the <code>nn.Conv2d()</code> performs a conovlutional operation on the data (see this in action on the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer webpage</a>).</p>
<blockquote class="blockquote">
<p><strong>Exercise:</strong> What do you think the <a href="https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html"><code>nn.AvgPool2d()</code></a> layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Extra-curriculum:</strong> Lookup “most common convolutional neural networks”, what architectures do you find? Are any of them contained within the <a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a> library? What do you think you could do with these?</p>
</blockquote>
</section>
<section id="setup-a-loss-function-and-optimizer-for-model_2" class="level3" data-number="5.11.4">
<h3 data-number="5.11.4" class="anchored" data-anchor-id="setup-a-loss-function-and-optimizer-for-model_2"><span class="header-section-number">5.11.4</span> 7.3 Setup a loss function and optimizer for <code>model_2</code></h3>
<p>We’ve stepped through the layers in our first CNN enough.</p>
<p>But remember, if something still isn’t clear, try starting small.</p>
<p>Pick a single layer of a model, pass some data through it and see what happens.</p>
<p>Now it’s time to move forward and get to training!</p>
<p>Let’s setup a loss function and an optimizer.</p>
<p>We’ll use the functions as before, <code>nn.CrossEntropyLoss()</code> as the loss function (since we’re working with multi-class classification data).</p>
<p>And <code>torch.optim.SGD()</code> as the optimizer to optimize <code>model_2.parameters()</code> with a learning rate of <code>0.1</code>.</p>
<div id="06a76a1b-5f6f-4018-bf7b-8388b385476f" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup loss and optimizer</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_2.parameters(), </span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>                             lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="training-and-testing-model_2-using-our-training-and-test-functions" class="level3" data-number="5.11.5">
<h3 data-number="5.11.5" class="anchored" data-anchor-id="training-and-testing-model_2-using-our-training-and-test-functions"><span class="header-section-number">5.11.5</span> 7.4 Training and testing <code>model_2</code> using our training and test functions</h3>
<p>Loss and optimizer ready!</p>
<p>Time to train and test.</p>
<p>We’ll use our <code>train_step()</code> and <code>test_step()</code> functions we created before.</p>
<p>We’ll also measure the time to compare it to our other models.</p>
<div id="861d126e-d876-40b3-9b7a-66cfc2f1bf05" class="cell" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure time</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>train_time_start_model_2 <span class="op">=</span> timer()</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train and test model </span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ch">\n</span><span class="ss">---------"</span>)</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>    train_step(data_loader<span class="op">=</span>train_dataloader, </span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_2, </span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn,</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>    test_step(data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_2,</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn,</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>train_time_end_model_2 <span class="op">=</span> timer()</span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>total_train_time_model_2 <span class="op">=</span> print_train_time(start<span class="op">=</span>train_time_start_model_2,</span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>                                           end<span class="op">=</span>train_time_end_model_2,</span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>                                           device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fc69ce706c5c45099e57919dfcae065c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0
---------
Train loss: 0.59411 | Train accuracy: 78.41%
Test loss: 0.39967 | Test accuracy: 85.70%

Epoch: 1
---------
Train loss: 0.36450 | Train accuracy: 86.81%
Test loss: 0.34607 | Test accuracy: 87.48%

Epoch: 2
---------
Train loss: 0.32553 | Train accuracy: 88.33%
Test loss: 0.32664 | Test accuracy: 88.23%

Train time on cuda: 21.099 seconds</code></pre>
</div>
</div>
<p>Woah! Looks like the convolutional and max pooling layers helped improve performance a little.</p>
<p>Let’s evaluate <code>model_2</code>’s results with our <code>eval_model()</code> function.</p>
<div id="c1bf8b89-1389-4395-a1c4-9c6e94d9e71c" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get model_2 results </span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>model_2_results <span class="op">=</span> eval_model(</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model_2,</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>model_2_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>{'model_name': 'FashionMNISTModelV2',
 'model_loss': 0.32664385437965393,
 'model_acc': 88.22883386581469}</code></pre>
</div>
</div>
</section>
</section>
<section id="compare-model-results-and-training-time" class="level2" data-number="5.12">
<h2 data-number="5.12" class="anchored" data-anchor-id="compare-model-results-and-training-time"><span class="header-section-number">5.12</span> 8. Compare model results and training time</h2>
<p>We’ve trained three different models.</p>
<ol type="1">
<li><code>model_0</code> - our baseline model with two <code>nn.Linear()</code> layers.</li>
<li><code>model_1</code> - the same setup as our baseline model except with <code>nn.ReLU()</code> layers in between the <code>nn.Linear()</code> layers.</li>
<li><code>model_2</code> - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.</li>
</ol>
<p>This is a regular practice in machine learning.</p>
<p>Building multiple models and performing multiple training experiments to see which performs best.</p>
<p>Let’s combine our model results dictionaries into a DataFrame and find out.</p>
<div id="52d84ee1-1ad4-4860-b147-f8912c1febc7" class="cell" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>compare_results <span class="op">=</span> pd.DataFrame([model_0_results, model_1_results, model_2_results])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>compare_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model_name</th>
<th data-quarto-table-cell-role="th">model_loss</th>
<th data-quarto-table-cell-role="th">model_acc</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>FashionMNISTModelV0</td>
<td>0.476639</td>
<td>83.426518</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>FashionMNISTModelV1</td>
<td>0.685001</td>
<td>75.019968</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>FashionMNISTModelV2</td>
<td>0.326644</td>
<td>88.228834</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Nice!</p>
<p>We can add the training time values too.</p>
<div id="297af38f-e69f-4c6f-9027-fcaf0482a55c" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add training times to results comparison</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>compare_results[<span class="st">"training_time"</span>] <span class="op">=</span> [total_train_time_model_0,</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                                    total_train_time_model_1,</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>                                    total_train_time_model_2]</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>compare_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model_name</th>
<th data-quarto-table-cell-role="th">model_loss</th>
<th data-quarto-table-cell-role="th">model_acc</th>
<th data-quarto-table-cell-role="th">training_time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>FashionMNISTModelV0</td>
<td>0.476639</td>
<td>83.426518</td>
<td>14.974887</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>FashionMNISTModelV1</td>
<td>0.685001</td>
<td>75.019968</td>
<td>16.942553</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>FashionMNISTModelV2</td>
<td>0.326644</td>
<td>88.228834</td>
<td>21.098929</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>It looks like our CNN (<code>FashionMNISTModelV2</code>) model performed the best (lowest loss, highest accuracy) but had the longest training time.</p>
<p>And our baseline model (<code>FashionMNISTModelV0</code>) performed better than <code>model_1</code> (<code>FashionMNISTModelV1</code>) but took longer to train (this is likely because we used a CPU to train <code>model_0</code> but a GPU to train <code>model_1</code>).</p>
<p>The tradeoffs here are known as the <strong>performance-speed</strong> tradeoff.</p>
<p>Generally, you get better performance out of a larger, more complex model (like we did with <code>model_2</code>).</p>
<p>However, this performance increase often comes at a sacrifice of training speed and inference speed.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> The training times you get will be very dependant on the hardware you use.</p>
<p>Generally, the more CPU cores you have, the faster your models will train on CPU. And similar for GPUs.</p>
<p>Newer hardware (in terms of age) will also often train models faster due to incorporating technology advances.</p>
</blockquote>
<p>How about we get visual?</p>
<div id="5eb0df60-9318-47d0-adce-f8788ed3999e" class="cell" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize our model results</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>compare_results.set_index(<span class="st">"model_name"</span>)[<span class="st">"model_acc"</span>].plot(kind<span class="op">=</span><span class="st">"barh"</span>)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"accuracy (%)"</span>)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"model"</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-46-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="make-and-evaluate-random-predictions-with-best-model" class="level2" data-number="5.13">
<h2 data-number="5.13" class="anchored" data-anchor-id="make-and-evaluate-random-predictions-with-best-model"><span class="header-section-number">5.13</span> 9. Make and evaluate random predictions with best model</h2>
<p>Alright, we’ve compared our models to each other, let’s further evaluate our best performing model, <code>model_2</code>.</p>
<p>To do so, let’s create a function <code>make_predictions()</code> where we can pass the model and some data for it to predict on.</p>
<div id="d1d5d3e7-9601-4141-8bd7-9abbd016bf6c" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_predictions(model: torch.nn.Module, data: <span class="bu">list</span>, device: torch.device <span class="op">=</span> device):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    pred_probs <span class="op">=</span> []</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sample <span class="kw">in</span> data:</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Prepare sample</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>            sample <span class="op">=</span> torch.unsqueeze(sample, dim<span class="op">=</span><span class="dv">0</span>).to(device) <span class="co"># Add an extra dimension and send sample to device</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass (model outputs raw logit)</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>            pred_logit <span class="op">=</span> model(sample)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get prediction probability (logit -&gt; prediction probability)</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>            pred_prob <span class="op">=</span> torch.softmax(pred_logit.squeeze(), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get pred_prob off GPU for further calculations</span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>            pred_probs.append(pred_prob.cpu())</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack the pred_probs to turn list into a tensor</span></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.stack(pred_probs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="420c7461-eaa9-4459-9e68-53574c758765" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>test_samples <span class="op">=</span> []</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> []</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample, label <span class="kw">in</span> random.sample(<span class="bu">list</span>(test_data), k<span class="op">=</span><span class="dv">9</span>):</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    test_samples.append(sample)</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    test_labels.append(label)</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a><span class="co"># View the first test sample shape and label</span></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test sample image shape: </span><span class="sc">{</span>test_samples[<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">Test sample label: </span><span class="sc">{</span>test_labels[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>class_names[test_labels[<span class="dv">0</span>]]<span class="sc">}</span><span class="ss">)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test sample image shape: torch.Size([1, 28, 28])
Test sample label: 5 (Sandal)</code></pre>
</div>
</div>
<p>And now we can use our <code>make_predictions()</code> function to predict on <code>test_samples</code>.</p>
<div id="79de2ac1-7d4b-4f81-ae8a-90099bca2a3d" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on test samples with model 2</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>pred_probs<span class="op">=</span> make_predictions(model<span class="op">=</span>model_2, </span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>                             data<span class="op">=</span>test_samples)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a><span class="co"># View first two prediction probabilities list</span></span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>pred_probs[:<span class="dv">2</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>tensor([[2.3550e-07, 1.7185e-08, 4.6618e-07, 6.1371e-08, 5.1185e-08, 9.9957e-01,
         3.7702e-07, 1.5924e-05, 3.7681e-05, 3.7831e-04],
        [7.3275e-02, 6.7410e-01, 3.7231e-03, 8.8129e-02, 1.0114e-01, 6.9186e-05,
         5.8674e-02, 4.2595e-04, 3.8635e-04, 7.1354e-05]])</code></pre>
</div>
</div>
<p>Excellent!</p>
<p>And now we can go from prediction probabilities to prediction labels by taking the <code>torch.argmax()</code> of the output of the <code>torch.softmax()</code> activation function.</p>
<div id="f9d97bcc-4310-4851-a1f8-6bcd757e9b26" class="cell" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn the prediction probabilities into prediction labels by taking the argmax()</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>pred_classes <span class="op">=</span> pred_probs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>pred_classes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])</code></pre>
</div>
</div>
<div id="1141af97-0990-4920-83d4-c13cca3f9abc" class="cell" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Are our predictions in the same form as our test labels? </span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>test_labels, pred_classes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))</code></pre>
</div>
</div>
<p>Now our predicted classes are in the same format as our test labels, we can compare.</p>
<p>Since we’re dealing with image data, let’s stay true to the data explorer’s motto.</p>
<p>“Visualize, visualize, visualize!”</p>
<div id="679cb5f7-bb66-42dd-a4d6-400b27b7c019" class="cell" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot predictions</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>nrows <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>ncols <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(test_samples):</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create a subplot</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>  plt.subplot(nrows, ncols, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot the target image</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>  plt.imshow(sample.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find the prediction label (in text form, e.g. "Sandal")</span></span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>  pred_label <span class="op">=</span> class_names[pred_classes[i]]</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Get the truth label (in text form, e.g. "T-shirt")</span></span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>  truth_label <span class="op">=</span> class_names[test_labels[i]] </span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create the title text of the plot</span></span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>  title_text <span class="op">=</span> <span class="ss">f"Pred: </span><span class="sc">{</span>pred_label<span class="sc">}</span><span class="ss"> | Truth: </span><span class="sc">{</span>truth_label<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check for equality and change title colour accordingly</span></span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> pred_label <span class="op">==</span> truth_label:</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a>      plt.title(title_text, fontsize<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span><span class="st">"g"</span>) <span class="co"># green text if correct</span></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb87-25"><a href="#cb87-25" aria-hidden="true" tabindex="-1"></a>      plt.title(title_text, fontsize<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span><span class="st">"r"</span>) <span class="co"># red text if wrong</span></span>
<span id="cb87-26"><a href="#cb87-26" aria-hidden="true" tabindex="-1"></a>  plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Well, well, well, doesn’t that look good!</p>
<p>Not bad for a couple dozen lines of PyTorch code!</p>
</section>
<section id="making-a-confusion-matrix-for-further-prediction-evaluation" class="level2" data-number="5.14">
<h2 data-number="5.14" class="anchored" data-anchor-id="making-a-confusion-matrix-for-further-prediction-evaluation"><span class="header-section-number">5.14</span> 10. Making a confusion matrix for further prediction evaluation</h2>
<p>There are many <a href="https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics">different evaluation metrics</a> we can use for classification problems.</p>
<p>One of the most visual is a <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">confusion matrix</a>.</p>
<p>A confusion matrix shows you where your classification model got confused between predicitons and true labels.</p>
<p>To make a confusion matrix, we’ll go through three steps: 1. Make predictions with our trained model, <code>model_2</code> (a confusion matrix compares predictions to true labels). 2. Make a confusion matrix using <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code>torch.ConfusionMatrix</code></a>. 3. Plot the confusion matrix using <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code>mlxtend.plotting.plot_confusion_matrix()</code></a>.</p>
<p>Let’s start by making predictions with our trained model.</p>
<div id="065b8090-c9c5-43df-b5c1-b45ba33af1be" class="cell" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import tqdm for progress bar</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Make predictions with trained model</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> []</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>model_2.<span class="bu">eval</span>()</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> X, y <span class="kw">in</span> tqdm(test_dataloader, desc<span class="op">=</span><span class="st">"Making predictions"</span>):</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Send data and targets to target device</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Do the forward pass</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    y_logit <span class="op">=</span> model_2(X)</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels</span></span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.softmax(y_logit.squeeze(), dim<span class="op">=</span><span class="dv">0</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Put predictions on CPU for evaluation</span></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>    y_preds.append(y_pred.cpu())</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate list of predictions into a tensor</span></span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>y_pred_tensor <span class="op">=</span> torch.cat(y_preds)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ddb83e1f77d840b981ecebc11d584069","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Wonderful!</p>
<p>Now we’ve got predictions, let’s go through steps 2 &amp; 3: 2. Make a confusion matrix using <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code>torchmetrics.ConfusionMatrix</code></a>. 3. Plot the confusion matrix using <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code>mlxtend.plotting.plot_confusion_matrix()</code></a>.</p>
<p>First we’ll need to make sure we’ve got <code>torchmetrics</code> and <code>mlxtend</code> installed (these two libraries will help us make and visual a confusion matrix).</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> If you’re using Google Colab, the default version of <code>mlxtend</code> installed is 0.14.0 (as of March 2022), however, for the parameters of the <code>plot_confusion_matrix()</code> function we’d like use, we need 0.19.0 or higher.</p>
</blockquote>
<div id="e6c0a05d-d3e0-4b86-9ef7-ee6ea5629b07" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See if torchmetrics exists, if not, install it</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchmetrics, mlxtend</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mlxtend version: </span><span class="sc">{</span>mlxtend<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">int</span>(mlxtend.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">19</span>, <span class="st">"mlxtend verison should be 0.19.0 or higher"</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">-</span>q torchmetrics <span class="op">-</span>U mlxtend <span class="co"># &lt;- Note: If you're using Google Colab, this may require restarting the runtime</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchmetrics, mlxtend</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mlxtend version: </span><span class="sc">{</span>mlxtend<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>mlxtend version: 0.19.0</code></pre>
</div>
</div>
<p>To plot the confusion matrix, we need to make sure we’ve got and <a href="http://rasbt.github.io/mlxtend/"><code>mlxtend</code></a> version of 0.19.0 or higher.</p>
<div id="21383f88-a2dd-4678-94c6-479c592da0ab" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import mlxtend upgraded version</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlxtend </span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mlxtend.__version__)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(mlxtend.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">19</span> <span class="co"># should be version 0.19.0 or higher</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.19.0</code></pre>
</div>
</div>
<p><code>torchmetrics</code> and <code>mlxtend</code> installed, let’s make a confusion matrix!</p>
<p>First we’ll create a <code>torchmetrics.ConfusionMatrix</code> instance telling it how many classes we’re dealing with by setting <code>num_classes=len(class_names)</code>.</p>
<p>Then we’ll create a confusion matrix (in tensor format) by passing our instance our model’s predictions (<code>preds=y_pred_tensor</code>) and targets (<code>target=test_data.targets</code>).</p>
<p>Finally we can plot our confision matrix using the <code>plot_confusion_matrix()</code> function from <code>mlxtend.plotting</code>.</p>
<div id="7aed6d76-ad1c-429e-b8e0-c80572e3ebf4" class="cell" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics <span class="im">import</span> ConfusionMatrix</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_confusion_matrix</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Setup confusion matrix instance and compare predictions to targets</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>confmat <span class="op">=</span> ConfusionMatrix(num_classes<span class="op">=</span><span class="bu">len</span>(class_names))</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>confmat_tensor <span class="op">=</span> confmat(preds<span class="op">=</span>y_pred_tensor,</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>                         target<span class="op">=</span>test_data.targets)</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Plot the confusion matrix</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plot_confusion_matrix(</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>    conf_mat<span class="op">=</span>confmat_tensor.numpy(), <span class="co"># matplotlib likes working with NumPy </span></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>    class_names<span class="op">=</span>class_names, <span class="co"># turn the row and column labels into class names</span></span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a>    figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>)</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Woah! Doesn’t that look good?</p>
<p>We can see our model does fairly well since most of the dark squares are down the diagonal from top left to bottom right (and ideal model will have only values in these squares and 0 everywhere else).</p>
<p>The model gets most “confused” on classes that are similar, for example predicting “Pullover” for images that are actually labelled “Shirt”.</p>
<p>And the same for predicting “Shirt” for classes that are actually labelled “T-shirt/top”.</p>
<p>This kind of information is often more helpful that a single accuracy metric because it tells use <em>where</em> a model is getting things wrong.</p>
<p>It also hints at <em>why</em> the model may be getting certain things wrong.</p>
<p>It’s understandable the model sometimes predicts “Shirt” for images labelled “T-shirt/top”.</p>
<p>We can use this kind of information to further inspect our models and data to see how it could be improved.</p>
<blockquote class="blockquote">
<p><strong>Exercise:</strong> Use the trained <code>model_2</code> to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image should’ve been. After visualing these predictions do you think it’s more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g.&nbsp;a “Shirt” label is too close to “T-shirt/top”)?</p>
</blockquote>
</section>
<section id="save-and-load-best-performing-model" class="level2" data-number="5.15">
<h2 data-number="5.15" class="anchored" data-anchor-id="save-and-load-best-performing-model"><span class="header-section-number">5.15</span> 11. Save and load best performing model</h2>
<p>Let’s finish this section off by saving and loading in our best performing model.</p>
<p>Recall from <a href="https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model">notebook 01</a> we can save and load a PyTorch model using a combination of: * <code>torch.save</code> - a function to save a whole PyTorch model or a model’s <code>state_dict()</code>. * <code>torch.load</code> - a function to load in a saved PyTorch object. * <code>torch.nn.Module.load_state_dict()</code> - a function to load a saved <code>state_dict()</code> into an existing model instance.</p>
<p>You can see more of these three in the <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch saving and loading models documentation</a>.</p>
<p>For now, let’s save our <code>model_2</code>’s <code>state_dict()</code> then load it back in and evaluate it to make sure the save and load went correctly.</p>
<div id="d058e8fa-560f-4350-a154-49593ff403c9" class="cell" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> Path(<span class="st">"models"</span>)</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>MODEL_PATH.mkdir(parents<span class="op">=</span><span class="va">True</span>, <span class="co"># create parent directories if needed</span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>                 exist_ok<span class="op">=</span><span class="va">True</span> <span class="co"># if models directory already exists, don't error</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model save path</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"03_pytorch_computer_vision_model_2.pth"</span></span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>MODEL_SAVE_PATH <span class="op">=</span> MODEL_PATH <span class="op">/</span> MODEL_NAME</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model state dict</span></span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Saving model to: </span><span class="sc">{</span>MODEL_SAVE_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>torch.save(obj<span class="op">=</span>model_2.state_dict(), <span class="co"># only saving the state_dict() only saves the learned parameters</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>           f<span class="op">=</span>MODEL_SAVE_PATH)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Saving model to: models/03_pytorch_computer_vision_model_2.pth</code></pre>
</div>
</div>
<p>Now we’ve got a saved model <code>state_dict()</code> we can load it back in using a combination of <code>load_state_dict()</code> and <code>torch.load()</code>.</p>
<p>Since we’re using <code>load_state_dict()</code>, we’ll need to create a new instance of <code>FashionMNISTModelV2()</code> with the same input parameters as our saved model <code>state_dict()</code>.</p>
<div id="634a8f7a-3013-4b45-b365-49b286d3c478" class="cell" data-execution_count="61">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: loading model will error if the shapes here aren't the same as the saved version</span></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>loaded_model_2 <span class="op">=</span> FashionMNISTModelV2(input_shape<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>                                    hidden_units<span class="op">=</span><span class="dv">10</span>, <span class="co"># try changing this to 128 and seeing what happens </span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>                                    output_shape<span class="op">=</span><span class="dv">10</span>) </span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load in the saved state_dict()</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>loaded_model_2.load_state_dict(torch.load(f<span class="op">=</span>MODEL_SAVE_PATH))</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Send model to GPU</span></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>loaded_model_2 <span class="op">=</span> loaded_model_2.to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>And now we’ve got a loaded model we can evaluate it with <code>eval_model()</code> to make sure its parameters work similarly to <code>model_2</code> prior to saving.</p>
<div id="3e3bcd06-d99b-47bc-8828-9e3903285599" class="cell" data-execution_count="62">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate loaded model</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>loaded_model_2_results <span class="op">=</span> eval_model(</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>loaded_model_2,</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, </span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>    accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>loaded_model_2_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>{'model_name': 'FashionMNISTModelV2',
 'model_loss': 0.32664385437965393,
 'model_acc': 88.22883386581469}</code></pre>
</div>
</div>
<p>Do these results look the same as <code>model_2_results</code>?</p>
<div id="68544254-c99a-47ec-a32f-9816c21a993e" class="cell" data-execution_count="63">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>model_2_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>{'model_name': 'FashionMNISTModelV2',
 'model_loss': 0.32664385437965393,
 'model_acc': 88.22883386581469}</code></pre>
</div>
</div>
<p>We can find out if two tensors are close to each other using <code>torch.isclose()</code> and passing in a tolerance level of closeness via the parameters <code>atol</code> (absolute tolerance) and <code>rtol</code> (relative tolerance).</p>
<p>If our model’s results are close, the output of <code>torch.isclose()</code> should be true.</p>
<div id="48dcf0ba-7e00-4406-8aaa-41918856361a" class="cell" data-execution_count="64">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check to see if results are close to each other (if they are very far away, there may be an error)</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(torch.tensor(model_2_results[<span class="st">"model_loss"</span>]), </span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>              torch.tensor(loaded_model_2_results[<span class="st">"model_loss"</span>]),</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>              atol<span class="op">=</span><span class="fl">1e-08</span>, <span class="co"># absolute tolerance</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>              rtol<span class="op">=</span><span class="fl">0.0001</span>) <span class="co"># relative tolerance</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>tensor(True)</code></pre>
</div>
</div>
</section>
<section id="exercises" class="level2" data-number="5.16">
<h2 data-number="5.16" class="anchored" data-anchor-id="exercises"><span class="header-section-number">5.16</span> Exercises</h2>
<p>All of the exercises are focused on practicing the code in the sections above.</p>
<p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p>
<p>All exercises should be completed using <a href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">device-agnostic code</a>.</p>
<p><strong>Resources:</strong> * <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb">Exercise template notebook for 03</a> * <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/03_pytorch_computer_vision_exercise_solutions.ipynb">Example solutions notebook for 03</a> (try the exercises <em>before</em> looking at this)</p>
<ol type="1">
<li>What are 3 areas in industry where computer vision is currently being used?</li>
<li>Search “what is overfitting in machine learning” and write down a sentence about what you find.</li>
<li>Search “ways to prevent overfitting in machine learning”, write down 3 of the things you find and a sentence about each. <strong>Note:</strong> there are lots of these, so don’t worry too much about all of them, just pick 3 and start with those.</li>
<li>Spend 20-minutes reading and clicking through the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer website</a>.
<ul>
<li>Upload your own example image using the “upload” button and see what happens in each layer of a CNN as your image passes through it.</li>
</ul></li>
<li>Load the <a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST"><code>torchvision.datasets.MNIST()</code></a> train and test datasets.</li>
<li>Visualize at least 5 different samples of the MNIST training dataset.</li>
<li>Turn the MNIST train and test datasets into dataloaders using <code>torch.utils.data.DataLoader</code>, set the <code>batch_size=32</code>.</li>
<li>Recreate <code>model_2</code> used in this notebook (the same model from the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer website</a>, also known as TinyVGG) capable of fitting on the MNIST dataset.</li>
<li>Train the model you built in exercise 8. on CPU and GPU and see how long it takes on each.</li>
<li>Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label.</li>
<li>Plot a confusion matrix comparing your model’s predictions to the truth labels.</li>
<li>Create a random tensor of shape <code>[1, 3, 64, 64]</code> and pass it through a <code>nn.Conv2d()</code> layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the <code>kernel_size</code> parameter goes up and down?</li>
<li>Use a model similar to the trained <code>model_2</code> from this notebook to make predictions on the test <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html"><code>torchvision.datasets.FashionMNIST</code></a> dataset.
<ul>
<li>Then plot some predictions where the model was wrong alongside what the label of the image should’ve been.</li>
<li>After visualing these predictions do you think it’s more of a modelling error or a data error?</li>
<li>As in, could the model do better or are the labels of the data too close to each other (e.g.&nbsp;a “Shirt” label is too close to “T-shirt/top”)?</li>
</ul></li>
</ol>
</section>
<section id="extra-curriculum" class="level2" data-number="5.17">
<h2 data-number="5.17" class="anchored" data-anchor-id="extra-curriculum"><span class="header-section-number">5.17</span> Extra-curriculum</h2>
<ul>
<li><strong>Watch:</strong> <a href="https://www.youtube.com/watch?v=iaSUYvmCekI&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=3">MIT’s Introduction to Deep Computer Vision</a> lecture. This will give you a great intuition behind convolutional neural networks.</li>
<li>Spend 10-minutes clicking thorugh the different options of the <a href="https://pytorch.org/vision/stable/index.html">PyTorch vision library</a>, what different modules are available?</li>
<li>Lookup “most common convolutional neural networks”, what architectures do you find? Are any of them contained within the <a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a> library? What do you think you could do with these?</li>
<li>For a large number of pretrained PyTorch computer vision models as well as many different extensions to PyTorch’s computer vision functionalities check out the <a href="https://github.com/rwightman/pytorch-image-models/">PyTorch Image Models library <code>timm</code></a> (Torch Image Models) by Ross Wightman.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_pytorch_classification.html" class="pagination-link" aria-label="02 - PyTorch 신경망 분류">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">02 - PyTorch 신경망 분류</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04_pytorch_custom_datasets.html" class="pagination-link" aria-label="04 - PyTorch 사용자 정의 데이터셋">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">04 - PyTorch 사용자 정의 데이터셋</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>