<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>03 - PyTorch 컴퓨터 비전 – 파이토치 딥러닝 입문</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04_pytorch_custom_datasets.html" rel="next">
<link href="./02_pytorch_classification.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-841b73d05e5bc75123d26cb7b2f11c52.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03_pytorch_computer_vision.html"><span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">파이토치 딥러닝 입문</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">딥러닝을 위한 PyTorch 배우기</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_pytorch_fundamentals.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">00 - PyTorch 기초</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_pytorch_workflow.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">01 - PyTorch 워크플로우</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_pytorch_classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">02 - PyTorch 신경망 분류</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_pytorch_computer_vision.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_pytorch_custom_datasets.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">04 - PyTorch 사용자 정의 데이터셋</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_pytorch_going_modular.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">05 - PyTorch 모듈화</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_pytorch_transfer_learning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">06 - PyTorch 전이 학습</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_pytorch_experiment_tracking.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">07 - PyTorch 실험 추적</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_pytorch_paper_replicating.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">08 - PyTorch 논문 복제</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_pytorch_model_deployment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">09 - PyTorch 모델 배포</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">참고 문헌</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#컴퓨터-비전은-어디에-사용되나요" id="toc-컴퓨터-비전은-어디에-사용되나요" class="nav-link active" data-scroll-target="#컴퓨터-비전은-어디에-사용되나요">컴퓨터 비전은 어디에 사용되나요?</a></li>
  <li><a href="#이번-장에서-다룰-내용" id="toc-이번-장에서-다룰-내용" class="nav-link" data-scroll-target="#이번-장에서-다룰-내용">이번 장에서 다룰 내용</a></li>
  <li><a href="#도움을-받을-수-있는-곳" id="toc-도움을-받을-수-있는-곳" class="nav-link" data-scroll-target="#도움을-받을-수-있는-곳">도움을 받을 수 있는 곳</a></li>
  <li><a href="#pytorch의-컴퓨터-비전-라이브러리" id="toc-pytorch의-컴퓨터-비전-라이브러리" class="nav-link" data-scroll-target="#pytorch의-컴퓨터-비전-라이브러리">0. PyTorch의 컴퓨터 비전 라이브러리</a></li>
  <li><a href="#데이터셋-가져오기" id="toc-데이터셋-가져오기" class="nav-link" data-scroll-target="#데이터셋-가져오기">1. 데이터셋 가져오기</a>
  <ul class="collapse">
  <li><a href="#컴퓨터-비전-모델의-입력-및-출력-모양" id="toc-컴퓨터-비전-모델의-입력-및-출력-모양" class="nav-link" data-scroll-target="#컴퓨터-비전-모델의-입력-및-출력-모양">1.1 컴퓨터 비전 모델의 입력 및 출력 모양</a></li>
  <li><a href="#데이터-시각화하기" id="toc-데이터-시각화하기" class="nav-link" data-scroll-target="#데이터-시각화하기">1.2 데이터 시각화하기</a></li>
  </ul></li>
  <li><a href="#dataloader-준비하기" id="toc-dataloader-준비하기" class="nav-link" data-scroll-target="#dataloader-준비하기">2. DataLoader 준비하기</a></li>
  <li><a href="#모델-0-베이스라인-모델-구축하기" id="toc-모델-0-베이스라인-모델-구축하기" class="nav-link" data-scroll-target="#모델-0-베이스라인-모델-구축하기">3. 모델 0: 베이스라인 모델 구축하기</a>
  <ul class="collapse">
  <li><a href="#손실-함수-옵티마이저-및-평가-지표-설정" id="toc-손실-함수-옵티마이저-및-평가-지표-설정" class="nav-link" data-scroll-target="#손실-함수-옵티마이저-및-평가-지표-설정">3.1 손실 함수, 옵티마이저 및 평가 지표 설정</a></li>
  <li><a href="#실험-시간을-측정하는-함수-만들기" id="toc-실험-시간을-측정하는-함수-만들기" class="nav-link" data-scroll-target="#실험-시간을-측정하는-함수-만들기">3.2 실험 시간을 측정하는 함수 만들기</a></li>
  <li><a href="#훈련-루프-생성-및-데이터-배치로-모델-훈련하기" id="toc-훈련-루프-생성-및-데이터-배치로-모델-훈련하기" class="nav-link" data-scroll-target="#훈련-루프-생성-및-데이터-배치로-모델-훈련하기">3.3 훈련 루프 생성 및 데이터 배치로 모델 훈련하기</a></li>
  </ul></li>
  <li><a href="#예측-수행-및-모델-0-결과-가져오기" id="toc-예측-수행-및-모델-0-결과-가져오기" class="nav-link" data-scroll-target="#예측-수행-및-모델-0-결과-가져오기">4. 예측 수행 및 모델 0 결과 가져오기</a></li>
  <li><a href="#장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시" id="toc-장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시" class="nav-link" data-scroll-target="#장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시">5. 장치 중립적(device-agnostic) 코드 설정 (GPU 사용 가능 시)</a></li>
  <li><a href="#모델-1-비선형성을-이용한-더-나은-모델-구축하기" id="toc-모델-1-비선형성을-이용한-더-나은-모델-구축하기" class="nav-link" data-scroll-target="#모델-1-비선형성을-이용한-더-나은-모델-구축하기">6. 모델 1: 비선형성을 이용한 더 나은 모델 구축하기</a>
  <ul class="collapse">
  <li><a href="#손실-함수-옵티마이저-및-평가-지표-설정-1" id="toc-손실-함수-옵티마이저-및-평가-지표-설정-1" class="nav-link" data-scroll-target="#손실-함수-옵티마이저-및-평가-지표-설정-1">6.1 손실 함수, 옵티마이저 및 평가 지표 설정</a></li>
  <li><a href="#훈련-및-테스트-루프-기능화하기" id="toc-훈련-및-테스트-루프-기능화하기" class="nav-link" data-scroll-target="#훈련-및-테스트-루프-기능화하기">6.2 훈련 및 테스트 루프 기능화하기</a></li>
  </ul></li>
  <li><a href="#모델-2-합성곱-신경망-cnn-구축하기" id="toc-모델-2-합성곱-신경망-cnn-구축하기" class="nav-link" data-scroll-target="#모델-2-합성곱-신경망-cnn-구축하기">7. 모델 2: 합성곱 신경망 (CNN) 구축하기</a>
  <ul class="collapse">
  <li><a href="#어떤-모델을-사용해야-하나요" id="toc-어떤-모델을-사용해야-하나요" class="nav-link" data-scroll-target="#어떤-모델을-사용해야-하나요">어떤 모델을 사용해야 하나요?</a></li>
  <li><a href="#nn.conv2d-단계별로-살펴보기" id="toc-nn.conv2d-단계별로-살펴보기" class="nav-link" data-scroll-target="#nn.conv2d-단계별로-살펴보기">7.1 <code>nn.Conv2d()</code> 단계별로 살펴보기</a></li>
  <li><a href="#stepping-through-nn.maxpool2d" id="toc-stepping-through-nn.maxpool2d" class="nav-link" data-scroll-target="#stepping-through-nn.maxpool2d">7.2 Stepping through <code>nn.MaxPool2d()</code></a></li>
  <li><a href="#model_2를-위한-손실-함수-및-옵티마이저-설정" id="toc-model_2를-위한-손실-함수-및-옵티마이저-설정" class="nav-link" data-scroll-target="#model_2를-위한-손실-함수-및-옵티마이저-설정">7.3 <code>model_2</code>를 위한 손실 함수 및 옵티마이저 설정</a></li>
  <li><a href="#훈련-및-테스트-함수를-사용하여-model_2-훈련-및-테스트하기" id="toc-훈련-및-테스트-함수를-사용하여-model_2-훈련-및-테스트하기" class="nav-link" data-scroll-target="#훈련-및-테스트-함수를-사용하여-model_2-훈련-및-테스트하기">7.4 훈련 및 테스트 함수를 사용하여 <code>model_2</code> 훈련 및 테스트하기</a></li>
  </ul></li>
  <li><a href="#모델-결과-및-훈련-시간-비교하기" id="toc-모델-결과-및-훈련-시간-비교하기" class="nav-link" data-scroll-target="#모델-결과-및-훈련-시간-비교하기">8. 모델 결과 및 훈련 시간 비교하기</a></li>
  <li><a href="#make-and-evaluate-random-predictions-with-best-model" id="toc-make-and-evaluate-random-predictions-with-best-model" class="nav-link" data-scroll-target="#make-and-evaluate-random-predictions-with-best-model">9. Make and evaluate random predictions with best model</a></li>
  <li><a href="#making-a-confusion-matrix-for-further-prediction-evaluation" id="toc-making-a-confusion-matrix-for-further-prediction-evaluation" class="nav-link" data-scroll-target="#making-a-confusion-matrix-for-further-prediction-evaluation">10. Making a confusion matrix for further prediction evaluation</a></li>
  <li><a href="#최적의-모델로-무작위-예측-수행-및-평가하기" id="toc-최적의-모델로-무작위-예측-수행-및-평가하기" class="nav-link" data-scroll-target="#최적의-모델로-무작위-예측-수행-및-평가하기">9. 최적의 모델로 무작위 예측 수행 및 평가하기</a></li>
  <li><a href="#추가-예측-평가를-위해-혼동-행렬-만들기" id="toc-추가-예측-평가를-위해-혼동-행렬-만들기" class="nav-link" data-scroll-target="#추가-예측-평가를-위해-혼동-행렬-만들기">10. 추가 예측 평가를 위해 혼동 행렬 만들기</a></li>
  <li><a href="#가장-성능이-좋은-모델-저장-및-불러오기" id="toc-가장-성능이-좋은-모델-저장-및-불러오기" class="nav-link" data-scroll-target="#가장-성능이-좋은-모델-저장-및-불러오기">11. 가장 성능이 좋은 모델 저장 및 불러오기</a></li>
  <li><a href="#연습-문제" id="toc-연습-문제" class="nav-link" data-scroll-target="#연습-문제">연습 문제</a></li>
  <li><a href="#추가-학습-자료" id="toc-추가-학습-자료" class="nav-link" data-scroll-target="#추가-학습-자료">추가 학습 자료</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<p><a href="https://en.wikipedia.org/wiki/Computer_vision">컴퓨터 비전</a>은 컴퓨터가 볼 수 있도록 가르치는 예술입니다.</p>
<p>예를 들어, 사진이 고양이인지 강아지인지 분류하는 모델을 구축하는 것(<a href="https://developers.google.com/machine-learning/glossary#binary-classification">이진 분류</a>)이 포함될 수 있습니다.</p>
<p>또는 사진이 고양이, 강아지, 닭 중 무엇인지 분류하는 것(<a href="https://developers.google.com/machine-learning/glossary#multi-class-classification">다중 클래스 분류</a>)도 포함됩니다.</p>
<p>비디오 프레임에서 자동차가 어디에 나타나는지 식별하는 것(<a href="https://en.wikipedia.org/wiki/Object_detection">객체 탐지</a>)이나,</p>
<p>이미지에서 서로 다른 물체가 어디에서 분리되는지 알아내는 것(<a href="https://arxiv.org/abs/1801.00868">전경 분할(Panoptic Segmentation)</a>)도 컴퓨터 비전의 영역입니다.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png" class="img-fluid" alt="컴퓨터 비전 문제 예시"> <em>이진 분류, 다중 클래스 분류, 객체 탐지 및 분할에 대한 컴퓨터 비전 문제 예시입니다.</em></p>
<section id="컴퓨터-비전은-어디에-사용되나요" class="level2">
<h2 class="anchored" data-anchor-id="컴퓨터-비전은-어디에-사용되나요">컴퓨터 비전은 어디에 사용되나요?</h2>
<p>스마트폰을 사용한다면 이미 컴퓨터 비전을 사용하고 있는 것입니다.</p>
<p>카메라 및 사진 앱은 <a href="https://machinelearning.apple.com/research/panoptic-segmentation">컴퓨터 비전을 사용하여 이미지를 개선</a>하고 정렬합니다.</p>
<p>현대 자동차는 다른 자동차를 피하고 차선을 유지하기 위해 <a href="https://youtu.be/j0z4FweCy4M?t=2989">컴퓨터 비전</a>을 사용합니다.</p>
<p>제조업체는 컴퓨터 비전을 사용하여 다양한 제품의 결함을 식별합니다.</p>
<p>보안 카메라는 컴퓨터 비전을 사용하여 잠재적인 침입자를 감지합니다.</p>
<p>본질적으로 시각적으로 설명할 수 있는 모든 것은 잠재적인 컴퓨터 비전 문제가 될 수 있습니다.</p>
</section>
<section id="이번-장에서-다룰-내용" class="level2">
<h2 class="anchored" data-anchor-id="이번-장에서-다룰-내용">이번 장에서 다룰 내용</h2>
<p>지난 몇 개 섹션에서 배웠던 PyTorch 워크플로우를 컴퓨터 비전에 적용해 보겠습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png" class="img-fluid figure-img"></p>
<figcaption>컴퓨터 비전에 중점을 둔 PyTorch 워크플로우</figcaption>
</figure>
</div>
<p>구체적으로 다음 내용을 다룹니다:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>주제</strong></th>
<th><strong>내용</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>0. PyTorch의 컴퓨터 비전 라이브러리</strong></td>
<td>PyTorch에는 내장된 유용한 컴퓨터 비전 라이브러리가 많이 있습니다. 하나씩 살펴보겠습니다.</td>
</tr>
<tr class="even">
<td><strong>1. 데이터 로드</strong></td>
<td>컴퓨터 비전 연습을 위해 <a href="https://github.com/zalandoresearch/fashion-mnist">FashionMNIST</a>의 다양한 의류 이미지로 시작해 보겠습니다.</td>
</tr>
<tr class="odd">
<td><strong>2. 데이터 준비</strong></td>
<td>이미지를 가져와서 <a href="https://pytorch.org/docs/stable/data.html">PyTorch <code>DataLoader</code></a>로 로드하여 훈련 루프에서 사용할 수 있도록 준비합니다.</td>
</tr>
<tr class="even">
<td><strong>3. 모델 0: 베이스라인 모델 구축</strong></td>
<td>데이터에서 패턴을 학습하기 위한 다중 클래스 분류 모델을 만들고, <strong>손실 함수</strong>, <strong>옵티마이저</strong>를 선택하고 <strong>훈련 루프</strong>를 구축합니다.</td>
</tr>
<tr class="odd">
<td><strong>4. 모델 0의 예측 및 평가</strong></td>
<td>베이스라인 모델로 예측을 수행하고 평가해 봅니다.</td>
</tr>
<tr class="even">
<td><strong>5. 향후 모델을 위한 장치 중립적 코드 설정</strong></td>
<td>장치 중립적(device-agnostic) 코드를 작성하는 것이 가장 좋으므로, 이를 설정해 봅니다.</td>
</tr>
<tr class="odd">
<td><strong>6. 모델 1: 비선형성 추가</strong></td>
<td>실험은 머신러닝의 큰 부분입니다. 비선형 레이어를 추가하여 베이스라인 모델을 개선해 봅니다.</td>
</tr>
<tr class="even">
<td><strong>7. 모델 2: 합성곱 신경망 (CNN)</strong></td>
<td>컴퓨터 비전에 특화된 강력한 합성곱 신경망 아키텍처를 소개합니다.</td>
</tr>
<tr class="odd">
<td><strong>8. 모델 비교</strong></td>
<td>세 가지 서로 다른 모델을 구축했으므로, 이들을 비교해 봅니다.</td>
</tr>
<tr class="even">
<td><strong>9. 최적의 모델 평가</strong></td>
<td>무작위 이미지에 대해 예측을 수행하고 최적의 모델을 평가해 봅니다.</td>
</tr>
<tr class="odd">
<td><strong>10. 혼동 행렬 만들기</strong></td>
<td>혼동 행렬은 분류 모델을 평가하는 좋은 방법입니다. 어떻게 만드는지 살펴보겠습니다.</td>
</tr>
<tr class="even">
<td><strong>11. 가장 성능이 좋은 모델 저장 및 불러오기</strong></td>
<td>나중에 모델을 사용하고 싶을 수 있으므로 저장하고 올바르게 불러와지는지 확인합니다.</td>
</tr>
</tbody>
</table>
</section>
<section id="도움을-받을-수-있는-곳" class="level2">
<h2 class="anchored" data-anchor-id="도움을-받을-수-있는-곳">도움을 받을 수 있는 곳</h2>
<p>이 과정의 모든 자료는 <a href="https://github.com/mrdbourke/pytorch-deep-learning">GitHub</a>에 있습니다.</p>
<p>문제가 발생하면 해당 페이지의 <a href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">Discussions 페이지</a>에서 질문할 수 있습니다.</p>
<p>또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 <a href="https://pytorch.org/docs/stable/index.html">PyTorch 문서</a>와 <a href="https://discuss.pytorch.org/">PyTorch 개발자 포럼</a>도 있습니다.</p>
</section>
<section id="pytorch의-컴퓨터-비전-라이브러리" class="level2">
<h2 class="anchored" data-anchor-id="pytorch의-컴퓨터-비전-라이브러리">0. PyTorch의 컴퓨터 비전 라이브러리</h2>
<p>코드를 작성하기 전에 알아야 할 몇 가지 PyTorch 컴퓨터 비전 라이브러리에 대해 이야기해 보겠습니다.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>PyTorch 모듈</th>
<th>역할</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://pytorch.org/vision/stable/index.html"><code>torchvision</code></a></td>
<td>컴퓨터 비전 문제에 자주 사용되는 데이터셋, 모델 아키텍처 및 이미지 변환이 포함되어 있습니다.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/vision/stable/datasets.html"><code>torchvision.datasets</code></a></td>
<td>이미지 분류, 객체 탐지, 이미지 캡셔닝, 비디오 분류 등 다양한 문제에 대한 많은 예시 컴퓨터 비전 데이터셋이 있습니다. 또한 <a href="https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets">커스텀 데이터셋을 만들기 위한 일련의 기본 클래스</a>도 포함되어 있습니다.</td>
</tr>
<tr class="odd">
<td><a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a></td>
<td>PyTorch로 구현된 성능이 뛰어나고 일반적으로 사용되는 컴퓨터 비전 모델 아키텍처가 포함되어 있어 자신의 문제에 사용할 수 있습니다.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/vision/stable/transforms.html"><code>torchvision.transforms</code></a></td>
<td>모델에 사용하기 전에 이미지를 변환(숫자로 변환/처리/증강)해야 하는 경우가 많으며, 일반적인 이미지 변환 기능이 여기에 있습니다.</td>
</tr>
<tr class="odd">
<td><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.Dataset</code></a></td>
<td>PyTorch를 위한 기본 데이터셋 클래스입니다.</td>
</tr>
<tr class="even">
<td><a href="https://pytorch.org/docs/stable/data.html#module-torch.utils.data"><code>torch.utils.data.DataLoader</code></a></td>
<td>데이터셋(<code>torch.utils.data.Dataset</code>으로 생성)에 대해 파이썬 이터러블(iterable)을 생성합니다.</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>참고:</strong> <code>torch.utils.data.Dataset</code> 및 <code>torch.utils.data.DataLoader</code> 클래스는 PyTorch에서 컴퓨터 비전 전용이 아니며, 다양한 유형의 데이터를 처리할 수 있습니다.</p>
</blockquote>
<p>이제 가장 중요한 PyTorch 컴퓨터 비전 라이브러리 몇 가지를 살펴보았으니 관련 종속성을 임포트해 보겠습니다.</p>
<div id="c263a60d-d788-482f-b9e7-9cab4f6b1f72" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch 임포트</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># torchvision 임포트</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms <span class="im">import</span> ToTensor</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 시각화를 위한 matplotlib 임포트</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 버전 확인</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 참고: PyTorch 버전은 1.10.0 이상, torchvision 버전은 0.11 이상이어야 합니다.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ch">\n</span><span class="ss">torchvision version: </span><span class="sc">{</span>torchvision<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 1.11.0
torchvision version: 0.12.0</code></pre>
</div>
</div>
</section>
<section id="데이터셋-가져오기" class="level2">
<h2 class="anchored" data-anchor-id="데이터셋-가져오기">1. 데이터셋 가져오기</h2>
<p>컴퓨터 비전 문제를 시작하기 위해 컴퓨터 비전 데이터셋을 가져와 보겠습니다.</p>
<p>먼저 FashionMNIST로 시작합니다.</p>
<p>MNIST는 Modified National Institute of Standards and Technology의 약자입니다.</p>
<p><a href="https://en.wikipedia.org/wiki/MNIST_database">오리지널 MNIST 데이터셋</a>에는 수천 개의 손글씨 숫자(0~9) 예시가 포함되어 있으며 우편 서비스를 위한 숫자 식별 컴퓨터 비전 모델을 구축하는 데 사용되었습니다.</p>
<p>Zalando Research에서 만든 <a href="https://github.com/zalandoresearch/fashion-mnist">FashionMNIST</a>는 비슷한 설정입니다.</p>
<p>다만 10가지 서로 다른 종류의 의류를 나타내는 회색조 이미지가 포함되어 있습니다.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png" class="img-fluid" alt="FashionMNIST 예시 이미지"> <em><code>torchvision.datasets</code>에는 컴퓨터 비전 코드 작성을 연습할 수 있는 많은 예시 데이터셋이 포함되어 있습니다. FashionMNIST는 그중 하나입니다. 10개의 서로 다른 이미지 클래스(서로 다른 유형의 의류)가 있으므로 다중 클래스 분류 문제입니다.</em></p>
<p>나중에 이 이미지들에서 서로 다른 스타일의 의류를 식별하는 컴퓨터 비전 신경망을 구축할 것입니다.</p>
<p>PyTorch에는 <code>torchvision.datasets</code>에 저장된 많은 공통 컴퓨터 비전 데이터셋이 있습니다.</p>
<p><a href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html"><code>torchvision.datasets.FashionMNIST()</code></a>에 있는 FashionMNIST를 포함해서요.</p>
<p>이를 다운로드하기 위해 다음 매개변수를 제공합니다: * <code>root: str</code> - 데이터를 어느 폴더에 다운로드할 것인가요? * <code>train: Bool</code> - 훈련 세트 또는 테스트 세트 중 무엇을 원하나요? * <code>download: Bool</code> - 데이터를 다운로드해야 하나요? * <code>transform: torchvision.transforms</code> - 데이터에 어떤 변환을 적용하고 싶나요? * <code>target_transform</code> - 원하는 경우 타겟(레이블)도 변환할 수 있습니다.</p>
<p><code>torchvision</code>의 다른 많은 데이터셋에도 이러한 매개변수 옵션이 있습니다.</p>
<div id="486f8377-6810-4367-859d-69dccc7aef95" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 훈련 데이터 설정</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>, <span class="co"># 데이터를 어디에 다운로드할까요?</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>, <span class="co"># 훈련 데이터를 가져옵니다.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>, <span class="co"># 디스크에 데이터가 없으면 다운로드합니다.</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor(), <span class="co"># 이미지는 PIL 형식으로 제공되므로 Torch 텐서로 변환하려고 합니다.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    target_transform<span class="op">=</span><span class="va">None</span> <span class="co"># 레이블도 변환할 수 있습니다.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 테스트 데이터 설정</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> datasets.FashionMNIST(</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">"data"</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>, <span class="co"># 테스트 데이터를 가져옵니다.</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>ToTensor()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>훈련 데이터의 첫 번째 샘플을 확인해 보겠습니다.</p>
<div id="43bfd3d9-a132-41e8-8ccd-5ae25a7da59a" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 첫 번째 훈련 샘플 확인</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>image, label <span class="op">=</span> train_data[<span class="dv">0</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>image, label</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,
           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0039, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,
           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,
           0.0157, 0.0000, 0.0000, 0.0118],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,
           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0471, 0.0392, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,
           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,
           0.3020, 0.5098, 0.2824, 0.0588],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,
           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,
           0.5529, 0.3451, 0.6745, 0.2588],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,
           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,
           0.4824, 0.7686, 0.8980, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,
           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,
           0.8745, 0.9608, 0.6784, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,
           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,
           0.8627, 0.9529, 0.7922, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,
           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,
           0.8863, 0.7725, 0.8196, 0.2039],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,
           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,
           0.9608, 0.4667, 0.6549, 0.2196],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,
           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,
           0.8510, 0.8196, 0.3608, 0.0000],
          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,
           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,
           0.8549, 1.0000, 0.3020, 0.0000],
          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,
           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,
           0.8784, 0.9569, 0.6235, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,
           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,
           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,
           0.9137, 0.9333, 0.8431, 0.0000],
          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,
           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,
           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,
           0.8627, 0.9098, 0.9647, 0.0000],
          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,
           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,
           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,
           0.8706, 0.8941, 0.8824, 0.0000],
          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,
           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,
           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,
           0.8745, 0.8784, 0.8980, 0.1137],
          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,
           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,
           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,
           0.8627, 0.8667, 0.9020, 0.2627],
          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,
           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,
           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,
           0.7098, 0.8039, 0.8078, 0.4510],
          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,
           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,
           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,
           0.6549, 0.6941, 0.8235, 0.3608],
          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,
           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,
           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,
           0.7529, 0.8471, 0.6667, 0.0000],
          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,
           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,
           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,
           0.3882, 0.2275, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,
           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000],
          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.0000]]]),
 9)</code></pre>
</div>
</div>
<section id="컴퓨터-비전-모델의-입력-및-출력-모양" class="level3">
<h3 class="anchored" data-anchor-id="컴퓨터-비전-모델의-입력-및-출력-모양">1.1 컴퓨터 비전 모델의 입력 및 출력 모양</h3>
<p>이미지를 나타내는 큰 값의 텐서가 있고, 타겟(레이블)을 나타내는 단일 값이 있습니다.</p>
<p>이미지 모양을 확인해 보겠습니다.</p>
<div id="c2997d9f-b574-4d23-aa34-1a4df1751226" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지의 모양은 무엇인가요?</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>torch.Size([1, 28, 28])</code></pre>
</div>
</div>
<p>이미지 텐서의 모양은 <code>[1, 28, 28]</code>이며 더 구체적으로는 다음과 같습니다:</p>
<pre><code>[색상_채널=1, 높이=28, 너비=28]</code></pre>
<p><code>색상_채널=1</code>은 이미지가 회색조임을 의미합니다.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png" class="img-fluid" alt="FashionMNIST 문제의 예시 입력 및 출력 모양"> <em>다양한 문제에는 다양한 입력 및 출력 모양이 있습니다. 하지만 전제는 동일합니다. 데이터를 숫자로 인코딩하고, 해당 숫자에서 패턴을 찾기 위한 모델을 구축하고, 그 패턴을 의미 있는 것으로 변환하는 것입니다.</em></p>
<p><code>색상_채널=3</code>인 경우 이미지는 빨간색, 녹색, 파란색의 픽셀 값으로 제공됩니다(이를 <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB 색상 모델</a>이라고도 함).</p>
<p>현재 텐서의 순서는 종종 <code>CHW</code>(색상 채널, 높이, 너비)라고 불립니다.</p>
<p>이미지를 <code>CHW</code>(색상 채널 우선)로 표시해야 하는지 아니면 <code>HWC</code>(색상 채널 마지막)로 표시해야 하는지에 대한 논의가 있습니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> <code>N</code>이 <em>이미지 수</em>를 나타내는 <code>NCHW</code> 및 <code>NHWC</code> 형식도 볼 수 있습니다. 예를 들어 <code>batch_size=32</code>인 경우 텐서 모양은 <code>[32, 1, 28, 28]</code>이 될 수 있습니다. 배치 크기는 나중에 다루겠습니다.</p>
</blockquote>
<p>PyTorch는 일반적으로 많은 연산자에서 <code>NCHW</code>(채널 우선)를 기본값으로 허용합니다.</p>
<p>그러나 PyTorch는 <code>NHWC</code>(채널 마지막)가 성능이 더 좋으며 <a href="https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice">권장 사항(best practice)</a>으로 간주된다고 설명합니다.</p>
<p>지금은 데이터셋과 모델이 상대적으로 작기 때문에 큰 차이가 없을 것입니다.</p>
<p>하지만 나중에 더 큰 이미지 데이터셋을 다루고 합성곱 신경망을 사용할 때 이를 염두에 두세요.</p>
<p>데이터의 모양을 더 확인해 보겠습니다.</p>
<div id="fc4f768c-c3f6-454d-a633-673ad1d6eca0" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 샘플은 몇 개인가요?</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(train_data.data), <span class="bu">len</span>(train_data.targets), <span class="bu">len</span>(test_data.data), <span class="bu">len</span>(test_data.targets)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(60000, 60000, 10000, 10000)</code></pre>
</div>
</div>
<p>따라서 60,000개의 훈련 샘플과 10,000개의 테스트 샘플이 있습니다.</p>
<p>어떤 클래스가 있나요?</p>
<p><code>.classes</code> 속성을 통해 이를 찾을 수 있습니다.</p>
<div id="e22849c6-d93f-4b38-8403-5ebf0deaf008" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 클래스 확인</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> train_data.classes</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>class_names</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>['T-shirt/top',
 'Trouser',
 'Pullover',
 'Dress',
 'Coat',
 'Sandal',
 'Shirt',
 'Sneaker',
 'Bag',
 'Ankle boot']</code></pre>
</div>
</div>
<p>좋습니다! 10가지 서로 다른 종류의 옷을 다루고 있는 것 같네요.</p>
<p>10개의 서로 다른 클래스를 다루고 있기 때문에 우리의 문제는 <strong>다중 클래스 분류</strong>입니다.</p>
<p>이제 시각화를 해보겠습니다.</p>
</section>
<section id="데이터-시각화하기" class="level3">
<h3 class="anchored" data-anchor-id="데이터-시각화하기">1.2 데이터 시각화하기</h3>
<div id="b1df1f2c-28c9-43bf-aaef-cf996c9ae1c5" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>image, label <span class="op">=</span> train_data[<span class="dv">0</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image shape: </span><span class="sc">{</span>image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.squeeze()) <span class="co"># image shape is [1, 28, 28] (colour channels, height, width)</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.title(label)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image shape: torch.Size([1, 28, 28])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can turn the image into grayscale using the <code>cmap</code> parameter of <code>plt.imshow()</code>.</p>
<div id="92f09917-88f7-4446-b65f-baae586914c9" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p>
<p>Let’s view a few more.</p>
<div id="7188ed7a-5959-48c4-ac7f-19129a2adc83" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot more images</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>rows, cols <span class="op">=</span> <span class="dv">4</span>, <span class="dv">4</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, rows <span class="op">*</span> cols <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    random_idx <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(train_data), size<span class="op">=</span>[<span class="dv">1</span>]).item()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    img, label <span class="op">=</span> train_data[random_idx]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    fig.add_subplot(rows, cols, i)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    plt.imshow(img.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    plt.title(class_names[label])</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Hmmm, this dataset doesn’t look too aesthetic.</p>
<p>But the principles we’re going to learn on how to build a model for it will be similar across a wide range of computer vision problems.</p>
<p>In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.</p>
<p>Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?</p>
<p>You probably could.</p>
<p>But I think coding a model in PyTorch would be faster.</p>
<blockquote class="blockquote">
<p><strong>Question:</strong> Do you think the above data can be model with only straight (linear) lines? Or do you think you’d also need non-straight (non-linear) lines?</p>
</blockquote>
</section>
</section>
<section id="dataloader-준비하기" class="level2">
<h2 class="anchored" data-anchor-id="dataloader-준비하기">2. DataLoader 준비하기</h2>
<p>이제 데이터셋이 준비되었습니다.</p>
<p>다음 단계는 <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><code>torch.utils.data.DataLoader</code></a> 또는 줄여서 <code>DataLoader</code>를 사용하여 준비하는 것입니다.</p>
<p><code>DataLoader</code>는 이름에서 짐작할 수 있는 역할을 합니다.</p>
<p>모델에 데이터를 로드하는 것을 돕습니다.</p>
<p>훈련과 추론을 위해서요.</p>
<p>큰 <code>Dataset</code>을 작은 덩어리의 파이썬 이터러블로 변환합니다.</p>
<p>이러한 작은 덩어리를 <strong>배치(batch)</strong> 또는 <strong>미니 배치(mini-batch)</strong>라고 하며 <code>batch_size</code> 매개변수로 설정할 수 있습니다.</p>
<p>왜 이렇게 할까요?</p>
<p>컴퓨팅 효율성이 더 높기 때문입니다.</p>
<p>이상적인 세상에서는 모든 데이터를 한 번에 순전파 및 역전파할 수 있을 것입니다.</p>
<p>하지만 정말 큰 데이터셋을 사용하기 시작하면 무한한 컴퓨팅 파워가 없는 한 데이터를 배치로 나누는 것이 더 쉽습니다.</p>
<p>또한 모델이 개선될 기회를 더 많이 제공합니다.</p>
<p><strong>미니 배치</strong>(데이터의 작은 부분)를 사용하면 에포크(epoch)당 한 번이 아니라 미니 배치당 한 번씩 경사 하강법이 더 자주 수행됩니다.</p>
<p>좋은 배치 크기는 얼마일까요?</p>
<p><a href="https://twitter.com/ylecun/status/989610208497360896?s=20&amp;t=N96J_jotN--PYuJk2WcjMw">32는 시작하기 좋은 지점</a>입니다.</p>
<p>하지만 이는 사용자가 설정할 수 있는 값(<strong>하이퍼파라미터</strong>)이므로 모든 종류의 값을 시도해 볼 수 있습니다. 다만 일반적으로 2의 거듭제곱(예: 32, 64, 128, 256, 512)이 가장 자주 사용됩니다.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png" class="img-fluid" alt="배치된 데이터셋의 예시"> <em>FashionMNIST를 배치 크기 32로 배치하고 셔플을 켠 모습입니다. 다른 데이터셋에 대해서도 유사한 배치 프로세스가 발생하지만 배치 크기에 따라 달라집니다.</em></p>
<p>훈련 세트와 테스트 세트를 위한 <code>DataLoader</code>를 만들어 보겠습니다.</p>
<div id="bb2dbf90-a326-43cb-b25b-71af142fafeb" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the batch size hyperparameter</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn datasets into iterables (batches)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_data, <span class="co"># dataset to turn into iterable</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE, <span class="co"># how many samples per batch? </span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span> <span class="co"># shuffle data every epoch?</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_data,</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span> <span class="co"># don't necessarily have to shuffle the testing data</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's check out what we've created</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataloaders: </span><span class="sc">{</span>train_dataloader<span class="sc">,</span> test_dataloader<span class="sc">}</span><span class="ss">"</span>) </span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Length of train dataloader: </span><span class="sc">{</span><span class="bu">len</span>(train_dataloader)<span class="sc">}</span><span class="ss"> batches of </span><span class="sc">{</span>BATCH_SIZE<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Length of test dataloader: </span><span class="sc">{</span><span class="bu">len</span>(test_dataloader)<span class="sc">}</span><span class="ss"> batches of </span><span class="sc">{</span>BATCH_SIZE<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193a8a90&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193b0700&gt;)
Length of train dataloader: 1875 batches of 32
Length of test dataloader: 313 batches of 32</code></pre>
</div>
</div>
<div id="7a925ee7-484b-4149-be8f-3ad790172a5f" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out what's inside the training dataloader</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>train_features_batch, train_labels_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>train_features_batch.shape, train_labels_batch.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</code></pre>
</div>
</div>
<p>And we can see that the data remains unchanged by checking a single sample.</p>
<div id="c863d66a-49be-43be-84dc-372a5d6fc2c2" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Show a sample</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>random_idx <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(train_features_batch), size<span class="op">=</span>[<span class="dv">1</span>]).item()</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>img, label <span class="op">=</span> train_features_batch[random_idx], train_labels_batch[random_idx]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(img.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"Off"</span>)<span class="op">;</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image size: </span><span class="sc">{</span>img<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, label size: </span><span class="sc">{</span>label<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image size: torch.Size([1, 28, 28])
Label: 6, label size: torch.Size([])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="모델-0-베이스라인-모델-구축하기" class="level2">
<h2 class="anchored" data-anchor-id="모델-0-베이스라인-모델-구축하기">3. 모델 0: 베이스라인 모델 구축하기</h2>
<p>데이터 로드 및 준비 완료!</p>
<p>이제 <code>nn.Module</code>을 상속받아 <strong>베이스라인 모델</strong>을 구축할 시간입니다.</p>
<p><strong>베이스라인 모델</strong>은 상상할 수 있는 가장 간단한 모델 중 하나입니다.</p>
<p>베이스라인을 시작점으로 사용하고 이후의 더 복잡한 모델로 이를 개선하려고 노력합니다.</p>
<p>우리의 베이스라인은 두 개의 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code>nn.Linear()</code></a> 레이어로 구성됩니다.</p>
<p>이전 섹션에서 이 작업을 수행했지만 한 가지 약간의 차이점이 있습니다.</p>
<p>이미지 데이터를 다루고 있기 때문에 시작을 위해 다른 레이어를 사용할 것입니다.</p>
<p>바로 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code>nn.Flatten()</code></a> 레이어입니다.</p>
<p><code>nn.Flatten()</code>은 텐서의 차원을 단일 벡터로 압축합니다.</p>
<p>이것은 직접 보면 이해하기 더 쉽습니다.</p>
<div id="405319f1-f242-4bd9-90f5-3abdc50782ac" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten 레이어 생성</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>flatten_model <span class="op">=</span> nn.Flatten() <span class="co"># 모든 nn 모듈은 모델로 작동합니다(순전파를 수행할 수 있음)</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 단일 샘플 가져오기</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> train_features_batch[<span class="dv">0</span>]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 샘플을 평탄화(flatten)</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> flatten_model(x) <span class="co"># 순전파 수행</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과 출력</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"평탄화 전 모양: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [color_channels, height, width]"</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"평탄화 후 모양: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [color_channels, height*width]"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 아래 주석을 해제하고 어떤 일이 일어나는지 확인해 보세요</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co">#print(x)</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co">#print(output)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]
Shape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]</code></pre>
</div>
</div>
<p><code>nn.Flatten()</code> 레이어는 모양을 <code>[color_channels, height, width]</code>에서 <code>[color_channels, height*width]</code>로 바꿨습니다.</p>
<p>왜 이렇게 할까요?</p>
<p>높이와 너비 차원의 픽셀 데이터를 하나의 긴 <strong>특성 벡터(feature vector)</strong>로 변환했기 때문입니다.</p>
<p>그리고 <code>nn.Linear()</code> 레이어는 입력이 특성 벡터 형태인 것을 선호합니다.</p>
<p>첫 번째 레이어로 <code>nn.Flatten()</code>을 사용하여 첫 번째 모델을 만들어 보겠습니다.</p>
<div id="1449f427-6859-41ae-8133-50b58ffbce72" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FashionMNISTModelV0(nn.Module):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), <span class="co"># 신경망은 입력이 벡터 형태인 것을 선호합니다.</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>input_shape, out_features<span class="op">=</span>hidden_units), <span class="co"># in_features = 데이터 샘플의 특성 수(784픽셀)</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units, out_features<span class="op">=</span>output_shape)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_stack(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>멋지네요!</p>
<p>이제 사용할 수 있는 베이스라인 모델 클래스가 생겼으니 모델을 인스턴스화해 보겠습니다.</p>
<p>다음 매개변수를 설정해야 합니다: * <code>input_shape=784</code> - 모델에 들어가는 특성 수입니다. 우리의 경우 대상 이미지의 각 픽셀에 대해 하나씩입니다(높이 28픽셀 x 너비 28픽셀 = 784개의 특성). * <code>hidden_units=10</code> - 은닉 레이어의 유닛/뉴런 수입니다. 이 숫자는 원하는 대로 설정할 수 있지만 모델을 작게 유지하기 위해 <code>10</code>으로 시작합니다. * <code>output_shape=len(class_names)</code> - 다중 클래스 분류 문제를 다루고 있으므로 데이터셋의 각 클래스당 하나의 출력 뉴런이 필요합니다.</p>
<p>이제 모델 인스턴스를 만들고 지금은 CPU로 보냅니다(곧 CPU에서 <code>model_0</code>을 실행하는 것과 GPU에서 유사한 모델을 실행하는 것에 대한 작은 테스트를 수행할 것입니다).</p>
<div id="dd18384a-76f9-4b5a-a013-fda077f16865" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 입력 매개변수로 모델 설정</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>model_0 <span class="op">=</span> FashionMNISTModelV0(input_shape<span class="op">=</span><span class="dv">784</span>, <span class="co"># 모든 픽셀에 대해 하나씩 (28x28)</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    hidden_units<span class="op">=</span><span class="dv">10</span>, <span class="co"># 은닉 레이어의 유닛 수</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    output_shape<span class="op">=</span><span class="bu">len</span>(class_names) <span class="co"># 각 클래스에 대해 하나씩</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>model_0.to(<span class="st">"cpu"</span>) <span class="co"># 우선 모델을 CPU에 둡니다. </span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>FashionMNISTModelV0(
  (layer_stack): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=10, bias=True)
    (2): Linear(in_features=10, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<section id="손실-함수-옵티마이저-및-평가-지표-설정" class="level3">
<h3 class="anchored" data-anchor-id="손실-함수-옵티마이저-및-평가-지표-설정">3.1 손실 함수, 옵티마이저 및 평가 지표 설정</h3>
<p>분류 문제를 다루고 있으므로 <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"><code>helper_functions.py</code> 스크립트</a>를 가져오고, 이어서 <a href="https://www.learnpytorch.io/02_pytorch_classification/">노트북 02</a>에서 정의한 <code>accuracy_fn()</code>을 가져오겠습니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> 자체 정확도 함수나 평가 지표를 임포트하여 사용하는 대신 <a href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics 패키지</a>에서 다양한 평가 지표를 임포트할 수도 있습니다.</p>
</blockquote>
<div id="31c91f17-d810-46a4-97c3-c734f93430b1" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path </span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Learn PyTorch 저장소에서 helper functions 다운로드 (이미 다운로드되지 않은 경우)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> Path(<span class="st">"helper_functions.py"</span>).is_file():</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"helper_functions.py already exists, skipping download"</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"Downloading helper_functions.py"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 참고: 이것이 작동하려면 "raw" GitHub URL이 필요합니다.</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>  request <span class="op">=</span> requests.get(<span class="st">"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py"</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"helper_functions.py"</span>, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    f.write(request.content)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>helper_functions.py already exists, skipping download</code></pre>
</div>
</div>
<div id="ce3d13b8-f018-4b44-8bba-375074dc4c5f" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 정확도 지표 임포트</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> accuracy_fn <span class="co"># 참고: torchmetrics.Accuracy()를 사용할 수도 있습니다.</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 함수 및 옵티마이저 설정</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss() <span class="co"># 이것은 일부 장소에서 "criterion" 또는 "cost function"으로도 불립니다.</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_0.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="실험-시간을-측정하는-함수-만들기" class="level3">
<h3 class="anchored" data-anchor-id="실험-시간을-측정하는-함수-만들기">3.2 실험 시간을 측정하는 함수 만들기</h3>
<p>손실 함수와 옵티마이저가 준비되었습니다!</p>
<p>이제 모델 훈련을 시작할 시간입니다.</p>
<p>하지만 훈련하는 동안 작은 실험을 하나 해보면 어떨까요?</p>
<p>모델이 CPU에서 훈련되는 시간과 GPU를 사용하여 훈련되는 시간을 측정하는 타이밍 함수를 만들어 보겠습니다.</p>
<p>이 모델은 CPU에서 훈련하지만 다음 모델은 GPU에서 훈련하고 어떤 일이 일어나는지 살펴보겠습니다.</p>
<p>우리의 타이밍 함수는 파이썬 <a href="https://docs.python.org/3/library/timeit.html"><code>timeit</code> 모듈</a>에서 <a href="https://docs.python.org/3/library/timeit.html#timeit.default_timer"><code>timeit.default_timer()</code> 함수</a>를 임포트할 것입니다.</p>
<div id="31adc3fe-ce90-4b4e-b0d4-3613abae5714" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_train_time(start: <span class="bu">float</span>, end: <span class="bu">float</span>, device: torch.device <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""시작 시간과 종료 시간 사이의 차이를 출력합니다.</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co">        start (float): 계산 시작 시간 (timeit 형식 선호). </span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co">        end (float): 계산 종료 시간.</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="co">        device ([type], optional): 계산이 실행되는 장치. 기본값은 None.</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co">        float: 시작 시간과 종료 시간 사이의 초 단위 시간 (높을수록 더 긺).</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    total_time <span class="op">=</span> end <span class="op">-</span> start</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Train time on </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>total_time<span class="sc">:.3f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_time</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="훈련-루프-생성-및-데이터-배치로-모델-훈련하기" class="level3">
<h3 class="anchored" data-anchor-id="훈련-루프-생성-및-데이터-배치로-모델-훈련하기">3.3 훈련 루프 생성 및 데이터 배치로 모델 훈련하기</h3>
<p>아름답네요!</p>
<p>타이머, 손실 함수, 옵티마이저, 모델, 그리고 가장 중요한 데이터까지 모든 퍼즐 조각이 준비된 것 같습니다.</p>
<p>이제 모델을 훈련하고 평가하기 위해 훈련 루프와 테스트 루프를 만들어 보겠습니다.</p>
<p>이전 노트북과 동일한 단계를 사용하겠지만, 데이터가 이제 배치 형태이므로 데이터 배치를 반복하기 위한 또 다른 루프를 추가할 것입니다.</p>
<p>데이터 배치는 각각 훈련 및 테스트 데이터 분할을 위한 <code>DataLoader</code>인 <code>train_dataloader</code>와 <code>test_dataloader</code>에 포함되어 있습니다.</p>
<p>하나의 배치는 <code>BATCH_SIZE</code>개의 <code>X</code>(특성)와 <code>y</code>(레이블) 샘플이며, <code>BATCH_SIZE=32</code>를 사용하고 있으므로 배치는 32개의 이미지와 타겟 샘플을 가집니다.</p>
<p>그리고 데이터 배치에 대해 계산을 수행하므로 손실 및 평가 지표는 전체 데이터셋이 아니라 <strong>배치당</strong> 계산됩니다.</p>
<p>즉, 손실 및 정확도 값을 각 데이터셋의 해당 데이터로더에 있는 배치 수로 나누어야 함을 의미합니다.</p>
<p>단계별로 살펴보겠습니다: 1. 에포크를 반복합니다. 2. 훈련 배치를 반복하고, 훈련 단계를 수행하며, <em>배치당</em> 훈련 손실을 계산합니다. 3. 테스트 배치를 반복하고, 테스트 단계를 수행하며, <em>배치당</em> 테스트 손실을 계산합니다. 4. 진행 상황을 출력합니다. 5. 전체 시간을 측정합니다(재미로요).</p>
<p>몇 가지 단계가 있지만…</p>
<p>…의심스러우면 코드로 구현해 보세요.</p>
<div id="c07bbf10-81e3-47f0-990d-9a4a838276ab" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 진행률 표시줄을 위한 tqdm 임포트</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 시드 설정 및 타이머 시작</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>train_time_start_on_cpu <span class="op">=</span> timer()</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 에포크 수 설정 (빠른 훈련 시간을 위해 작게 유지)</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 훈련 및 테스트 루프 생성</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"에포크: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ch">\n</span><span class="ss">-------"</span>)</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">### 훈련</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 훈련 배치를 반복하기 위한 루프 추가</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        model_0.train() </span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. 순전파</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model_0(X)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. 손실 계산 (배치당)</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss <span class="co"># 에포크당 손실을 누적해서 더함</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. 옵티마이저 제로 그라디언트</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. 손실 역전파</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. 옵티마이저 단계 수행</span></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 지금까지 본 샘플 수 출력</span></span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">400</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>batch <span class="op">*</span> <span class="bu">len</span>(X)<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(train_dataloader.dataset)<span class="sc">}</span><span class="ss"> 샘플 확인"</span>)</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 전체 훈련 손실을 훈련 데이터로더의 길이로 나눔 (에포크당 배치당 평균 손실)</span></span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">/=</span> <span class="bu">len</span>(train_dataloader)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">### 테스트</span></span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 손실 및 정확도를 누적해서 더하기 위한 변수 설정</span></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>    test_loss, test_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span> </span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>    model_0.<span class="bu">eval</span>()</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> test_dataloader:</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 1. 순전파</span></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>            test_pred <span class="op">=</span> model_0(X)</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 2. 손실 계산 (누적)</span></span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(test_pred, y) <span class="co"># 에포크당 손실을 누적해서 더함</span></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 3. 정확도 계산 (예측값이 y_true와 같아야 함)</span></span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a>            test_acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y, y_pred<span class="op">=</span>test_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 테스트 지표 계산은 torch.inference_mode() 내부에서 이루어져야 합니다.</span></span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 전체 테스트 손실을 테스트 데이터로더의 길이로 나눔 (배치당)</span></span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">/=</span> <span class="bu">len</span>(test_dataloader)</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 전체 정확도를 테스트 데이터로더의 길이로 나눔 (배치당)</span></span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">/=</span> <span class="bu">len</span>(test_dataloader)</span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a>    <span class="co">## 진행 상황 출력</span></span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">훈련 손실: </span><span class="sc">{</span>train_loss<span class="sc">:.5f}</span><span class="ss"> | 테스트 손실: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">, 테스트 정확도: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a><span class="co"># 훈련 시간 계산</span></span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>train_time_end_on_cpu <span class="op">=</span> timer()</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>total_train_time_model_0 <span class="op">=</span> print_train_time(start<span class="op">=</span>train_time_start_on_cpu, </span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>                                           end<span class="op">=</span>train_time_end_on_cpu,</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>                                           device<span class="op">=</span><span class="bu">str</span>(<span class="bu">next</span>(model_0.parameters()).device))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0132aa6e08d747e7af202440f9615a86","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%

Epoch: 1
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%

Epoch: 2
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%

Train time on cpu: 14.975 seconds</code></pre>
</div>
</div>
<p>좋습니다! 베이스라인 모델이 꽤 잘 작동하는 것 같네요.</p>
<p>CPU에서도 훈련하는 데 그리 오래 걸리지 않았습니다. GPU에서는 더 빨라질까요?</p>
<p>이제 모델을 평가하는 코드를 작성해 보겠습니다.</p>
</section>
</section>
<section id="예측-수행-및-모델-0-결과-가져오기" class="level2">
<h2 class="anchored" data-anchor-id="예측-수행-및-모델-0-결과-가져오기">4. 예측 수행 및 모델 0 결과 가져오기</h2>
<p>앞으로 몇 가지 모델을 구축할 것이므로, 모두 유사한 방식으로 평가하는 코드를 작성하는 것이 좋습니다.</p>
<p>구체적으로, 훈련된 모델, <code>DataLoader</code>, 손실 함수 및 정확도 함수를 입력으로 받는 함수를 만들어 보겠습니다.</p>
<p>이 함수는 모델을 사용하여 <code>DataLoader</code>의 데이터에 대해 예측을 수행한 다음, 손실 함수와 정확도 함수를 사용하여 해당 예측을 평가합니다.</p>
<div id="8317dd04-9de2-4fd7-97bd-1e202621397d" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model: torch.nn.Module, </span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>               data_loader: torch.utils.data.DataLoader, </span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>               loss_fn: torch.nn.Module, </span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>               accuracy_fn):</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""data_loader에 대한 모델의 예측 결과를 딕셔너리로 반환합니다.</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): data_loader에 대해 예측을 수행할 수 있는 PyTorch 모델.</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">        data_loader (torch.utils.data.DataLoader): 예측 대상 데이터셋.</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_fn (torch.nn.Module): 모델의 손실 함수.</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">        accuracy_fn: 모델의 예측을 실제 레이블과 비교하는 정확도 함수.</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">        (dict): data_loader에 대한 모델의 예측 결과.</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> data_loader:</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 모델로 예측 수행</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(X)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 배치당 손실 및 정확도 값 누적</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> loss_fn(y_pred, y)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y, </span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>                                y_pred<span class="op">=</span>y_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>)) <span class="co"># 정확도를 위해 예측 레이블이 필요함 (logits -&gt; pred_prob -&gt; pred_labels)</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 배치당 평균 손실/정확도를 찾기 위해 손실 및 정확도 스케일링</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"model_name"</span>: model.<span class="va">__class__</span>.<span class="va">__name__</span>, <span class="co"># 모델이 클래스로 생성된 경우에만 작동함</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_loss"</span>: loss.item(),</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_acc"</span>: acc}</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 테스트 데이터셋에 대해 모델 0 결과 계산</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>model_0_results <span class="op">=</span> eval_model(model<span class="op">=</span>model_0, data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>model_0_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>{'model_name': 'FashionMNISTModelV0',
 'model_loss': 0.47663894295692444,
 'model_acc': 83.42651757188499}</code></pre>
</div>
</div>
<p>좋아 보이네요!</p>
<p>이 딕셔너리를 사용하여 나중에 베이스라인 모델 결과를 다른 모델과 비교할 수 있습니다.</p>
</section>
<section id="장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시" class="level2">
<h2 class="anchored" data-anchor-id="장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시">5. 장치 중립적(device-agnostic) 코드 설정 (GPU 사용 가능 시)</h2>
<p>CPU에서 60,000개 샘플에 대해 PyTorch 모델을 훈련하는 데 얼마나 걸리는지 확인했습니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> 모델 훈련 시간은 사용된 하드웨어에 따라 다릅니다. 일반적으로 프로세서가 많을수록 훈련 속도가 빨라지며, 작은 데이터셋의 작은 모델은 종종 큰 데이터셋의 큰 모델보다 빠르게 훈련됩니다.</p>
</blockquote>
<p>이제 모델과 데이터를 GPU(사용 가능한 경우)에서 실행할 수 있도록 <a href="https://pytorch.org/docs/stable/notes/cuda.html#best-practices">장치 중립적 코드</a>를 설정해 보겠습니다.</p>
<p>Google Colab에서 이 노트북을 실행 중이고 아직 GPU를 켜지 않았다면, 지금 <code>런타임 -&gt; 런타임 유형 변경 -&gt; 하드웨어 가속기 -&gt; GPU</code>를 통해 켜야 할 때입니다. 이 작업을 수행하면 런타임이 재설정될 가능성이 높으므로 <code>런타임 -&gt; 이전 셀 실행</code>을 통해 위의 모든 셀을 다시 실행해야 합니다.</p>
<div id="17b69fe9-f974-4538-922c-20c5cc8220cc" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 장치 중립적 코드 설정</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>'cuda'</code></pre>
</div>
</div>
<p>멋지네요!</p>
<p>이제 다른 모델을 구축해 보겠습니다.</p>
</section>
<section id="모델-1-비선형성을-이용한-더-나은-모델-구축하기" class="level2">
<h2 class="anchored" data-anchor-id="모델-1-비선형성을-이용한-더-나은-모델-구축하기">6. 모델 1: 비선형성을 이용한 더 나은 모델 구축하기</h2>
<p><a href="https://www.learnpytorch.io/02_pytorch_classification/#6-the-missing-piece-non-linearity">노트북 02에서 비선형성의 힘</a>에 대해 배웠습니다.</p>
<p>우리가 다루고 있는 데이터를 보았을 때, 비선형 함수가 필요하다고 생각하시나요?</p>
<p>선형은 직선을, 비선형은 직선이 아님을 의미한다는 것을 기억하세요.</p>
<p>한번 확인해 봅시다.</p>
<p>이전과 유사한 모델을 만들되, 이번에는 각 선형 레이어 사이에 비선형 함수(<code>nn.ReLU()</code>)를 추가해 보겠습니다.</p>
<div id="2ccce5f2-b1e5-47a6-a7f3-6bc096b35ffb" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 비선형 및 선형 레이어가 있는 모델 생성</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FashionMNISTModelV1(nn.Module):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(), <span class="co"># 입력을 단일 벡터로 평탄화</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>input_shape, out_features<span class="op">=</span>hidden_units),</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units, out_features<span class="op">=</span>output_shape),</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_stack(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>좋아 보이네요.</p>
<p>이제 이전과 동일한 설정으로 인스턴스화해 보겠습니다.</p>
<p><code>input_shape=784</code>(이미지 데이터의 특성 수와 동일), <code>hidden_units=10</code>(작게 시작하며 베이스라인 모델과 동일), <code>output_shape=len(class_names)</code>(클래스당 하나의 출력 유닛)가 필요합니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> 한 가지 변화(비선형 레이어 추가)를 제외하고 모델의 대부분의 설정을 동일하게 유지한 것에 주목하세요. 이것은 일련의 머신러닝 실험을 수행할 때 표준적인 관행입니다. 한 가지를 바꾸고 어떤 일이 일어나는지 확인한 다음, 다시 반복하는 것이죠.</p>
</blockquote>
<div id="907091ec-7e46-470b-a305-788a3009b837" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>model_1 <span class="op">=</span> FashionMNISTModelV1(input_shape<span class="op">=</span><span class="dv">784</span>, <span class="co"># 입력 특성 수</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    hidden_units<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    output_shape<span class="op">=</span><span class="bu">len</span>(class_names) <span class="co"># 원하는 출력 클래스 수</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>).to(device) <span class="co"># 사용 가능한 경우 모델을 GPU로 보냅니다.</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="bu">next</span>(model_1.parameters()).device <span class="co"># 모델 장치 확인</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>device(type='cuda', index=0)</code></pre>
</div>
</div>
<section id="손실-함수-옵티마이저-및-평가-지표-설정-1" class="level3">
<h3 class="anchored" data-anchor-id="손실-함수-옵티마이저-및-평가-지표-설정-1">6.1 손실 함수, 옵티마이저 및 평가 지표 설정</h3>
<p>평소와 같이 손실 함수, 옵티마이저 및 평가 지표를 설정하겠습니다(여러 평가 지표를 사용할 수 있지만 지금은 정확도를 고수하겠습니다).</p>
<div id="fe7e463b-d46c-4f00-853c-fdf0a28d74c8" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> accuracy_fn</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_1.parameters(), </span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                            lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="훈련-및-테스트-루프-기능화하기" class="level3">
<h3 class="anchored" data-anchor-id="훈련-및-테스트-루프-기능화하기">6.2 훈련 및 테스트 루프 기능화하기</h3>
<p>지금까지는 훈련 및 테스트 루프를 반복해서 작성했습니다.</p>
<p>이제 이를 다시 작성하되, 반복해서 호출할 수 있도록 함수에 넣어 보겠습니다.</p>
<p>그리고 이제 장치 중립적 코드를 사용하고 있으므로 특성(<code>X</code>) 및 타겟(<code>y</code>) 텐서에 대해 <code>.to(device)</code>를 호출해야 합니다.</p>
<p>훈련 루프를 위해 모델, <code>DataLoader</code>, 손실 함수 및 옵티마이저를 입력으로 받는 <code>train_step()</code> 함수를 만들겠습니다.</p>
<p>테스트 루프도 비슷하게 만들되 <code>test_step()</code>이라고 부르고 모델, <code>DataLoader</code>, 손실 함수 및 평가 함수를 입력으로 받겠습니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> 함수이기 때문에 원하는 방식으로 커스터마이징할 수 있습니다. 여기서 만드는 것은 특정 분류 사용 사례를 위한 기본적인 훈련 및 테스트 함수라고 볼 수 있습니다.</p>
</blockquote>
<div id="3d239ed2-4028-4603-8db3-ffca2b727819" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(model: torch.nn.Module,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>               data_loader: torch.utils.data.DataLoader,</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>               loss_fn: torch.nn.Module,</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>               optimizer: torch.optim.Optimizer,</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>               accuracy_fn,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>               device: torch.device <span class="op">=</span> device):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    train_loss, train_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(data_loader):</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 데이터를 GPU로 보냄</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. 순전파</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. 손실 계산</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        train_loss <span class="op">+=</span> loss</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y,</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>                                 y_pred<span class="op">=</span>y_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>)) <span class="co"># logits -&gt; pred labels로 변환</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. 옵티마이저 제로 그라디언트</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. 손실 역전파</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. 옵티마이저 단계 수행</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 에포크당 손실 및 정확도 계산 및 출력</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    train_acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"훈련 손실: </span><span class="sc">{</span>train_loss<span class="sc">:.5f}</span><span class="ss"> | 훈련 정확도: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_step(data_loader: torch.utils.data.DataLoader,</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>              model: torch.nn.Module,</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>              loss_fn: torch.nn.Module,</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>              accuracy_fn,</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>              device: torch.device <span class="op">=</span> device):</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>    test_loss, test_acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>() <span class="co"># 모델을 평가 모드로 설정</span></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 추론 모드 컨텍스트 매니저 켜기</span></span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode(): </span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> data_loader:</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 데이터를 GPU로 보냄</span></span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>            X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 1. 순전파</span></span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>            test_pred <span class="op">=</span> model(X)</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 2. 손실 및 정확도 계산</span></span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(test_pred, y)</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>            test_acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y,</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>                y_pred<span class="op">=</span>test_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># logits -&gt; pred labels로 변환</span></span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 지표 조정 및 출력</span></span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"테스트 손실: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss"> | 테스트 정확도: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>야호!</p>
<p>이제 모델을 훈련하고 테스트하기 위한 함수가 생겼으니 실행해 보겠습니다.</p>
<p>각 에포크에 대해 또 다른 루프 내부에서 이를 수행할 것입니다.</p>
<p>그렇게 하면 각 에포크마다 훈련 및 테스트 단계를 거치게 됩니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> 테스트 단계를 얼마나 자주 수행할지 커스텀할 수 있습니다. 때로는 5에포크 또는 10에포크마다 수행하기도 하지만, 여기서는 매 에포크마다 수행합니다.</p>
</blockquote>
<p>GPU에서 코드를 실행하는 데 걸리는 시간도 측정해 보겠습니다.</p>
<div id="2bb8094b-01a0-4b84-9526-ba8888d04901" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 시간 측정</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>train_time_start_on_gpu <span class="op">=</span> timer()</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"에포크: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ch">\n</span><span class="ss">---------"</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    train_step(data_loader<span class="op">=</span>train_dataloader, </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_1, </span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    test_step(data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_1,</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>train_time_end_on_gpu <span class="op">=</span> timer()</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>total_train_time_model_1 <span class="op">=</span> print_train_time(start<span class="op">=</span>train_time_start_on_gpu,</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>                                            end<span class="op">=</span>train_time_end_on_gpu,</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>                                            device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"03eed0dd4e134aea983b4c5383502cf0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0
---------
Train loss: 1.09199 | Train accuracy: 61.34%
Test loss: 0.95636 | Test accuracy: 65.00%

Epoch: 1
---------
Train loss: 0.78101 | Train accuracy: 71.93%
Test loss: 0.72227 | Test accuracy: 73.91%

Epoch: 2
---------
Train loss: 0.67027 | Train accuracy: 75.94%
Test loss: 0.68500 | Test accuracy: 75.02%

Train time on cuda: 16.943 seconds</code></pre>
</div>
</div>
<p>훌륭합니다!</p>
<p>모델이 훈련되었지만 훈련 시간이 더 오래 걸렸나요?</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> CUDA 대 CPU 훈련 시간은 주로 사용하는 CPU/GPU의 품질에 따라 달라집니다. 더 자세한 설명은 아래를 읽어보세요.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>질문:</strong> “GPU를 사용했는데 모델 훈련 속도가 빨라지지 않았습니다. 이유가 무엇일까요?”</p>
<p><strong>답변:</strong> 한 가지 이유는 데이터셋과 모델이 모두 매우 작기 때문에(우리가 다루는 데이터셋과 모델처럼), GPU를 사용함으로써 얻는 이점보다 실제로 데이터를 GPU로 전송하는 데 걸리는 시간이 더 크기 때문일 수 있습니다.</p>
<p>CPU 메모리(기본값)에서 GPU 메모리로 데이터를 복사하는 사이에 작은 병목 현상이 발생합니다.</p>
<p>따라서 작은 모델과 데이터셋의 경우 실제로는 CPU가 계산하기에 최적의 장소일 수 있습니다.</p>
<p>하지만 큰 데이터셋과 모델의 경우 GPU가 제공할 수 있는 계산 속도는 대개 데이터를 전송하는 비용보다 훨씬 큽니다.</p>
<p>그러나 이것은 주로 사용 중인 하드웨어에 따라 달라집니다. 연습을 통해 모델을 훈련하기에 가장 좋은 장소가 어디인지 익숙해질 것입니다.</p>
</blockquote>
<p>이제 <code>eval_model()</code> 함수를 사용하여 훈련된 <code>model_1</code>을 평가하고 어떻게 되었는지 확인해 보겠습니다.</p>
<div id="32a544e3-9dbe-4aa1-b074-22e28b8f2f2a" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 참고: This will error due to `eval_model()` not using device agnostic code </span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>model_1_results <span class="op">=</span> eval_model(model<span class="op">=</span>model_1, </span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, </span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    accuracy_fn<span class="op">=</span>accuracy_fn) </span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>model_1_results </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">RuntimeError</span>                              Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_1084458/2906876561.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg ansi-bold">      2</span> 
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-red-fg"># Note: This will error due to `eval_model()` not using device agnostic code</span>
<span class="ansi-green-fg">----&gt; 4</span><span class="ansi-red-fg"> model_1_results = eval_model(model=model_1, 
</span><span class="ansi-green-fg ansi-bold">      5</span>     data_loader<span class="ansi-blue-fg">=</span>test_dataloader<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg ansi-bold">      6</span>     loss_fn<span class="ansi-blue-fg">=</span>loss_fn<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">/tmp/ipykernel_1084458/2300884397.py</span> in <span class="ansi-cyan-fg">eval_model</span><span class="ansi-blue-fg">(model, data_loader, loss_fn, accuracy_fn)</span>
<span class="ansi-green-fg ansi-bold">     20</span>         <span class="ansi-green-fg">for</span> X<span class="ansi-blue-fg">,</span> y <span class="ansi-green-fg">in</span> data_loader<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg ansi-bold">     21</span>             <span class="ansi-red-fg"># Make predictions with the model</span>
<span class="ansi-green-fg">---&gt; 22</span><span class="ansi-red-fg">             </span>y_pred <span class="ansi-blue-fg">=</span> model<span class="ansi-blue-fg">(</span>X<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">     23</span> 
<span class="ansi-green-fg ansi-bold">     24</span>             <span class="ansi-red-fg"># Accumulate the loss and accuracy values per batch</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1108</span>         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1109</span>                 or _global_forward_hooks or _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1110</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> forward_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">   1111</span>         <span class="ansi-red-fg"># Do not call functions when jit is used</span>
<span class="ansi-green-fg ansi-bold">   1112</span>         full_backward_hooks<span class="ansi-blue-fg">,</span> non_full_backward_hooks <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">/tmp/ipykernel_1084458/3744982926.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, x)</span>
<span class="ansi-green-fg ansi-bold">     12</span> 
<span class="ansi-green-fg ansi-bold">     13</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">:</span> torch<span class="ansi-blue-fg">.</span>Tensor<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 14</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>layer_stack<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1108</span>         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1109</span>                 or _global_forward_hooks or _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1110</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> forward_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">   1111</span>         <span class="ansi-red-fg"># Do not call functions when jit is used</span>
<span class="ansi-green-fg ansi-bold">   1112</span>         full_backward_hooks<span class="ansi-blue-fg">,</span> non_full_backward_hooks <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/container.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, input)</span>
<span class="ansi-green-fg ansi-bold">    139</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> input<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg ansi-bold">    140</span>         <span class="ansi-green-fg">for</span> module <span class="ansi-green-fg">in</span> self<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 141</span><span class="ansi-red-fg">             </span>input <span class="ansi-blue-fg">=</span> module<span class="ansi-blue-fg">(</span>input<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">    142</span>         <span class="ansi-green-fg">return</span> input
<span class="ansi-green-fg ansi-bold">    143</span> 

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-fg">(self, *input, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1108</span>         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1109</span>                 or _global_forward_hooks or _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1110</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> forward_call<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>input<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">   1111</span>         <span class="ansi-red-fg"># Do not call functions when jit is used</span>
<span class="ansi-green-fg ansi-bold">   1112</span>         full_backward_hooks<span class="ansi-blue-fg">,</span> non_full_backward_hooks <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">]</span>

<span class="ansi-green-fg">~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/linear.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-fg">(self, input)</span>
<span class="ansi-green-fg ansi-bold">    101</span> 
<span class="ansi-green-fg ansi-bold">    102</span>     <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> input<span class="ansi-blue-fg">:</span> Tensor<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> Tensor<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 103</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> F<span class="ansi-blue-fg">.</span>linear<span class="ansi-blue-fg">(</span>input<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>weight<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>bias<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg ansi-bold">    104</span> 
<span class="ansi-green-fg ansi-bold">    105</span>     <span class="ansi-green-fg">def</span> extra_repr<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> str<span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">RuntimeError</span>: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre>
</div>
</div>
</div>
<p>안돼요!</p>
<p><code>eval_model()</code> 함수에서 다음과 같은 오류가 발생합니다.</p>
<blockquote class="blockquote">
<p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code></p>
</blockquote>
<p>그 이유는 데이터와 모델은 장치 중립적 코드를 사용하도록 설정했지만 평가 함수는 그렇지 않았기 때문입니다.</p>
<p><code>eval_model()</code> 함수에 타겟 <code>device</code> 매개변수를 전달하여 이를 해결해 볼까요?</p>
<p>그런 다음 결과를 다시 계산해 보겠습니다.</p>
<div id="f3665d99-1adc-4d9f-bfc6-e5601a80691c" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Move values to device</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_model(model: torch.nn.Module, </span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>               data_loader: torch.utils.data.DataLoader, </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>               loss_fn: torch.nn.Module, </span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>               accuracy_fn, </span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>               device: torch.device <span class="op">=</span> device):</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluates a given model on a given dataset.</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="co">        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co">        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co">        loss_fn (torch.nn.Module): The loss function of model.</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="co">        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="co">        device (str, optional): Target device to compute on. Defaults to device.</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a><span class="co">        (dict): Results of model making predictions on data_loader.</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> data_loader:</span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Send data to the target device</span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>            X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(X)</span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> loss_fn(y_pred, y)</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">+=</span> accuracy_fn(y_true<span class="op">=</span>y, y_pred<span class="op">=</span>y_pred.argmax(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale loss and acc</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">/=</span> <span class="bu">len</span>(data_loader)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"model_name"</span>: model.<span class="va">__class__</span>.<span class="va">__name__</span>, <span class="co"># only works when model was created with a class</span></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_loss"</span>: loss.item(),</span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>            <span class="st">"model_acc"</span>: acc}</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate model 1 results with device-agnostic code </span></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>model_1_results <span class="op">=</span> eval_model(model<span class="op">=</span>model_1, data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, accuracy_fn<span class="op">=</span>accuracy_fn,</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a>model_1_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>{'model_name': 'FashionMNISTModelV1',
 'model_loss': 0.6850008964538574,
 'model_acc': 75.01996805111821}</code></pre>
</div>
</div>
<div id="a9e916cf-f873-4481-a983-bac26ce4cac2" class="cell" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check baseline results</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>model_0_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>{'model_name': 'FashionMNISTModelV0',
 'model_loss': 0.47663894295692444,
 'model_acc': 83.42651757188499}</code></pre>
</div>
</div>
<p>이런, 이 경우에는 모델에 비선형성을 추가했음에도 베이스라인보다 성능이 떨어졌습니다.</p>
<p>머신러닝에서 주의해야 할 점은, 효과가 있을 것이라고 생각했던 것이 그렇지 않을 때가 있고, 그 반대의 경우도 있다는 것입니다.</p>
<p>이것은 과학이기도 하지만 예술이기도 합니다.</p>
<p>겉보기에는 우리 모델이 훈련 데이터에 <strong>과적합(overfitting)</strong>된 것으로 보입니다.</p>
<p>과적합은 모델이 훈련 데이터는 잘 학습하지만 그 패턴이 테스트 데이터로 일반화되지 않는 것을 의미합니다.</p>
<p>과적합을 해결하는 두 가지 주요 방법은 다음과 같습니다. 1. 더 작거나 다른 모델을 사용합니다(일부 모델은 특정 종류의 데이터에 다른 모델보다 더 잘 맞습니다). 2. 더 큰 데이터셋을 사용합니다(데이터가 많을수록 모델이 일반화 가능한 패턴을 학습할 기회가 더 많아집니다).</p>
<p>더 많은 방법이 있지만, 그것은 여러분이 탐구해 볼 과제로 남겨두겠습니다.</p>
<p>온라인에서 “머신러닝에서 과적합을 방지하는 방법”을 검색하여 무엇이 나오는지 확인해 보세요.</p>
<p>그동안 우리는 1번 방법인 다른 모델 사용하기를 살펴보겠습니다.</p>
</section>
</section>
<section id="모델-2-합성곱-신경망-cnn-구축하기" class="level2">
<h2 class="anchored" data-anchor-id="모델-2-합성곱-신경망-cnn-구축하기">7. 모델 2: 합성곱 신경망 (CNN) 구축하기</h2>
<p>좋습니다. 이제 한 단계 더 나아가 볼 시간입니다.</p>
<p>이제 <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">합성곱 신경망(Convolutional Neural Network)</a>(CNN 또는 ConvNet)을 만들 차례입니다.</p>
<p>CNN은 시각적 데이터에서 패턴을 찾는 능력으로 잘 알려져 있습니다.</p>
<p>우리는 시각적 데이터를 다루고 있으므로 CNN 모델을 사용하여 베이스라인을 개선할 수 있는지 확인해 보겠습니다.</p>
<p>우리가 사용할 CNN 모델은 <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> 웹사이트의 TinyVGG로 알려진 모델입니다.</p>
<p>이 모델은 합성곱 신경망의 전형적인 구조를 따릅니다.</p>
<p><code>입력 레이어 -&gt; [합성곱 레이어 -&gt; 활성화 레이어 -&gt; 풀링 레이어] -&gt; 출력 레이어</code></p>
<p><code>[합성곱 레이어 -&gt; 활성화 레이어 -&gt; 풀링 레이어]</code>의 내용은 요구 사항에 따라 여러 번 반복되고 확장될 수 있습니다.</p>
<section id="어떤-모델을-사용해야-하나요" class="level3">
<h3 class="anchored" data-anchor-id="어떤-모델을-사용해야-하나요">어떤 모델을 사용해야 하나요?</h3>
<blockquote class="blockquote">
<p><strong>질문:</strong> 잠깐만요, CNN이 이미지에 좋다고 하셨는데, 제가 알아야 할 다른 모델 유형이 있나요?</p>
</blockquote>
<p>좋은 질문입니다.</p>
<p>이 표는 어떤 모델을 사용할지에 대한 일반적인 가이드입니다(예외는 있습니다).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>문제 유형</strong></th>
<th><strong>사용할 모델 (일반적으로)</strong></th>
<th><strong>코드 예시</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>정형 데이터 (Excel 스프레드시트, 행 및 열 데이터)</td>
<td>그라디언트 부스팅 모델, 랜덤 포레스트, XGBoost</td>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"><code>sklearn.ensemble</code></a>, <a href="https://xgboost.readthedocs.io/en/stable/">XGBoost 라이브러리</a></td>
</tr>
<tr class="even">
<td>비정형 데이터 (이미지, 오디오, 언어)</td>
<td>합성곱 신경망, 트랜스포머</td>
<td><a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a>, <a href="https://huggingface.co/docs/transformers/index">HuggingFace Transformers</a></td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>참고:</strong> 위의 표는 참고용일 뿐이며, 결국 사용하게 될 모델은 작업 중인 문제와 제약 조건(데이터 양, 지연 시간 요구 사항)에 따라 크게 달라집니다.</p>
</blockquote>
<p>모델에 대한 이야기는 이쯤 하고, 이제 <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer 웹사이트</a>의 모델을 복제하는 CNN을 구축해 보겠습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-cnn-explainer-model.png" class="img-fluid figure-img"></p>
<figcaption>TinyVGG 아키텍처, CNN Explainer 웹사이트의 설정</figcaption>
</figure>
</div>
<p>이를 위해 <code>torch.nn</code>의 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>nn.Conv2d()</code></a> 및 <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"><code>nn.MaxPool2d()</code></a> 레이어를 활용할 것입니다.</p>
<div id="dce60214-63fd-46e2-89ba-125445ac76b7" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a convolutional neural network </span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FashionMNISTModelV2(nn.Module):</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Model architecture copying TinyVGG from: </span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co">    https://poloclub.github.io/cnn-explainer/</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape: <span class="bu">int</span>, hidden_units: <span class="bu">int</span>, output_shape: <span class="bu">int</span>):</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span>input_shape, </span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>                      out_channels<span class="op">=</span>hidden_units, </span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span><span class="dv">3</span>, <span class="co"># how big is the square that's going over the image?</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>                      stride<span class="op">=</span><span class="dv">1</span>, <span class="co"># default</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>                      padding<span class="op">=</span><span class="dv">1</span>),<span class="co"># options = "valid" (no padding) or "same" (output has same shape as input) or int for specific number </span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(in_channels<span class="op">=</span>hidden_units, </span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>                      out_channels<span class="op">=</span>hidden_units,</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>                      kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>                      stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>                      padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>                         stride<span class="op">=</span><span class="dv">2</span>) <span class="co"># default stride value is same as kernel_size</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(hidden_units, hidden_units, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(hidden_units, hidden_units, <span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool2d(<span class="dv">2</span>)</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Where did this in_features shape come from? </span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># It's because each layer of our network compresses and changes the shape of our inputs data.</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units<span class="op">*</span><span class="dv">7</span><span class="op">*</span><span class="dv">7</span>, </span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>                      out_features<span class="op">=</span>output_shape)</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor):</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block_1(x)</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(x.shape)</span></span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block_2(x)</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(x.shape)</span></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(x.shape)</span></span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>model_2 <span class="op">=</span> FashionMNISTModelV2(input_shape<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>    hidden_units<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>    output_shape<span class="op">=</span><span class="bu">len</span>(class_names)).to(device)</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>model_2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>FashionMNISTModelV2(
  (block_1): Sequential(
    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block_2): Sequential(
    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=490, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<p>Nice!</p>
<p>Our biggest model yet!</p>
<p>What we’ve done is a common practice in machine learning.</p>
<p>Find a model architecture somewhere and replicate it with code.</p>
</section>
<section id="nn.conv2d-단계별로-살펴보기" class="level3">
<h3 class="anchored" data-anchor-id="nn.conv2d-단계별로-살펴보기">7.1 <code>nn.Conv2d()</code> 단계별로 살펴보기</h3>
<p>위에서 만든 모델을 바로 사용할 수도 있지만, 먼저 새로 추가한 두 개의 레이어를 단계별로 살펴보겠습니다. * <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>nn.Conv2d()</code></a>: 합성곱 레이어(convolutional layer)라고도 합니다. * <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"><code>nn.MaxPool2d()</code></a>: 최대 풀링 레이어(max pooling layer)라고도 합니다.</p>
<blockquote class="blockquote">
<p><strong>질문:</strong> <code>nn.Conv2d()</code>에서 “2d”는 무엇을 의미하나요?</p>
<p>2d는 2차원(2-dimensional) 데이터를 의미합니다. 우리의 이미지는 높이와 너비라는 두 개의 차원을 가집니다. 색상 채널 차원도 있지만, 각 색상 채널 차원 자체도 높이와 너비라는 두 개의 차원을 가집니다.</p>
<p>다른 차원의 데이터(텍스트의 경우 1D, 3D 객체의 경우 3D)를 위해 <code>nn.Conv1d()</code> 및 <code>nn.Conv3d()</code>도 존재합니다.</p>
</blockquote>
<p>레이어를 테스트하기 위해 CNN Explainer에서 사용된 데이터와 유사한 장난감 데이터(toy data)를 만들어 보겠습니다.</p>
<div id="058b01ac-3f6a-4472-bcbf-3377974e3254" class="cell" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 이미지 배치와 동일한 크기의 무작위 숫자 샘플 배치 생성</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> torch.randn(size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">64</span>, <span class="dv">64</span>)) <span class="co"># [batch_size, color_channels, height, width]</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>test_image <span class="op">=</span> images[<span class="dv">0</span>] <span class="co"># 테스트를 위한 단일 이미지 가져오기</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"이미지 배치 모양: </span><span class="sc">{</span>images<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, color_channels, height, width]"</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"단일 이미지 모양: </span><span class="sc">{</span>test_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [color_channels, height, width]"</span>) </span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"단일 이미지 픽셀 값:</span><span class="ch">\n</span><span class="sc">{</span>test_image<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>다양한 매개변수를 사용하여 <code>nn.Conv2d()</code> 예시를 만들어 보겠습니다. * <code>in_channels</code> (int) - 입력 이미지의 채널 수. * <code>out_channels</code> (int) - 합성곱에 의해 생성된 채널 수. * <code>kernel_size</code> (int 또는 tuple) - 합성곱 커널/필터의 크기. * <code>stride</code> (int 또는 tuple, 선택 사항) - 합성곱 커널이 한 번에 이동하는 단계의 크기. 기본값: 1. * <code>padding</code> (int, tuple, str) - 입력의 네 면 모두에 추가되는 패딩. 기본값: 0.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/03-conv2d-layer.png" class="img-fluid figure-img"></p>
<figcaption>Conv2d 레이어의 다양한 매개변수를 살펴보는 예시</figcaption>
</figure>
</div>
<p><em><code>nn.Conv2d()</code> 레이어의 하이퍼파라미터를 변경할 때 일어나는 일의 예시입니다.</em></p>
<div id="ebd39562-1dad-40e3-90f5-750a5dac24e2" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># TinyVGG와 동일한 차원의 합성곱 레이어 생성</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="co"># (매개변수를 변경해 보며 어떤 일이 일어나는지 확인해 보세요)</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>conv_layer <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>                       out_channels<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>                       kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>                       stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>                       padding<span class="op">=</span><span class="dv">0</span>) <span class="co"># 여기서 "valid" 또는 "same"도 사용해 보세요 </span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 합성곱 레이어에 데이터 통과</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>conv_layer(test_image) <span class="co"># 참고: PyTorch &lt;1.11.0 버전을 실행 중인 경우 모양 문제로 인해 오류가 발생합니다(nn.Conv2d()는 4D 텐서를 입력으로 기대함) </span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>단일 이미지를 통과시키려고 하면 모양 불일치 오류가 발생합니다.</p>
<blockquote class="blockquote">
<p><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead</code></p>
<p><strong>참고:</strong> PyTorch 1.11.0 이상 버전을 실행 중이라면 이 오류는 발생하지 않습니다.</p>
</blockquote>
<p>이는 <code>nn.Conv2d()</code> 레이어가 <code>(N, C, H, W)</code> 또는 <code>[batch_size, color_channels, height, width]</code> 크기의 4차원 텐서를 입력으로 기대하기 때문입니다.</p>
<p>현재 단일 이미지 <code>test_image</code>는 <code>[color_channels, height, width]</code> 또는 <code>[3, 64, 64]</code> 모양만 가지고 있습니다.</p>
<p><code>test_image.unsqueeze(dim=0)</code>을 사용하여 <code>N</code>에 대한 추가 차원을 더함으로써 이를 해결할 수 있습니다.</p>
<div id="abba741d-a1ed-44ed-ba53-41d589433a2c" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 테스트 이미지에 추가 차원 더하기</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c7280a49-4ee0-452b-a514-61115b6a444c" class="cell" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 추가 차원이 있는 테스트 이미지를 conv_layer에 통과시키기</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>conv_layer(test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Hmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a>), we get different channel sizes as well as different pixel sizes.</p>
<p>What if we changed the values of <code>conv_layer</code>?</p>
<div id="04445d45-cf2f-4c1d-b215-bc50865a207a" class="cell" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new conv_layer with different values (try setting these to whatever you like)</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>conv_layer_2 <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, <span class="co"># same number of color channels as our input image</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>                         out_channels<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>                         kernel_size<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">5</span>), <span class="co"># kernel is usually a square so a tuple also works</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>                         stride<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>                         padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>conv_layer_2(test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>torch.Size([1, 10, 30, 30])</code></pre>
</div>
</div>
<p>Woah, we get another shape change.</p>
<p>Now our image is of shape <code>[1, 10, 30, 30]</code> (it will be different if you use different values) or <code>[batch_size=1, color_channels=10, height=30, width=30]</code>.</p>
<p>What’s going on here?</p>
<p>Behind the scenes, our <code>nn.Conv2d()</code> is compressing the information stored in the image.</p>
<p>It does this by performing operations on the input (our test image) against its internal parameters.</p>
<p>The goal of this is similar to all of the other neural networks we’ve been building.</p>
<p>Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.</p>
<p>The only difference is <em>how</em> the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer <code>forward()</code> method.</p>
<p>If we check out our <code>conv_layer_2.state_dict()</code> we’ll find a similar weight and bias setup as we’ve seen before.</p>
<div id="46027ed1-c3a7-46bd-bab7-17f8c20e354b" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out the conv_layer_2 internal parameters</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conv_layer_2.state_dict())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],
          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],
          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],
          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],
          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],

         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],
          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],
          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],
          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],
          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],

         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],
          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],
          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],
          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],
          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],


        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],
          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],
          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],
          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],
          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],

         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],
          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],
          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],
          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],
          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],

         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],
          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],
          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],
          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],
          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],


        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],
          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],
          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],
          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],
          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],

         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],
          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],
          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],
          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],
          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],

         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],
          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],
          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],
          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],
          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],


        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],
          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],
          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],
          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],
          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],

         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],
          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],
          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],
          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],
          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],

         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],
          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],
          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],
          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],
          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],


        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],
          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],
          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],
          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],
          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],

         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],
          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],
          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],
          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],
          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],

         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],
          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],
          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],
          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],
          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],


        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],
          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],
          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],
          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],
          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],

         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],
          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],
          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],
          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],
          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],

         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],
          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],
          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],
          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],
          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],


        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],
          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],
          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],
          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],
          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],

         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],
          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],
          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],
          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],
          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],

         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],
          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],
          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],
          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],
          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],


        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],
          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],
          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],
          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],
          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],

         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],
          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],
          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],
          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],
          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],

         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],
          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],
          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],
          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],
          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],


        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],
          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],
          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],
          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],
          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],

         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],
          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],
          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],
          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],
          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],

         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],
          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],
          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],
          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],
          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],


        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],
          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],
          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],
          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],
          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],

         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],
          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],
          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],
          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],
          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],

         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],
          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],
          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],
          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],
          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,
        -0.0797,  0.1121]))])</code></pre>
</div>
</div>
<p>Look at that! A bunch of random numbers for a weight and bias tensor.</p>
<p>The shapes of these are manipulated by the inputs we passed to <code>nn.Conv2d()</code> when we set it up.</p>
<p>Let’s check them out.</p>
<div id="e5518d61-c0b7-4351-b5ea-4d6b6144291a" class="cell" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get shapes of weight and bias tensors within conv_layer_2</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"conv_layer_2 weight shape: </span><span class="ch">\n</span><span class="sc">{</span>conv_layer_2<span class="sc">.</span>weight<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]"</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">conv_layer_2 bias shape: </span><span class="ch">\n</span><span class="sc">{</span>conv_layer_2<span class="sc">.</span>bias<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [out_channels=10]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>conv_layer_2 weight shape: 
torch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]

conv_layer_2 bias shape: 
torch.Size([10]) -&gt; [out_channels=10]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Question:</strong> What should we set the parameters of our <code>nn.Conv2d()</code> layers?</p>
<p>That’s a good one. But similar to many other things in machine learning, the values of these aren’t set in stone (and recall, because these values are ones we can set ourselves, they’re referred to as “<strong>hyperparameters</strong>”).</p>
<p>The best way to find out is to try out different values and see how they effect your model’s performance.</p>
<p>Or even better, find a working example on a problem similar to yours (like we’ve done with TinyVGG) and copy it.</p>
</blockquote>
<p>We’re working with a different of layer here to what we’ve seen before.</p>
<p>But the premise remains the same: start with random numbers and update them to better represent the data.</p>
</section>
<section id="stepping-through-nn.maxpool2d" class="level3">
<h3 class="anchored" data-anchor-id="stepping-through-nn.maxpool2d">7.2 Stepping through <code>nn.MaxPool2d()</code></h3>
<p>Now let’s check out what happens when we move data through <code>nn.MaxPool2d()</code>.</p>
<div id="1164c753-19d9-43b7-a04f-017d0f7188c3" class="cell" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out original image shape without and with unsqueezed dimension</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test image original shape: </span><span class="sc">{</span>test_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test image with unsqueezed dimension: </span><span class="sc">{</span>test_image<span class="sc">.</span>unsqueeze(dim<span class="op">=</span><span class="dv">0</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a sample nn.MaxPoo2d() layer</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>max_pool_layer <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass data through just the conv_layer</span></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>test_image_through_conv <span class="op">=</span> conv_layer(test_image.unsqueeze(dim<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after going through conv_layer(): </span><span class="sc">{</span>test_image_through_conv<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass data through the max pool layer</span></span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>test_image_through_conv_and_max_pool <span class="op">=</span> max_pool_layer(test_image_through_conv)</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape after going through conv_layer() and max_pool_layer(): </span><span class="sc">{</span>test_image_through_conv_and_max_pool<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test image original shape: torch.Size([3, 64, 64])
Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])
Shape after going through conv_layer(): torch.Size([1, 10, 62, 62])
Shape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])</code></pre>
</div>
</div>
<p>Notice the change in the shapes of what’s happening in and out of a <code>nn.MaxPool2d()</code> layer.</p>
<p>The <code>kernel_size</code> of the <code>nn.MaxPool2d()</code> layer will effects the size of the output shape.</p>
<p>In our case, the shape halves from a <code>62x62</code> image to <code>31x31</code> image.</p>
<p>Let’s see this work with a smaller tensor.</p>
<div id="e6a2b196-4845-4b40-9212-e75406e88875" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a random tensor with a similiar number of dimensions to our images</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>random_tensor <span class="op">=</span> torch.randn(size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor:</span><span class="ch">\n</span><span class="sc">{</span>random_tensor<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random tensor shape: </span><span class="sc">{</span>random_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a max pool layer</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>max_pool_layer <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>) <span class="co"># see what happens when you change the kernel_size value </span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the random tensor through the max pool layer</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>max_pool_tensor <span class="op">=</span> max_pool_layer(random_tensor)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Max pool tensor:</span><span class="ch">\n</span><span class="sc">{</span>max_pool_tensor<span class="sc">}</span><span class="ss"> &lt;- this is the maximum value from random_tensor"</span>)</span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Max pool tensor shape: </span><span class="sc">{</span>max_pool_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random tensor:
tensor([[[[0.3367, 0.1288],
          [0.2345, 0.2303]]]])
Random tensor shape: torch.Size([1, 1, 2, 2])

Max pool tensor:
tensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor
Max pool tensor shape: torch.Size([1, 1, 1, 1])</code></pre>
</div>
</div>
<p><code>random_tensor</code>와 <code>max_pool_tensor</code> 사이의 마지막 두 차원을 주목해 보세요. <code>[2, 2]</code>에서 <code>[1, 1]</code>로 바뀌었습니다.</p>
<p>본질적으로 절반으로 줄어든 것입니다.</p>
<p>그리고 이 변화는 <code>nn.MaxPool2d()</code>의 <code>kernel_size</code> 값에 따라 달라질 것입니다.</p>
<p>또한 <code>max_pool_tensor</code>에 남은 값은 <code>random_tensor</code>에서 <strong>최댓값(maximum)</strong>이라는 점도 주목하세요.</p>
<p>여기서 무슨 일이 일어나고 있는 걸까요?</p>
<p>이것은 신경망 퍼즐의 또 다른 중요한 조각입니다.</p>
<p>기본적으로 <strong>신경망의 모든 레이어는 고차원 공간에서 저차원 공간으로 데이터를 압축하려고 시도합니다.</strong></p>
<p>즉, 많은 숫자(원시 데이터)를 가져와서 해당 숫자들에서 패턴을 학습하는 것입니다. 이 패턴은 예측 능력을 갖추면서도 원래 값보다 크기가 <em>작은</em> 패턴입니다.</p>
<p>인공지능의 관점에서 보면 신경망의 전체 목표를 정보의 <em>압축</em>이라고 볼 수 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png" class="img-fluid figure-img"></p>
<figcaption>신경망의 각 레이어는 원래 입력 데이터를 더 작은 표현으로 압축하며, 이는 (바라건대) 미래의 입력 데이터에 대해 예측을 수행할 수 있는 능력을 갖춥니다</figcaption>
</figure>
</div>
<p>즉, 신경망의 관점에서 지능은 압축입니다.</p>
<p>이것이 <code>nn.MaxPool2d()</code> 레이어를 사용하는 아이디어입니다. 텐서의 일부에서 최댓값을 가져오고 나머지는 무시하는 것이죠.</p>
<p>본질적으로 정보의 상당 부분을 (바라건대) 유지하면서 텐서의 차원을 낮추는 것입니다.</p>
<p><code>nn.Conv2d()</code> 레이어의 경우도 마찬가지입니다.</p>
<p>다만 최댓값만 가져오는 대신, <code>nn.Conv2d()</code>는 데이터에 대해 합성곱 연산을 수행합니다(<a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer 웹페이지</a>에서 이를 실제로 확인해 보세요).</p>
<blockquote class="blockquote">
<p><strong>과제:</strong> <a href="https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html"><code>nn.AvgPool2d()</code></a> 레이어는 무엇을 한다고 생각하시나요? 위에서 했던 것처럼 무작위 텐서를 만들어 통과시켜 보세요. 입력 및 출력 모양과 입력 및 출력 값을 확인해 보세요.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>추가 학습 자료:</strong> “가장 흔한 합성곱 신경망”을 검색해 보세요. 어떤 아키텍처를 찾았나요? 그중 <a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a> 라이브러리에 포함된 것이 있나요? 이것들로 무엇을 할 수 있을 것 같나요?</p>
</blockquote>
</section>
<section id="model_2를-위한-손실-함수-및-옵티마이저-설정" class="level3">
<h3 class="anchored" data-anchor-id="model_2를-위한-손실-함수-및-옵티마이저-설정">7.3 <code>model_2</code>를 위한 손실 함수 및 옵티마이저 설정</h3>
<p>첫 번째 CNN의 레이어들을 충분히 살펴보았습니다.</p>
<p>하지만 여전히 명확하지 않은 부분이 있다면 작게 시작해 보세요.</p>
<p>모델의 단일 레이어를 선택하고 일부 데이터를 통과시켜 어떤 일이 일어나는지 확인해 보세요.</p>
<p>이제 앞으로 나아가 훈련을 시작할 시간입니다!</p>
<p>손실 함수와 옵티마이저를 설정해 보겠습니다.</p>
<p>이전과 동일하게 다중 클래스 분류 데이터를 다루고 있으므로 <code>nn.CrossEntropyLoss()</code>를 손실 함수로 사용합니다.</p>
<p>그리고 <code>model_2.parameters()</code>를 학습률 <code>0.1</code>로 최적화하기 위해 <code>torch.optim.SGD()</code>를 옵티마이저로 사용합니다.</p>
<div id="06a76a1b-5f6f-4018-bf7b-8388b385476f" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 손실 함수 및 옵티마이저 설정</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_2.parameters(), </span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>                             lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="훈련-및-테스트-함수를-사용하여-model_2-훈련-및-테스트하기" class="level3">
<h3 class="anchored" data-anchor-id="훈련-및-테스트-함수를-사용하여-model_2-훈련-및-테스트하기">7.4 훈련 및 테스트 함수를 사용하여 <code>model_2</code> 훈련 및 테스트하기</h3>
<p>손실 함수와 옵티마이저가 준비되었습니다!</p>
<p>이제 훈련하고 테스트할 시간입니다.</p>
<p>이전에 만든 <code>train_step()</code> 및 <code>test_step()</code> 함수를 사용하겠습니다.</p>
<p>또한 다른 모델과 비교하기 위해 시간을 측정하겠습니다.</p>
<div id="861d126e-d876-40b3-9b7a-66cfc2f1bf05" class="cell" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 시간 측정</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> timeit <span class="im">import</span> default_timer <span class="im">as</span> timer</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>train_time_start_model_2 <span class="op">=</span> timer()</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델 훈련 및 테스트</span></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(epochs)):</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"에포크: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ch">\n</span><span class="ss">---------"</span>)</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    train_step(data_loader<span class="op">=</span>train_dataloader, </span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_2, </span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn,</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    test_step(data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>model_2,</span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>        loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>        accuracy_fn<span class="op">=</span>accuracy_fn,</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>        device<span class="op">=</span>device</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a>train_time_end_model_2 <span class="op">=</span> timer()</span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>total_train_time_model_2 <span class="op">=</span> print_train_time(start<span class="op">=</span>train_time_start_model_2,</span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>                                           end<span class="op">=</span>train_time_end_model_2,</span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>                                           device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"fc69ce706c5c45099e57919dfcae065c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0
---------
Train loss: 0.59411 | Train accuracy: 78.41%
Test loss: 0.39967 | Test accuracy: 85.70%

Epoch: 1
---------
Train loss: 0.36450 | Train accuracy: 86.81%
Test loss: 0.34607 | Test accuracy: 87.48%

Epoch: 2
---------
Train loss: 0.32553 | Train accuracy: 88.33%
Test loss: 0.32664 | Test accuracy: 88.23%

Train time on cuda: 21.099 seconds</code></pre>
</div>
</div>
<p>와! 합성곱 레이어와 최대 풀링 레이어가 성능을 약간 향상시킨 것 같네요.</p>
<p><code>eval_model()</code> 함수를 사용하여 <code>model_2</code>의 결과를 평가해 보겠습니다.</p>
<div id="c1bf8b89-1389-4395-a1c4-9c6e94d9e71c" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model_2 결과 가져오기</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>model_2_results <span class="op">=</span> eval_model(</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model_2,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>model_2_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="모델-결과-및-훈련-시간-비교하기" class="level2">
<h2 class="anchored" data-anchor-id="모델-결과-및-훈련-시간-비교하기">8. 모델 결과 및 훈련 시간 비교하기</h2>
<p>우리는 세 가지 다른 모델을 훈련했습니다.</p>
<ol type="1">
<li><code>model_0</code> - 두 개의 <code>nn.Linear()</code> 레이어가 있는 베이스라인 모델.</li>
<li><code>model_1</code> - 베이스라인 모델과 동일한 설정이지만 <code>nn.Linear()</code> 레이어 사이에 <code>nn.ReLU()</code> 레이어가 추가된 모델.</li>
<li><code>model_2</code> - CNN Explainer 웹사이트의 TinyVGG 아키텍처를 모방한 첫 번째 CNN 모델.</li>
</ol>
<p>이것은 머신러닝에서 일반적인 관행입니다.</p>
<p>여러 모델을 구축하고 여러 번의 훈련 실험을 수행하여 어느 모델이 가장 좋은 성능을 내는지 확인하는 것이죠.</p>
<p>모델 결과 딕셔너리를 DataFrame으로 결합하여 확인해 보겠습니다.</p>
<div id="52d84ee1-1ad4-4860-b147-f8912c1febc7" class="cell" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>compare_results <span class="op">=</span> pd.DataFrame([model_0_results, model_1_results, model_2_results])</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>compare_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model_name</th>
<th data-quarto-table-cell-role="th">model_loss</th>
<th data-quarto-table-cell-role="th">model_acc</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>FashionMNISTModelV0</td>
<td>0.476639</td>
<td>83.426518</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>FashionMNISTModelV1</td>
<td>0.685001</td>
<td>75.019968</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>FashionMNISTModelV2</td>
<td>0.326644</td>
<td>88.228834</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>좋네요!</p>
<p>훈련 시간 값도 추가할 수 있습니다.</p>
<div id="297af38f-e69f-4c6f-9027-fcaf0482a55c" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과 비교에 훈련 시간 추가</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>compare_results[<span class="st">"training_time"</span>] <span class="op">=</span> [total_train_time_model_0,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>                                    total_train_time_model_1,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>                                    total_train_time_model_2]</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>compare_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>우리 CNN(<code>FashionMNISTModelV2</code>) 모델이 가장 성능이 좋았지만(가장 낮은 손실, 가장 높은 정확도), 훈련 시간은 가장 길었습니다.</p>
<p>그리고 베이스라인 모델(<code>FashionMNISTModelV0</code>)은 <code>model_1</code>(<code>FashionMNISTModelV1</code>)보다 성능이 좋았지만 훈련 시간이 더 오래 걸렸습니다(이는 <code>model_0</code> 훈련에는 CPU를, <code>model_1</code> 훈련에는 GPU를 사용했기 때문일 가능성이 큽니다).</p>
<p>여기서 발생하는 상충 관계를 <strong>성능-속도(performance-speed)</strong> 상충 관계라고 합니다.</p>
<p>일반적으로 더 크고 복잡한 모델(우리가 <code>model_2</code>로 했던 것처럼)에서 더 나은 성능을 얻을 수 있습니다.</p>
<p>하지만 이러한 성능 향상은 종종 훈련 속도와 추론 속도의 희생을 수반합니다.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> 훈련 시간은 사용 중인 하드웨어에 따라 크게 달라집니다.</p>
<p>일반적으로 CPU 코어가 많을수록 CPU에서의 모델 훈련 속도가 빨라집니다. GPU의 경우도 마찬가지입니다.</p>
<p>최신 하드웨어는 기술 발전을 통합하기 때문에 대개 모델 훈련 속도가 더 빠릅니다.</p>
</blockquote>
<p>이제 시각화를 해볼까요?</p>
<div id="5eb0df60-9318-47d0-adce-f8788ed3999e" class="cell" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize our model results</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>compare_results.set_index(<span class="st">"model_name"</span>)[<span class="st">"model_acc"</span>].plot(kind<span class="op">=</span><span class="st">"barh"</span>)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"accuracy (%)"</span>)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"model"</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-46-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="make-and-evaluate-random-predictions-with-best-model" class="level2">
<h2 class="anchored" data-anchor-id="make-and-evaluate-random-predictions-with-best-model">9. Make and evaluate random predictions with best model</h2>
<p>Alright, we’ve compared our models to each other, let’s further evaluate our best performing model, <code>model_2</code>.</p>
<p>To do so, let’s create a function <code>make_predictions()</code> where we can pass the model and some data for it to predict on.</p>
<div id="d1d5d3e7-9601-4141-8bd7-9abbd016bf6c" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_predictions(model: torch.nn.Module, data: <span class="bu">list</span>, device: torch.device <span class="op">=</span> device):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    pred_probs <span class="op">=</span> []</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sample <span class="kw">in</span> data:</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Prepare sample</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>            sample <span class="op">=</span> torch.unsqueeze(sample, dim<span class="op">=</span><span class="dv">0</span>).to(device) <span class="co"># Add an extra dimension and send sample to device</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass (model outputs raw logit)</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>            pred_logit <span class="op">=</span> model(sample)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get prediction probability (logit -&gt; prediction probability)</span></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>            pred_prob <span class="op">=</span> torch.softmax(pred_logit.squeeze(), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get pred_prob off GPU for further calculations</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>            pred_probs.append(pred_prob.cpu())</span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack the pred_probs to turn list into a tensor</span></span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.stack(pred_probs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="420c7461-eaa9-4459-9e68-53574c758765" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>test_samples <span class="op">=</span> []</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> []</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample, label <span class="kw">in</span> random.sample(<span class="bu">list</span>(test_data), k<span class="op">=</span><span class="dv">9</span>):</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    test_samples.append(sample)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    test_labels.append(label)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a><span class="co"># View the first test sample shape and label</span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test sample image shape: </span><span class="sc">{</span>test_samples[<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">Test sample label: </span><span class="sc">{</span>test_labels[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>class_names[test_labels[<span class="dv">0</span>]]<span class="sc">}</span><span class="ss">)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test sample image shape: torch.Size([1, 28, 28])
Test sample label: 5 (Sandal)</code></pre>
</div>
</div>
<p>And now we can use our <code>make_predictions()</code> function to predict on <code>test_samples</code>.</p>
<div id="79de2ac1-7d4b-4f81-ae8a-90099bca2a3d" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on test samples with model 2</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>pred_probs<span class="op">=</span> make_predictions(model<span class="op">=</span>model_2, </span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>                             data<span class="op">=</span>test_samples)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="co"># View first two prediction probabilities list</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>pred_probs[:<span class="dv">2</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>tensor([[2.3550e-07, 1.7185e-08, 4.6618e-07, 6.1371e-08, 5.1185e-08, 9.9957e-01,
         3.7702e-07, 1.5924e-05, 3.7681e-05, 3.7831e-04],
        [7.3275e-02, 6.7410e-01, 3.7231e-03, 8.8129e-02, 1.0114e-01, 6.9186e-05,
         5.8674e-02, 4.2595e-04, 3.8635e-04, 7.1354e-05]])</code></pre>
</div>
</div>
<p>Excellent!</p>
<p>And now we can go from prediction probabilities to prediction labels by taking the <code>torch.argmax()</code> of the output of the <code>torch.softmax()</code> activation function.</p>
<div id="f9d97bcc-4310-4851-a1f8-6bcd757e9b26" class="cell" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn the prediction probabilities into prediction labels by taking the argmax()</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>pred_classes <span class="op">=</span> pred_probs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>pred_classes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])</code></pre>
</div>
</div>
<div id="1141af97-0990-4920-83d4-c13cca3f9abc" class="cell" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Are our predictions in the same form as our test labels? </span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>test_labels, pred_classes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))</code></pre>
</div>
</div>
<p>Now our predicted classes are in the same format as our test labels, we can compare.</p>
<p>Since we’re dealing with image data, let’s stay true to the data explorer’s motto.</p>
<p>“Visualize, visualize, visualize!”</p>
<div id="679cb5f7-bb66-42dd-a4d6-400b27b7c019" class="cell" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot predictions</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>nrows <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>ncols <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(test_samples):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create a subplot</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>  plt.subplot(nrows, ncols, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot the target image</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>  plt.imshow(sample.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find the prediction label (in text form, e.g. "Sandal")</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>  pred_label <span class="op">=</span> class_names[pred_classes[i]]</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Get the truth label (in text form, e.g. "T-shirt")</span></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>  truth_label <span class="op">=</span> class_names[test_labels[i]] </span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create the title text of the plot</span></span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>  title_text <span class="op">=</span> <span class="ss">f"Pred: </span><span class="sc">{</span>pred_label<span class="sc">}</span><span class="ss"> | Truth: </span><span class="sc">{</span>truth_label<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check for equality and change title colour accordingly</span></span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> pred_label <span class="op">==</span> truth_label:</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>      plt.title(title_text, fontsize<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span><span class="st">"g"</span>) <span class="co"># green text if correct</span></span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>      plt.title(title_text, fontsize<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span><span class="st">"r"</span>) <span class="co"># red text if wrong</span></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>  plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-52-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Well, well, well, doesn’t that look good!</p>
<p>Not bad for a couple dozen lines of PyTorch code!</p>
</section>
<section id="making-a-confusion-matrix-for-further-prediction-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="making-a-confusion-matrix-for-further-prediction-evaluation">10. Making a confusion matrix for further prediction evaluation</h2>
<p>There are many <a href="https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics">different evaluation metrics</a> we can use for classification problems.</p>
<p>One of the most visual is a <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">confusion matrix</a>.</p>
<p>A confusion matrix shows you where your classification model got confused between predicitons and true labels.</p>
<p>To make a confusion matrix, we’ll go through three steps: 1. Make predictions with our trained model, <code>model_2</code> (a confusion matrix compares predictions to true labels). 2. Make a confusion matrix using <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code>torch.ConfusionMatrix</code></a>. 3. Plot the confusion matrix using <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code>mlxtend.plotting.plot_confusion_matrix()</code></a>.</p>
<p>Let’s start by making predictions with our trained model.</p>
<div id="065b8090-c9c5-43df-b5c1-b45ba33af1be" class="cell" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import tqdm for progress bar</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Make predictions with trained model</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> []</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>model_2.<span class="bu">eval</span>()</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> X, y <span class="kw">in</span> tqdm(test_dataloader, desc<span class="op">=</span><span class="st">"Making predictions"</span>):</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Send data and targets to target device</span></span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Do the forward pass</span></span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a>    y_logit <span class="op">=</span> model_2(X)</span>
<span id="cb83-13"><a href="#cb83-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels</span></span>
<span id="cb83-14"><a href="#cb83-14" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.softmax(y_logit.squeeze(), dim<span class="op">=</span><span class="dv">0</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb83-15"><a href="#cb83-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Put predictions on CPU for evaluation</span></span>
<span id="cb83-16"><a href="#cb83-16" aria-hidden="true" tabindex="-1"></a>    y_preds.append(y_pred.cpu())</span>
<span id="cb83-17"><a href="#cb83-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate list of predictions into a tensor</span></span>
<span id="cb83-18"><a href="#cb83-18" aria-hidden="true" tabindex="-1"></a>y_pred_tensor <span class="op">=</span> torch.cat(y_preds)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ddb83e1f77d840b981ecebc11d584069","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Wonderful!</p>
<p>Now we’ve got predictions, let’s go through steps 2 &amp; 3: 2. Make a confusion matrix using <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code>torchmetrics.ConfusionMatrix</code></a>. 3. Plot the confusion matrix using <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code>mlxtend.plotting.plot_confusion_matrix()</code></a>.</p>
<p>First we’ll need to make sure we’ve got <code>torchmetrics</code> and <code>mlxtend</code> installed (these two libraries will help us make and visual a confusion matrix).</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> If you’re using Google Colab, the default version of <code>mlxtend</code> installed is 0.14.0 (as of March 2022), however, for the parameters of the <code>plot_confusion_matrix()</code> function we’d like use, we need 0.19.0 or higher.</p>
</blockquote>
<div id="e6c0a05d-d3e0-4b86-9ef7-ee6ea5629b07" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See if torchmetrics exists, if not, install it</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchmetrics, mlxtend</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mlxtend version: </span><span class="sc">{</span>mlxtend<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">int</span>(mlxtend.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">19</span>, <span class="st">"mlxtend verison should be 0.19.0 or higher"</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">-</span>q torchmetrics <span class="op">-</span>U mlxtend <span class="co"># &lt;- 참고: If you're using Google Colab, this may require restarting the runtime</span></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchmetrics, mlxtend</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mlxtend version: </span><span class="sc">{</span>mlxtend<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>mlxtend version: 0.19.0</code></pre>
</div>
</div>
<p>To plot the confusion matrix, we need to make sure we’ve got and <a href="http://rasbt.github.io/mlxtend/"><code>mlxtend</code></a> version of 0.19.0 or higher.</p>
<div id="21383f88-a2dd-4678-94c6-479c592da0ab" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import mlxtend upgraded version</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlxtend </span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mlxtend.__version__)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(mlxtend.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">19</span> <span class="co"># should be version 0.19.0 or higher</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.19.0</code></pre>
</div>
</div>
<p><code>torchmetrics</code> and <code>mlxtend</code> installed, let’s make a confusion matrix!</p>
<p>First we’ll create a <code>torchmetrics.ConfusionMatrix</code> instance telling it how many classes we’re dealing with by setting <code>num_classes=len(class_names)</code>.</p>
<p>Then we’ll create a confusion matrix (in tensor format) by passing our instance our model’s predictions (<code>preds=y_pred_tensor</code>) and targets (<code>target=test_data.targets</code>).</p>
<p>Finally we can plot our confision matrix using the <code>plot_confusion_matrix()</code> function from <code>mlxtend.plotting</code>.</p>
<div id="7aed6d76-ad1c-429e-b8e0-c80572e3ebf4" class="cell" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics <span class="im">import</span> ConfusionMatrix</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_confusion_matrix</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Setup confusion matrix instance and compare predictions to targets</span></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>confmat <span class="op">=</span> ConfusionMatrix(num_classes<span class="op">=</span><span class="bu">len</span>(class_names))</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>confmat_tensor <span class="op">=</span> confmat(preds<span class="op">=</span>y_pred_tensor,</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>                         target<span class="op">=</span>test_data.targets)</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Plot the confusion matrix</span></span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plot_confusion_matrix(</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a>    conf_mat<span class="op">=</span>confmat_tensor.numpy(), <span class="co"># matplotlib likes working with NumPy </span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>    class_names<span class="op">=</span>class_names, <span class="co"># turn the row and column labels into class names</span></span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>    figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>)</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="03_pytorch_computer_vision_files/figure-html/cell-56-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="최적의-모델로-무작위-예측-수행-및-평가하기" class="level2">
<h2 class="anchored" data-anchor-id="최적의-모델로-무작위-예측-수행-및-평가하기">9. 최적의 모델로 무작위 예측 수행 및 평가하기</h2>
<p>좋습니다. 모델들을 서로 비교해 보았으니, 이제 가장 성능이 좋은 모델인 <code>model_2</code>를 더 평가해 보겠습니다.</p>
<p>이를 위해 모델과 예측할 데이터를 전달할 수 있는 <code>make_predictions()</code> 함수를 만들어 보겠습니다.</p>
<div id="d1d5d3e7-9601-4141-8bd7-9abbd016bf6c" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_predictions(model: torch.nn.Module, data: <span class="bu">list</span>, device: torch.device <span class="op">=</span> device):</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    pred_probs <span class="op">=</span> []</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sample <span class="kw">in</span> data:</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 샘플 준비</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>            sample <span class="op">=</span> torch.unsqueeze(sample, dim<span class="op">=</span><span class="dv">0</span>).to(device) <span class="co"># 추가 차원을 더하고 샘플을 장치로 보냄</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 순전파 (모델은 가공되지 않은 로짓을 출력함)</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>            pred_logit <span class="op">=</span> model(sample)</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 예측 확률 가져오기 (로짓 -&gt; 예측 확률)</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>            pred_prob <span class="op">=</span> torch.softmax(pred_logit.squeeze(), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 후속 계산을 위해 pred_prob를 GPU에서 내림</span></span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>            pred_probs.append(pred_prob.cpu())</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pred_probs 리스트를 텐서로 변환하기 위해 스택(stack) 수행</span></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.stack(pred_probs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="420c7461-eaa9-4459-9e68-53574c758765" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>test_samples <span class="op">=</span> []</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>test_labels <span class="op">=</span> []</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample, label <span class="kw">in</span> random.sample(<span class="bu">list</span>(test_data), k<span class="op">=</span><span class="dv">9</span>):</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>    test_samples.append(sample)</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    test_labels.append(label)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 첫 번째 테스트 샘플의 모양과 레이블 확인</span></span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"테스트 샘플 이미지 모양: </span><span class="sc">{</span>test_samples[<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">테스트 샘플 레이블: </span><span class="sc">{</span>test_labels[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>class_names[test_labels[<span class="dv">0</span>]]<span class="sc">}</span><span class="ss">)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>이제 <code>make_predictions()</code> 함수를 사용하여 <code>test_samples</code>에 대해 예측을 수행할 수 있습니다.</p>
<div id="79de2ac1-7d4b-4f81-ae8a-90099bca2a3d" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델 2로 테스트 샘플에 대해 예측 수행</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>pred_probs<span class="op">=</span> make_predictions(model<span class="op">=</span>model_2, </span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>                             data<span class="op">=</span>test_samples)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 처음 두 개의 예측 확률 리스트 확인</span></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>pred_probs[:<span class="dv">2</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>훌륭합니다!</p>
<p>이제 <code>torch.softmax()</code> 활성화 함수의 출력값에 <code>torch.argmax()</code>를 취하여 예측 확률에서 예측 레이블로 변환할 수 있습니다.</p>
<div id="f9d97bcc-4310-4851-a1f8-6bcd757e9b26" class="cell" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># argmax()를 사용하여 예측 확률을 예측 레이블로 변환</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>pred_classes <span class="op">=</span> pred_probs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>pred_classes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1141af97-0990-4920-83d4-c13cca3f9abc" class="cell" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측값이 테스트 레이블과 동일한 형식인가요? </span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>test_labels, pred_classes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>이제 예측 클래스가 테스트 레이블과 동일한 형식이 되었으므로 비교할 수 있습니다.</p>
<p>이미지 데이터를 다루고 있으니 데이터 탐색가의 모토를 따릅시다.</p>
<p>“시각화, 시각화, 시각화!”</p>
<div id="679cb5f7-bb66-42dd-a4d6-400b27b7c019" class="cell" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측 시각화</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">9</span>))</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>nrows <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>ncols <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(test_samples):</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 서브플롯 생성</span></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>  plt.subplot(nrows, ncols, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 대상 이미지 그리기</span></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>  plt.imshow(sample.squeeze(), cmap<span class="op">=</span><span class="st">"gray"</span>)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 예측 레이블 찾기 (텍스트 형식, 예: "Sandal")</span></span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>  pred_label <span class="op">=</span> class_names[pred_classes[i]]</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 실제 레이블 가져오기 (텍스트 형식, 예: "T-shirt")</span></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>  truth_label <span class="op">=</span> class_names[test_labels[i]] </span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 플롯의 제목 텍스트 생성</span></span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>  title_text <span class="op">=</span> <span class="ss">f"예측: </span><span class="sc">{</span>pred_label<span class="sc">}</span><span class="ss"> | 실제: </span><span class="sc">{</span>truth_label<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 일치 여부를 확인하고 그에 따라 제목 색상 변경</span></span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> pred_label <span class="op">==</span> truth_label:</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>      plt.title(title_text, fontsize<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span><span class="st">"g"</span>) <span class="co"># 맞으면 초록색 텍스트</span></span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>      plt.title(title_text, fontsize<span class="op">=</span><span class="dv">10</span>, c<span class="op">=</span><span class="st">"r"</span>) <span class="co"># 틀리면 빨간색 텍스트</span></span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>  plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>와, 정말 좋아 보이네요!</p>
<p>PyTorch 코드 수십 줄 치고는 나쁘지 않죠!</p>
</section>
<section id="추가-예측-평가를-위해-혼동-행렬-만들기" class="level2">
<h2 class="anchored" data-anchor-id="추가-예측-평가를-위해-혼동-행렬-만들기">10. 추가 예측 평가를 위해 혼동 행렬 만들기</h2>
<p>분류 문제에 사용할 수 있는 <a href="https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics">다양한 평가 지표</a>가 많이 있습니다.</p>
<p>가장 시각적인 것 중 하나는 <a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">혼동 행렬(confusion matrix)</a>입니다.</p>
<p>혼동 행렬은 분류 모델이 예측값과 실제 레이블 사이에서 어디서 혼동을 일으켰는지 보여줍니다.</p>
<p>혼동 행렬을 만들기 위해 세 단계를 거칩니다. 1. 훈련된 모델인 <code>model_2</code>로 예측을 수행합니다(혼동 행렬은 예측값을 실제 레이블과 비교합니다). 2. <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code>torchmetrics.ConfusionMatrix</code></a>를 사용하여 혼동 행렬을 만듭니다. 3. <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code>mlxtend.plotting.plot_confusion_matrix()</code></a>를 사용하여 혼동 행렬을 그립니다.</p>
<p>먼저 훈련된 모델로 예측을 수행해 보겠습니다.</p>
<div id="065b8090-c9c5-43df-b5c1-b45ba33af1be" class="cell" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 진행률 표시줄을 위한 tqdm 임포트</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 훈련된 모델로 예측 수행</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> []</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>model_2.<span class="bu">eval</span>()</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> X, y <span class="kw">in</span> tqdm(test_dataloader, desc<span class="op">=</span><span class="st">"예측 수행 중"</span>):</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 데이터와 타겟을 타겟 장치로 보냄</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> X.to(device), y.to(device)</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 순전파 수행</span></span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>    y_logit <span class="op">=</span> model_2(X)</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 예측값을 로짓 -&gt; 예측 확률 -&gt; 예측 레이블로 변환</span></span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.softmax(y_logit.squeeze(), dim<span class="op">=</span><span class="dv">0</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 평가를 위해 예측값을 CPU에 둠</span></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    y_preds.append(y_pred.cpu())</span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 예측 리스트를 텐서로 결합</span></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>y_pred_tensor <span class="op">=</span> torch.cat(y_preds)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>멋지네요!</p>
<p>이제 예측값이 생겼으니 2단계와 3단계를 진행해 보겠습니다. 2. <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html?highlight=confusion#confusionmatrix"><code>torchmetrics.ConfusionMatrix</code></a>를 사용하여 혼동 행렬을 만듭니다. 3. <a href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"><code>mlxtend.plotting.plot_confusion_matrix()</code></a>를 사용하여 혼동 행렬을 그립니다.</p>
<p>먼저 <code>torchmetrics</code>와 <code>mlxtend</code>가 설치되어 있는지 확인해야 합니다(이 두 라이브러리는 혼동 행렬을 만들고 시각화하는 데 도움을 줍니다).</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> Google Colab을 사용 중이라면 <code>mlxtend</code>의 기본 설치 버전은 0.14.0(2022년 3월 기준)입니다. 하지만 우리가 사용하려는 <code>plot_confusion_matrix()</code> 함수의 매개변수를 위해서는 0.19.0 이상의 버전이 필요합니다.</p>
</blockquote>
<div id="e6c0a05d-d3e0-4b86-9ef7-ee6ea5629b07" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># torchmetrics가 있는지 확인하고, 없으면 설치합니다.</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchmetrics, mlxtend</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mlxtend 버전: </span><span class="sc">{</span>mlxtend<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">int</span>(mlxtend.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">19</span>, <span class="st">"mlxtend 버전은 0.19.0 이상이어야 합니다."</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">-</span>q torchmetrics <span class="op">-</span>U mlxtend <span class="co"># &lt;- 참고: Google Colab을 사용하는 경우 런타임을 다시 시작해야 할 수도 있습니다.</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchmetrics, mlxtend</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"mlxtend 버전: </span><span class="sc">{</span>mlxtend<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>혼동 행렬을 그리려면 <a href="http://rasbt.github.io/mlxtend/"><code>mlxtend</code></a> 버전이 0.19.0 이상이어야 합니다.</p>
<div id="21383f88-a2dd-4678-94c6-479c592da0ab" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 업그레이드된 mlxtend 버전 임포트</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlxtend </span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mlxtend.__version__)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(mlxtend.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">19</span> <span class="co"># 0.19.0 이상 버전이어야 함</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><code>torchmetrics</code>와 <code>mlxtend</code>가 설치되었으니 혼동 행렬을 만들어 봅시다!</p>
<p>먼저 <code>num_classes=len(class_names)</code>로 설정하여 우리가 다루는 클래스 수를 알려주는 <code>torchmetrics.ConfusionMatrix</code> 인스턴스를 생성합니다.</p>
<p>그런 다음 모델의 예측값(<code>preds=y_pred_tensor</code>)과 실제 타겟(<code>target=test_data.targets</code>)을 인스턴스에 전달하여 텐서 형식의 혼동 행렬을 생성합니다.</p>
<p>마지막으로 <code>mlxtend.plotting</code>의 <code>plot_confusion_matrix()</code> 함수를 사용하여 혼동 행렬을 시각화할 수 있습니다.</p>
<div id="7aed6d76-ad1c-429e-b8e0-c80572e3ebf4" class="cell" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics <span class="im">import</span> ConfusionMatrix</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_confusion_matrix</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 혼동 행렬 인스턴스를 설정하고 예측값과 타겟을 비교합니다.</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>confmat <span class="op">=</span> ConfusionMatrix(num_classes<span class="op">=</span><span class="bu">len</span>(class_names))</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>confmat_tensor <span class="op">=</span> confmat(preds<span class="op">=</span>y_pred_tensor,</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>                         target<span class="op">=</span>test_data.targets)</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 혼동 행렬을 그립니다.</span></span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plot_confusion_matrix(</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>    conf_mat<span class="op">=</span>confmat_tensor.numpy(), <span class="co"># matplotlib은 NumPy와 함께 작동하는 것을 선호합니다. </span></span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a>    class_names<span class="op">=</span>class_names, <span class="co"># 행과 열 레이블을 클래스 이름으로 바꿉니다.</span></span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a>    figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>)</span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>와! 정말 좋아 보이지 않나요?</p>
<p>대부분의 어두운 사각형이 왼쪽 위에서 오른쪽 아래로 이어지는 대각선에 몰려 있는 것을 통해 우리 모델이 꽤 잘 작동하고 있음을 알 수 있습니다(이상적인 모델은 이 대각선 사각형에만 값이 있고 나머지는 모두 0일 것입니다).</p>
<p>모델은 서로 비슷한 클래스에서 가장 많이 “혼동”을 일으킵니다. 예를 들어 실제로는 “Shirt”로 레이블이 지정된 이미지에 대해 “Pullover”라고 예측하는 경우입니다.</p>
<p>실제로 “T-shirt/top”으로 레이블이 지정된 클래스에 대해 “Shirt”라고 예측하는 경우도 마찬가지입니다.</p>
<p>이러한 정보는 단일 정확도 지표보다 훨씬 더 유용할 때가 많습니다. 모델이 <em>어디서</em> 틀리고 있는지 알려주기 때문입니다.</p>
<p>또한 모델이 <em>왜</em> 특정한 실수를 하는지에 대한 힌트도 제공합니다.</p>
<p>모델이 “T-shirt/top”으로 레이블이 지정된 이미지를 가끔 “Shirt”라고 예측하는 것은 충분히 이해할 수 있는 일입니다.</p>
<p>이러한 정보를 사용하여 모델과 데이터를 더 자세히 조사하고 어떻게 개선할 수 있을지 파악할 수 있습니다.</p>
<blockquote class="blockquote">
<p><strong>과제:</strong> 훈련된 <code>model_2</code>를 사용하여 테스트용 FashionMNIST 데이터셋에 대해 예측을 수행해 보세요. 그런 다음 모델이 틀린 몇 가지 예측을 해당 이미지의 실제 레이블과 함께 시각화해 보세요. 이러한 예측을 시각화한 후, 이것이 모델링 오류에 가까운지 아니면 데이터 오류에 가까운지 생각해 보세요. 즉, 모델이 더 잘할 수 있었을까요, 아니면 데이터의 레이블이 서로 너무 비슷했나요(예: “Shirt” 레이블과 “T-shirt/top”이 너무 비슷함)?</p>
</blockquote>
</section>
<section id="가장-성능이-좋은-모델-저장-및-불러오기" class="level2">
<h2 class="anchored" data-anchor-id="가장-성능이-좋은-모델-저장-및-불러오기">11. 가장 성능이 좋은 모델 저장 및 불러오기</h2>
<p>가장 성능이 좋은 모델을 저장하고 불러오는 것으로 이 섹션을 마무리하겠습니다.</p>
<p><a href="https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model">노트북 01</a>에서 보았듯이 다음 함수들을 조합하여 PyTorch 모델을 저장하고 불러올 수 있습니다. * <code>torch.save</code>: 전체 PyTorch 모델 또는 모델의 <code>state_dict()</code>를 저장하는 함수입니다. * <code>torch.load</code>: 저장된 PyTorch 객체를 불러오는 함수입니다. * <code>torch.nn.Module.load_state_dict()</code>: 저장된 <code>state_dict()</code>를 기존 모델 인스턴스로 불러오는 함수입니다.</p>
<p>이 세 가지에 대한 자세한 내용은 <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch 모델 저장 및 불러오기 문서</a>에서 확인할 수 있습니다.</p>
<p>이제 <code>model_2</code>의 <code>state_dict()</code>를 저장한 다음, 다시 불러와서 평가하여 저장과 불러오기가 올바르게 수행되었는지 확인해 보겠습니다.</p>
<div id="d058e8fa-560f-4350-a154-49593ff403c9" class="cell" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델 디렉토리 생성 (이미 존재하지 않는 경우), 참고: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>MODEL_PATH <span class="op">=</span> Path(<span class="st">"models"</span>)</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>MODEL_PATH.mkdir(parents<span class="op">=</span><span class="va">True</span>, <span class="co"># 필요한 경우 부모 디렉토리 생성</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>                 exist_ok<span class="op">=</span><span class="va">True</span> <span class="co"># 모델 디렉토리가 이미 존재해도 오류를 발생시키지 않음</span></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델 저장 경로 생성</span></span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"03_pytorch_computer_vision_model_2.pth"</span></span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>MODEL_SAVE_PATH <span class="op">=</span> MODEL_PATH <span class="op">/</span> MODEL_NAME</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델 state dict 저장</span></span>
<span id="cb99-14"><a href="#cb99-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"모델 저장 중: </span><span class="sc">{</span>MODEL_SAVE_PATH<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb99-15"><a href="#cb99-15" aria-hidden="true" tabindex="-1"></a>torch.save(obj<span class="op">=</span>model_2.state_dict(), <span class="co"># state_dict()만 저장하면 학습된 매개변수만 저장됩니다.</span></span>
<span id="cb99-16"><a href="#cb99-16" aria-hidden="true" tabindex="-1"></a>           f<span class="op">=</span>MODEL_SAVE_PATH)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>저장된 모델 <code>state_dict()</code>가 있으므로 <code>load_state_dict()</code>와 <code>torch.load()</code>를 조합하여 다시 불러올 수 있습니다.</p>
<p><code>load_state_dict()</code>를 사용하므로 저장된 모델 <code>state_dict()</code>와 동일한 입력 매개변수를 사용하여 <code>FashionMNISTModelV2()</code>의 새 인스턴스를 만들어야 합니다.</p>
<div id="634a8f7a-3013-4b45-b365-49b286d3c478" class="cell" data-execution_count="61">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># FashionMNISTModelV2의 새 인스턴스 생성 (저장된 state_dict()와 동일한 클래스)</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 참고: 여기서의 모양이 저장된 버전과 같지 않으면 모델 로드 시 오류가 발생합니다.</span></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>loaded_model_2 <span class="op">=</span> FashionMNISTModelV2(input_shape<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>                                    hidden_units<span class="op">=</span><span class="dv">10</span>, <span class="co"># 이것을 128로 변경하고 어떤 일이 일어나는지 확인해 보세요 </span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>                                    output_shape<span class="op">=</span><span class="dv">10</span>) </span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 저장된 state_dict() 불러오기</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>loaded_model_2.load_state_dict(torch.load(f<span class="op">=</span>MODEL_SAVE_PATH))</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 모델을 GPU로 보냄</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>loaded_model_2 <span class="op">=</span> loaded_model_2.to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>이제 불러온 모델이 있으므로 <code>eval_model()</code>로 평가하여 해당 매개변수가 저장 전의 <code>model_2</code>와 유사하게 작동하는지 확인해 보겠습니다.</p>
<div id="3e3bcd06-d99b-47bc-8828-9e3903285599" class="cell" data-execution_count="62">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 불러온 모델 평가</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>loaded_model_2_results <span class="op">=</span> eval_model(</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>loaded_model_2,</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>    data_loader<span class="op">=</span>test_dataloader,</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>    loss_fn<span class="op">=</span>loss_fn, </span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>    accuracy_fn<span class="op">=</span>accuracy_fn</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>loaded_model_2_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>이 결과가 <code>model_2_results</code>와 동일하게 보이나요?</p>
<div id="68544254-c99a-47ec-a32f-9816c21a993e" class="cell" data-execution_count="63">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>model_2_results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><code>torch.isclose()</code>를 사용하고 <code>atol</code>(절대 허용 오차) 및 <code>rtol</code>(상대 허용 오차) 매개변수를 통해 근접도 허용 수준을 전달하여 두 텐서가 서로 가까운지 확인할 수 있습니다.</p>
<p>모델의 결과가 가깝다면 <code>torch.isclose()</code>의 출력은 True여야 합니다.</p>
<div id="48dcf0ba-7e00-4406-8aaa-41918856361a" class="cell" data-execution_count="64">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 결과가 서로 가까운지 확인 (너무 멀리 떨어져 있으면 오류가 있을 수 있음)</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(torch.tensor(model_2_results[<span class="st">"model_loss"</span>]), </span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>              torch.tensor(loaded_model_2_results[<span class="st">"model_loss"</span>]),</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>              atol<span class="op">=</span><span class="fl">1e-08</span>, <span class="co"># 절대 허용 오차</span></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>              rtol<span class="op">=</span><span class="fl">0.0001</span>) <span class="co"># 상대 허용 오차</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="연습-문제" class="level2">
<h2 class="anchored" data-anchor-id="연습-문제">연습 문제</h2>
<p>모든 연습 문제는 위 섹션의 코드를 연습하는 데 중점을 둡니다.</p>
<p>각 섹션을 참조하거나 링크된 리소스를 따라가며 완료할 수 있어야 합니다.</p>
<p>모든 연습 문제는 <a href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">장치 중립적 코드</a>를 사용하여 완료해야 합니다.</p>
<p><strong>리소스:</strong> * <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb">03 연습 문제 템플릿 노트북</a> * <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/03_pytorch_computer_vision_exercise_solutions.ipynb">03 연습 문제 예시 솔루션 노트북</a> (솔루션을 보기 전에 직접 풀어보세요)</p>
<ol type="1">
<li>현재 컴퓨터 비전이 사용되고 있는 산업 분야 3가지는 무엇인가요?</li>
<li>“머신러닝에서 과적합(overfitting)이란 무엇인가”를 검색하고 찾은 내용에 대해 한 문장으로 적어보세요.</li>
<li>“머신러닝에서 과적합을 방지하는 방법”을 검색하여 3가지를 적고 각각에 대해 한 문장으로 설명하세요. <strong>참고:</strong> 방법이 아주 많으므로 너무 걱정하지 말고 3가지만 골라 시작해 보세요.</li>
<li><a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer 웹사이트</a>를 20분 동안 읽고 클릭해 보세요.
<ul>
<li>“upload” 버튼을 사용하여 자신의 예시 이미지를 업로드하고 이미지가 CNN의 각 레이어를 통과할 때 어떤 일이 일어나는지 확인해 보세요.</li>
</ul></li>
<li><a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST"><code>torchvision.datasets.MNIST()</code></a> 훈련 및 테스트 데이터셋을 로드하세요.</li>
<li>MNIST 훈련 데이터셋에서 적어도 5개의 서로 다른 샘플을 시각화하세요.</li>
<li><code>torch.utils.data.DataLoader</code>를 사용하여 MNIST 훈련 및 테스트 데이터셋을 데이터로더로 변환하고 <code>batch_size=32</code>로 설정하세요.</li>
<li>MNIST 데이터셋에 적합한, 이 노트북에서 사용된 <code>model_2</code>(<a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer 웹사이트</a>의 모델, TinyVGG로도 알려짐)를 재현하세요.</li>
<li>8번 연습 문제에서 구축한 모델을 CPU와 GPU에서 훈련하고 각각 얼마나 걸리는지 확인해 보세요.</li>
<li>훈련된 모델을 사용하여 예측을 수행하고 적어도 5개를 시각화하여 예측값과 실제 레이블을 비교해 보세요.</li>
<li>모델의 예측값을 실제 레이블과 비교하는 혼동 행렬을 그리세요.</li>
<li>모양이 <code>[1, 3, 64, 64]</code>인 무작위 텐서를 만들고 다양한 하이퍼파라미터 설정(원하는 설정 가능)으로 <code>nn.Conv2d()</code> 레이어에 통과시키세요. <code>kernel_size</code> 매개변수가 커지거나 작아지면 어떤 변화가 나타나나요?</li>
<li>이 노트북의 훈련된 <code>model_2</code>와 유사한 모델을 사용하여 테스트용 <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html"><code>torchvision.datasets.FashionMNIST</code></a> 데이터셋에 대해 예측을 수행하세요.
<ul>
<li>그런 다음 모델이 틀린 몇 가지 예측을 해당 이미지의 실제 레이블과 함께 시각화하세요.</li>
<li>이러한 예측을 시각화한 후, 이것이 모델링 오류에 가까운지 아니면 데이터 오류에 가까운지 생각해 보세요.</li>
<li>즉, 모델이 더 잘할 수 있었을까요, 아니면 데이터의 레이블이 서로 너무 비슷했나요(예: “Shirt” 레이블과 “T-shirt/top”이 너무 비슷함)?</li>
</ul></li>
</ol>
</section>
<section id="추가-학습-자료" class="level2">
<h2 class="anchored" data-anchor-id="추가-학습-자료">추가 학습 자료</h2>
<ul>
<li><strong>시청:</strong> <a href="https://www.youtube.com/watch?v=iaSUYvmCekI&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=3">MIT의 딥 컴퓨터 비전 입문</a> 강의. 합성곱 신경망 뒤에 숨겨진 훌륭한 직관을 제공할 것입니다.</li>
<li><a href="https://pytorch.org/vision/stable/index.html">PyTorch vision 라이브러리</a>의 다양한 옵션을 10분 동안 클릭해 보세요. 어떤 모듈들을 사용할 수 있나요?</li>
<li>“가장 흔한 합성곱 신경망”을 검색해 보세요. 어떤 아키텍처를 찾았나요? 그중 <a href="https://pytorch.org/vision/stable/models.html"><code>torchvision.models</code></a> 라이브러리에 포함된 것이 있나요? 이것들로 무엇을 할 수 있을 것 같나요?</li>
<li>수많은 사전 훈련된 PyTorch 컴퓨터 비전 모델과 PyTorch의 컴퓨터 비전 기능에 대한 다양한 확장 기능은 Ross Wightman의 <a href="https://github.com/rwightman/pytorch-image-models/">PyTorch Image Models 라이브러리 <code>timm</code></a>(Torch Image Models)을 확인해 보세요.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_pytorch_classification.html" class="pagination-link" aria-label="02 - PyTorch 신경망 분류">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">02 - PyTorch 신경망 분류</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04_pytorch_custom_datasets.html" class="pagination-link" aria-label="04 - PyTorch 사용자 정의 데이터셋">
        <span class="nav-page-text"><span class="chapter-title">04 - PyTorch 사용자 정의 데이터셋</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>