<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; 08 - PyTorch 논문 복제 – 파이토치 딥러닝 입문</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./09_pytorch_model_deployment.html" rel="next">
<link href="./07_pytorch_experiment_tracking.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-803ae4f7a8810f475153517dc3c4ebae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./08_pytorch_paper_replicating.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">08 - PyTorch 논문 복제</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">파이토치 딥러닝 입문</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">딥러닝을 위한 PyTorch 배우기</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_pytorch_fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">00 - PyTorch 기초</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_pytorch_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">01 - PyTorch 워크플로우</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_pytorch_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">02 - PyTorch 신경망 분류</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_pytorch_computer_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_pytorch_custom_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">04 - PyTorch 사용자 정의 데이터셋</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_pytorch_going_modular.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">05 - PyTorch 모듈화</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_pytorch_transfer_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">06 - PyTorch 전이 학습</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_pytorch_experiment_tracking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">07 - PyTorch 실험 추적</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_pytorch_paper_replicating.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">08 - PyTorch 논문 복제</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_pytorch_model_deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">09 - PyTorch 모델 배포</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">참고 문헌</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#tk---what-is-paper-replicating" id="toc-tk---what-is-paper-replicating" class="nav-link active" data-scroll-target="#tk---what-is-paper-replicating"><span class="header-section-number">10.1</span> TK - What is paper replicating?</a></li>
  <li><a href="#tk---what-is-a-machine-learning-research-paper" id="toc-tk---what-is-a-machine-learning-research-paper" class="nav-link" data-scroll-target="#tk---what-is-a-machine-learning-research-paper"><span class="header-section-number">10.2</span> TK - What is a machine learning research paper?</a></li>
  <li><a href="#tk---why-replicate-a-machine-learning-research-paper" id="toc-tk---why-replicate-a-machine-learning-research-paper" class="nav-link" data-scroll-target="#tk---why-replicate-a-machine-learning-research-paper"><span class="header-section-number">10.3</span> TK - Why replicate a machine learning research paper?</a></li>
  <li><a href="#tk---where-can-you-find-code-examples-for-machine-learning-research-papers" id="toc-tk---where-can-you-find-code-examples-for-machine-learning-research-papers" class="nav-link" data-scroll-target="#tk---where-can-you-find-code-examples-for-machine-learning-research-papers"><span class="header-section-number">10.4</span> TK - Where can you find code examples for machine learning research papers?</a></li>
  <li><a href="#tk---이번-장에서-다룰-내용" id="toc-tk---이번-장에서-다룰-내용" class="nav-link" data-scroll-target="#tk---이번-장에서-다룰-내용"><span class="header-section-number">10.5</span> TK - 이번 장에서 다룰 내용</a></li>
  <li><a href="#tk---where-can-you-get-help" id="toc-tk---where-can-you-get-help" class="nav-link" data-scroll-target="#tk---where-can-you-get-help"><span class="header-section-number">10.6</span> TK - Where can you get help?</a></li>
  <li><a href="#tk-0.-getting-setup" id="toc-tk-0.-getting-setup" class="nav-link" data-scroll-target="#tk-0.-getting-setup"><span class="header-section-number">10.7</span> TK 0. Getting setup</a></li>
  <li><a href="#tk-1.-get-data" id="toc-tk-1.-get-data" class="nav-link" data-scroll-target="#tk-1.-get-data"><span class="header-section-number">10.8</span> TK 1. Get Data</a></li>
  <li><a href="#tk-2.-create-datasets-and-dataloaders" id="toc-tk-2.-create-datasets-and-dataloaders" class="nav-link" data-scroll-target="#tk-2.-create-datasets-and-dataloaders"><span class="header-section-number">10.9</span> TK 2. Create Datasets and DataLoaders</a>
  <ul class="collapse">
  <li><a href="#prepare-transforms-for-images" id="toc-prepare-transforms-for-images" class="nav-link" data-scroll-target="#prepare-transforms-for-images"><span class="header-section-number">10.9.1</span> 2.1 Prepare transforms for images</a></li>
  <li><a href="#turn-images-into-dataloaders" id="toc-turn-images-into-dataloaders" class="nav-link" data-scroll-target="#turn-images-into-dataloaders"><span class="header-section-number">10.9.2</span> 2.2 Turn images into <code>DataLoader</code>’s</a></li>
  <li><a href="#tk-2.3-visualize-a-single-image" id="toc-tk-2.3-visualize-a-single-image" class="nav-link" data-scroll-target="#tk-2.3-visualize-a-single-image"><span class="header-section-number">10.9.3</span> TK 2.3 Visualize a single image</a></li>
  </ul></li>
  <li><a href="#tk-3.-replicating-the-vit-paper-an-overview" id="toc-tk-3.-replicating-the-vit-paper-an-overview" class="nav-link" data-scroll-target="#tk-3.-replicating-the-vit-paper-an-overview"><span class="header-section-number">10.10</span> TK 3. Replicating the ViT paper: an overview</a>
  <ul class="collapse">
  <li><a href="#inputs-and-outputs-layers-and-blocks" id="toc-inputs-and-outputs-layers-and-blocks" class="nav-link" data-scroll-target="#inputs-and-outputs-layers-and-blocks"><span class="header-section-number">10.10.1</span> 3.1 Inputs and outputs, layers and blocks</a></li>
  <li><a href="#getting-specific-whats-vit-made-of" id="toc-getting-specific-whats-vit-made-of" class="nav-link" data-scroll-target="#getting-specific-whats-vit-made-of"><span class="header-section-number">10.10.2</span> 3.2 Getting specific: What’s ViT made of?</a></li>
  <li><a href="#tk---3.3-my-workflow-for-replicating-papers" id="toc-tk---3.3-my-workflow-for-replicating-papers" class="nav-link" data-scroll-target="#tk---3.3-my-workflow-for-replicating-papers"><span class="header-section-number">10.10.3</span> TK - 3.3 My workflow for replicating papers</a></li>
  </ul></li>
  <li><a href="#tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding" id="toc-tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding" class="nav-link" data-scroll-target="#tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding"><span class="header-section-number">10.11</span> TK 4. Equation 1: Split data into patches and creating the class, position and patch embedding</a>
  <ul class="collapse">
  <li><a href="#tk---4.1-calculating-patch-embedding-input-and-output-shapes-by-hand" id="toc-tk---4.1-calculating-patch-embedding-input-and-output-shapes-by-hand" class="nav-link" data-scroll-target="#tk---4.1-calculating-patch-embedding-input-and-output-shapes-by-hand"><span class="header-section-number">10.11.1</span> TK - 4.1 Calculating patch embedding input and output shapes by hand</a></li>
  <li><a href="#tk---4.2-turning-a-single-image-into-patches" id="toc-tk---4.2-turning-a-single-image-into-patches" class="nav-link" data-scroll-target="#tk---4.2-turning-a-single-image-into-patches"><span class="header-section-number">10.11.2</span> TK - 4.2 Turning a single image into patches</a></li>
  <li><a href="#tk---4.3-creating-image-patches-with-torch.nn.conv2d" id="toc-tk---4.3-creating-image-patches-with-torch.nn.conv2d" class="nav-link" data-scroll-target="#tk---4.3-creating-image-patches-with-torch.nn.conv2d"><span class="header-section-number">10.11.3</span> TK - 4.3 Creating image patches with <code>torch.nn.Conv2d()</code></a></li>
  <li><a href="#tk---4.4-flattening-the-patch-embedding-with-torch.nn.flatten" id="toc-tk---4.4-flattening-the-patch-embedding-with-torch.nn.flatten" class="nav-link" data-scroll-target="#tk---4.4-flattening-the-patch-embedding-with-torch.nn.flatten"><span class="header-section-number">10.11.4</span> TK - 4.4 Flattening the patch embedding with <code>torch.nn.Flatten()</code></a></li>
  <li><a href="#tk---4.5-turning-the-vit-patch-embedding-layer-into-a-pytorch-module" id="toc-tk---4.5-turning-the-vit-patch-embedding-layer-into-a-pytorch-module" class="nav-link" data-scroll-target="#tk---4.5-turning-the-vit-patch-embedding-layer-into-a-pytorch-module"><span class="header-section-number">10.11.5</span> TK - 4.5 Turning the ViT patch embedding layer into a PyTorch module</a></li>
  <li><a href="#tk-4.6-creating-the-class-token-embedding" id="toc-tk-4.6-creating-the-class-token-embedding" class="nav-link" data-scroll-target="#tk-4.6-creating-the-class-token-embedding"><span class="header-section-number">10.11.6</span> TK 4.6 Creating the class token embedding</a></li>
  <li><a href="#tk-4.7-creating-the-position-embedding" id="toc-tk-4.7-creating-the-position-embedding" class="nav-link" data-scroll-target="#tk-4.7-creating-the-position-embedding"><span class="header-section-number">10.11.7</span> TK 4.7 Creating the position embedding</a></li>
  <li><a href="#tk-4.8-전체-과정-합치기-from-image-to-embedding" id="toc-tk-4.8-전체-과정-합치기-from-image-to-embedding" class="nav-link" data-scroll-target="#tk-4.8-전체-과정-합치기-from-image-to-embedding"><span class="header-section-number">10.11.8</span> TK 4.8 전체 과정 합치기: from image to embedding</a></li>
  </ul></li>
  <li><a href="#tk.-5.-equation-2-multi-head-attention-msa" id="toc-tk.-5.-equation-2-multi-head-attention-msa" class="nav-link" data-scroll-target="#tk.-5.-equation-2-multi-head-attention-msa"><span class="header-section-number">10.12</span> TK. 5. Equation 2: Multi-Head Attention (MSA)</a>
  <ul class="collapse">
  <li><a href="#the-layernorm-ln-layer" id="toc-the-layernorm-ln-layer" class="nav-link" data-scroll-target="#the-layernorm-ln-layer"><span class="header-section-number">10.12.1</span> 5.1 The LayerNorm (LN) layer</a></li>
  <li><a href="#the-multi-head-self-attention-msa-layer" id="toc-the-multi-head-self-attention-msa-layer" class="nav-link" data-scroll-target="#the-multi-head-self-attention-msa-layer"><span class="header-section-number">10.12.2</span> 5.2 The Multi-Head Self Attention (MSA) layer</a></li>
  <li><a href="#replicating-equation-2-with-pytorch-layers" id="toc-replicating-equation-2-with-pytorch-layers" class="nav-link" data-scroll-target="#replicating-equation-2-with-pytorch-layers"><span class="header-section-number">10.12.3</span> 5.3 Replicating Equation 2 with PyTorch layers</a></li>
  </ul></li>
  <li><a href="#tk-6.-equation-3-multilayer-perceptron-mlp" id="toc-tk-6.-equation-3-multilayer-perceptron-mlp" class="nav-link" data-scroll-target="#tk-6.-equation-3-multilayer-perceptron-mlp"><span class="header-section-number">10.13</span> TK 6. Equation 3: Multilayer Perceptron (MLP)</a></li>
  <li><a href="#tk-7.-create-the-transformer-encoder" id="toc-tk-7.-create-the-transformer-encoder" class="nav-link" data-scroll-target="#tk-7.-create-the-transformer-encoder"><span class="header-section-number">10.14</span> TK 7. Create the Transformer Encoder</a></li>
  <li><a href="#tk-8.-전체-과정-합치기-to-create-vit" id="toc-tk-8.-전체-과정-합치기-to-create-vit" class="nav-link" data-scroll-target="#tk-8.-전체-과정-합치기-to-create-vit"><span class="header-section-number">10.15</span> TK 8. 전체 과정 합치기 to create ViT</a></li>
  <li><a href="#tk-9.-inspect-the-model" id="toc-tk-9.-inspect-the-model" class="nav-link" data-scroll-target="#tk-9.-inspect-the-model"><span class="header-section-number">10.16</span> TK 9. Inspect the model</a></li>
  <li><a href="#tk-10.-train-model" id="toc-tk-10.-train-model" class="nav-link" data-scroll-target="#tk-10.-train-model"><span class="header-section-number">10.17</span> TK 10. Train model</a></li>
  <li><a href="#tk-11.-evaluate-model" id="toc-tk-11.-evaluate-model" class="nav-link" data-scroll-target="#tk-11.-evaluate-model"><span class="header-section-number">10.18</span> TK 11. Evaluate model</a></li>
  <li><a href="#tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset" id="toc-tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset" class="nav-link" data-scroll-target="#tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset"><span class="header-section-number">10.19</span> TK 12. Bring in pretrained ViT from <code>torchvision.models</code> on same dataset</a></li>
  <li><a href="#tk---things-this-replication-misses-out-on" id="toc-tk---things-this-replication-misses-out-on" class="nav-link" data-scroll-target="#tk---things-this-replication-misses-out-on"><span class="header-section-number">10.20</span> TK - Things this replication misses out on</a></li>
  <li><a href="#tk---연습-문제" id="toc-tk---연습-문제" class="nav-link" data-scroll-target="#tk---연습-문제"><span class="header-section-number">10.21</span> TK - 연습 문제</a></li>
  <li><a href="#tk---추가-학습-자료" id="toc-tk---추가-학습-자료" class="nav-link" data-scroll-target="#tk---추가-학습-자료"><span class="header-section-number">10.22</span> TK - 추가 학습 자료</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">08 - PyTorch 논문 복제</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/08_pytorch_paper_replicating.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<p>TK intro</p>
<p>Want to recreate ViT paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” - https://arxiv.org/abs/2010.11929 - TK will refer to this as “ViT paper” throughout.</p>
<ul>
<li><p>TK what is ViT?</p></li>
<li><p>TK - The name Transformer comes from the architecture name in the paper where it was originally introduced, <a href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a>. An architecture is usually considered a Transformer variant if it uses attention layers in a specific pattern. Since the Transformer architecture originally focused on text data, the goal of the ViT paper was to bring it to the vision.</p></li>
<li><p>TK - The original transformer was made to work on sequences of text (1D), Vision Transformer turns images into sequences of “patches”.</p></li>
<li><p>TK - original ViT also called “vanilla vision transformer”</p></li>
</ul>
<section id="tk---what-is-paper-replicating" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="tk---what-is-paper-replicating"><span class="header-section-number">10.1</span> TK - What is paper replicating?</h2>
<p>It’s no secret machine learning is advancing fast.</p>
<p>Many of these advances get published in machine learning research papers.</p>
<p>And the goal of <strong>paper replicating</strong> is to take replicate these advances with code so you can use the techniques for your own problem.</p>
<p>For example, let’s say a new model architecture gets released that performs better than any other architecture before on various benchmarks, wouldn’t it be nice to try that architecture on your own problems?</p>
<ul>
<li>TK image: paper replicating = research paper (language + diagrams + math) -&gt; code (turn language, diagrams and math into usable code) / (translate a research paper into usable code)</li>
</ul>
</section>
<section id="tk---what-is-a-machine-learning-research-paper" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="tk---what-is-a-machine-learning-research-paper"><span class="header-section-number">10.2</span> TK - What is a machine learning research paper?</h2>
<p>A machine learning research paper is a scientific paper that details findings of a research group on a specific area.</p>
<p>The contents of a machine learning research paper can vary from paper to paper but they generally follow the structure:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Section</strong></th>
<th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Abstract</strong></td>
<td>An overview/summary of the paper’s main findings/contributions.</td>
</tr>
<tr class="even">
<td><strong>Introduction</strong></td>
<td>What’s the paper’s main problem and what are previous methods used to try and solve it?</td>
</tr>
<tr class="odd">
<td><strong>Method</strong></td>
<td>How did the researchers go about conducting their research? For example, what model(s) were used, data sources, training setups, etc.</td>
</tr>
<tr class="even">
<td><strong>Results</strong></td>
<td>What are the outcomes of the paper? If a new type of model or training setup was used, how did the results of findings compare to previous works (this is where <strong>experiment tracking</strong> comes in handy)?</td>
</tr>
<tr class="odd">
<td><strong>Conclusion</strong></td>
<td>What are the limitations of the suggested methods? What are some next steps for the research community?</td>
</tr>
<tr class="even">
<td><strong>References</strong></td>
<td>What resources/other papers did the researchers look at to build their own body of work?</td>
</tr>
<tr class="odd">
<td><strong>Appendix</strong></td>
<td>Are there any extra resources/findings to look at that weren’t included in any of the above sections?</td>
</tr>
</tbody>
</table>
</section>
<section id="tk---why-replicate-a-machine-learning-research-paper" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="tk---why-replicate-a-machine-learning-research-paper"><span class="header-section-number">10.3</span> TK - Why replicate a machine learning research paper?</h2>
<p>A machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.</p>
<p>And if these experiments lead to better results in an area related to the problem you’re working on, it’d be nice to try them out.</p>
<p>Also, replicating the work of others is a fantastic way to practice your skills.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-george-hotz-quote.png" width="600" alt="george hotz quote saying to get better at being a machine learning engineer, download a paper, implement it and keep going until you have skills"></p>
<p><em>George Hotz is founder of <a href="https://comma.ai/">comma.ai</a>, a self-driving car company and livestreams machine learning coding on <a href="https://www.twitch.tv/georgehotz">Twitch</a> and those videos get posted in full to <a href="https://www.youtube.com/c/georgehotzarchive">YouTube</a>. I pulled this quote from one of his livestreams. The “٭” is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).</em></p>
<p>When you first start trying to replicate research papers, you’ll likely be overwhelmed.</p>
<p>That’s normal.</p>
<p>Research teams spend weeks, months and sometimes years creating these works so it makes sense if it takes you sometime to even read let alone reproduce the works.</p>
<p>Replicating research is such a tough problem, phenomenal machine learning libraries and tools such as, <a href="https://huggingface.co/">HuggingFace</a>, <a href="https://github.com/rwightman/pytorch-image-models">PyTorch Image Models</a> (<code>timm</code> library) and <a href="https://www.fast.ai/">fast.ai</a> have been born out of making machine learning research more accessible.</p>
</section>
<section id="tk---where-can-you-find-code-examples-for-machine-learning-research-papers" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="tk---where-can-you-find-code-examples-for-machine-learning-research-papers"><span class="header-section-number">10.4</span> TK - Where can you find code examples for machine learning research papers?</h2>
<p>One of the first things you’ll notice when it comes to machine learning research is: there’s a lot of it.</p>
<p>So beware, trying to stay on top of it is like trying to outrun a hamster wheel.</p>
<p>Follow your interest, pick a few things that stand out to you.</p>
<p>In saying this, there are several places to find and read machine learning research papers: * <a href="https://arxiv.org/">arXiv</a> - Pronounced “archive”, arXiv is a free and open resource for reading technical articles on everything from physics to computer science (inlcuding machine learning). * <a href="https://paperswithcode.com/">Papers with Code</a> - A curated collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models. * <a href="https://twitter.com/ak92501">AK Twitter</a> - The AK Twitter account publishes machine learning research highlights, often with live demos almost every day. I don’t understand 9/10 posts but I find it fun to explore every so often. * <a href="https://github.com/lucidrains/vit-pytorch">lucidrains’ <code>vit-pytorch</code> GitHub repository</a> - Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale looks like. The <code>vit-pytorch</code> repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code (much of the inspiration for this notebook was gathered from this repository).</p>
<p>TK image: showcase the above</p>
</section>
<section id="tk---이번-장에서-다룰-내용" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="tk---이번-장에서-다룰-내용"><span class="header-section-number">10.5</span> TK - 이번 장에서 다룰 내용</h2>
<p>TODO</p>
<ul>
<li>ViT -&gt; FoodVision Mini</li>
<li>Layers = collections of functions to manipulate data -&gt; Architectures = collections of layers (blocks) -&gt; All layers (and blocks) have inputs and outputs
<ul>
<li>Replicating research papers starts by figuring out the inputs and outputs of your layers -&gt; blocks -&gt; model</li>
</ul></li>
</ul>
</section>
<section id="tk---where-can-you-get-help" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="tk---where-can-you-get-help"><span class="header-section-number">10.6</span> TK - Where can you get help?</h2>
<p>All of the materials for this course <a href="https://github.com/mrdbourke/pytorch-deep-learning">are available on GitHub</a>.</p>
<p>If you run into trouble, you can ask a question on the course <a href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">GitHub Discussions page</a>.</p>
<p>And of course, there’s the <a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a> and <a href="https://discuss.pytorch.org/">PyTorch developer forums</a>, a very helpful place for all things PyTorch.</p>
</section>
<section id="tk-0.-getting-setup" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="tk-0.-getting-setup"><span class="header-section-number">10.7</span> TK 0. Getting setup</h2>
<p>As we’ve done previously, let’s make sure we’ve got all of the modules we’ll need for this section.</p>
<p>We’ll import the Python scripts (such as <code>data_setup.py</code> and <code>engine.py</code>) we created in <a href="https://www.learnpytorch.io/05_pytorch_going_modular/">05. PyTorch Going Modular</a>.</p>
<p>To do so, we’ll download <a href="https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular"><code>going_modular</code></a> directory from the <code>pytorch-deep-learning</code> repository (if we don’t already have it).</p>
<p>We’ll also get the <a href="https://github.com/TylerYep/torchinfo"><code>torchinfo</code></a> package if it’s not available.</p>
<p><code>torchinfo</code> will help later on to give us a visual representation of our model.</p>
<p>And since later on we’ll be using a newer version of the <code>torchvision</code> package (as of June 2022), we’ll make sure we’ve got the latest versions.</p>
<div id="ebe46d77-6c4d-4102-9994-2cb89f633f18" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchvision</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">int</span>(torch.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">12</span>, <span class="st">"torch version should be 1.12+"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">int</span>(torchvision.__version__.split(<span class="st">"."</span>)[<span class="dv">1</span>]) <span class="op">&gt;=</span> <span class="dv">13</span>, <span class="st">"torchvision version should be 0.13+"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"torch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"torchvision version: </span><span class="sc">{</span>torchvision<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[INFO] torch/torchvision versions not as required, installing nightly versions."</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip3 install <span class="op">-</span>U <span class="op">--</span>pre torch torchvision torchaudio <span class="op">--</span>extra<span class="op">-</span>index<span class="op">-</span>url https:<span class="op">//</span>download.pytorch.org<span class="op">/</span>whl<span class="op">/</span>nightly<span class="op">/</span>cu113</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torchvision</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"torch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"torchvision version: </span><span class="sc">{</span>torchvision<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch version: 1.12.0+cu102
torchvision version: 0.13.0+cu102</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>참고:</strong> If you’re using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you’ve got the right versions of <code>torch</code> and <code>torchvision</code>.</p>
</blockquote>
<p>Now we’ll continue with the regular imports, setting up device agnostic code and this time we’ll also get the <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"><code>helper_functions.py</code></a> script from GitHub.</p>
<p>The <code>helper_functions.py</code> script contains several functions we created in previous sections: * <code>set_seeds()</code> to set the random seeds (created in <a href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds">07. PyTorch Experiment Tracking section 0</a>). * <code>download_data()</code> to download a data source given a link (created in <a href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data">07. PyTorch Experiment Tracking section 1</a>). * <code>plot_loss_curves()</code> to inspect our model’s training results (created in <a href="https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0">04. PyTorch Custom Datasets section 7.8</a>)</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> It may be a better idea for many of the functions in the <code>helper_functions.py</code> script to be merged into <code>going_modular/going_modular/utils.py</code>, perhaps that’s an extension you’d like to try.</p>
</blockquote>
<div id="960eb156-c1b1-4e76-a812-01bf045835bd" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Continue with regular imports</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Try to get torchinfo, install it if it doesn't work</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"[INFO] Couldn't find torchinfo... installing it."</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>pip install <span class="op">-</span>q torchinfo</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Try to import the going_modular directory, download it from GitHub if it doesn't work</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> going_modular.going_modular <span class="im">import</span> data_setup, engine</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> helper_functions <span class="im">import</span> download_data, set_seeds, plot_loss_curves</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the going_modular scripts</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub."</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>git clone https:<span class="op">//</span>github.com<span class="op">/</span>mrdbourke<span class="op">/</span>pytorch<span class="op">-</span>deep<span class="op">-</span>learning</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>mv pytorch<span class="op">-</span>deep<span class="op">-</span>learning<span class="op">/</span>going_modular .</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>mv pytorch<span class="op">-</span>deep<span class="op">-</span>learning<span class="op">/</span>helper_functions.py . <span class="co"># get the helper_functions.py script</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="op">!</span>rm <span class="op">-</span>rf pytorch<span class="op">-</span>deep<span class="op">-</span>learning</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> going_modular.going_modular <span class="im">import</span> data_setup, engine</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> helper_functions <span class="im">import</span> download_data, set_seeds, plot_loss_curves</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<blockquote class="blockquote">
<p><strong>참고:</strong> If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>.</p>
</blockquote>
<div id="5e246f92-e509-474e-b6c7-c82cf11cb8ca" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>'cuda'</code></pre>
</div>
</div>
</section>
<section id="tk-1.-get-data" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="tk-1.-get-data"><span class="header-section-number">10.8</span> TK 1. Get Data</h2>
<p>Since we’re continuing on with FoodVision Mini, let’s download the pizza, steak and sushi image dataset we’ve been using.</p>
<p>To do so we can use the <code>download_data()</code> function from <code>helper_functions.py</code> that we created in <a href="https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data">07. PyTorch Experiment Tracking section 1</a>.</p>
<p>We’ll <code>source</code> to the raw GitHub link of the <a href="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip"><code>pizza_steak_sushi.zip</code> data</a> and the <code>destination</code> to <code>pizza_steak_sushi</code>.</p>
<div id="37b5ffc0-7093-481e-8081-dbdfac4c24f0" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download pizza, steak, sushi images from GitHub</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> download_data(source<span class="op">=</span><span class="st">"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                           destination<span class="op">=</span><span class="st">"pizza_steak_sushi"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>image_path</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] data/pizza_steak_sushi directory exists, skipping download.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>PosixPath('data/pizza_steak_sushi')</code></pre>
</div>
</div>
<p>Beautiful! Data downloaded, let’s setup the training and test directories.</p>
<div id="92a426b6-df22-4a58-9d5e-b382c73c6048" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup directory paths to train and test images</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>train_dir <span class="op">=</span> image_path <span class="op">/</span> <span class="st">"train"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>test_dir <span class="op">=</span> image_path <span class="op">/</span> <span class="st">"test"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="tk-2.-create-datasets-and-dataloaders" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="tk-2.-create-datasets-and-dataloaders"><span class="header-section-number">10.9</span> TK 2. Create Datasets and DataLoaders</h2>
<p>Since we’ve got some data, let’s now turn it into <code>DataLoader</code>’s.</p>
<p>To do so we can use the <code>create_dataloaders()</code> function in <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py"><code>data_setup.py</code></a>.</p>
<p>First, we’ll create a transform to prepare our images.</p>
<p>This where one of the first references to the ViT paper will come in.</p>
<p>In Table 3, the training resolution is mentioned as being 224 (height=224, width=224).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-image-size-and-batch-size.png" width="900" alt="Table 3 from the Vision Transformer paper showing the image size and batch size"></p>
<p><em>You can often find various hyperparameter settings listed in a table. In this case we’re still preparing our data, so we’re mainly concerned with things like image size and batch size. Source: Table 3 in <a href="https://arxiv.org/abs/2010.11929">ViT paper</a>.</em></p>
<p>So we’ll make sure our transform resizes our images appropriately.</p>
<p>And since we’ll be training our model from scratch (no transfer learning to begin with), we won’t provide a <code>normalize</code> transform like we did in <a href="https://www.learnpytorch.io/06_pytorch_transfer_learning/#21-creating-a-transform-for-torchvisionmodels-manual-creation">06. PyTorch Transfer Learning section 2.1</a>.</p>
<section id="prepare-transforms-for-images" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="prepare-transforms-for-images"><span class="header-section-number">10.9.1</span> 2.1 Prepare transforms for images</h3>
<div id="a45ea650-c3fa-479c-8767-48bc3a1f1267" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create image size (from Table 3 in the ViT paper) </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>IMG_SIZE <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create transform pipeline manually</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>manual_transforms <span class="op">=</span> transforms.Compose([</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((IMG_SIZE, IMG_SIZE)),</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>])           </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Manually created transforms: </span><span class="sc">{</span>manual_transforms<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Manually created transforms: Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)
    ToTensor()
)</code></pre>
</div>
</div>
</section>
<section id="turn-images-into-dataloaders" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="turn-images-into-dataloaders"><span class="header-section-number">10.9.2</span> 2.2 Turn images into <code>DataLoader</code>’s</h3>
<p>Transforms created!</p>
<p>Let’s now create our <code>DataLoader</code>’s.</p>
<p>The ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size we’ve been using (32).</p>
<p>We’re going to stick with a batch size of 32.</p>
<p>Why?</p>
<p>Because some hardware (including the free tier of Google Colab) may not be able to handle a batch size of 4096.</p>
<p>Having a batch size of 4096 means that 4096 images need to fit into the GPU memory at a time.</p>
<p>This works when you’ve got the hardware to handle it like a research team from Google often does but when you’re running on a single GPU (such as using Google Colab), making sure things work with smaller batch size first is a good idea.</p>
<p>An extension of this project could be to try a higher batch size value and see what happens.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> We’re using the <code>pin_memory=True</code> parameter in the <code>create_dataloaders()</code> function to speed up computation. <code>pin_memory=True</code> avoids unnecessary copying of memory between the CPU and GPU memory by “pinning” examples that have been seen before. For more on this concept. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). See the PyTorch <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>torch.utils.data.DataLoader</code> documentation</a> or <a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr from First Principles</a> by Horace He for more.</p>
</blockquote>
<div id="d0ac8145-f89a-490f-82e3-d4b22225d163" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the batch size</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span> <span class="co"># this is lower than the ViT paper but it's because we're starting small</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data loaders</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader, class_names <span class="op">=</span> data_setup.create_dataloaders(</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    train_dir<span class="op">=</span>train_dir,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    test_dir<span class="op">=</span>test_dir,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>manual_transforms, <span class="co"># use manually created transforms</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>train_dataloader, test_dataloader, class_names</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa8cc731550&gt;,
 &lt;torch.utils.data.dataloader.DataLoader at 0x7fa8cc731520&gt;,
 ['pizza', 'steak', 'sushi'])</code></pre>
</div>
</div>
</section>
<section id="tk-2.3-visualize-a-single-image" class="level3" data-number="10.9.3">
<h3 data-number="10.9.3" class="anchored" data-anchor-id="tk-2.3-visualize-a-single-image"><span class="header-section-number">10.9.3</span> TK 2.3 Visualize a single image</h3>
<p>Now we’ve loaded our data, let’s <em>visualize, visualize, visualize!</em></p>
<p>An important step in the ViT paper is preparing the images into patches.</p>
<p>We’ll get to what this means in a second but for now, let’s view a single image and its label.</p>
<p>To do so, let’s get a single image and label from a batch of data and inspect their shapes.</p>
<div id="b5734a22-ded5-403e-84f5-d7a90ed3f085" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a batch of images</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>image_batch, label_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataloader))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a single image from the batch</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>image, label <span class="op">=</span> image_batch[<span class="dv">0</span>], label_batch[<span class="dv">0</span>]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View the batch shapes</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>image.shape, label</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(torch.Size([3, 224, 224]), tensor(0))</code></pre>
</div>
</div>
<p>Wonderful!</p>
<p>Now let’s plot the image and its label with <code>matplotlib</code>.</p>
<div id="afe85fae-38fd-4f34-a52c-29d02cce09c1" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot image with matplotlib</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)) <span class="co"># rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice!</p>
<p>Looks like our images are importing correctly, let’s continue with the paper replication.</p>
</section>
</section>
<section id="tk-3.-replicating-the-vit-paper-an-overview" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="tk-3.-replicating-the-vit-paper-an-overview"><span class="header-section-number">10.10</span> TK 3. Replicating the ViT paper: an overview</h2>
<p>Before we write anymore code, let’s discuss what we’re doing.</p>
<p>We’d like to replicate the ViT paper for our own problem, FoodVision Mini.</p>
<p>So our inputs are: images of pizza, steak and sushi.</p>
<p>And our ideal model outputs are: predicted labels of pizza, steak or sushi.</p>
<p>No different to what we’ve been doing throughout the previous sections.</p>
<p>The question is: how do we go from our inputs to the desired outputs?</p>
<section id="inputs-and-outputs-layers-and-blocks" class="level3" data-number="10.10.1">
<h3 data-number="10.10.1" class="anchored" data-anchor-id="inputs-and-outputs-layers-and-blocks"><span class="header-section-number">10.10.1</span> 3.1 Inputs and outputs, layers and blocks</h3>
<p>ViT is a deep learning neural network architecture.</p>
<p>And any neural network architecture is generally comprised of <strong>layers</strong>.</p>
<p>And a collection of layers is often referred to as a <strong>block</strong>.</p>
<p>And stacking many blocks together is what gives us the whole architecture.</p>
<p>A <strong>layer</strong> takes an input (say an image tensor), performs some kind of function on it (for example what’s in the layer’s <code>forward()</code> method) and then returns an output.</p>
<p>So if a <strong>single layer</strong> takes an input and gives an output, then a collection of layers or a <strong>block</strong> also takes an input and gives an output.</p>
<p>Let’s make this concrete: * <strong>Layer</strong> - takes an input, performs a function on it, returns an output. * <strong>Block</strong> - a collection of layers, takes an input, performs a series of functions on it, returns an output. * <strong>Architecture (or model)</strong> - a collection of blocks, takes an input, performs a series of functions on it, returns an output.</p>
<p>This ideology is what we’re going to be using to replicate the ViT paper.</p>
<p>We’re going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.</p>
<p>The reason we do this is because looking at a whole research paper can be intimidating.</p>
<p>So for a better understanding, we’ll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.</p>
<p>TK image: stacking the network together like lego (functions + layers + blocks = model).</p>
</section>
<section id="getting-specific-whats-vit-made-of" class="level3" data-number="10.10.2">
<h3 data-number="10.10.2" class="anchored" data-anchor-id="getting-specific-whats-vit-made-of"><span class="header-section-number">10.10.2</span> 3.2 Getting specific: What’s ViT made of?</h3>
<p>There are many little details about the ViT model sprinkled throughout the paper.</p>
<p>Finding them all is like one big treasure hunt!</p>
<p>Remember, a research paper is often months of work compressed into a few pages so it’s understandable for it to take of practice to replicate.</p>
<p>However, the main three resources we’ll be looking at for the architecture design are: 1. <strong>Figure 1</strong> - This gives an overview of the model in a graphical sense, you could <em>almost</em> recreate the architecture with this figure alone. 2. <strong>Four equations in section 3.1</strong> - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1. 3. <strong>Table 1</strong> - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We’ll be focused on the smallest version, ViT-Base.</p>
<section id="tk-3.2.1-exploring-figure-1" class="level4" data-number="10.10.2.1">
<h4 data-number="10.10.2.1" class="anchored" data-anchor-id="tk-3.2.1-exploring-figure-1"><span class="header-section-number">10.10.2.1</span> TK 3.2.1 Exploring Figure 1</h4>
<p>Let’s start by going through Figure 1 of the ViT Paper.</p>
<p>The main things we’ll be paying attention to are: 1. <strong>Layers</strong> - takes an <strong>input</strong>, performs an operation or function, produces an <strong>output</strong>. 2. <strong>Blocks</strong> - a collection of layers, which in turn also takes an <strong>input</strong> and produces an <strong>output</strong>.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs.png" width="900" alt="figure 1 from the original vision transformer paper"></p>
<p><em>Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.</em></p>
<p>The ViT architecture is comprised of several stages: * <strong>Patch + Position Embedding (inputs)</strong> - Turns the input image into a sequence of image patches and add a position number what order the patch comes in. * <strong>Linear projection of flattened patches (Embedded Patches)</strong> - The image patches get turned into an <strong>embedding</strong>, the benefit of using an embedding rather than just the image values is that an embedding is a <em>learnable</em> representation (typically in the form of a vector) of the image that can improve with training. * <strong>Norm</strong> - This is short for “<a href="https://paperswithcode.com/method/layer-normalization">Layer Normalization</a>” or “LayerNorm”, a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"><code>torch.nn.LayerNorm()</code></a>. * <strong>Multi-Head Attention</strong> - This is a <a href="https://paperswithcode.com/method/multi-head-attention">Multi-Headed Self-Attention layer</a> or “MSA” for short. You can create an MSA layer via the PyTorch layer <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"><code>torch.nn.MultiheadAttention()</code></a>. * <strong>MLP (or <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a>)</strong> - A MLP can often refer to any collection of feedforward layers (or in PyTorch’s case, a collection of layers with a <code>forward()</code> method). In the ViT Paper, the authors refer to the MLP as “MLP block” and it contains two <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code>torch.nn.Linear()</code></a> layers with a <a href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html"><code>torch.nn.GELU()</code></a> non-linearity activation in between them (section 3.1) and a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"><code>torch.nn.Dropout()</code></a> layer after each (Appendex B.1). * <strong>Transformer Encoder</strong> - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the “+” symbols) meaning the layer’s inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother. * <strong>MLP Head</strong> - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we’re working on image classification, you could also call this the “classifier head”. The structure of the MLP Head is similar to the MLP block.</p>
<p>You might notice that many of the pieces of the ViT architecture can be created with existing PyTorch layers.</p>
<p>This is because of how PyTorch is designed, it’s one of the main purposes of PyTorch to create reusable neural network layers for both researchers and machine learning practitioners.</p>
<blockquote class="blockquote">
<p><strong>Question:</strong> Why not code everything from scratch?</p>
<p>You could definitely do that by reproducing all of the math equations from the paper with custom PyTorch layers and that would certainly be an educative exercise, however, using pre-existing PyTorch layers is usually favoured as pre-existing layers have often been extensively tested and performance checked to make sure they run correctly and fast.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>참고:</strong> We’re going to focused on write PyTorch code to create these layers, for the background on what each of these layers does, I’d suggest reading the ViT Paper in full or reading the linked resources for each layer.</p>
</blockquote>
<p>Let’s take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs-food-mini.png" width="900" alt="figure 1 from the original vision transformer paper adapted to work with food images, an image of pizza goes in and gets classified as 'pizza'"></p>
<p><em>Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class “pizza” is returned.</em></p>
</section>
<section id="tk---3.2.2-exploring-the-four-equations" class="level4" data-number="10.10.2.2">
<h4 data-number="10.10.2.2" class="anchored" data-anchor-id="tk---3.2.2-exploring-the-four-equations"><span class="header-section-number">10.10.2.2</span> TK - 3.2.2 Exploring the Four Equations</h4>
<p>The next main part(s) of the ViT paper we’re going to look at are the four equations in section 3.1.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png" width="650" alt="four mathematical equations from the vision transformer machine learning paper"></p>
<p><em>These four equations represent the math behind the four major parts of the ViT architecture.</em></p>
<p>Section 3.1 describes each of these (some of the text has been omitted for brevity, bolded text is mine):</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Equation number</strong></th>
<th><strong>Description from ViT paper section 3.1</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>…The Transformer uses constant latent vector size <span class="math inline">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math inline">\(D\)</span> dimensions with a <strong>trainable linear projection</strong> (Eq. 1). We refer to the output of this projection as the <strong>patch embeddings</strong>.</td>
</tr>
<tr class="even">
<td>2</td>
<td>The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). <strong>Layernorm (LN) is applied before every block</strong>, and <strong>residual connections after every block</strong> (Wang et al., 2019; Baevski &amp; Auli, 2019).</td>
</tr>
<tr class="odd">
<td>3</td>
<td>See above.</td>
</tr>
<tr class="even">
<td>4</td>
<td>Similar to BERT’s [ class ] token, we <strong>prepend a learnable embedding to the sequence of embedded patches</strong> <span class="math inline">\(\left(\mathbf{z}_{0}^{0}=\mathbf{x}_{\text {class }}\right)\)</span>, whose state at the output of the Transformer encoder <span class="math inline">\(\left(\mathbf{z}_{L}^{0}\right)\)</span> serves as the image representation <span class="math inline">\(\mathbf{y}\)</span> (Eq. 4)…</td>
</tr>
</tbody>
</table>
<p>Let’s map these descriptions to the ViT architecture in Figure 1.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-mapping-the-four-equations-to-figure-1.png" width="1000" alt="mapping the vision transformer paper figure 1 to the four equations listed in the paper"></p>
<p><em>Connecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks. Some details such as “residual connections after every block” are referred to in Figure 1 and in the text but not in the equations.</em></p>
<p>There’s a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.</p>
<p>How about we break down each equation further (it will be our goal to recreate these with code)?</p>
<p>In all equations (except equation 4), “<span class="math inline">\(\mathbf{z}\)</span>” is the raw output of a particular layer:</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{z}_{0}\)</span> is “z zero” (this is the output of the initial patch embedding layer)</li>
<li><span class="math inline">\(\mathbf{z}_{\ell}^{\prime}\)</span> is “z of a particular layer <em>prime</em>” (or an intermediary value of z)</li>
<li><span class="math inline">\(\mathbf{z}_{\ell}\)</span> is “z of a particular layer”</li>
</ol>
<p>And <span class="math inline">\(\mathbf{y}\)</span> is the overall output of the architecture.</p>
<p><strong>Equation 1</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{0} &amp;=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{\text {pos }} \in \mathbb{R}^{(N+1) \times D}
\end{aligned}
\]</span></p>
<p>This equation deals with the class token, patch embedding and position embedding (<span class="math inline">\(\mathbf{E}\)</span> is for embedding) of the input image.</p>
<p>In vector form, the embedding might look something like:</p>
<p>TK - update the vector form to reflect a real exmaple</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x_input <span class="op">=</span> [class_token, image_patch_1, image_patch_2, image_patch_3...] <span class="op">+</span> [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Where each of the elements in the vector is learnable (their <code>requires_grad=True</code>).</p>
<p><strong>Equation 2</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{\ell}^{\prime} &amp;=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, &amp; &amp; \ell=1 \ldots L
\end{aligned}
\]</span></p>
<p>This says that for every layer from <span class="math inline">\(1\)</span> through to <span class="math inline">\(L\)</span> (the total number of layers), there’s a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).</p>
<p>The addition on the end is the equivalent of adding the input to the output and forming a <a href="https://paperswithcode.com/method/residual-connection">skip/residual connection</a>.</p>
<p>We’ll call this layer the “MSA block”.</p>
<p>In pseudocode, this might look like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>x_output_MSA_block <span class="op">=</span> MSA_layer(LN_layer(x_input)) <span class="op">+</span> x_input</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>
<p><strong>Equation 3</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{\ell} &amp;=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L \\
\end{aligned}
\]</span></p>
<p>This says that for every layer from <span class="math inline">\(1\)</span> through to <span class="math inline">\(L\)</span> (the total number of layers), there’s also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).</p>
<p>The addition on the end is showing the presence of a skip/residual connection.</p>
<p>We’ll call this layer the “MLP block”.</p>
<p>In pseudocode, this might look like:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>x_output_MLP_block <span class="op">=</span> MLP_layer(LN_layer(x_output_MSA_block)) <span class="op">+</span> x_output_MSA_block</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Notice the skip connection on the end (adding the input of the layers to the output of the layers).</p>
<p><strong>Equation 4</strong></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} &amp;=\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right) &amp; &amp;
\end{aligned}
\]</span></p>
<p>This says for the last layer <span class="math inline">\(L\)</span>, the output <span class="math inline">\(y\)</span> is the 0 index token of <span class="math inline">\(z\)</span> wrapped in a LayerNorm layer (LN).</p>
<p>Or in our case, the 0 index of <code>x_output_MLP_block</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> LN_layer(Linear_layer(x_output_MLP_block[<span class="dv">0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Of course there are some simplifications above but we’ll take care of those when we start to write PyTorch code for each section.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> The above section covers alot of information. But don’t forget if something doesn’t make sense, you can always research it further. By asking questions like “what is a residual connection?”.</p>
</blockquote>
</section>
<section id="tk---3.2.3-exploring-table-1" class="level4" data-number="10.10.2.3">
<h4 data-number="10.10.2.3" class="anchored" data-anchor-id="tk---3.2.3-exploring-table-1"><span class="header-section-number">10.10.2.3</span> TK - 3.2.3 Exploring Table 1</h4>
<p>The final piece of the ViT architecture puzzle we’ll focus on (for now) is Table 1.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">Hidden size <span class="math inline">\(D\)</span></th>
<th style="text-align: center;">MLP size</th>
<th style="text-align: center;">Heads</th>
<th style="text-align: center;">Params</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ViT-Base</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">3072</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;"><span class="math inline">\(86M\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">ViT-Large</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"><span class="math inline">\(307M\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ViT-Huge</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">1280</td>
<td style="text-align: center;">5120</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;"><span class="math inline">\(632M\)</span></td>
</tr>
</tbody>
</table>
<div data-align="center">
<pre><code>&lt;i&gt;Table 1: Details of Vision Transformer model variants. Source: &lt;a href="https://arxiv.org/abs/2010.11929"&gt;ViT paper&lt;/a&gt;.&lt;/i&gt;</code></pre>
</div>
<p><br></p>
<p>This table showcasing the various hyperparameters of each of the ViT architectures.</p>
<p>You can see the numbers gradually increase from ViT-Base to ViT-Huge.</p>
<p>We’re going to focus on replicating ViT-Base (start small and scale up when necessary) but we’ll be writing code that could easily scale up to the larger variants.</p>
<p>Breaking the hyperparameters down: * <strong>Layers</strong> - How many Transformer encoder blocks are there? (each of these will contain a MSA block and MLP block) * <strong>Hidden size <span class="math inline">\(D\)</span></strong> - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more compute. * <strong>MLP size</strong> - What are the number of hidden units in the MLP layers? * <strong>Heads</strong> - How many heads are there in the Multi-Head Attention layers? * <strong>Params</strong> - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more compute. You’ll notice even ViT-Base has far more parameters than any other model we’ve used so far.</p>
<p>We’ll use these values as the hyperparameter settings for our ViT architecture.</p>
</section>
</section>
<section id="tk---3.3-my-workflow-for-replicating-papers" class="level3" data-number="10.10.3">
<h3 data-number="10.10.3" class="anchored" data-anchor-id="tk---3.3-my-workflow-for-replicating-papers"><span class="header-section-number">10.10.3</span> TK - 3.3 My workflow for replicating papers</h3>
<p>When I start working on replicating a paper, I go through the following steps:</p>
<ol type="1">
<li>Read the whole paper end-to-end once (to get an idea of the main concepts).</li>
<li>Go back through each section and see how they line up with each other and start thinking about how they might be turned into code (just like above).</li>
<li>Repeat step 2 until I’ve got a fairly good outline.</li>
<li>Use <a href="https://mathpix.com/">mathpix.com</a> (a very handy tool) to turn any sections of the paper into markdown/LaTeX to put into notebooks.</li>
<li>Replicate the simplest version of the model possible.</li>
<li>If I get stuck, look up other examples.</li>
</ol>
<p>TK - gif of mathpix</p>
<p>We’ve already gone through the first few steps above (and if you haven’t read the full paper yet, I’d encourage you to give it a go) but what we’ll be focusing on next is step 5: replicating the simplest version fo the model possible.</p>
<p>This is why we’re starting with ViT-Base.</p>
<p>Replicating the smallest version of the architecture possible, get it working and then we can scale up if we wanted to.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> If you’ve never read a research paper before, many of the above steps can be intimidating. But don’t worry, like anything, your skills at reading <em>and</em> replicating papers will improve with practice. Don’t forget, a research paper is often <em>months</em> of work by many people compressed into a few pages. So trying to replicate it on your own is no small feat.</p>
</blockquote>
</section>
</section>
<section id="tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding" class="level2" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding"><span class="header-section-number">10.11</span> TK 4. Equation 1: Split data into patches and creating the class, position and patch embedding</h2>
<p>I remember one of my machine learning engineer friends used to say “it’s all about the embedding.”</p>
<p>As in, if you can represent your data in a good, learnable way (as embeddings are learnable representations), chances are a learning algorithm will be able to perform well on them.</p>
<p>So with that being said, let’s start by creating the class, position and patch embeddings for the ViT architecture.</p>
<p>We’ll start with the <strong>patch embedding</strong>.</p>
<p>This means we’ll be turning our input images in a sequence of patches and then embedding those patches.</p>
<p>Recall that an <strong>embedding</strong> is a learnable representation of some form and is often a vector. The term learnable is important because this means the representation of an input image can be improved and learned over time.</p>
<p>We’ll begin by following the opening paragraph of section 3.1 of the ViT paper (bold mine):</p>
<blockquote class="blockquote">
<p>표준 트랜스포머는 토큰 임베딩의 1D 시퀀스를 입력으로 받습니다. To handle 2D images, we reshape the image <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{H \times W \times C}\)</span> into a sequence of flattened 2D patches <span class="math inline">\(\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}\)</span>, where <span class="math inline">\((H, W)\)</span> is the resolution of the original image, <span class="math inline">\(C\)</span> is the number of channels, <span class="math inline">\((P, P)\)</span> is the resolution of each image patch, and <span class="math inline">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size <span class="math inline">\(D\)</span> through all of its layers, so we flatten the patches and map to <span class="math inline">\(D\)</span> dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the <strong>patch embeddings</strong>.</p>
</blockquote>
<p>And size we’re dealing with image shapes, let’s keep in mind the line from Table 3 of the ViT paper:</p>
<blockquote class="blockquote">
<p>Training resolution is <strong>224</strong>.</p>
</blockquote>
<p>Let’s break down the text above.</p>
<ul>
<li><span class="math inline">\(D\)</span> is the size of the <strong>patch embeddings</strong>, different values for <span class="math inline">\(D\)</span> can be found in Table 1.</li>
<li>The image starts as 2D with size <span class="math inline">\({H \times W \times C}\)</span>.</li>
<li>The image gets converted to a sequence of flattened 2D patches with size <span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span>.
<ul>
<li><span class="math inline">\((H, W)\)</span> is the resolution of the original image.</li>
<li><span class="math inline">\(C\)</span> is the number of channels.</li>
<li><span class="math inline">\((P, P)\)</span> is the resolution of each image patch (<strong>patch size</strong>).</li>
<li><span class="math inline">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</li>
</ul></li>
</ul>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-annotated.png" width="900" alt="mapping the vit architecture diagram positional and patch embeddings portion to the relative mathematical equation describing what's going on"></p>
<p><em>Mapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.</em></p>
<section id="tk---4.1-calculating-patch-embedding-input-and-output-shapes-by-hand" class="level3" data-number="10.11.1">
<h3 data-number="10.11.1" class="anchored" data-anchor-id="tk---4.1-calculating-patch-embedding-input-and-output-shapes-by-hand"><span class="header-section-number">10.11.1</span> TK - 4.1 Calculating patch embedding input and output shapes by hand</h3>
<p>How about we start by calculating these input and output shape values by hand?</p>
<p>To do so, let’s create some variables to mimic each of the terms (such as <span class="math inline">\(H\)</span>, <span class="math inline">\(W\)</span> etc) above.</p>
<p>We’ll use a patch size (<span class="math inline">\(P\)</span>) of 16 since it’s the best performing version of ViT-Base uses (see column “ViT-B/16” of Table 5 in the ViT paper for more).</p>
<div id="bb10d7f1-1aca-416f-b5a4-3abe722ff207" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create example values</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>height <span class="op">=</span> <span class="dv">224</span> <span class="co"># H ("The training resolution is 224.")</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="dv">224</span> <span class="co"># W</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>color_channels <span class="op">=</span> <span class="dv">3</span> <span class="co"># C</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># P</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate N (number of patches)</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>number_of_patches <span class="op">=</span> <span class="bu">int</span>((height <span class="op">*</span> width) <span class="op">/</span> patch_size<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of patches (N) with image height (H=</span><span class="sc">{</span>height<span class="sc">}</span><span class="ss">), width (W=</span><span class="sc">{</span>width<span class="sc">}</span><span class="ss">) and patch size (P=</span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">): </span><span class="sc">{</span>number_of_patches<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196</code></pre>
</div>
</div>
<p>We’ve got the number of patches, how about we create the image output size as well?</p>
<p>Better yet, let’s replicate the input and output shapes of the patch embedding layer.</p>
<p>Recall:</p>
<ul>
<li><strong>Input:</strong> The image starts as 2D with size <span class="math inline">\({H \times W \times C}\)</span>.</li>
<li><strong>Output:</strong> The image gets converted to a sequence of flattened 2D patches with size <span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</li>
</ul>
<div id="1f684bab-1e4e-4251-99b7-839b0b69dbd3" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input shape</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>input_shape <span class="op">=</span> (height, width, color_channels)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Output shape</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>output_shape <span class="op">=</span> (number_of_patches, patch_size<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> color_channels)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape (2D image): </span><span class="sc">{</span>input_shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape (flattened 2D patches): </span><span class="sc">{</span>output_shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape (2D image): (224, 224, 3)
Output shape (flattened 2D patches): (196, 768)</code></pre>
</div>
</div>
<p>Input and output shapes acquired!</p>
</section>
<section id="tk---4.2-turning-a-single-image-into-patches" class="level3" data-number="10.11.2">
<h3 data-number="10.11.2" class="anchored" data-anchor-id="tk---4.2-turning-a-single-image-into-patches"><span class="header-section-number">10.11.2</span> TK - 4.2 Turning a single image into patches</h3>
<p>Now we know the ideal input and output shapes for our <strong>patch embedding</strong> layer.</p>
<p>What we’re doing here is breaking the overall architecture down into smaller pieces, focusing on the inputs and outputs of individual layers.</p>
<p>So how do we create the patch embedding layer?</p>
<p>We’ll get to that shortly, first, let’s <em>visualize, visualize, visualize!</em> what it looks like to turn an image into patches.</p>
<p>Let’s start with our single image.</p>
<div id="336e1b36-9849-4104-8cb9-bb64a20ffc48" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View single image</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)) <span class="co"># adjust for matplotlib</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We want to turn this image into patches of itself inline with Figure 1 of the ViT paper.</p>
<p>How about we start by just visualizing the top row of patched pixels?</p>
<p>We can do this by indexing on the different image dimensions.</p>
<div id="bcd2e784-7989-40e5-b8f5-64de18f1fe3d" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) </span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>image_permuted <span class="op">=</span> image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Index to plot the top row of patched pixels</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(patch_size, patch_size))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(image_permuted[:patch_size, :, :])<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now we’ve got the top row, let’s turn it into patches.</p>
<p>We can do this by iterating through the number of patches there’d be in the top row.</p>
<div id="93210158-3dcb-4d1f-b728-c7c9c3df99dd" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup hyperparameters and make sure img_size and patch_size are compatible</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>img_size <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>num_patches <span class="op">=</span> img_size<span class="op">/</span>patch_size </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> img_size <span class="op">%</span> patch_size <span class="op">==</span> <span class="dv">0</span>, <span class="st">"Image size must be divisible by patch size"</span> </span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of patches per row: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a series of subplots</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>                        ncols<span class="op">=</span>img_size <span class="op">//</span> patch_size, <span class="co"># one column for each patch</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>                        figsize<span class="op">=</span>(num_patches, num_patches),</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>                        sharex<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>                        sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through number of patches in the top row</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, patch <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(<span class="dv">0</span>, img_size, patch_size)):</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    axs[i].imshow(image_permuted[:patch_size, patch:patch<span class="op">+</span>patch_size, :])<span class="op">;</span> <span class="co"># keep height index constant, alter the width index</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    axs[i].set_xlabel(i<span class="op">+</span><span class="dv">1</span>) <span class="co"># set the label</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    axs[i].set_xticks([])</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    axs[i].set_yticks([])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of patches per row: 14.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Those are some nice looking patches!</p>
<p>How about we do it for the whole image?</p>
<p>This time we’ll iterate through the indexs for height and width and plot each patch as it’s own subplot.</p>
<div id="7d45e15a-eb50-4c46-8055-2acaaacb881c" class="cell" data-tags="[]" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup hyperparameters and make sure img_size and patch_size are compatible</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>img_size <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>num_patches <span class="op">=</span> img_size<span class="op">/</span>patch_size </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> img_size <span class="op">%</span> patch_size <span class="op">==</span> <span class="dv">0</span>, <span class="st">"Image size must be divisible by patch size"</span> </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of patches per row: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ch">\n</span><span class="ss">Number of patches per column: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ch">\n</span><span class="ss">Total patches: </span><span class="sc">{</span>num_patches<span class="op">*</span>num_patches<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a series of subplots</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(nrows<span class="op">=</span>img_size <span class="op">//</span> patch_size, <span class="co"># need int not float</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>                        ncols<span class="op">=</span>img_size <span class="op">//</span> patch_size, </span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>                        figsize<span class="op">=</span>(num_patches, num_patches),</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>                        sharex<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>                        sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop through height and width of image</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, patch_height <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(<span class="dv">0</span>, img_size, patch_size)): <span class="co"># iterate through height</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j, patch_width <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">range</span>(<span class="dv">0</span>, img_size, patch_size)): <span class="co"># iterate through width</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        axs[i, j].imshow(image_permuted[patch_height:patch_height<span class="op">+</span>patch_size, <span class="co"># iterate through height </span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>                                        patch_width:patch_width<span class="op">+</span>patch_size, <span class="co"># iterate through width</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>                                        :]) <span class="co"># get all color channels</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set up label information, remove the ticks for clarity and set labels to outside</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        axs[i, j].set_ylabel(i<span class="op">+</span><span class="dv">1</span>, </span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>                             rotation<span class="op">=</span><span class="st">"horizontal"</span>, </span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>                             horizontalalignment<span class="op">=</span><span class="st">"right"</span>, </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>                             verticalalignment<span class="op">=</span><span class="st">"center"</span>) </span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>        axs[i, j].set_xlabel(j<span class="op">+</span><span class="dv">1</span>) </span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        axs[i, j].set_xticks([])</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        axs[i, j].set_yticks([])</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>        axs[i, j].label_outer()</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a super title</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="ss">f"</span><span class="sc">{</span>class_names[label]<span class="sc">}</span><span class="ss"> -&gt; Patchified"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of patches per row: 14.0
Number of patches per column: 14.0
Total patches: 196.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Image patchified!</p>
<p>Woah, that looks cool.</p>
<p>Now how do we turn each of these patches into an embedding and convert them into a sequence?</p>
<p>Hint: we can use PyTorch layers. Can you guess which?</p>
</section>
<section id="tk---4.3-creating-image-patches-with-torch.nn.conv2d" class="level3" data-number="10.11.3">
<h3 data-number="10.11.3" class="anchored" data-anchor-id="tk---4.3-creating-image-patches-with-torch.nn.conv2d"><span class="header-section-number">10.11.3</span> TK - 4.3 Creating image patches with <code>torch.nn.Conv2d()</code></h3>
<p>It’s time to start moving towards replicating the patch embedding layers with PyTorch.</p>
<p>To visualize our single image we wrote code to loop through the different height and width dimensions of a single image and plot individual patches.</p>
<p>This operation is very similar to the convolutional operation we saw in <a href="https://www.learnpytorch.io/03_pytorch_computer_vision/#71-stepping-through-nnconv2d">03. PyTorch Computer Vision section 7.1: Stepping through <code>nn.Conv2d()</code></a>.</p>
<p>In fact, the authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):</p>
<blockquote class="blockquote">
<p><strong>Hybrid Architecture.</strong> As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection <span class="math inline">\(\mathbf{E}\)</span> (Eq. 1) is applied to patches extracted from a <strong>CNN feature map</strong>. As a special case, the patches can have spatial size <span class="math inline">\(1 \times 1\)</span>, which means that the <strong>input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension</strong>. The classification input embedding and position embeddings are added as described above.</p>
</blockquote>
<p>The “<strong>feature map</strong>” they’re refering to are the weights/activations produced by a convolutional layer passing over a given image.</p>
<p><img src="images/08-vit-paper-patch-embedding-animation.png" width="900" alt="example of creating a patch embedding by passing a convolutional layer over a single image"></p>
<p><em>By setting the <code>kernel_size</code> and <code>stride</code> parameters of a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>torch.nn.Conv2d()</code></a> layer equal to the <code>patch_size</code>, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a “Linear Projection” in the ViT paper) of each patch.</em></p>
<p>Remember our ideal input and output shapes for the patch embedding layer?</p>
<ul>
<li><strong>Input:</strong> The image starts as 2D with size <span class="math inline">\({H \times W \times C}\)</span>.</li>
<li><strong>Output:</strong> The image gets converted to a sequence of flattened 2D patches with size <span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</li>
</ul>
<p>Or for an image size of 224 and patch size of 16:</p>
<ul>
<li><strong>Input (2D image):</strong> (224, 224, 3)</li>
<li><strong>Output (flattened 2D patches):</strong> (196, 768)</li>
</ul>
<p>We can recreate these with: * <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><code>torch.nn.Conv2d()</code></a> for turning our image into patches of CNN feature maps. * <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code>torch.nn.Flatten()</code></a> for flattening the spatial dimensions of the feature map.</p>
<p>Let’s start with the <code>torch.nn.Conv2d()</code> layer.</p>
<p>We can replicate the creation of patches by setting the <code>kernel_size</code> and <code>stride</code> equal to <code>patch_size</code>.</p>
<p>This means each convolutional kernel will be of size <code>(patch_size x patch_size)</code> or if <code>patch_size=16</code>, <code>(16 x 16)</code> (the equivalent of one whole patch)</p>
<p>And each step or <code>stride</code> of the convolutional kernel will be <code>patch_size</code> pixels long or <code>16</code> pixels long (equivalent of stepping to the next patch).</p>
<p>We’ll set <code>in_channels=3</code> for the number of color channels in our image and we’ll set <code>out_channels=768</code>, the same as the <span class="math inline">\(D\)</span> value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a vector of size 768).</p>
<div id="3d4fd046-6b51-4ac0-8d39-e67fb333a18a" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the patch size</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>patch_size<span class="op">=</span><span class="dv">16</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Conv2d layer with hyperparameters from the ViT paper</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>conv2d <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span><span class="dv">3</span>, <span class="co"># number of color channels</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>                   out_channels<span class="op">=</span><span class="dv">768</span>, <span class="co"># from Table 1: Hidden size D, this is the embedding size</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>                   kernel_size<span class="op">=</span>patch_size, <span class="co"># could also use (patch_size, patch_size)</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>                   stride<span class="op">=</span>patch_size,</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>                   padding<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now we’ve got a convoluational layer, let’s see what happens when we pass a single image through it.</p>
<div id="1d3424d2-2cfa-431c-9fd0-e2afdf15fc9c" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View single image</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)) <span class="co"># adjust for matplotlib</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="a72a5614-f1cb-4c01-9696-24f07bf2a219" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass the image through the convolutional layer </span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>image_out_of_conv <span class="op">=</span> conv2d(image.unsqueeze(<span class="dv">0</span>)) <span class="co"># add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(image_out_of_conv.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1, 768, 14, 14])</code></pre>
</div>
</div>
<p>Passing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or <span class="math inline">\(D\)</span>) feature/activation maps.</p>
<p>So its output shape can be read as:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">768</span>, <span class="dv">14</span>, <span class="dv">14</span>]) <span class="op">-&gt;</span> [batch_size, embedding_dim, feature_map_height, feature_map_width]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Let’s visualize five random feature maps and see what they look like.</p>
<div id="af5b58ca-0d73-4c62-b4af-4e8867b764e2" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot random 5 convolutional feature maps</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>random_indexes <span class="op">=</span> random.sample(<span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">758</span>), k<span class="op">=</span><span class="dv">5</span>) <span class="co"># pick 5 numbers between 0 and the embedding size</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Showing random convolutional feature maps from indexes: </span><span class="sc">{</span>random_indexes<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plot</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot random image feature maps</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(random_indexes):</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    image_conv_feature_map <span class="op">=</span> image_out_of_conv[:, idx, :, :] <span class="co"># index on the output tensor of the convolutional layer</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    axs[i].<span class="bu">set</span>(xticklabels<span class="op">=</span>[], yticklabels<span class="op">=</span>[], xticks<span class="op">=</span>[], yticks<span class="op">=</span>[])<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Showing random convolutional feature maps from indexes: [180, 39, 286, 72, 105]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how the feature maps all kind of represent the original image, visualizing a few you can see the different major outlines and some major features.</p>
<p>The important thing to note is that these features may change over time as the neural network learns.</p>
<p>And because of these, these feature maps can be considered a <strong>learnable embedding</strong> of our image.</p>
<p>Let’s check one out in numerical form.</p>
<div id="94f6f5b9-a1c7-4aa1-9780-7cd06457b2b3" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a single feature map in tensor form</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>single_feature_map <span class="op">=</span> image_out_of_conv[:, <span class="dv">0</span>, :, :]</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>single_feature_map, single_feature_map.requires_grad</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>(tensor([[[0.2642, 1.1367, 1.0221, 0.9712, 1.0950, 1.2478, 1.2884, 1.1481,
           0.9640, 0.6204, 0.5996, 0.5855, 0.5560, 0.4992],
          [1.1578, 1.0633, 0.9593, 1.1931, 1.3194, 1.1306, 0.7317, 0.4322,
           0.6025, 0.7246, 0.7891, 0.5383, 0.4786, 0.7098],
          [1.0163, 0.9368, 1.1675, 1.3373, 1.0429, 0.7497, 0.7445, 0.4270,
           0.7430, 0.8750, 0.6105, 0.5147, 1.0207, 0.7852],
          [1.0669, 1.0157, 1.3291, 1.0117, 0.4848, 0.6802, 0.8365, 0.7736,
           0.8618, 0.9144, 0.8926, 0.9795, 0.7475, 0.7585],
          [0.9371, 1.1937, 1.0068, 0.6377, 0.7283, 0.9625, 1.0372, 0.8920,
           0.9372, 0.9034, 0.9683, 0.9405, 0.5958, 0.8740],
          [0.9419, 0.8599, 0.5429, 0.6954, 1.0202, 0.9093, 1.0003, 0.7619,
           0.8472, 0.8062, 0.6418, 0.7741, 0.5791, 0.9816],
          [0.7965, 0.7202, 0.6424, 0.9137, 0.8264, 1.0243, 1.0920, 0.9548,
           0.9166, 0.7937, 0.4675, 0.5346, 0.7774, 1.1001],
          [0.3298, 0.4832, 0.5324, 0.7486, 0.8303, 0.8101, 0.9969, 0.9931,
           1.0058, 0.6002, 0.6643, 0.7254, 0.8453, 1.1323],
          [0.5384, 0.4798, 0.6725, 0.8014, 0.7044, 0.7988, 0.8185, 0.8911,
           0.9720, 0.8939, 0.6234, 0.5674, 0.5775, 1.0011],
          [0.6199, 0.6465, 0.6503, 0.6215, 0.8154, 0.7950, 0.8647, 0.9872,
           0.8513, 0.8833, 0.5799, 0.5914, 0.6936, 1.0554],
          [0.5140, 0.6462, 0.6982, 0.7445, 0.7394, 0.8124, 0.7462, 0.9183,
           0.7471, 0.9436, 0.7147, 0.6396, 0.5795, 1.0201],
          [0.5467, 0.7408, 0.6854, 0.6624, 0.7465, 0.5077, 0.7633, 0.8709,
           1.0026, 0.7276, 0.7847, 0.5811, 0.5521, 1.0318],
          [0.8041, 0.8868, 0.5559, 0.5889, 0.7236, 0.6976, 0.7940, 0.9365,
           0.9110, 0.8182, 0.7013, 0.4890, 0.8364, 1.0031],
          [0.1976, 1.0262, 1.1979, 0.9982, 0.9644, 0.8868, 0.9556, 1.0204,
           1.0060, 0.9586, 0.9351, 0.8819, 0.9290, 0.9289]]],
        grad_fn=&lt;SliceBackward0&gt;),
 True)</code></pre>
</div>
</div>
<p>The <code>grad_fn</code> output of the <code>single_feature_map</code> and the <code>required_grad=True</code> attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training.</p>
</section>
<section id="tk---4.4-flattening-the-patch-embedding-with-torch.nn.flatten" class="level3" data-number="10.11.4">
<h3 data-number="10.11.4" class="anchored" data-anchor-id="tk---4.4-flattening-the-patch-embedding-with-torch.nn.flatten"><span class="header-section-number">10.11.4</span> TK - 4.4 Flattening the patch embedding with <code>torch.nn.Flatten()</code></h3>
<p>We’ve turned our image into patch embeddings but they’re still in 2D format.</p>
<p>How do we get them into the desired output shape of the patch embedding layer of the ViT model?</p>
<ul>
<li><strong>Desried output (flattened 2D patches):</strong> (196, 768) -&gt; <span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span></li>
</ul>
<p>Let’s check the current shape.</p>
<div id="c8219029-6162-4046-8702-0c2cb42f2378" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Current tensor shape</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Current tensor shape: </span><span class="sc">{</span>image_out_of_conv<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Current tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]</code></pre>
</div>
</div>
<p>Well we’ve got the 768 part ( <span class="math inline">\((P^{2} \cdot C)\)</span> ) but we still need the number of patches (<span class="math inline">\(N\)</span>).</p>
<p>Reading back through section 3.1 of the ViT paper it says (bold mine):</p>
<blockquote class="blockquote">
<p>As a special case, the patches can have spatial size <span class="math inline">\(1 \times 1\)</span>, which means that the <strong>input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension</strong>.</p>
</blockquote>
<p>Flattening the spatial dimensions of the feature map hey?</p>
<p>What layer do we have in PyTorch that can flatten?</p>
<p>How about <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code>torch.nn.Flatten()</code></a>?</p>
<p>But we don’t want to flatten the whole tensor, we only want to flatten the “spatial dimensions of the feature map”.</p>
<p>Which in our case is the <code>feature_map_height</code> and <code>feature_map_width</code> dimensions of <code>image_out_of_conv</code>.</p>
<p>So how about we create a <code>torch.nn.Flatten()</code> layer to only flatten those dimensions, we can use the <code>start_dim</code> and <code>end_dim</code> parameters to set that up?</p>
<div id="ed82899d-7bbc-49f9-a423-8fa6344b8e99" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create flatten layer</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>flatten <span class="op">=</span> nn.Flatten(start_dim<span class="op">=</span><span class="dv">2</span>, <span class="co"># flatten feature_map_height (dimension 2)</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>                     end_dim<span class="op">=</span><span class="dv">3</span>) <span class="co"># flatten feature_map_width (dimension 3)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Nice! Now let’s put it all together!</p>
<p>We’ll: 1. Take a single image. 2. Put in through the convolutional layer (<code>conv2d</code>) to turn the image into 2D feature maps (patch embeddings). 3. Flatten the 2D feature map into a single sequence.</p>
<div id="e3fa363b-1923-4e27-a0b5-980d885fcda2" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. View single image</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)) <span class="co"># adjust for matplotlib</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>plt.title(class_names[label])</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original image shape: </span><span class="sc">{</span>image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Turn image into feature maps</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>image_out_of_conv <span class="op">=</span> conv2d(image.unsqueeze(<span class="dv">0</span>)) <span class="co"># add batch dimension to avoid shape errors</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image feature map shape: </span><span class="sc">{</span>image_out_of_conv<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Flatten the feature maps</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>image_out_of_conv_flattened <span class="op">=</span> flatten(image_out_of_conv)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Flattened image feature map shape: </span><span class="sc">{</span>image_out_of_conv_flattened<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original image shape: torch.Size([3, 224, 224])
Image feature map shape: torch.Size([1, 768, 14, 14])
Flattened image feature map shape: torch.Size([1, 768, 196])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-24-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Woohoo! It looks like our <code>image_out_of_conv_flattened</code> shape is very close to our desired output shape:</p>
<ul>
<li><strong>Desried output (flattened 2D patches):</strong> (196, 768) -&gt; <span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span></li>
<li><strong>Current shape:</strong> (1, 768, 196)</li>
</ul>
<p>The only difference is our current shape has a batch size and the dimensions are in a different order to the desired output.</p>
<p>How could we fix this?</p>
<p>Well, how about we rearrange the dimensions?</p>
<p>We can do so with <code>torch.Tensor.permute()</code> just like we do when rearranging image tensors to plot them with matplotlib.</p>
<p>Let’s try.</p>
<div id="47f571a1-2303-4981-85f5-33936b39cf14" class="cell" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get flattened image patch embeddings in right shape </span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>image_out_of_conv_flattened_reshaped <span class="op">=</span> image_out_of_conv_flattened.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="co"># [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embedding sequence shape: </span><span class="sc">{</span>image_out_of_conv_flattened_reshaped<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, num_patches, embedding_size]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Patch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]</code></pre>
</div>
</div>
<p>Yes!!!</p>
<p>We’ve now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers.</p>
<p>How about we visualize one of the flattened feature maps?</p>
<div id="e4204163-8689-4b9e-8828-4e1e20d9316e" class="cell" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a single flattened feature map</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>single_flattened_feature_map <span class="op">=</span> image_out_of_conv_flattened_reshaped[:, :, <span class="dv">0</span>]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the flattened feature map visually</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">22</span>, <span class="dv">22</span>))</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(single_flattened_feature_map.detach().numpy())</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"Flattened feature map shape: </span><span class="sc">{</span>single_flattened_feature_map<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Hmm, the flattened feature map doesn’t look like much visually, but that’s not what we’re concerned about, this is what will be the output of the patching embedding layer and the input to the rest of the ViT architecture.</p>
<p>TK image - single image -&gt; conv2d -&gt; flatten -&gt; get the output above (show the workflow and transformation, this could be the gif we’ve but using but extended to work with the flatten section)</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> The <a href="https://arxiv.org/abs/1706.03762">original Transformer architecture</a> was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why the input to the ViT architecture is processed in the way it is. We’re essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.</p>
</blockquote>
<p>How about we view the flattened feature map in tensor form?</p>
<div id="0cdeb08e-948d-4810-ae8d-c607b3b9feec" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># See the flattened feature map as a tensor</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>(tensor([[0.2642, 1.1367, 1.0221, 0.9712, 1.0950, 1.2478, 1.2884, 1.1481, 0.9640,
          0.6204, 0.5996, 0.5855, 0.5560, 0.4992, 1.1578, 1.0633, 0.9593, 1.1931,
          1.3194, 1.1306, 0.7317, 0.4322, 0.6025, 0.7246, 0.7891, 0.5383, 0.4786,
          0.7098, 1.0163, 0.9368, 1.1675, 1.3373, 1.0429, 0.7497, 0.7445, 0.4270,
          0.7430, 0.8750, 0.6105, 0.5147, 1.0207, 0.7852, 1.0669, 1.0157, 1.3291,
          1.0117, 0.4848, 0.6802, 0.8365, 0.7736, 0.8618, 0.9144, 0.8926, 0.9795,
          0.7475, 0.7585, 0.9371, 1.1937, 1.0068, 0.6377, 0.7283, 0.9625, 1.0372,
          0.8920, 0.9372, 0.9034, 0.9683, 0.9405, 0.5958, 0.8740, 0.9419, 0.8599,
          0.5429, 0.6954, 1.0202, 0.9093, 1.0003, 0.7619, 0.8472, 0.8062, 0.6418,
          0.7741, 0.5791, 0.9816, 0.7965, 0.7202, 0.6424, 0.9137, 0.8264, 1.0243,
          1.0920, 0.9548, 0.9166, 0.7937, 0.4675, 0.5346, 0.7774, 1.1001, 0.3298,
          0.4832, 0.5324, 0.7486, 0.8303, 0.8101, 0.9969, 0.9931, 1.0058, 0.6002,
          0.6643, 0.7254, 0.8453, 1.1323, 0.5384, 0.4798, 0.6725, 0.8014, 0.7044,
          0.7988, 0.8185, 0.8911, 0.9720, 0.8939, 0.6234, 0.5674, 0.5775, 1.0011,
          0.6199, 0.6465, 0.6503, 0.6215, 0.8154, 0.7950, 0.8647, 0.9872, 0.8513,
          0.8833, 0.5799, 0.5914, 0.6936, 1.0554, 0.5140, 0.6462, 0.6982, 0.7445,
          0.7394, 0.8124, 0.7462, 0.9183, 0.7471, 0.9436, 0.7147, 0.6396, 0.5795,
          1.0201, 0.5467, 0.7408, 0.6854, 0.6624, 0.7465, 0.5077, 0.7633, 0.8709,
          1.0026, 0.7276, 0.7847, 0.5811, 0.5521, 1.0318, 0.8041, 0.8868, 0.5559,
          0.5889, 0.7236, 0.6976, 0.7940, 0.9365, 0.9110, 0.8182, 0.7013, 0.4890,
          0.8364, 1.0031, 0.1976, 1.0262, 1.1979, 0.9982, 0.9644, 0.8868, 0.9556,
          1.0204, 1.0060, 0.9586, 0.9351, 0.8819, 0.9290, 0.9289]],
        grad_fn=&lt;SelectBackward0&gt;),
 True,
 torch.Size([1, 196]))</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>We’ve turned our single 2D image into a single 1D learnable embedding vector (or “Linear Projection of Flattned Patches” in Figure 1 of the ViT paper).</p>
</section>
<section id="tk---4.5-turning-the-vit-patch-embedding-layer-into-a-pytorch-module" class="level3" data-number="10.11.5">
<h3 data-number="10.11.5" class="anchored" data-anchor-id="tk---4.5-turning-the-vit-patch-embedding-layer-into-a-pytorch-module"><span class="header-section-number">10.11.5</span> TK - 4.5 Turning the ViT patch embedding layer into a PyTorch module</h3>
<p>Time to put everything we’ve done for creating the patch embedding into a single PyTorch layer.</p>
<p>We can do so by subclassing <code>nn.Module</code> and creating a small PyTorch “model” to do all of the steps above.</p>
<p>Specifically we’ll: 1. Create a class called <code>PatchEmbedding</code> which subclasses <code>nn.Module</code> (so it can be used a PyTorch layer). 2. Initialize the class with the parameters <code>in_channels=3</code>, <code>patch_size=16</code> (for ViT-Base) and <code>embedding_dim=768</code> (this is <span class="math inline">\(D\)</span> for ViT-Base from Table 1). 3. Create a layer to turn an image into patches using <code>nn.Conv2d()</code> (just like in 4.3 above). 4. Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above). 5. Define a <code>forward()</code> method to take an input and pass it through the layers created in 3 and 4. 6. Make sure the output shape reflects the required output shape of the ViT architecture (<span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span>).</p>
<p>Let’s do it!</p>
<div id="3ef75c7e" class="cell" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a class which subclasses nn.Module</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PatchEmbedding(nn.Module):</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Turns a 2D input image into a 1D sequence learnable embedding vector.</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co">        in_channels (int): Number of color channels for the input images. Defaults to 3.</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co">        patch_size (int): Size of patches to convert input image into. Defaults to 16.</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co">        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span> </span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Initialize the class with appropriate variables</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>                 in_channels:<span class="bu">int</span><span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>                 patch_size:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>                 embedding_dim:<span class="bu">int</span><span class="op">=</span><span class="dv">768</span>):</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Create a layer to turn an image into patches</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patcher <span class="op">=</span> nn.Conv2d(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>                                 out_channels<span class="op">=</span>embedding_dim,</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>                                 kernel_size<span class="op">=</span>patch_size,</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>                                 stride<span class="op">=</span>patch_size,</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>                                 padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Create a layer to flatten the patch feature maps into a single dimension</span></span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten(start_dim<span class="op">=</span><span class="dv">2</span>, <span class="co"># only flatten the feature map dimensions into a single vector</span></span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>                                  end_dim<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Define the forward method </span></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create assertion to check that inputs are the correct shape</span></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>        image_resolution <span class="op">=</span> x.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> image_resolution <span class="op">%</span> patch_size <span class="op">==</span> <span class="dv">0</span>, <span class="ss">f"Input image size must be divisble by patch size, image shape: </span><span class="sc">{</span>image_resolution<span class="sc">}</span><span class="ss">, patch size: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform the forward pass</span></span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>        x_patched <span class="op">=</span> <span class="va">self</span>.patcher(x)</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>        x_flattened <span class="op">=</span> <span class="va">self</span>.flatten(x_patched) </span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 6. Make sure the output shape has the right order </span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x_flattened.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="co"># adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -&gt; [batch_size, N, P^2•C]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><code>PatchEmbedding</code> layer created!</p>
<p>Let’s try it out on a single image.</p>
<div id="a5599575-44cc-46c9-95a4-e65eb1379a59" class="cell" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>set_seeds()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of patch embedding layer</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>patchify <span class="op">=</span> PatchEmbedding(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>                          patch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>                          embedding_dim<span class="op">=</span><span class="dv">768</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass a single image through</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input image shape: </span><span class="sc">{</span>image<span class="sc">.</span>unsqueeze(<span class="dv">0</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>patch_embedded_image <span class="op">=</span> patchify(image.unsqueeze(<span class="dv">0</span>)) <span class="co"># add an extra batch dimension on the 0th index, otherwise will error</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output patch embedding shape: </span><span class="sc">{</span>patch_embedded_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input image shape: torch.Size([1, 3, 224, 224])
Output patch embedding shape: torch.Size([1, 196, 768])</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>The output shape matches the ideal input and output shapes we’d like to see from the patch embedding layer:</p>
<ul>
<li><strong>Input:</strong> The image starts as 2D with size <span class="math inline">\({H \times W \times C}\)</span>.</li>
<li><strong>Output:</strong> The image gets converted to a sequence of flattened 2D patches with size <span class="math inline">\({N \times\left(P^{2} \cdot C\right)}\)</span>.</li>
</ul>
<p>Where: * <span class="math inline">\((H, W)\)</span> is the resolution of the original image. * <span class="math inline">\(C\)</span> is the number of channels. * <span class="math inline">\((P, P)\)</span> is the resolution of each image patch (<strong>patch size</strong>). * <span class="math inline">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.</p>
<p>We’ve now replicated the patch embedding for equation 1 but not the class token/position embedding.</p>
<p>We’ll get to these later on.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-replicating-the-patch-embedding-layer.png" width="900" alt="replicating the vision transformer architecture patch embedding layer"></p>
<p><em>Our <code>PatchEmbedding</code> class (right) replicates the patch embedding of the ViT architecture from Figure 1 and Equation 1 from the ViT paper (left). However, the learnable class embedding and position embeddings haven’t been created yet. These will come soon.</em></p>
<p>Let’s now get a summary of our <code>PatchEmbedding</code> layer.</p>
<div id="e440be53-d72c-42b8-87c8-1c31c4262c16" class="cell" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random input sizes</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>random_input_image <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>random_input_image_error <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">250</span>, <span class="dv">250</span>) <span class="co"># will error because image size is incompatible with patch_size</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a summary of the input and outputs of PatchEmbedding</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>summary(PatchEmbedding(), </span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>        input_size<span class="op">=</span>random_input_image, <span class="co"># try swapping this for "random_input_image_error" </span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        col_names<span class="op">=</span>[<span class="st">"input_size"</span>, <span class="st">"output_size"</span>, <span class="st">"num_params"</span>, <span class="st">"trainable"</span>],</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>        col_width<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>        row_settings<span class="op">=</span>[<span class="st">"var_names"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
PatchEmbedding (PatchEmbedding)          [1, 3, 224, 224]     [1, 196, 768]        --                   True
├─Conv2d (patcher)                       [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True
├─Flatten (flatten)                      [1, 768, 14, 14]     [1, 768, 196]        --                   --
========================================================================================================================
Total params: 590,592
Trainable params: 590,592
Non-trainable params: 0
Total mult-adds (M): 115.76
========================================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 1.20
Params size (MB): 2.36
Estimated Total Size (MB): 4.17
========================================================================================================================</code></pre>
</div>
</div>
</section>
<section id="tk-4.6-creating-the-class-token-embedding" class="level3" data-number="10.11.6">
<h3 data-number="10.11.6" class="anchored" data-anchor-id="tk-4.6-creating-the-class-token-embedding"><span class="header-section-number">10.11.6</span> TK 4.6 Creating the class token embedding</h3>
<p>Okay we’ve made the image patch embedding, time to get to work on the class token embedding.</p>
<p>Or <span class="math inline">\(\mathbf{x}_\text {class }\)</span> from equation 1.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-the-class-token.png" width="900" alt="class token embedding highlight from the vision transformer paper figure 1 and section 3.1"></p>
<p><em>Left: Figure 1 from the ViT paper with the “classification token” or <code>[class]</code> embedding token we’re going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.</em></p>
<p>Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:</p>
<blockquote class="blockquote">
<p>Similar to BERT’s <code>[ class ]</code> token, we prepend a learnable embedding to the sequence of embedded patches <span class="math inline">\(\left(\mathbf{z}_{0}^{0}=\mathbf{x}_{\text {class }}\right)\)</span>, whose state at the output of the Transformer encoder <span class="math inline">\(\left(\mathbf{z}_{L}^{0}\right)\)</span> serves as the image representation <span class="math inline">\(\mathbf{y}\)</span> (Eq. 4).</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>참고:</strong> <a href="https://arxiv.org/abs/1810.04805">BERT</a> (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a <code>[ class ]</code> token at the start of a sequence originated, class being a description for the “classification” class the sequence belonged to.</p>
</blockquote>
<p>So we need to “preprend a learnable embedding to the sequence of embedded patches”.</p>
<p>Let’s start by viewing our sequence of embedded patches tensor (created in 4.5) and its shape.</p>
<div id="50381789-d73d-4648-9144-4d48da87318f" class="cell" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View the patch embedding and patch embedding shape</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(patch_embedded_image) </span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embedding shape: </span><span class="sc">{</span>patch_embedded_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, number_of_patches, embedding_dimension]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],
         [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],
         [-0.6015,  0.1235, -0.2506,  ...,  0.6307, -0.4673,  0.2756],
         ...,
         [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],
         [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],
         [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],
       grad_fn=&lt;PermuteBackward0&gt;)
Patch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]</code></pre>
</div>
</div>
<p>To “prepend a learnable embedding to the sequence of embedded patches” we need to create a learnable embedding in the shape of the <code>embedding_dimension</code> (<span class="math inline">\(D\)</span>) and then add it to the <code>number_of_patches</code> dimension.</p>
<p>Or in pseudocode:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>patch_embedding <span class="op">=</span> [image_patch_1, image_patch_2, image_patch_3...]</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>class_token <span class="op">=</span> learnable_embedding</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>patch_embedding_with_class_token <span class="op">=</span> torch.cat((class_token, patch_embedding), dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Notice the concatenation (<code>torch.cat()</code>) happens on <code>dim=1</code> (the <code>number_of_patches</code> dimension).</p>
<p>Let’s create a learnable embedding for the class token.</p>
<p>To do so, we’ll get the batch size and embedding dimension shape and then we’ll create a <code>torch.ones()</code> tensor in the shape <code>[batch_size, 1, embedding_dimension]</code>.</p>
<p>And we’ll make the tensor learnable by passing it to <code>nn.Parameter()</code> with <code>requires_grad=True</code>.</p>
<div id="cc0bb859-e62e-41a8-9a47-339e4272a152" class="cell" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the batch size and embedding dimension</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> patch_embedded_image.shape[<span class="dv">0</span>]</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>embedding_dimension <span class="op">=</span> patch_embedded_image.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>class_token <span class="op">=</span> nn.Parameter(torch.ones(batch_size, <span class="dv">1</span>, embedding_dimension), <span class="co"># [batch_size, number_of_patches, embedding_dimension]</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>                           requires_grad<span class="op">=</span><span class="va">True</span>) <span class="co"># make sure the embedding is learnable</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the first 10 examples of the class_token</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_token[:, :, :<span class="dv">10</span>])</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the class_token shape</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Class token shape: </span><span class="sc">{</span>class_token<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, number_of_tokens, embedding_dimension]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)
Class token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>참고:</strong> Here we’re only creating the class token embedding as <a href="https://pytorch.org/docs/stable/generated/torch.ones.html"><code>torch.ones()</code></a> for demonstration purposes, in reality, you’d likely create the class token embedding with <a href="https://pytorch.org/docs/stable/generated/torch.randn.html"><code>torch.randn()</code></a> (start with a random number).</p>
</blockquote>
<p>See how the <code>number_of_patches</code> dimension of <code>class_token</code> is <code>1</code> since we only want to prepend one class token value to the start of the patch embedding sequence.</p>
<p>Now we’ve got the class token embedding, let’s prepend it to our sequence of image patches, <code>patch_embedded_image</code>.</p>
<p>We can so using <a href="https://pytorch.org/docs/stable/generated/torch.cat.html"><code>torch.cat()</code></a> and set <code>dim=1</code> (so <code>class_token</code>’s <code>number_of_patches</code> dimension is preprended to <code>patch_embedded_image</code>’s <code>number_of_patches</code> dimension).</p>
<div id="a7287b01-76cb-4371-ab07-981f7bbf2be5" class="cell" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the class token embedding to the front of the patch embedding</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>patch_embedded_image_with_class_embedding <span class="op">=</span> torch.cat((class_token, patch_embedded_image), </span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>                                                      dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># concat on first dimension</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the sequence of patch embeddings with the prepended class token embedding</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(patch_embedded_image_with_class_embedding)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sequence of patch embeddings with class token prepended shape: </span><span class="sc">{</span>patch_embedded_image_with_class_embedding<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, number_of_patches, embedding_dimension]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
         [-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],
         [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],
         ...,
         [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],
         [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],
         [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],
       grad_fn=&lt;CatBackward0&gt;)
Sequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]</code></pre>
</div>
</div>
<p>Nice! Learnable class token prepended!</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-prepending-the-learnable-class-token.png" width="900" alt="going from a sequence of patch embeddings, creating a learnable class token and then prepending it to the patch embeddings"></p>
<p><em>Reviewing what we’ve done to create the learnable class token, we start with a sequence of image patch embeddings created by <code>PatchEmbedding()</code> on single image, we then created a learnable class token with one value for each of the embedding dimensions and then prepended it to the original sequence of patch embeddings. 참고: Using <code>torch.ones()</code> to create the learnable class token is mostly for demonstration purposes only, in practice, you’d like create it with <code>torch.randn()</code>.</em></p>
</section>
<section id="tk-4.7-creating-the-position-embedding" class="level3" data-number="10.11.7">
<h3 data-number="10.11.7" class="anchored" data-anchor-id="tk-4.7-creating-the-position-embedding"><span class="header-section-number">10.11.7</span> TK 4.7 Creating the position embedding</h3>
<p>Well, we’ve got the class token embedding and the patch embedding, now how might we create the position embedding?</p>
<p>Or <span class="math inline">\(\mathbf{E}_{\text {pos }}\)</span> from equation 1 where <span class="math inline">\(E\)</span> stands for “embedding”.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-the-position-embeddings.png" width="900" alt="extracting the position embeddings from the vision transformer architecture and comparing them to other sections of the vision transformer paper"></p>
<p><em>Left: Figure 1 from the ViT paper with the position embedding we’re going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.</em></p>
<p>Let’s find out more by reading section 3.1 of the ViT paper (bold mine):</p>
<blockquote class="blockquote">
<p>Position embeddings are added to the patch embeddings to retain positional information. We use <strong>standard learnable 1D position embeddings</strong>, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.</p>
</blockquote>
<p>To start creating the position embeddings, let’s view our current embeddings.</p>
<div id="33e5e5bf-e744-4249-ac9b-08986be5ef81" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View the sequence of patch embeddings with the prepended class embedding</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],
          [-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],
          [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],
          ...,
          [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],
          [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],
          [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],
        grad_fn=&lt;CatBackward0&gt;),
 torch.Size([1, 197, 768]))</code></pre>
</div>
</div>
<p>Equation 1 states that the position embeddings should have the shape <span class="math inline">\((N + 1) \times D\)</span> where: * <span class="math inline">\(N=H W / P^{2}\)</span> is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. * <span class="math inline">\(D\)</span> is the size of the <strong>patch embeddings</strong>, different values for <span class="math inline">\(D\)</span> can be found in Table 1.</p>
<p>Luckily we’ve got both of these values already.</p>
<p>So let’s make a learnable 1D embedding with <code>torch.ones()</code> to create <span class="math inline">\(\mathbf{E}_{\text {pos }}\)</span>.</p>
<div id="5bb7f6d1-0824-47eb-a059-6854da5c7433" class="cell" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate N (number of patches)</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>number_of_patches <span class="op">=</span> <span class="bu">int</span>((height <span class="op">*</span> width) <span class="op">/</span> patch_size<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get embedding dimension</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>embedding_dimension <span class="op">=</span> patch_embedded_image_with_class_embedding.shape[<span class="dv">2</span>]</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the learnable 1D position embedding</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>position_embedding <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>,</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>                                             number_of_patches<span class="op">+</span><span class="dv">1</span>, </span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>                                             embedding_dimension),</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>                                  requires_grad<span class="op">=</span><span class="va">True</span>) <span class="co"># make sure it's learnable</span></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding</span></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(position_embedding[:, :<span class="dv">10</span>, :<span class="dv">10</span>])</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Position embeddding shape: </span><span class="sc">{</span>position_embedding<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, number_of_patches, embedding_dimension]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)
Position embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>참고:</strong> Only creating the position embedding as <code>torch.ones()</code> for demonstration purposes, in reality, you’d likely create the position embedding with <code>torch.randn()</code> (start with a random number and improve via gradient descent).</p>
</blockquote>
<p>Position embeddings created!</p>
<p>Let’s add them to our sequence of patch embeddings with a prepended class token.</p>
<div id="03370370-e2c2-4e46-bc20-302b97fba9d7" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the position embedding to the patch and class token embedding</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>patch_and_position_embedding <span class="op">=</span> patch_embedded_image_with_class_embedding <span class="op">+</span> position_embedding</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(patch_and_position_embedding)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embeddings, class token prepended and positional embeddings added shape: </span><span class="sc">{</span>patch_and_position_embedding<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, number_of_patches, embedding_dimension]"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[[2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000],
         [0.5077, 1.0265, 0.9091,  ..., 1.1478, 0.9014, 1.2243],
         [0.0151, 1.3805, 0.6362,  ..., 1.6115, 0.9195, 1.2097],
         ...,
         [0.3332, 1.1713, 0.8289,  ..., 1.4699, 0.7119, 1.2599],
         [0.3017, 1.1949, 0.8116,  ..., 1.5152, 0.6874, 1.2151],
         [0.3111, 1.1862, 0.8556,  ..., 1.5019, 0.6436, 1.2378]]],
       grad_fn=&lt;AddBackward0&gt;)
Patch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]</code></pre>
</div>
</div>
<p>Notice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with <code>torch.ones()</code>).</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> We could put both the class token embedding and position embedding into their own layer if we wanted to. But we’ll see later on how they can be incorporated into the overall ViT architecture’s <code>forward()</code> method.</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-patch-embeddings-with-learnable-class-token-and-position-embeddings.png" width="900" alt="patch embeddings with learnable class token and position embeddings"></p>
<p><em>The workflow we’ve used for adding the position embeddings to the sequence of patch embeddings and class token. 참고: <code>torch.ones()</code> only used to create embeddings for illustration purposes, in practice, you’d likely use <code>torch.randn()</code> to start with a random number.</em></p>
</section>
<section id="tk-4.8-전체-과정-합치기-from-image-to-embedding" class="level3" data-number="10.11.8">
<h3 data-number="10.11.8" class="anchored" data-anchor-id="tk-4.8-전체-과정-합치기-from-image-to-embedding"><span class="header-section-number">10.11.8</span> TK 4.8 전체 과정 합치기: from image to embedding</h3>
<p>Alright, we’ve come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{0} &amp;=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{\text {pos }} \in \mathbb{R}^{(N+1) \times D}
\end{aligned}
\]</span></p>
<p>Let’s now put everything together in a single code cell and go from input image (<span class="math inline">\(x\)</span>) to output embedding <span class="math inline">\({z}_0\)</span>.</p>
<p>We can do so by: 1. Setting the patch size (we’ll use <code>16</code> as it’s widely used throughout the paper and for ViT-Base). 2. Getting a single image, printing it’s shape and storing its height and width. 3. Adding a batch dimension to the single image so it’s compatible with our <code>PatchEmbedding</code> layer. 4. Creating a <code>PatchEmbedding</code> layer with a <code>patch_size=16</code> and <code>embedding_dim=768</code> (from Table 1 for ViT-Base). 5. Passing the single image through the <code>PatchEmbedding</code> layer in 4 to create a sequence of patch embeddings. 6. Creating a class token embedding like in section 4.6. 7. Prepending the class token emebdding to the patch embeddings created in step 5. 8. Creating a position embedding like in section 4.7. 9. Adding the position embedding to the class token and patch embeddings created in step 7.</p>
<p>We’ll also make sure to set the random seeds with <code>set_seeds()</code> and print out the shapes of different tensors along the way.</p>
<div id="8de90548-e6b0-4123-90ca-a23b0fab52a9" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>set_seeds()</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Set patch size</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Print shape of original image tensor and get the image dimensions</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image tensor shape: </span><span class="sc">{</span>image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>height, width <span class="op">=</span> image.shape[<span class="dv">1</span>], image.shape[<span class="dv">2</span>]</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Get image tensor and add batch dimension</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> image.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input image with batch dimension shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Create patch embedding layer</span></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>patch_embedding_layer <span class="op">=</span> PatchEmbedding(in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>                                       patch_size<span class="op">=</span>patch_size,</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>                                       embedding_dim<span class="op">=</span><span class="dv">768</span>)</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Pass image through patch embedding layer</span></span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>patch_embedding <span class="op">=</span> patch_embedding_layer(x)</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patching embedding shape: </span><span class="sc">{</span>patch_embedding<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Create class token embedding</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> patch_embedding.shape[<span class="dv">0</span>]</span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>embedding_dimension <span class="op">=</span> patch_embedding.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>class_token <span class="op">=</span> nn.Parameter(torch.ones(batch_size, <span class="dv">1</span>, embedding_dimension),</span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a>                           requires_grad<span class="op">=</span><span class="va">True</span>) <span class="co"># make sure it's learnable</span></span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Class token embedding shape: </span><span class="sc">{</span>class_token<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Prepend class token embedding to patch embedding</span></span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a>patch_embedding_class_token <span class="op">=</span> torch.cat((class_token, patch_embedding), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embedding with class token shape: </span><span class="sc">{</span>patch_embedding_class_token<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 8. Create position embedding</span></span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>number_of_patches <span class="op">=</span> <span class="bu">int</span>((height <span class="op">*</span> width) <span class="op">/</span> patch_size<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a>position_embedding <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">1</span>, number_of_patches<span class="op">+</span><span class="dv">1</span>, embedding_dimension),</span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a>                                  requires_grad<span class="op">=</span><span class="va">True</span>) <span class="co"># make sure it's learnable</span></span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 9. Add position embedding to patch embedding with class token</span></span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a>patch_and_position_embedding <span class="op">=</span> patch_embedding_class_token <span class="op">+</span> position_embedding</span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch and position embedding shape: </span><span class="sc">{</span>patch_and_position_embedding<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image tensor shape: torch.Size([3, 224, 224])
Input image with batch dimension shape: torch.Size([1, 3, 224, 224])
Patching embedding shape: torch.Size([1, 196, 768])
Class token embedding shape: torch.Size([1, 1, 768])
Patch embedding with class token shape: torch.Size([1, 197, 768])
Patch and position embedding shape: torch.Size([1, 197, 768])</code></pre>
</div>
</div>
<p>Woohoo!</p>
<p>From a single image to patch and position embeddings in a single cell of code.</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-putting-it-all-together.png" width="900" alt="mapping equation 1 from the vision transformer paper to pytorch code"></p>
<p><em>Mapping equation 1 from the ViT paper to our PyTorch code. This is the essence of paper replicating, taking a research paper and turning it into usable code.</em></p>
<p>Now we’ve got a way to encode our images and pass them to the Transformer Encoder in Figure 1 of the ViT paper.</p>
<p><img src="images/08-vit-paper-architecture-animation-full-architecture.png" alt="Vision transformer architecture animation, going from a single image and passing it through a patch embedidng layer and then passing it through the transformer encoder." width="900/"></p>
<p><em>Animating the entire ViT workflow: from patch embeddings to transformer encoder to MLP head.</em></p>
<p>From a code perspective, creating the patch embedding is probably the largest section of replicating the ViT paper.</p>
<p>Many of the other parts of the ViT paper such as the Multi-Head Attention and Norm layers can be created using existing PyTorch layers.</p>
<p>Onwards!</p>
</section>
</section>
<section id="tk.-5.-equation-2-multi-head-attention-msa" class="level2" data-number="10.12">
<h2 data-number="10.12" class="anchored" data-anchor-id="tk.-5.-equation-2-multi-head-attention-msa"><span class="header-section-number">10.12</span> TK. 5. Equation 2: Multi-Head Attention (MSA)</h2>
<p>We’ve got our input data patchified and embedded, now let’s move onto the next part of the ViT architecture.</p>
<p>To start, we’ll break down the Transformer Encoder section into two parts (start small and increase when necessary).</p>
<p>The first being equation 2 and the second being equation 3.</p>
<p>Recall equation 2 states:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{\ell}^{\prime} &amp;=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1}, &amp; &amp; \ell=1 \ldots L
\end{aligned}
\]</span></p>
<p>This indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-2-msa-block-mapped-to-equation.png" alt="mapping equation 2 from the ViT paper to the ViT architecture diagram in figure 1" width="900/"></p>
<p><em>Left: Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.</em></p>
<p>Many layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch.</p>
<p>In saying this, to replicate these layers and residual connection with PyTorch code we can use: * Multi-Head Self Attention (MSA) - <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"><code>torch.nn.MultiheadAttention()</code></a>. * Norm (LN or LayerNorm) - <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"><code>torch.nn.LayerNorm()</code></a>. * Residual connection - add the input to output (we’ll see this later on when we create the full Transformer Encoder block).</p>
<section id="the-layernorm-ln-layer" class="level3" data-number="10.12.1">
<h3 data-number="10.12.1" class="anchored" data-anchor-id="the-layernorm-ln-layer"><span class="header-section-number">10.12.1</span> 5.1 The LayerNorm (LN) layer</h3>
<p><a href="https://paperswithcode.com/method/layer-normalization">Layer Normalization</a> (<code>torch.nn.LayerNorm()</code> or Norm or LayerNorm or LN) normalizes an input over the last dimension.</p>
<p>You can set <code>normalized_shape</code> to be equal to the dimension size you’d like to noramlize over (in our case it’ll be <span class="math inline">\(D\)</span> or <code>768</code> for ViT-Base).</p>
<p>You can find the formal definition of <code>torch.nn.LayerNorm()</code> in the <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">PyTorch documentation</a>.</p>
<p>What does it do?</p>
<p>Layer Normalization helps improve training time and model generalization (ability to adapt to unseen data).</p>
<p>I like to think of any kind of normalization as “getting the data into a similar format” or “getting data samples into a similar distribution”.</p>
<p>Imagine trying to walk up (or down) a set of stairs all with differing heights and lengths.</p>
<p>It’d take some adjustment each step right?</p>
<p>And what you learn for each step wouldn’t necessary help with the next one since they all differ.</p>
<p>Normalization (including Layer Normalization) is the equivalent of making all the stairs the same height and length except the stairs are your data samples.</p>
<p>So just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard-deviations) easier than those with varying distributions.</p>
</section>
<section id="the-multi-head-self-attention-msa-layer" class="level3" data-number="10.12.2">
<h3 data-number="10.12.2" class="anchored" data-anchor-id="the-multi-head-self-attention-msa-layer"><span class="header-section-number">10.12.2</span> 5.2 The Multi-Head Self Attention (MSA) layer</h3>
<p>The power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the <a href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a> research paper.</p>
<p>There are many resources online to learn more about the Transformer architeture and attention mechanism online such as Jay Alammar’s wonderful <a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer post</a> and <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Illustrated Attention post</a>.</p>
<p>But we’re going to focus more on coding an existing PyTorch MSA implementation than creating our own.</p>
<p>However, you can find the formal defintion of the ViT paper’s MSA implementation is defined in Appendix A:</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-2-appendix-A.png" alt="vision transformer paper figure 1 highlighted with equation 2 and appendix A" width="900/"></p>
<p><em>Left: Vision Transformer architecture overview from Figure 1 of the ViT paper. Right: Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.</em></p>
<p>The image above highlights the triple input to the MSA layer.</p>
<p>This is known as <strong>query, key, value</strong> input or <strong>qkv</strong> for short which is fundamental to the self-attention mechanism.</p>
<p>In our case, the triple input will be three versions of the output of the Norm layer.</p>
<p>Or three versions of our layer-normalized image patch and position embeddings created in section 4.8.</p>
<p>We can implement the MSA layer in PyTorch with <code>torch.nn.MultiheadAttention()</code> with the parameters: * <code>embed_dim</code> - the embedding dimension from Table 1 (Hidden size <span class="math inline">\(D\)</span>). * <code>num_heads</code> - how many attention heads to use (this is where the term “multihead” comes from), this value is also in Table 1 (Heads). * <code>dropout</code> - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isn’t used after the qkv-projections).</p>
</section>
<section id="replicating-equation-2-with-pytorch-layers" class="level3" data-number="10.12.3">
<h3 data-number="10.12.3" class="anchored" data-anchor-id="replicating-equation-2-with-pytorch-layers"><span class="header-section-number">10.12.3</span> 5.3 Replicating Equation 2 with PyTorch layers</h3>
<p>Let’s put everything we’ve discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.</p>
<p>To do so, we’ll:</p>
<ol type="1">
<li>Create a class called <code>MultiheadSelfAttentionBlock()</code> that inherits from <code>torch.nn.Module</code>.</li>
<li>Initialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.</li>
<li>Create a layer normalization (LN) layer with <code>torch.nn.LayerNorm()</code> with the <code>normalized_shape</code> parameter the same as our embedding dimension (<span class="math inline">\(D\)</span> from Table 1).</li>
<li>Create a multi-head attention (MSA) layer with the appropriate <code>embed_dim</code>, <code>num_heads</code>, <code>dropout</code> and <code>batch_first</code> parameters.</li>
<li>Create a <code>forward()</code> method for our class passing the in the inputs through the LN layer and MSA layer.</li>
</ol>
<div id="b76ae98c" class="cell" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a class that inherits from nn.Module</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiheadSelfAttentionBlock(nn.Module):</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Creates a multi-head self-attention block ("MSA block" for short).</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Initialize the class with hyperparameters from Table 1</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>                 embedding_dim:<span class="bu">int</span><span class="op">=</span><span class="dv">768</span>, <span class="co"># from Table 1 for ViT-Base</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>                 num_heads:<span class="bu">int</span><span class="op">=</span><span class="dv">12</span>, <span class="co"># from Table 1 for ViT-Base</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>                 attn_dropout:<span class="bu">int</span><span class="op">=</span><span class="dv">0</span>): <span class="co"># doesn't look like the paper uses any dropout in MSABlocks</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Create the Norm layer (LN)</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span>embedding_dim)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Create the Multi-Head Attention (MSA) layer</span></span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.multihead_attn <span class="op">=</span> nn.MultiheadAttention(embed_dim<span class="op">=</span>embedding_dim,</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>                                                    num_heads<span class="op">=</span>num_heads,</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>                                                    dropout<span class="op">=</span>attn_dropout,</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>                                                    batch_first<span class="op">=</span><span class="va">True</span>) <span class="co"># does our batch dimension come first?</span></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Create a forward() method to pass the data throguh the layers</span></span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer_norm(x)</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>        attn_output, _ <span class="op">=</span> <span class="va">self</span>.multihead_attn(query<span class="op">=</span>x, <span class="co"># query embeddings </span></span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>                                             key<span class="op">=</span>x, <span class="co"># key embeddings</span></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>                                             value<span class="op">=</span>x, <span class="co"># value embeddings</span></span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>                                             need_weights<span class="op">=</span><span class="va">False</span>) <span class="co"># do we need the weights or just the layer outputs?</span></span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> attn_output</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<blockquote class="blockquote">
<p><strong>참고:</strong> Unlike Figure 1, our <code>MultiheadSelfAttentionBlock()</code> doesn’t include a skip or residual connection (“<span class="math inline">\(+\mathbf{z}_{\ell-1}\)</span>” in equation 2), we’ll include this when we create the entire Transformer encoder later on.</p>
</blockquote>
<p>MSABlock created!</p>
<p>Let’s try it out by create an instance of our <code>MultiheadSelfAttentionBlock</code> and passing through the <code>patch_and_position_embedding</code> variable we created in section 4.8.</p>
<div id="ceb1dfc0-40ad-4cee-bc54-e9a5cec56895" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of MSABlock</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>multihead_self_attention_block <span class="op">=</span> MultiheadSelfAttentionBlock(embedding_dim<span class="op">=</span><span class="dv">768</span>, <span class="co"># from Table 1 </span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>                                                             num_heads<span class="op">=</span><span class="dv">12</span>) <span class="co"># from Table 1</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass patch and position image embedding through MSABlock</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>patched_image_through_msa_block <span class="op">=</span> multihead_self_attention_block(patch_and_position_embedding)</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape of MSA block: </span><span class="sc">{</span>patch_and_position_embedding<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape MSA block: </span><span class="sc">{</span>patched_image_through_msa_block<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape of MSA block: torch.Size([1, 197, 768])
Output shape MSA block: torch.Size([1, 197, 768])</code></pre>
</div>
</div>
<p>Notice how the input and output shape of our data stays the same when it goes through the MSA block.</p>
<p>This doesn’t mean the data doesn’t change as it goes through.</p>
<p>You could try printing the input and output tensor to see how it changes (though this change will be across <code>1 * 197 * 768</code> values).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-2-in-code.png" width="900" alt="vision transformer paper with equation 2 of figure 1 highlighted and equation 2 turned into code"></p>
<p><em>Left: Vision Transformer architecture from Figure 1 with Multi-Head Attention and LayerNorm layers highlighted, these layers make up equation 2 from section 3.1 of the paper. Right: Replicating equation 2 (without the skip connection on the end) using PyTorch layers.</em></p>
<p>We’ve now officially replicated equation 2 (except for the residual connection on the end but we’ll get to this in section 7)!</p>
<p>Onto the next!</p>
</section>
</section>
<section id="tk-6.-equation-3-multilayer-perceptron-mlp" class="level2" data-number="10.13">
<h2 data-number="10.13" class="anchored" data-anchor-id="tk-6.-equation-3-multilayer-perceptron-mlp"><span class="header-section-number">10.13</span> TK 6. Equation 3: Multilayer Perceptron (MLP)</h2>
<p>UPTOHERE: * Replicate equation 3 like replicating equation 2</p>
<ul>
<li>TK also called “feedforward”</li>
</ul>
<blockquote class="blockquote">
<p>Dropout, when used, is applied <strong>after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.</strong></p>
</blockquote>
<blockquote class="blockquote">
<p>The MLP contains two layers with a GELU non-linearity</p>
</blockquote>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{\ell} &amp;=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L
\end{aligned}
\]</span></p>
<ul>
<li>TK - GELU in PyTorch – https://pytorch.org/docs/stable/generated/torch.nn.GELU.html</li>
</ul>
<div id="68d9dbfe" class="cell" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Could also call this "FeedForward"</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLPBlock(nn.Module):</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Creates an MLPBlock of the Vision Transformer architecture."""</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>                 embedding_dim, <span class="co"># embedding dimension (Hidden Size D in Table 1)</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>                 mlp_size, <span class="co"># MLP size in Table 1</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>                 dropout<span class="op">=</span><span class="dv">0</span>): <span class="co"># "Dropout... is applied to every dense layer... (Appendix B.1)"</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span>embedding_dim)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>embedding_dim,</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>                      out_features<span class="op">=</span>mlp_size),</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>            nn.GELU(), <span class="co"># "The MLP contains two layers with a GELU non-linearity (section 3.1)."</span></span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span>dropout),</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>mlp_size, <span class="co"># needs to take same in_features as out_features of layer above</span></span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a>                      out_features<span class="op">=</span>embedding_dim), <span class="co"># take back to embedding_dim</span></span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer_norm(x)</span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mlp(x)</span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="442fb987" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>mlp_block <span class="op">=</span> MLPBlock(embedding_dim<span class="op">=</span><span class="dv">768</span>, <span class="co"># Table 1 </span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>                     mlp_size<span class="op">=</span><span class="dv">3072</span>) <span class="co"># Table 1</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>patched_image_through_mlp_block <span class="op">=</span> mlp_block(patched_image_through_msa_block)</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>patched_image_through_mlp_block.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>torch.Size([1, 197, 768])</code></pre>
</div>
</div>
</section>
<section id="tk-7.-create-the-transformer-encoder" class="level2" data-number="10.14">
<h2 data-number="10.14" class="anchored" data-anchor-id="tk-7.-create-the-transformer-encoder"><span class="header-section-number">10.14</span> TK 7. Create the Transformer Encoder</h2>
<ul>
<li>Tk - what is an “encoder”?</li>
<li>Tk - “transformer block” or “transformer encoder”? - line this up with the paper</li>
</ul>
<p>See here for pre-built transformer blocks/layers: https://pytorch.org/docs/stable/nn.html#transformer-layers</p>
<div id="2c43855c" class="cell" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderBlock(nn.Module):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Creates a Transformer Encoder block."""</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>                 embedding_dim<span class="op">=</span><span class="dv">768</span>, <span class="co"># From Table 1</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>                 num_heads<span class="op">=</span><span class="dv">12</span>, <span class="co"># From Table 1</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>                 mlp_size<span class="op">=</span><span class="dv">3072</span>, <span class="co"># From Table 1</span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>                 mlp_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>                 attn_dropout<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create MSA Block (for equation 2)</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.msa_block <span class="op">=</span> MultiheadSelfAttentionBlock(embedding_dim<span class="op">=</span>embedding_dim,</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>                                                     num_heads<span class="op">=</span>num_heads,</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>                                                     attn_dropout<span class="op">=</span>attn_dropout)</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create MLP Block (for equation 3)</span></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp_block <span class="op">=</span>  MLPBlock(embedding_dim<span class="op">=</span>embedding_dim,</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>                                   mlp_size<span class="op">=</span>mlp_size,</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>                                   dropout<span class="op">=</span>mlp_dropout)</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.msa_block(x) <span class="op">+</span> x <span class="co"># Create skip connection</span></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mlp_block(x) <span class="op">+</span> x <span class="co"># Create skip connection</span></span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="tk-8.-전체-과정-합치기-to-create-vit" class="level2" data-number="10.15">
<h2 data-number="10.15" class="anchored" data-anchor-id="tk-8.-전체-과정-합치기-to-create-vit"><span class="header-section-number">10.15</span> TK 8. 전체 과정 합치기 to create ViT</h2>
<p>TK - replicate this with the TransformerEncoderLayer - https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/</p>
<p>Combine the transformer blocks and patched embedding into a ViT architecture.</p>
<div id="2df890d5" class="cell" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ViT(nn.Module):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Creates a Vision Transformer architecture."""</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>                 img_size<span class="op">=</span><span class="dv">224</span>, <span class="co"># From Table 3 in ViT paper</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>                 in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>                 patch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>                 num_transformer_layers<span class="op">=</span><span class="dv">12</span>, <span class="co"># From Table 1 in ViT paper</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>                 embedding_dim<span class="op">=</span><span class="dv">768</span>,</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>                 mlp_size<span class="op">=</span><span class="dv">3072</span>,</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>                 num_heads<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>                 attn_dropout<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>                 mlp_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>                 embedding_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>                 num_classes<span class="op">=</span><span class="dv">1000</span>): <span class="co"># default for ImageNet</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>() <span class="co"># don't forget the super().__init__()!</span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get image size</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.img_height, <span class="va">self</span>.img_width <span class="op">=</span> img_size, img_size</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate number of patches (height * width/patch^2)</span></span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> (<span class="va">self</span>.img_height <span class="op">*</span> <span class="va">self</span>.img_width) <span class="op">//</span> patch_size<span class="op">**</span><span class="dv">2</span></span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>                 </span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create class embedding (needs to go at front of sequence embedding)</span></span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_embedding <span class="op">=</span> nn.Parameter(data<span class="op">=</span>torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, embedding_dim),</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>                                            requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Create position embedding</span></span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> nn.Parameter(data<span class="op">=</span>torch.randn(<span class="dv">1</span>, <span class="va">self</span>.num_patches<span class="op">+</span><span class="dv">1</span>, embedding_dim),</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>                                               requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create embedding dropout</span></span>
<span id="cb78-33"><a href="#cb78-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding_dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span>embedding_dropout)</span>
<span id="cb78-34"><a href="#cb78-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-35"><a href="#cb78-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create patch embedding layer</span></span>
<span id="cb78-36"><a href="#cb78-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embedding <span class="op">=</span> PatchEmbedding(in_channels<span class="op">=</span>in_channels,</span>
<span id="cb78-37"><a href="#cb78-37" aria-hidden="true" tabindex="-1"></a>                                              patch_size<span class="op">=</span>patch_size,</span>
<span id="cb78-38"><a href="#cb78-38" aria-hidden="true" tabindex="-1"></a>                                              embedding_dim<span class="op">=</span>embedding_dim)</span>
<span id="cb78-39"><a href="#cb78-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb78-40"><a href="#cb78-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create transformer encoder blocks</span></span>
<span id="cb78-41"><a href="#cb78-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer_enedoder <span class="op">=</span> nn.Sequential(<span class="op">*</span>[TransformerEncoderBlock(embedding_dim<span class="op">=</span>embedding_dim,</span>
<span id="cb78-42"><a href="#cb78-42" aria-hidden="true" tabindex="-1"></a>                                                                            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb78-43"><a href="#cb78-43" aria-hidden="true" tabindex="-1"></a>                                                                            mlp_size<span class="op">=</span>mlp_size,</span>
<span id="cb78-44"><a href="#cb78-44" aria-hidden="true" tabindex="-1"></a>                                                                            mlp_dropout<span class="op">=</span>mlp_dropout) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_transformer_layers)])</span>
<span id="cb78-45"><a href="#cb78-45" aria-hidden="true" tabindex="-1"></a>       </span>
<span id="cb78-46"><a href="#cb78-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create classifier head (equation 4)</span></span>
<span id="cb78-47"><a href="#cb78-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Sequential(</span>
<span id="cb78-48"><a href="#cb78-48" aria-hidden="true" tabindex="-1"></a>            nn.LayerNorm(normalized_shape<span class="op">=</span>embedding_dim),</span>
<span id="cb78-49"><a href="#cb78-49" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>embedding_dim, </span>
<span id="cb78-50"><a href="#cb78-50" aria-hidden="true" tabindex="-1"></a>                      out_features<span class="op">=</span>num_classes)</span>
<span id="cb78-51"><a href="#cb78-51" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-52"><a href="#cb78-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb78-53"><a href="#cb78-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb78-54"><a href="#cb78-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get batch size</span></span>
<span id="cb78-55"><a href="#cb78-55" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb78-56"><a href="#cb78-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create class token embedding</span></span>
<span id="cb78-57"><a href="#cb78-57" aria-hidden="true" tabindex="-1"></a>        class_token <span class="op">=</span> <span class="va">self</span>.class_embedding.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb78-58"><a href="#cb78-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-59"><a href="#cb78-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create patch embedding</span></span>
<span id="cb78-60"><a href="#cb78-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embedding(x)</span>
<span id="cb78-61"><a href="#cb78-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-62"><a href="#cb78-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat class embedding and patch embedding (equation 1)</span></span>
<span id="cb78-63"><a href="#cb78-63" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat((class_token, x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb78-64"><a href="#cb78-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-65"><a href="#cb78-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add position embedding to patch embedding (equation 1) for every batch</span></span>
<span id="cb78-66"><a href="#cb78-66" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.position_embedding <span class="op">+</span> x</span>
<span id="cb78-67"><a href="#cb78-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-68"><a href="#cb78-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run embedding dropout</span></span>
<span id="cb78-69"><a href="#cb78-69" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding_dropout(x)</span>
<span id="cb78-70"><a href="#cb78-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-71"><a href="#cb78-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)</span></span>
<span id="cb78-72"><a href="#cb78-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer_enedoder(x)</span>
<span id="cb78-73"><a href="#cb78-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-74"><a href="#cb78-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Put 0 index logit through classifier (equation 4)</span></span>
<span id="cb78-75"><a href="#cb78-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.classifier(x[:, <span class="dv">0</span>]) <span class="co"># run on each sample in a batch at 0 index</span></span>
<span id="cb78-76"><a href="#cb78-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-77"><a href="#cb78-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb78-78"><a href="#cb78-78" aria-hidden="true" tabindex="-1"></a>        </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="7dc9f8ec" class="cell" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>class_tokens <span class="op">=</span> nn.Parameter(data<span class="op">=</span>torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">768</span>))</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>class_tokens.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>torch.Size([32, 1, 768])</code></pre>
</div>
</div>
<div id="f86190a3-ed1f-4ab3-881c-4eecea996912" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>set_seeds()</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>rand_image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="co"># vit = ViT(num_classes=len(class_names)) </span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>vit <span class="op">=</span> ViT(num_classes<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>vit(rand_image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="tk-9.-inspect-the-model" class="level2" data-number="10.16">
<h2 data-number="10.16" class="anchored" data-anchor-id="tk-9.-inspect-the-model"><span class="header-section-number">10.16</span> TK 9. Inspect the model</h2>
<blockquote class="blockquote">
<p><strong>참고:</strong> If you go too big, your hardware might not be able to handle it… (e.g.&nbsp;too high of a batch size…)</p>
</blockquote>
<p>TK - Number of parameters should be equivalent to: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 (<code>num_params=86,567,656</code>)</p>
<div id="494bde26-ed1e-45dc-b615-78ac268ca20e" class="cell" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - clean up the summary so it looks nice when it prints out </span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary using torchinfo (uncomment for actual output)</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>summary(model<span class="op">=</span>vit, </span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>        input_size<span class="op">=</span>(<span class="dv">128</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>), <span class="co"># make sure this is "input_size", not "input_shape"</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># col_names=["input_size"], # uncomment for smaller output</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>        col_names<span class="op">=</span>[<span class="st">"input_size"</span>, <span class="st">"output_size"</span>, <span class="st">"num_params"</span>, <span class="st">"trainable"</span>],</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>        col_width<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>        row_settings<span class="op">=</span>[<span class="st">"var_names"</span>]</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>======================================================================================================================================================
Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable
======================================================================================================================================================
ViT (ViT)                                                              [128, 3, 224, 224]   [128, 3]             152,064              True
├─Dropout (embedding_dropout)                                          [128, 197, 768]      [128, 197, 768]      --                   --
├─PatchEmbedding (patch_embedding)                                     [128, 3, 224, 224]   [128, 196, 768]      --                   True
│    └─Conv2d (patcher)                                                [128, 3, 224, 224]   [128, 768, 14, 14]   590,592              True
│    └─Flatten (flatten)                                               [128, 768, 14, 14]   [128, 768, 196]      --                   --
├─Dropout (embedding_dropout)                                          [128, 197, 768]      [128, 197, 768]      --                   --
├─Sequential (transformer_enedoder)                                    [128, 197, 768]      [128, 197, 768]      --                   True
│    └─TransformerEncoderBlock (0)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (1)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (2)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (3)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (4)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (5)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (6)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (7)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (8)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (9)                                     [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (10)                                    [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
│    └─TransformerEncoderBlock (11)                                    [128, 197, 768]      [128, 197, 768]      --                   True
│    │    └─MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True
│    │    └─MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True
├─Sequential (classifier)                                              [128, 768]           [128, 3]             --                   True
│    └─LayerNorm (0)                                                   [128, 768]           [128, 768]           1,536                True
│    └─Linear (1)                                                      [128, 768]           [128, 3]             2,307                True
======================================================================================================================================================
Total params: 85,800,963
Trainable params: 85,800,963
Non-trainable params: 0
Total mult-adds (G): 22.08
======================================================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 13168.81
Params size (MB): 257.55
Estimated Total Size (MB): 13503.43
======================================================================================================================================================</code></pre>
</div>
</div>
<ul>
<li>TK - same number of parameters as: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 -&gt; 86567656</li>
</ul>
<div id="3791f963-7d51-418b-b8bd-4c77341560e5" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>cls_embedding <span class="op">=</span> nn.Parameter(torch.randn(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">768</span>))</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="co"># See here: https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>cls_embedding.shape, cls_embedding.expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>).shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>(torch.Size([1, 1, 768]), torch.Size([32, 1, 768]))</code></pre>
</div>
</div>
</section>
<section id="tk-10.-train-model" class="level2" data-number="10.17">
<h2 data-number="10.17" class="anchored" data-anchor-id="tk-10.-train-model"><span class="header-section-number">10.17</span> TK 10. Train model</h2>
<div id="9107b068-f253-4026-ad21-83be41404043" class="cell" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> going_modular.going_modular <span class="im">import</span> engine</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>vit.parameters(), </span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>                             lr<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>                             betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.999</span>), <span class="co"># default</span></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>                             weight_decay<span class="op">=</span><span class="fl">0.1</span>) <span class="co"># from the ViT paper section 4.1</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>set_seeds()</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> engine.train(model<span class="op">=</span>vit,</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>                       train_dataloader<span class="op">=</span>train_dataloader,</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>                       test_dataloader<span class="op">=</span>test_dataloader,</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>                       optimizer<span class="op">=</span>optimizer,</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>                       loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>                       epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>                       device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0632771598bb43e7a49ed5ffa70c4490","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 1 | train_loss: 4.8759 | train_acc: 0.2891 | test_loss: 1.0465 | test_acc: 0.5417
Epoch: 2 | train_loss: 1.5900 | train_acc: 0.2617 | test_loss: 1.5876 | test_acc: 0.1979
Epoch: 3 | train_loss: 1.4644 | train_acc: 0.2617 | test_loss: 1.2738 | test_acc: 0.1979
Epoch: 4 | train_loss: 1.3159 | train_acc: 0.2773 | test_loss: 1.7498 | test_acc: 0.1979
Epoch: 5 | train_loss: 1.3114 | train_acc: 0.3008 | test_loss: 1.7444 | test_acc: 0.2604
Epoch: 6 | train_loss: 1.2445 | train_acc: 0.3008 | test_loss: 1.9704 | test_acc: 0.1979
Epoch: 7 | train_loss: 1.2050 | train_acc: 0.3984 | test_loss: 3.5480 | test_acc: 0.1979
Epoch: 8 | train_loss: 1.4368 | train_acc: 0.4258 | test_loss: 1.8324 | test_acc: 0.2604
Epoch: 9 | train_loss: 1.5757 | train_acc: 0.2344 | test_loss: 1.2848 | test_acc: 0.5417
Epoch: 10 | train_loss: 1.4658 | train_acc: 0.4023 | test_loss: 1.2389 | test_acc: 0.2604</code></pre>
</div>
</div>
</section>
<section id="tk-11.-evaluate-model" class="level2" data-number="10.18">
<h2 data-number="10.18" class="anchored" data-anchor-id="tk-11.-evaluate-model"><span class="header-section-number">10.18</span> TK 11. Evaluate model</h2>
<p>TK - plot the loss curves</p>
<div id="fcca1148-6475-4012-bfc9-cbad2706c22d" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> plot_loss_curves</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>plot_loss_curves(results)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-49-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>TK - why do the loss curves look the way they do? (too big of a model, not enough data)</p>
</section>
<section id="tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset" class="level2" data-number="10.19">
<h2 data-number="10.19" class="anchored" data-anchor-id="tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset"><span class="header-section-number">10.19</span> TK 12. Bring in pretrained ViT from <code>torchvision.models</code> on same dataset</h2>
<ul>
<li>Get a similar model from here - https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16</li>
</ul>
<div id="30de8333-74b0-49ae-a81e-0266e6325f26" class="cell" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The following requires torch v0.12+ and torchvision v0.13+</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.__version__) </span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchvision.__version__)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.12.0+cu102
0.13.0+cu102</code></pre>
</div>
</div>
<div id="b0b87f68-98cc-49f8-89bd-ff220a757f76" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>'cuda'</code></pre>
</div>
</div>
<div id="29887400-2f92-4c75-961e-0541bea6b73a" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seeds</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_seeds(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(seed)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    torch.cuda.manual_seed(seed)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="b8e2dda6-8af0-4255-815f-4d885fa4b477" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Requires torchvision &gt;= 0.13</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>pretrained_vit_weights <span class="op">=</span> torchvision.models.ViT_B_16_Weights.DEFAULT</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>pretrained_vit <span class="op">=</span> torchvision.models.vit_b_16(weights<span class="op">=</span>pretrained_vit_weights).to(device)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze the base parameters</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> parameter <span class="kw">in</span> pretrained_vit.parameters():</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>    parameter.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Change the classifier head</span></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>set_seeds()</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>pretrained_vit.heads <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">768</span>, out_features<span class="op">=</span><span class="bu">len</span>(class_names)).to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8fbd83a1" class="cell" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary using torchinfo (uncomment for actual output)</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>summary(model<span class="op">=</span>pretrained_vit, </span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>        input_size<span class="op">=</span>(<span class="dv">128</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>), <span class="co"># make sure this is "input_size", not "input_shape"</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># col_names=["input_size"], # uncomment for smaller output</span></span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>        col_names<span class="op">=</span>[<span class="st">"input_size"</span>, <span class="st">"output_size"</span>, <span class="st">"num_params"</span>, <span class="st">"trainable"</span>],</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>        col_width<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>        row_settings<span class="op">=</span>[<span class="st">"var_names"</span>]</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>======================================================================================================================================================
Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable
======================================================================================================================================================
VisionTransformer (VisionTransformer)                                  [128, 3, 224, 224]   [128, 3]             768                  Partial
├─Conv2d (conv_proj)                                                   [128, 3, 224, 224]   [128, 768, 14, 14]   (590,592)            False
├─Encoder (encoder)                                                    [128, 197, 768]      [128, 197, 768]      151,296              False
│    └─Dropout (dropout)                                               [128, 197, 768]      [128, 197, 768]      --                   --
│    └─Sequential (layers)                                             [128, 197, 768]      [128, 197, 768]      --                   False
│    │    └─EncoderBlock (encoder_layer_0)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_1)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_2)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_3)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_4)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_5)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_6)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_7)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_8)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_9)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_10)                            [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_11)                            [128, 197, 768]      [128, 197, 768]      (7,087,872)          False
│    └─LayerNorm (ln)                                                  [128, 197, 768]      [128, 197, 768]      (1,536)              False
├─Linear (heads)                                                       [128, 768]           [128, 3]             2,307                True
======================================================================================================================================================
Total params: 85,800,963
Trainable params: 2,307
Non-trainable params: 85,798,656
Total mult-adds (G): 22.08
======================================================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 13322.95
Params size (MB): 257.55
Estimated Total Size (MB): 13657.57
======================================================================================================================================================</code></pre>
</div>
</div>
<div id="2e4ed730-de72-415f-99d1-5dffd57e9dec" class="cell" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - the above output has the same number of parameters as our own created model</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="94cb3900" class="cell" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download pizza, steak, sushi images from GitHub</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> download_data(source<span class="op">=</span><span class="st">"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip"</span>,</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>                           destination<span class="op">=</span><span class="st">"pizza_steak_sushi"</span>)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>image_path</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] data/pizza_steak_sushi directory exists, skipping download.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>PosixPath('data/pizza_steak_sushi')</code></pre>
</div>
</div>
<div id="2e6ae0fe-73c0-4930-988a-e4df903084b6" class="cell" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>train_dir <span class="op">=</span> image_path <span class="op">/</span> <span class="st">"train"</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>test_dir <span class="op">=</span> image_path <span class="op">/</span> <span class="st">"test"</span> </span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>train_dir, test_dir</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>(PosixPath('data/pizza_steak_sushi/train'),
 PosixPath('data/pizza_steak_sushi/test'))</code></pre>
</div>
</div>
<div id="dd2f58ff-6182-453a-a802-70ff98c09557" class="cell" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset for pretrained ViT</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>pretrained_vit_transforms <span class="op">=</span> pretrained_vit_weights.transforms()</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pretrained_vit_transforms)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>train_dataloader_pretrained, test_dataloader_pretrained, class_names <span class="op">=</span> data_setup.create_dataloaders(train_dir<span class="op">=</span>train_dir,</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a>                                                                                                     test_dir<span class="op">=</span>test_dir,</span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a>                                                                                                     transform<span class="op">=</span>pretrained_vit_transforms,</span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>                                                                                                     batch_size<span class="op">=</span><span class="dv">1024</span>) <span class="co"># From here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>ImageClassification(
    crop_size=[224]
    resize_size=[256]
    mean=[0.485, 0.456, 0.406]
    std=[0.229, 0.224, 0.225]
    interpolation=InterpolationMode.BILINEAR
)</code></pre>
</div>
</div>
<div id="a49408b4-24d9-4bb1-90a2-dd61c08f78a4" class="cell" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train pretrained feature extractor ViT for 5 epochs on Pizza, Steak, Sushi</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TK - can probably increase the batch_size here because we're using feature extraction and not </span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="co"># training the whole model</span></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> going_modular.going_modular <span class="im">import</span> engine</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>pretrained_vit.parameters(), </span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>                             lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>set_seeds()</span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a>pretrained_vit_results <span class="op">=</span> engine.train(model<span class="op">=</span>pretrained_vit,</span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>                                      train_dataloader<span class="op">=</span>train_dataloader_pretrained,</span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>                                      test_dataloader<span class="op">=</span>test_dataloader_pretrained,</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>                                      optimizer<span class="op">=</span>optimizer,</span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>                                      loss_fn<span class="op">=</span>loss_fn,</span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>                                      epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>                                      device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7904829b8c1646d38bb6033a5b4367c4","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 1 | train_loss: 1.1490 | train_acc: 0.2356 | test_loss: 1.0584 | test_acc: 0.4667
Epoch: 2 | train_loss: 1.0017 | train_acc: 0.5289 | test_loss: 0.9194 | test_acc: 0.6400
Epoch: 3 | train_loss: 0.8716 | train_acc: 0.7244 | test_loss: 0.7983 | test_acc: 0.6667
Epoch: 4 | train_loss: 0.7583 | train_acc: 0.8089 | test_loss: 0.6942 | test_acc: 0.7733
Epoch: 5 | train_loss: 0.6608 | train_acc: 0.8622 | test_loss: 0.6060 | test_acc: 0.8800
Epoch: 6 | train_loss: 0.5777 | train_acc: 0.8889 | test_loss: 0.5318 | test_acc: 0.8933
Epoch: 7 | train_loss: 0.5076 | train_acc: 0.9156 | test_loss: 0.4700 | test_acc: 0.9067
Epoch: 8 | train_loss: 0.4487 | train_acc: 0.9244 | test_loss: 0.4188 | test_acc: 0.9333
Epoch: 9 | train_loss: 0.3993 | train_acc: 0.9378 | test_loss: 0.3765 | test_acc: 0.9333
Epoch: 10 | train_loss: 0.3580 | train_acc: 0.9378 | test_loss: 0.3417 | test_acc: 0.9467</code></pre>
</div>
</div>
<div id="3c0af18e-6419-4dd6-b8ea-f5830bbd63d5" class="cell" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the loss curves</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> plot_loss_curves</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>plot_loss_curves(pretrained_vit_results) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="08_pytorch_paper_replicating_files/figure-html/cell-60-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="0fd00943-01aa-4ef4-b366-3cb859a25b6f" class="cell" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> going_modular.going_modular <span class="im">import</span> utils</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>utils.save_model(model<span class="op">=</span>pretrained_vit,</span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>                 target_dir<span class="op">=</span><span class="st">"models"</span>,</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>                 model_name<span class="op">=</span><span class="st">"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth</code></pre>
</div>
</div>
<div id="f52ef12c-b88e-4796-84eb-981491a84334" class="cell" data-execution_count="61">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the model size in bytes then convert to megabytes</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>pretrained_vit_model_size <span class="op">=</span> Path(<span class="st">"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth"</span>).stat().st_size <span class="op">//</span> (<span class="dv">1024</span><span class="op">*</span><span class="dv">1024</span>)</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Pretrained ViT feature extractor model size: </span><span class="sc">{</span>pretrained_vit_model_size<span class="sc">}</span><span class="ss"> MB"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pretrained ViT feature extractor model size: 327 MB</code></pre>
</div>
</div>
</section>
<section id="tk---things-this-replication-misses-out-on" class="level2" data-number="10.20">
<h2 data-number="10.20" class="anchored" data-anchor-id="tk---things-this-replication-misses-out-on"><span class="header-section-number">10.20</span> TK - Things this replication misses out on</h2>
<p>TK Put down the difference in the paper vs this replication * Many of these things are in Table 3: * training data (ImageNet from scratch vs FoodVision Mini data) * LR warmup * LR decay * Weight decay * Number of epochs</p>
</section>
<section id="tk---연습-문제" class="level2" data-number="10.21">
<h2 data-number="10.21" class="anchored" data-anchor-id="tk---연습-문제"><span class="header-section-number">10.21</span> TK - 연습 문제</h2>
</section>
<section id="tk---추가-학습-자료" class="level2" data-number="10.22">
<h2 data-number="10.22" class="anchored" data-anchor-id="tk---추가-학습-자료"><span class="header-section-number">10.22</span> TK - 추가 학습 자료</h2>
<ul>
<li>layernorm</li>
<li>See the illustrated transformer for an overview of the Transformer model: https://jalammar.github.io/illustrated-transformer/ + https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</li>
<li>Attention is all you need paper - Yannic video</li>
<li>Vision transformer - yannic video</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./07_pytorch_experiment_tracking.html" class="pagination-link" aria-label="07 - PyTorch 실험 추적">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">07 - PyTorch 실험 추적</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./09_pytorch_model_deployment.html" class="pagination-link" aria-label="09 - PyTorch 모델 배포">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">09 - PyTorch 모델 배포</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>