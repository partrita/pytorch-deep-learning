<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; 02 - PyTorch 신경망 분류 – 파이토치 딥러닝 입문</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03_pytorch_computer_vision.html" rel="next">
<link href="./01_pytorch_workflow.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-803ae4f7a8810f475153517dc3c4ebae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02_pytorch_classification.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">02 - PyTorch 신경망 분류</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">파이토치 딥러닝 입문</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">딥러닝을 위한 PyTorch 배우기</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00_pytorch_fundamentals.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">00 - PyTorch 기초</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_pytorch_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">01 - PyTorch 워크플로우</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_pytorch_classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">02 - PyTorch 신경망 분류</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_pytorch_computer_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_pytorch_custom_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">04 - PyTorch 사용자 정의 데이터셋</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_pytorch_going_modular.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">05 - PyTorch 모듈화</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_pytorch_transfer_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">06 - PyTorch 전이 학습</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_pytorch_experiment_tracking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">07 - PyTorch 실험 추적</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_pytorch_paper_replicating.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">08 - PyTorch 논문 복제</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_pytorch_model_deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">09 - PyTorch 모델 배포</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">참고 문헌</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-a-classification-problem" id="toc-what-is-a-classification-problem" class="nav-link active" data-scroll-target="#what-is-a-classification-problem"><span class="header-section-number">4.1</span> What is a classification problem?</a></li>
  <li><a href="#이번-장에서-다룰-내용" id="toc-이번-장에서-다룰-내용" class="nav-link" data-scroll-target="#이번-장에서-다룰-내용"><span class="header-section-number">4.2</span> 이번 장에서 다룰 내용</a></li>
  <li><a href="#도움을-받을-수-있는-곳" id="toc-도움을-받을-수-있는-곳" class="nav-link" data-scroll-target="#도움을-받을-수-있는-곳"><span class="header-section-number">4.3</span> 도움을 받을 수 있는 곳</a></li>
  <li><a href="#architecture-of-a-classification-neural-network" id="toc-architecture-of-a-classification-neural-network" class="nav-link" data-scroll-target="#architecture-of-a-classification-neural-network"><span class="header-section-number">4.4</span> 0. Architecture of a classification neural network</a></li>
  <li><a href="#make-classification-data-and-get-it-ready" id="toc-make-classification-data-and-get-it-ready" class="nav-link" data-scroll-target="#make-classification-data-and-get-it-ready"><span class="header-section-number">4.5</span> 1. Make classification data and get it ready</a>
  <ul class="collapse">
  <li><a href="#input-and-output-shapes" id="toc-input-and-output-shapes" class="nav-link" data-scroll-target="#input-and-output-shapes"><span class="header-section-number">4.5.1</span> 1.1 Input and output shapes</a></li>
  <li><a href="#turn-data-into-tensors-and-create-train-and-test-splits" id="toc-turn-data-into-tensors-and-create-train-and-test-splits" class="nav-link" data-scroll-target="#turn-data-into-tensors-and-create-train-and-test-splits"><span class="header-section-number">4.5.2</span> 1.2 Turn data into tensors and create train and test splits</a></li>
  </ul></li>
  <li><a href="#모델-구축하기" id="toc-모델-구축하기" class="nav-link" data-scroll-target="#모델-구축하기"><span class="header-section-number">4.6</span> 2. 모델 구축하기</a>
  <ul class="collapse">
  <li><a href="#setup-loss-function-and-optimizer" id="toc-setup-loss-function-and-optimizer" class="nav-link" data-scroll-target="#setup-loss-function-and-optimizer"><span class="header-section-number">4.6.1</span> 2.1 Setup loss function and optimizer</a></li>
  </ul></li>
  <li><a href="#train-model" id="toc-train-model" class="nav-link" data-scroll-target="#train-model"><span class="header-section-number">4.7</span> 3. Train model</a>
  <ul class="collapse">
  <li><a href="#going-from-raw-model-outputs-to-predicted-labels-logits---prediction-probabilities---prediction-labels" id="toc-going-from-raw-model-outputs-to-predicted-labels-logits---prediction-probabilities---prediction-labels" class="nav-link" data-scroll-target="#going-from-raw-model-outputs-to-predicted-labels-logits---prediction-probabilities---prediction-labels"><span class="header-section-number">4.7.1</span> 3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)</a></li>
  <li><a href="#building-a-training-and-testing-loop" id="toc-building-a-training-and-testing-loop" class="nav-link" data-scroll-target="#building-a-training-and-testing-loop"><span class="header-section-number">4.7.2</span> 3.2 Building a training and testing loop</a></li>
  </ul></li>
  <li><a href="#make-predictions-and-evaluate-the-model" id="toc-make-predictions-and-evaluate-the-model" class="nav-link" data-scroll-target="#make-predictions-and-evaluate-the-model"><span class="header-section-number">4.8</span> 4. Make predictions and evaluate the model</a></li>
  <li><a href="#improving-a-model-from-a-model-perspective" id="toc-improving-a-model-from-a-model-perspective" class="nav-link" data-scroll-target="#improving-a-model-from-a-model-perspective"><span class="header-section-number">4.9</span> 5. Improving a model (from a model perspective)</a>
  <ul class="collapse">
  <li><a href="#preparing-data-to-see-if-our-model-can-model-a-straight-line" id="toc-preparing-data-to-see-if-our-model-can-model-a-straight-line" class="nav-link" data-scroll-target="#preparing-data-to-see-if-our-model-can-model-a-straight-line"><span class="header-section-number">4.9.1</span> 5.1 Preparing data to see if our model can model a straight line</a></li>
  <li><a href="#adjusting-model_1-to-fit-a-straight-line" id="toc-adjusting-model_1-to-fit-a-straight-line" class="nav-link" data-scroll-target="#adjusting-model_1-to-fit-a-straight-line"><span class="header-section-number">4.9.2</span> 5.2 Adjusting <code>model_1</code> to fit a straight line</a></li>
  </ul></li>
  <li><a href="#the-missing-piece-non-linearity" id="toc-the-missing-piece-non-linearity" class="nav-link" data-scroll-target="#the-missing-piece-non-linearity"><span class="header-section-number">4.10</span> 6. The missing piece: non-linearity</a>
  <ul class="collapse">
  <li><a href="#recreating-non-linear-data-red-and-blue-circles" id="toc-recreating-non-linear-data-red-and-blue-circles" class="nav-link" data-scroll-target="#recreating-non-linear-data-red-and-blue-circles"><span class="header-section-number">4.10.1</span> 6.1 Recreating non-linear data (red and blue circles)</a></li>
  <li><a href="#모델-구축하기-with-non-linearity" id="toc-모델-구축하기-with-non-linearity" class="nav-link" data-scroll-target="#모델-구축하기-with-non-linearity"><span class="header-section-number">4.10.2</span> 6.2 모델 구축하기 with non-linearity</a></li>
  <li><a href="#training-a-model-with-non-linearity" id="toc-training-a-model-with-non-linearity" class="nav-link" data-scroll-target="#training-a-model-with-non-linearity"><span class="header-section-number">4.10.3</span> 6.3 Training a model with non-linearity</a></li>
  <li><a href="#evaluating-a-model-trained-with-non-linear-activation-functions" id="toc-evaluating-a-model-trained-with-non-linear-activation-functions" class="nav-link" data-scroll-target="#evaluating-a-model-trained-with-non-linear-activation-functions"><span class="header-section-number">4.10.4</span> 6.4 Evaluating a model trained with non-linear activation functions</a></li>
  </ul></li>
  <li><a href="#replicating-non-linear-activation-functions" id="toc-replicating-non-linear-activation-functions" class="nav-link" data-scroll-target="#replicating-non-linear-activation-functions"><span class="header-section-number">4.11</span> 7. Replicating non-linear activation functions</a></li>
  <li><a href="#putting-things-together-by-building-a-multi-class-pytorch-model" id="toc-putting-things-together-by-building-a-multi-class-pytorch-model" class="nav-link" data-scroll-target="#putting-things-together-by-building-a-multi-class-pytorch-model"><span class="header-section-number">4.12</span> 8. Putting things together by building a multi-class PyTorch model</a>
  <ul class="collapse">
  <li><a href="#creating-mutli-class-classification-data" id="toc-creating-mutli-class-classification-data" class="nav-link" data-scroll-target="#creating-mutli-class-classification-data"><span class="header-section-number">4.12.1</span> 8.1 Creating mutli-class classification data</a></li>
  <li><a href="#building-a-multi-class-classification-model-in-pytorch" id="toc-building-a-multi-class-classification-model-in-pytorch" class="nav-link" data-scroll-target="#building-a-multi-class-classification-model-in-pytorch"><span class="header-section-number">4.12.2</span> 8.2 Building a multi-class classification model in PyTorch</a></li>
  <li><a href="#creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model" id="toc-creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model" class="nav-link" data-scroll-target="#creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model"><span class="header-section-number">4.12.3</span> 8.3 Creating a loss function and optimizer for a multi-class PyTorch model</a></li>
  <li><a href="#getting-prediction-probabilities-for-a-multi-class-pytorch-model" id="toc-getting-prediction-probabilities-for-a-multi-class-pytorch-model" class="nav-link" data-scroll-target="#getting-prediction-probabilities-for-a-multi-class-pytorch-model"><span class="header-section-number">4.12.4</span> 8.4 Getting prediction probabilities for a multi-class PyTorch model</a></li>
  <li><a href="#creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model" id="toc-creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model" class="nav-link" data-scroll-target="#creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model"><span class="header-section-number">4.12.5</span> 8.5 Creating a training and testing loop for a multi-class PyTorch model</a></li>
  <li><a href="#making-and-evaluating-predictions-with-a-pytorch-multi-class-model" id="toc-making-and-evaluating-predictions-with-a-pytorch-multi-class-model" class="nav-link" data-scroll-target="#making-and-evaluating-predictions-with-a-pytorch-multi-class-model"><span class="header-section-number">4.12.6</span> 8.6 Making and evaluating predictions with a PyTorch multi-class model</a></li>
  </ul></li>
  <li><a href="#more-classification-evaluation-metrics" id="toc-more-classification-evaluation-metrics" class="nav-link" data-scroll-target="#more-classification-evaluation-metrics"><span class="header-section-number">4.13</span> 9. More classification evaluation metrics</a></li>
  <li><a href="#연습-문제" id="toc-연습-문제" class="nav-link" data-scroll-target="#연습-문제"><span class="header-section-number">4.14</span> 연습 문제</a></li>
  <li><a href="#추가-학습-자료" id="toc-추가-학습-자료" class="nav-link" data-scroll-target="#추가-학습-자료"><span class="header-section-number">4.15</span> 추가 학습 자료</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">02 - PyTorch 신경망 분류</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/02_pytorch_classification.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<section id="what-is-a-classification-problem" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="what-is-a-classification-problem"><span class="header-section-number">4.1</span> What is a classification problem?</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Statistical_classification">classification problem</a> involves predicting whether something is one thing or another.</p>
<p>For example, you might want to:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Problem type</th>
<th>What is it?</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Binary classification</strong></td>
<td>Target can be one of two options, e.g.&nbsp;yes or no</td>
<td>Predict whether or not someone has heart disease based on their health parameters.</td>
</tr>
<tr class="even">
<td><strong>Multi-class classification</strong></td>
<td>Target can be one of more than two options</td>
<td>Decide whether a photo of is of food, a person or a dog.</td>
</tr>
<tr class="odd">
<td><strong>Multi-label classification</strong></td>
<td>Target can be assigned more than one option</td>
<td>Predict what categories should be assigned to a Wikipedia article (e.g.&nbsp;mathematics, science &amp; philosohpy).</td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-different-classification-problems.png" class="img-fluid figure-img"></p>
<figcaption>various different classification in machine learning such as binary classification, multiclass classification and multilabel classification</figcaption>
</figure>
</div>
<p>Classification, along with regression (predicting a number, covered in <a href="https://www.learnpytorch.io/01_pytorch_workflow/">notebook 01</a>) is one of the most common types of machine learning problems.</p>
<p>In this notebook, we’re going to work through a couple of different classification problems with PyTorch.</p>
<p>In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>
</section>
<section id="이번-장에서-다룰-내용" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="이번-장에서-다룰-내용"><span class="header-section-number">4.2</span> 이번 장에서 다룰 내용</h2>
<p>In this notebook we’re going to reiterate over the PyTorch workflow we coverd in notebook 01.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png" class="img-fluid figure-img"></p>
<figcaption>a pytorch workflow flowchart</figcaption>
</figure>
</div>
<p>Except instead of trying to predict a straight line (predicting a number, also called a regression problem), we’ll be working on a <strong>classification problem</strong>.</p>
<p>Specifically, we’re going to cover:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Topic</strong></th>
<th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>0. Architecture of a classification neural network</strong></td>
<td>Neural networks can come in almost any shape or size, but they typically follow a similar floor plan.</td>
</tr>
<tr class="even">
<td><strong>1. Getting binary classification data ready</strong></td>
<td>Data can be almost anything but to get started we’re going to create a simple binary classification dataset.</td>
</tr>
<tr class="odd">
<td><strong>2. Building a PyTorch classification model</strong></td>
<td>Here we’ll create a model to learn patterns in the data, we’ll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong> specific to classification.</td>
</tr>
<tr class="even">
<td><strong>3. 데이터에 모델 맞추기 (훈련)</strong></td>
<td>We’ve got data and a model, now let’s let the model (try to) find patterns in the (<strong>training</strong>) data.</td>
</tr>
<tr class="odd">
<td><strong>4. 예측 및 모델 평가 (추론)</strong></td>
<td>Our model’s found patterns in the data, let’s compare its findings to the actual (<strong>testing</strong>) data.</td>
</tr>
<tr class="even">
<td><strong>5. Improving a model (from a model perspective)</strong></td>
<td>We’ve trained an evaluated a model but it’s not working, let’s try a few things to improve it.</td>
</tr>
<tr class="odd">
<td><strong>6. Non-linearity</strong></td>
<td>So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines?</td>
</tr>
<tr class="even">
<td><strong>7. Replicating non-linear functions</strong></td>
<td>We used <strong>non-linear functions</strong> to help model non-linear data, but what do these look like?</td>
</tr>
<tr class="odd">
<td><strong>8. 전체 과정 합치기 with multi-class classification</strong></td>
<td>Let’s put everything we’ve done so far for binary classification together with a multi-class classification problem.</td>
</tr>
</tbody>
</table>
</section>
<section id="도움을-받을-수-있는-곳" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="도움을-받을-수-있는-곳"><span class="header-section-number">4.3</span> 도움을 받을 수 있는 곳</h2>
<p>All of the materials for this course <a href="https://github.com/mrdbourke/pytorch-deep-learning">live on GitHub</a>.</p>
<p>And if you run into trouble, you can ask a question on the <a href="https://github.com/mrdbourke/pytorch-deep-learning/discussions">Discussions page</a> there too.</p>
<p>There’s also the <a href="https://discuss.pytorch.org/">PyTorch developer forums</a>, a very helpful place for all things PyTorch.</p>
</section>
<section id="architecture-of-a-classification-neural-network" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="architecture-of-a-classification-neural-network"><span class="header-section-number">4.4</span> 0. Architecture of a classification neural network</h2>
<p>Before we get into writing code, let’s look at the general architecture of a classification neural network.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Hyperparameter</strong></th>
<th><strong>Binary Classification</strong></th>
<th><strong>Multiclass classification</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input layer shape</strong> (<code>in_features</code>)</td>
<td>Same as number of features (e.g.&nbsp;5 for age, sex, height, weight, smoking status in heart disease prediction)</td>
<td>Same as binary classification</td>
</tr>
<tr class="even">
<td><strong>Hidden layer(s)</strong></td>
<td>Problem specific, minimum = 1, maximum = unlimited</td>
<td>Same as binary classification</td>
</tr>
<tr class="odd">
<td><strong>Neurons per hidden layer</strong></td>
<td>Problem specific, generally 10 to 512</td>
<td>Same as binary classification</td>
</tr>
<tr class="even">
<td><strong>Output layer shape</strong> (<code>out_features</code>)</td>
<td>1 (one class or the other)</td>
<td>1 per class (e.g.&nbsp;3 for food, person or dog photo)</td>
</tr>
<tr class="odd">
<td><strong>Hidden layer activation</strong></td>
<td>Usually <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a> (rectified linear unit) but <a href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">can be many others</a></td>
<td>Same as binary classification</td>
</tr>
<tr class="even">
<td><strong>Output activation</strong></td>
<td><a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> (<a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html"><code>torch.sigmoid</code></a> in PyTorch)</td>
<td><a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html"><code>torch.softmax</code></a> in PyTorch)</td>
</tr>
<tr class="odd">
<td><strong>Loss function</strong></td>
<td><a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression">Binary crossentropy</a> (<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code>torch.nn.BCELoss</code></a> in PyTorch)</td>
<td>Cross entropy (<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code>torch.nn.CrossEntropyLoss</code></a> in PyTorch)</td>
</tr>
<tr class="even">
<td><strong>Optimizer</strong></td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD</a> (stochastic gradient descent), <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a> (see <a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a> for more options)</td>
<td>Same as binary classification</td>
</tr>
</tbody>
</table>
<p>Of course, this ingredient list of classification neural network components will vary depending on the problem you’re working on.</p>
<p>But it’s more than enough to get started.</p>
<p>We’re going to gets hands-on with this setup throughout this notebook.</p>
</section>
<section id="make-classification-data-and-get-it-ready" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="make-classification-data-and-get-it-ready"><span class="header-section-number">4.5</span> 1. Make classification data and get it ready</h2>
<p>Let’s begin by making some data.</p>
<p>We’ll use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html"><code>make_circles()</code></a> method from Scikit-Learn to generate two circles with different coloured dots.</p>
<div id="cell-6" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_circles</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Make 1000 samples </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create circles</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                    noise<span class="op">=</span><span class="fl">0.03</span>, <span class="co"># a little bit of noise to the dots</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                    random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># keep random state so we get the same values</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Alright, now let’s view the first 5 <code>X</code> and <code>y</code> values.</p>
<div id="cell-8" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b7316d88-7733-4981-9b4a-0a98c7cdd829" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 X features:</span><span class="ch">\n</span><span class="sc">{</span>X[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 5 y labels:</span><span class="ch">\n</span><span class="sc">{</span>y[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>First 5 X features:
[[ 0.75424625  0.23148074]
 [-0.75615888  0.15325888]
 [-0.81539193  0.17328203]
 [-0.39373073  0.69288277]
 [ 0.44220765 -0.89672343]]

First 5 y labels:
[1 1 1 1 0]</code></pre>
</div>
</div>
<p>Looks like there’s two <code>X</code> values per one <code>y</code> value.</p>
<p>Let’s keep following the data explorer’s motto of <em>visualize, visualize, visualize</em> and put them into a pandas DataFrame.</p>
<div id="cell-10" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:363}}" data-outputid="cd6ef4fe-cda3-48db-f2a5-9820660eab14" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make DataFrame of circle data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>circles <span class="op">=</span> pd.DataFrame({<span class="st">"X1"</span>: X[:, <span class="dv">0</span>],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"X2"</span>: X[:, <span class="dv">1</span>],</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"label"</span>: y</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>circles.head(<span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div id="df-08b6c806-a6a9-436c-8789-ef2c65a52811">
    <div class="colab-df-container">
      <div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">X1</th>
<th data-quarto-table-cell-role="th">X2</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>0.754246</td>
<td>0.231481</td>
<td>1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>-0.756159</td>
<td>0.153259</td>
<td>1</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>-0.815392</td>
<td>0.173282</td>
<td>1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>-0.393731</td>
<td>0.692883</td>
<td>1</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>0.442208</td>
<td>-0.896723</td>
<td>0</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">5</th>
<td>-0.479646</td>
<td>0.676435</td>
<td>1</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">6</th>
<td>-0.013648</td>
<td>0.803349</td>
<td>1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">7</th>
<td>0.771513</td>
<td>0.147760</td>
<td>1</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">8</th>
<td>-0.169322</td>
<td>-0.793456</td>
<td>1</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">9</th>
<td>-0.121486</td>
<td>1.021509</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-08b6c806-a6a9-436c-8789-ef2c65a52811')" title="Convert this dataframe to an interactive table." style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"></path>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"></path><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"></path>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-08b6c806-a6a9-436c-8789-ef2c65a52811 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-08b6c806-a6a9-436c-8789-ef2c65a52811');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
</div>
</div>
<p>It looks like each pair of <code>X</code> features (<code>X1</code> and <code>X2</code>) has a label (<code>y</code>) value of either 0 or 1.</p>
<p>This tells us that our problem is <strong>binary classification</strong> since there’s only two options (0 or 1).</p>
<p>How many values of each class is there?</p>
<div id="cell-12" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="dee5739f-1695-4e71-bb0b-de270f08b621" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check different labels</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>circles.label.value_counts()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>1    500
0    500
Name: label, dtype: int64</code></pre>
</div>
</div>
<p>500 each, nice and balanced.</p>
<p>Let’s plot them.</p>
<div id="cell-14" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:265}}" data-outputid="89b2f9ac-728c-481d-f4ba-6192a8334758" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with a plot</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(x<span class="op">=</span>X[:, <span class="dv">0</span>], </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>X[:, <span class="dv">1</span>], </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>            c<span class="op">=</span>y, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>plt.cm.RdYlBu)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Alrighty, looks like we’ve got a problem to solve.</p>
<p>Let’s find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> This dataset is often what’s considered a <strong>toy problem</strong> (a problem that’s used to try and test things out on) in machine learning.</p>
<p>But it represents the major key of classification, you have some kind of data represented as numerical values and you’d like to build a model that’s able to classify it, in our case, separate it into red or blue dots.</p>
</blockquote>
<section id="input-and-output-shapes" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="input-and-output-shapes"><span class="header-section-number">4.5.1</span> 1.1 Input and output shapes</h3>
<p>One of the most common errors in deep learning is shape errors.</p>
<p>Mismatching the shapes of tensors and tensor operations with result in errors in your models.</p>
<p>We’re going to see plenty of these throughout the course.</p>
<p>And there’s no surefire way to making sure they won’t happen, they will.</p>
<p>What you can do instead is continaully familiarize yourself with the shape of the data you’re working with.</p>
<p>I like referring to it as input and output shapes.</p>
<p>Ask yourself:</p>
<p>“What shapes are my inputs and what shapes are my outputs?”</p>
<p>Let’s find out.</p>
<div id="cell-17" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="16e65bb5-83e1-49c7-af5e-90ecede4eeae" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the shapes of our features and labels</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X.shape, y.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>((1000, 2), (1000,))</code></pre>
</div>
</div>
<p>Looks like we’ve got a match on the first dimension of each.</p>
<p>There’s 1000 <code>X</code> and 1000 <code>y</code>.</p>
<p>But what’s the second dimension on <code>X</code>?</p>
<p>It often helps to view the values and shapes of a single sample (features and labels).</p>
<p>Doing so will help you understand what input and output shapes you’d be expecting from your model.</p>
<div id="cell-19" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b3a96ca8-45f1-47d1-a98b-c2a7be79c0d5" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View the first example of features and labels</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>X_sample <span class="op">=</span> X[<span class="dv">0</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>y_sample <span class="op">=</span> y[<span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Values for one sample of X: </span><span class="sc">{</span>X_sample<span class="sc">}</span><span class="ss"> and the same for y: </span><span class="sc">{</span>y_sample<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shapes for one sample of X: </span><span class="sc">{</span>X_sample<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> and the same for y: </span><span class="sc">{</span>y_sample<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1
Shapes for one sample of X: (2,) and the same for y: ()</code></pre>
</div>
</div>
<p>This tells us the second dimension for <code>X</code> means it has two features (vector) where as <code>y</code> has a single feature (scalar).</p>
<p>We have two inputs for one output.</p>
</section>
<section id="turn-data-into-tensors-and-create-train-and-test-splits" class="level3" data-number="4.5.2">
<h3 data-number="4.5.2" class="anchored" data-anchor-id="turn-data-into-tensors-and-create-train-and-test-splits"><span class="header-section-number">4.5.2</span> 1.2 Turn data into tensors and create train and test splits</h3>
<p>We’ve investigated the input and output shapes of our data, now let’s prepare it for being used with PyTorch and for modelling.</p>
<p>Specifically, we’ll need to: 1. Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors). 2. Split our data into training and test sets (we’ll train a model on the training set to learn the patterns between <code>X</code> and <code>y</code> and then evaluate those learned patterns on the test dataset).</p>
<div id="cell-22" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="cd4e10c1-c358-4b74-81e0-bab7610197cc" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn data into tensors</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Otherwise this causes issues with computations later on</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.from_numpy(X).<span class="bu">type</span>(torch.<span class="bu">float</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.from_numpy(y).<span class="bu">type</span>(torch.<span class="bu">float</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View the first five samples</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X[:<span class="dv">5</span>], y[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(tensor([[ 0.7542,  0.2315],
         [-0.7562,  0.1533],
         [-0.8154,  0.1733],
         [-0.3937,  0.6929],
         [ 0.4422, -0.8967]]), tensor([1., 1., 1., 1., 0.]))</code></pre>
</div>
</div>
<p>Now our data is in tensor format, let’s split it into training and test sets.</p>
<p>To do so, let’s use the helpful function <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code>train_test_split()</code></a> from Scikit-Learn.</p>
<p>We’ll use <code>test_size=0.2</code> (80% training, 20% testing) and because the split happens randomly across the data, let’s use <code>random_state=42</code> so the split is reproducible.</p>
<div id="cell-24" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a2d3f3df-701e-44ef-a506-fd31d5443e90" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into train and test sets</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>                                                    y, </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>                                                    test_size<span class="op">=</span><span class="fl">0.2</span>, <span class="co"># 20% test, 80% train</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                                                    random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># make the random split reproducible</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(X_train), <span class="bu">len</span>(X_test), <span class="bu">len</span>(y_train), <span class="bu">len</span>(y_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(800, 200, 800, 200)</code></pre>
</div>
</div>
<p>Nice! Looks like we’ve now got 800 training samples and 200 testing samples.</p>
</section>
</section>
<section id="모델-구축하기" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="모델-구축하기"><span class="header-section-number">4.6</span> 2. 모델 구축하기</h2>
<p>We’ve got some data ready, now it’s time to build a model.</p>
<p>We’ll break it down into a few parts.</p>
<ol type="1">
<li>Setting up device agnostic code (so our model can run on CPU or GPU if it’s available).</li>
<li>Constructing a model by subclassing <code>nn.Module</code>.</li>
<li>Defining a loss function and optimizer.</li>
<li>Creating a training loop (this’ll be in the next section).</li>
</ol>
<p>The good news is we’ve been through all of the above steps before in notebook 01.</p>
<p>Except now we’ll be adjusting them so they work with a classification dataset.</p>
<p>Let’s start by importing PyTorch and <code>torch.nn</code> as well as setting up device agnostic code.</p>
<div id="cell-27" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:36}}" data-outputid="507bf4a3-b5ae-4943-aa21-4eb181b0d741" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard PyTorch imports</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Make device agnostic code</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>'cuda'</code></pre>
</div>
</div>
<p>Excellent, now <code>device</code> is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it’s available.</p>
<p>How about we create a model?</p>
<p>We’ll want a model capable of handling our <code>X</code> data as inputs and producing something in the shape of our <code>y</code> data as ouputs.</p>
<p>In other words, given <code>X</code> (features) we want our model to predict <code>y</code> (label).</p>
<p>This setup where you have features and labels is referred to as <strong>supervised learning</strong>. Because your data is telling your model what the outputs should be given a certain input.</p>
<p>To create such a model it’ll need to handle the input and output shapes of <code>X</code> and <code>y</code>.</p>
<p>Remember how I said input and output shapes are important? Here we’ll see why.</p>
<p>Let’s create a model class that: 1. Subclasses <code>nn.Module</code> (almost all PyTorch models are subclasses of <code>nn.Module</code>). 2. Creates 2 <code>nn.Linear</code> layers in the constructor capable of handling the input and output shapes of <code>X</code> and <code>y</code>. 3. Defines a <code>forward()</code> method containing the forward pass computation of the model. 4. Instantiates the model class and sends it to the target <code>device</code>.</p>
<div id="cell-29" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="d9c976ea-e23f-4993-c847-a15b0bf7b0b5" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Construct a model class that subclasses nn.Module</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CircleModelV0(nn.Module):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_1 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">5</span>) <span class="co"># takes in 2 features (X), produces 5 features</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_2 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">5</span>, out_features<span class="op">=</span><span class="dv">1</span>) <span class="co"># takes in 5 features, produces 1 feature (y)</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Define a forward method containing the forward pass computation</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the output of layer_2, a single feature, the same shape as y</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_2(<span class="va">self</span>.layer_1(x)) <span class="co"># computation goes through layer_1 first then the output of layer_1 goes through layer_2</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Create an instance of the model and send it to target device</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>model_0 <span class="op">=</span> CircleModelV0().to(device)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>model_0</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>CircleModelV0(
  (layer_1): Linear(in_features=2, out_features=5, bias=True)
  (layer_2): Linear(in_features=5, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<p>What’s going on here?</p>
<p>We’ve seen a few of these steps before.</p>
<p>The only major change is what’s happening between <code>self.layer_1</code> and <code>self.layer_2</code>.</p>
<p><code>self.layer_1</code> takes 2 input features <code>in_features=2</code> and produces 5 output features <code>out_features=5</code>.</p>
<p>This is known as having 5 <strong>hidden units</strong> or <strong>neurons</strong>.</p>
<p>This layer turns the input data from having 2 features to 5 features.</p>
<p>Why do this?</p>
<p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, <em>potentially</em> leading to better outputs.</p>
<p>I say potentially because sometimes it doesn’t work.</p>
<p>The number of hidden units you can use in neural network layers is a <strong>hyperparameter</strong> (a value you can set yourself) and there’s no set in stone value you have to use.</p>
<p>Generally more is better but there’s also such a thing as too much. The amount you choose will depend on your model type and dataset you’re working with.</p>
<p>Since our dataset is small and simple, we’ll keep it small.</p>
<p>The only rule with hidden units is that the next layer, in our case, <code>self.layer_2</code> has to take the same <code>in_features</code> as the previous layer <code>out_features</code>.</p>
<p>That’s why <code>self.layer_2</code> has <code>in_features=5</code>, it takes the <code>out_features=5</code> from <code>self.layer_1</code> and performs a linear computation on them, turning them into <code>out_features=1</code> (the same shape as <code>y</code>).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-tensorflow-playground-linear-activation.png" class="img-fluid" alt="A visual example of what a classification neural network with linear activation looks like on the tensorflow playground"> <em>A visual example of what a similar classificiation neural network to the one we’ve just built looks like. Try create one of your own on the <a href="https://playground.tensorflow.org/">TensorFlow Playground website</a>.</em></p>
<p>You can also do the same as above using <a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code>nn.Sequential</code></a>.</p>
<p><code>nn.Sequential</code> performs a forward pass computation of the input data through the layers in the order they appear.</p>
<div id="cell-31" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="def65504-863a-4361-816e-909dbfa4d624" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Replicate CircleModelV0 with nn.Sequential</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>model_0 <span class="op">=</span> nn.Sequential(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">5</span>),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">5</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>).to(device)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>model_0</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>Sequential(
  (0): Linear(in_features=2, out_features=5, bias=True)
  (1): Linear(in_features=5, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<p>Woah, that looks much simpler than subclassing <code>nn.Module</code>, why not just always use <code>nn.Sequential</code>?</p>
<p><code>nn.Sequential</code> is fantastic for straight-forward computations, however, as the namespace says, it <em>always</em> runs in sequential order.</p>
<p>So if you’d something else to happen (rather than just straight-forward sequential computation) you’ll want to define your own custom <code>nn.Module</code> subclass.</p>
<p>Now we’ve got a model, let’s see what happens when we pass some data through it.</p>
<div id="cell-33" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a4014167-4181-434e-ab75-905094229b3a" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions with the model</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>untrained_preds <span class="op">=</span> model_0(X_test.to(device))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Length of predictions: </span><span class="sc">{</span><span class="bu">len</span>(untrained_preds)<span class="sc">}</span><span class="ss">, Shape: </span><span class="sc">{</span>untrained_preds<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Length of test samples: </span><span class="sc">{</span><span class="bu">len</span>(y_test)<span class="sc">}</span><span class="ss">, Shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 10 predictions:</span><span class="ch">\n</span><span class="sc">{</span>untrained_preds[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">First 10 test labels:</span><span class="ch">\n</span><span class="sc">{</span>y_test[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Length of predictions: 200, Shape: torch.Size([200, 1])
Length of test samples: 200, Shape: torch.Size([200])

First 10 predictions:
tensor([[0.7311],
        [0.7607],
        [0.4336],
        [0.8163],
        [0.0846],
        [0.1053],
        [0.4653],
        [0.3110],
        [0.4488],
        [0.7589]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

First 10 test labels:
tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])</code></pre>
</div>
</div>
<p>Hmm, it seems there’s the same amount of predictions as there is test labels but the predictions don’t look like they’re in the same form or shape as the test labels.</p>
<p>We’ve got a couple steps we can do to fix this, we’ll see these later on.</p>
<section id="setup-loss-function-and-optimizer" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="setup-loss-function-and-optimizer"><span class="header-section-number">4.6.1</span> 2.1 Setup loss function and optimizer</h3>
<p>We’ve setup a loss (also called a criterion or cost function) and optimizer before in <a href="https://www.learnpytorch.io/01_pytorch_workflow/#creating-a-loss-function-and-optimizer-in-pytorch">notebook 01</a>.</p>
<p>But different problem types require different loss functions.</p>
<p>For example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.</p>
<p>And for a binary classification problem (like ours), you’ll often use <a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">binary cross entropy</a> as the loss function.</p>
<p>However, the same optimizer function can often be used across different problem spaces.</p>
<p>For example, the stochastic gradient descent optimizer (SGD, <code>torch.optim.SGD()</code>) can be used for a range of problems, so can too the Adam optimizer (<code>torch.optim.Adam()</code>).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Loss function/Optimizer</th>
<th>Problem type</th>
<th>PyTorch Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stochastic Gradient Descent (SGD) optimizer</td>
<td>Classification, regression, many others.</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"><code>torch.optim.SGD()</code></a></td>
</tr>
<tr class="even">
<td>Adam Optimizer</td>
<td>Classification, regression, many others.</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code>torch.optim.Adam()</code></a></td>
</tr>
<tr class="odd">
<td>Binary cross entropy loss</td>
<td>Binary classification</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code>torch.nn.BCELossWithLogits</code></a> or <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code>torch.nn.BCELoss</code></a></td>
</tr>
<tr class="even">
<td>Cross entropy loss</td>
<td>Mutli-class classification</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code>torch.nn.CrossEntropyLoss</code></a></td>
</tr>
<tr class="odd">
<td>Mean absolute error (MAE) or L1 Loss</td>
<td>Regression</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"><code>torch.nn.L1Loss</code></a></td>
</tr>
<tr class="even">
<td>Mean squared error (MSE) or L2 Loss</td>
<td>Regression</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss"><code>torch.nn.MSELoss</code></a></td>
</tr>
</tbody>
</table>
<p><em>Table of various loss functions and optimizers, there are more but these some common ones you’ll see.</em></p>
<p>Since we’re working with a binary classification problem, let’s use a binary cross entropy loss function.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> Recall a <strong>loss function</strong> is what measures how <em>wrong</em> your model predictions are, the higher the loss, the worse your model.</p>
<p>Also, PyTorch documentation often refers to loss functions as “loss criterion” or “criterion”, these are all different ways of describing the same thing.</p>
</blockquote>
<p>PyTorch has two binary cross entropy implementations: 1. <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code>torch.nn.BCELoss()</code></a> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features). 2. <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code>torch.nn.BCEWithLogitsLoss()</code></a> - This is the same as above except it has a sigmoid layer (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html"><code>nn.Sigmoid</code></a>) built-in (we’ll see what this means soon).</p>
<p>Which one should you use?</p>
<p>The <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">documentation for <code>torch.nn.BCEWithLogitsLoss()</code></a> states that it’s more numerically stable than using <code>torch.nn.BCELoss()</code> after a <code>nn.Sigmoid</code> layer.</p>
<p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code>nn.Sigmoid</code> and <code>torch.nn.BCELoss()</code> but that is beyond the scope of this notebook.</p>
<p>Knowing this, let’s create a loss function and an optimizer.</p>
<p>For the optimizer we’ll use <code>torch.optim.SGD()</code> to optimize the model parameters with learning rate 0.1.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> There’s a <a href="https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4">discussion on the PyTorch forums about the use of <code>nn.BCELoss</code> vs.&nbsp;<code>nn.BCEWithLogitsLoss</code></a>. It can be confusing at first but as with many things, it becomes easier with practice.</p>
</blockquote>
<div id="cell-36" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a loss function</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.BCEWithLogitsLoss() <span class="co"># BCEWithLogitsLoss = sigmoid built-in</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an optimizer</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params<span class="op">=</span>model_0.parameters(), </span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>                            lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now let’s also create an <strong>evaluation metric</strong>.</p>
<p>An evaluation metric can be used to offer another perspective on how your model is going.</p>
<p>If a loss function measures how <em>wrong</em> your model is, I like to think of evaluation metrics as measuring how <em>right</em> it is.</p>
<p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p>
<p>After all, when evaluating your models it’s good to look at things from multiple points of view.</p>
<p>There are several evaluation metrics that can be used for classification problems but let’s start out with <strong>accuracy</strong>.</p>
<p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p>
<p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p>
<p>Let’s write a function to do so.</p>
<div id="cell-38" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy (a classification metric)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> accuracy_fn(y_true, y_pred):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> torch.eq(y_true, y_pred).<span class="bu">sum</span>().item() <span class="co"># torch.eq() calculates where two tensors are equal</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> (correct <span class="op">/</span> <span class="bu">len</span>(y_pred)) <span class="op">*</span> <span class="dv">100</span> </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Excellent! We can now use this function whilst training our model to measure it’s performance alongside the loss.</p>
</section>
</section>
<section id="train-model" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="train-model"><span class="header-section-number">4.7</span> 3. Train model</h2>
<p>Okay, now we’ve got a loss function and optimizer ready to go, let’s train a model.</p>
<p>Do you remember the steps in a PyTorch training loop?</p>
<p>If not, here’s a reminder.</p>
<p>Steps in training:</p>
<details>
<summary>
PyTorch training loop steps
</summary>
<ol>
<li>
<b>Forward pass</b> - The model goes through all of the training data once, performing its <code>forward()</code> function calculations (<code>model(x_train)</code>).
</li>
<li>
<b>Calculate the loss</b> - The model’s outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).
</li>
<li>
<b>Zero gradients</b> - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (<code>optimizer.zero_grad()</code>).
</li>
<li>
<b>Perform backpropagation on the loss</b> - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with <code>requires_grad=True</code>). This is known as <b>backpropagation</b>, hence “backwards” (<code>loss.backward()</code>).
</li>
<li>
<b>Step the optimizer (gradient descent)</b> - Update the parameters with <code>requires_grad=True</code> with respect to the loss gradients in order to improve them (<code>optimizer.step()</code>).
</li>
</ol>
</details>
<section id="going-from-raw-model-outputs-to-predicted-labels-logits---prediction-probabilities---prediction-labels" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="going-from-raw-model-outputs-to-predicted-labels-logits---prediction-probabilities---prediction-labels"><span class="header-section-number">4.7.1</span> 3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)</h3>
<p>Before we the training loop steps, let’s see what comes out of our model during the forward pass (the forward pass is defined by the <code>foward()</code> method).</p>
<p>To do so, let’s pass the model some data.</p>
<div id="cell-42" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="829a0842-0a37-455a-b058-3e547200f836" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View the frist 5 outputs of the forward pass on the test data</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>y_logits <span class="op">=</span> model_0(X_test.to(device))[:<span class="dv">5</span>]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>y_logits</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([[0.7311],
        [0.7607],
        [0.4336],
        [0.8163],
        [0.0846]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Since our model hasn’t been trained, these outputs are basically random.</p>
<p>But <em>what</em> are they?</p>
<p>They’re the output of our <code>forward()</code> method.</p>
<p>Which implements two layers of <code>nn.Linear()</code> which internally calls the following equation:</p>
<p><span class="math display">\[
\mathbf{y} = x \cdot \mathbf{Weights}^T  + \mathbf{bias}
\]</span></p>
<p>The <em>raw outputs</em> (unmodified) of this equation (<span class="math inline">\(\mathbf{y}\)</span>) and in turn, the raw outputs of our model are often referred to as <a href="https://datascience.stackexchange.com/a/31045"><strong>logits</strong></a>.</p>
<p>That’s what our model is outputing above when it takes in the input data (<span class="math inline">\(x\)</span> in the equation or <code>X_test</code> in the code), logits.</p>
<p>However, these numbers are hard to interpret.</p>
<p>We’d like some numbers that are comparable to our truth labels.</p>
<p>To get our model’s raw outputs (logits) into such a form, we can use the <a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html">sigmoid activation function</a>.</p>
<p>Let’s try it out.</p>
<div id="cell-44" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="3564ee3b-e34f-40a3-a40a-b9c9b948da04" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use sigmoid on model logits</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>y_pred_probs <span class="op">=</span> torch.sigmoid(y_logits)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>y_pred_probs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([[0.6751],
        [0.6815],
        [0.6067],
        [0.6935],
        [0.5211]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>Okay, it seems like the outputs now have some kind of consistency (even though they’re still random).</p>
<p>They’re now in the form of <strong>prediction probabilities</strong> (I usually refer to these as <code>y_pred_probs</code>), in other words, the values are now how much the model thinks the data point belongs to one class or another.</p>
<p>In our case, since we’re dealing with binary classification, our ideal outputs are 0 or 1.</p>
<p>So these values can be viewed as a decision boundary.</p>
<p>The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.</p>
<p>More specificially: * If <code>y_pred_probs</code> &gt;= 0.5, <code>y=1</code> (class 1) * If <code>y_pred_probs</code> &lt; 0.5, <code>y=0</code> (class 0)</p>
<p>To turn our prediction probabilities in prediction labels, we can round the outputs of the sigmoid activation function.</p>
<div id="cell-46" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6134e47d-bbb2-46c3-e2ec-b4f42d517e39" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the predicted labels (round the prediction probabilities)</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> torch.<span class="bu">round</span>(y_pred_probs)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># In full</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>y_pred_labels <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(model_0(X_test.to(device))[:<span class="dv">5</span>]))</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Check for equality</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get rid of extra dimension</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>y_preds.squeeze()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([True, True, True, True, True], device='cuda:0')</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>tensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)</code></pre>
</div>
</div>
<p>Excellent! Now it looks like our model’s predictions are in the same form as our truth labels (<code>y_test</code>).</p>
<div id="cell-48" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="6b1cd7b4-f1d0-49b5-a8c3-338cf75c6cb0" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>y_test[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>tensor([1., 0., 1., 0., 1.])</code></pre>
</div>
</div>
<p>This means we’ll be able to compare our models predictions to the test labels to see how well it’s going.</p>
<p>To recap, we converted our model’s raw outputs (logits) to predicition probabilities using a sigmoid activation function.</p>
<p>And then converted the prediction probabilities to prediction labels by rounding them.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we’ll be looking at using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">softmax activation function</a> (this will come later on).</p>
<p>And the use of the sigmoid activation function is not required when passing our model’s raw outputs to the <code>nn.BCEWithLogitsLoss</code> (the “logits” in logits loss is because it works on the model’s raw logits output), this is because it has a sigmoid function built-in.</p>
</blockquote>
</section>
<section id="building-a-training-and-testing-loop" class="level3" data-number="4.7.2">
<h3 data-number="4.7.2" class="anchored" data-anchor-id="building-a-training-and-testing-loop"><span class="header-section-number">4.7.2</span> 3.2 Building a training and testing loop</h3>
<p>Alright, we’ve discussed how to take our raw model outputs and convert them to prediction labels, now let’s build a training loop.</p>
<p>Let’s start by training for 100 epochs and outputing the model’s progress every 10 epochs.</p>
<div id="cell-51" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e0341074-b603-41d5-a389-c401b4934d73" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of epochs</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Put data to target device</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> X_train.to(device), y_train.to(device)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> X_test.to(device), y_test.to(device)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Build training and evaluation loop</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    model_0.train()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Forward pass (model outputs raw logits)</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    y_logits <span class="op">=</span> model_0(X_train).squeeze() <span class="co"># squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device </span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(y_logits)) <span class="co"># turn logits -&gt; pred probs -&gt; pred labls</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate loss/accuracy</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                y_train) </span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_logits, <span class="co"># Using nn.BCEWithLogitsLoss works with raw logits</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>                   y_train) </span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_train, </span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>                      y_pred<span class="op">=</span>y_pred) </span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Loss backwards</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Optimizer step</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Testing</span></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>    model_0.<span class="bu">eval</span>()</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Forward pass</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>        test_logits <span class="op">=</span> model_0(X_test).squeeze() </span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>        test_pred <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(test_logits))</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Caculate loss/accuracy</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">=</span> loss_fn(test_logits,</span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>                            y_test)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_test,</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>                               y_pred<span class="op">=</span>test_pred)</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out what's happening every 10 epochs</span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">% | Test loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">, Test acc: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0 | Loss: 0.72095, Accuracy: 50.00% | Test loss: 0.72767, Test acc: 50.50%
Epoch: 10 | Loss: 0.70546, Accuracy: 53.87% | Test loss: 0.71203, Test acc: 52.50%
Epoch: 20 | Loss: 0.70011, Accuracy: 52.25% | Test loss: 0.70601, Test acc: 49.50%
Epoch: 30 | Loss: 0.69792, Accuracy: 51.50% | Test loss: 0.70315, Test acc: 49.50%
Epoch: 40 | Loss: 0.69682, Accuracy: 51.12% | Test loss: 0.70148, Test acc: 48.50%
Epoch: 50 | Loss: 0.69613, Accuracy: 50.75% | Test loss: 0.70034, Test acc: 49.50%
Epoch: 60 | Loss: 0.69565, Accuracy: 51.00% | Test loss: 0.69948, Test acc: 49.50%
Epoch: 70 | Loss: 0.69527, Accuracy: 50.75% | Test loss: 0.69880, Test acc: 50.00%
Epoch: 80 | Loss: 0.69497, Accuracy: 50.62% | Test loss: 0.69825, Test acc: 49.50%
Epoch: 90 | Loss: 0.69472, Accuracy: 50.62% | Test loss: 0.69779, Test acc: 49.50%</code></pre>
</div>
</div>
<p>Hmm, what do you notice about the performance of our model?</p>
<p>It looks like it went through the training and testing steps fine but the results don’t seem to have moved too much.</p>
<p>The accuracy barely moves above 50% on each data split.</p>
<p>And because we’re working with a balanced binary classification problem, it means our model is performing as good as random guessing (with 500 samples of class 0 and class 1 a model predicting class 1 every single time would achieve 50% accuracy).</p>
</section>
</section>
<section id="make-predictions-and-evaluate-the-model" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="make-predictions-and-evaluate-the-model"><span class="header-section-number">4.8</span> 4. Make predictions and evaluate the model</h2>
<p>From the metrics it looks like our model is random guessing.</p>
<p>How could we investigate this further?</p>
<p>I’ve got an idea.</p>
<p>The data explorer’s motto!</p>
<p>“Visualize, visualize, visualize!”</p>
<p>Let’s make a plot of our model’s predictions, the data it’s trying to predict on and the decision boundary it’s creating for whether something is class 0 or class 1.</p>
<p>To do so, we’ll write some code to download and import the <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"><code>helper_functions.py</code> script</a> from the <a href="https://github.com/mrdbourke/pytorch-deep-learning">Learn PyTorch for Deep Learning repo</a>.</p>
<p>It contains a helpful function called <code>plot_decision_boundary()</code> which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.</p>
<p>We’ll also import <code>plot_predictions()</code> which we wrote in notebook 01 to use later.</p>
<div id="cell-54" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4f63a8a3-5eae-49fb-e17e-df6c5721ab5a" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path </span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Download helper functions from Learn PyTorch repo (if not already downloaded)</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> Path(<span class="st">"helper_functions.py"</span>).is_file():</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"helper_functions.py already exists, skipping download"</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"Downloading helper_functions.py"</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>  request <span class="op">=</span> requests.get(<span class="st">"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py"</span>)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"helper_functions.py"</span>, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    f.write(request.content)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> helper_functions <span class="im">import</span> plot_predictions, plot_decision_boundary</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading helper_functions.py</code></pre>
</div>
</div>
<div id="cell-55" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:390}}" data-outputid="aa344fe8-0207-43df-f0e9-61b9302459a5" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot decision boundaries for training and test sets</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Train"</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_0, X_train, y_train)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Test"</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_0, X_test, y_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Oh wow, it seems like we’ve found the cause of model’s performance issue.</p>
<p>It’s currently trying to split the red and blue dots using a straight line…</p>
<p>That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.</p>
<p>In machine learning terms, our model is <strong>underfitting</strong>, meaning it’s not learning predictive patterns from the data.</p>
<p>How could we improve this?</p>
</section>
<section id="improving-a-model-from-a-model-perspective" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="improving-a-model-from-a-model-perspective"><span class="header-section-number">4.9</span> 5. Improving a model (from a model perspective)</h2>
<p>Let’s try to fix our model’s underfitting problem.</p>
<p>Focusing specifically on the model (not the data), there are a few ways we could do this.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model improvement technique*</th>
<th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Add more layers</strong></td>
<td>Each layer <em>potentially</em> increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network <em>deeper</em>.</td>
</tr>
<tr class="even">
<td><strong>Add more hidden units</strong></td>
<td>Similar to the above, more hidden units per layer means a <em>potential</em> increase in learning capabilities of the model, more hidden units is often referred to as making your neural network <em>wider</em>.</td>
</tr>
<tr class="odd">
<td><strong>Fitting for longer (more epochs)</strong></td>
<td>Your model might learn more if it had more opportunities to look at the data.</td>
</tr>
<tr class="even">
<td><strong>Changing the activation functions</strong></td>
<td>Some data just can’t be fit with only straight lines (like what we’ve seen), using non-linear activation functions can help with this (hint, hint).</td>
</tr>
<tr class="odd">
<td><strong>Change the learning rate</strong></td>
<td>Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn’t learn enough.</td>
</tr>
<tr class="even">
<td><strong>Change the loss function</strong></td>
<td>Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won’t work with a multi-class classification problem.</td>
</tr>
<tr class="odd">
<td><strong>Use transfer learning</strong></td>
<td>Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in <a href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">notebook 06</a>.</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>참고:</strong> *because you can adjust all of these by hand, they’re referred to as <strong>hyperparameters</strong>.</p>
<p>And this is also where machine learning’s half art half science comes in, there’s no real way to know here what the best combination of values is for your project, best to follow the data scientist’s motto of “experiment, experiment, experiment”.</p>
</blockquote>
<p>Let’s see what happens if we add an extra layer to our model, fit for longer (<code>epochs=1000</code> instead of <code>epochs=100</code>) and increase the number of hidden units from <code>5</code> to <code>10</code>.</p>
<p>We’ll follow the same steps we did above but with a few changed hyperparameters.</p>
<div id="cell-58" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9174e74b-862a-4bca-9f17-c7176d22f81e" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CircleModelV1(nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_1 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_2 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">10</span>) <span class="co"># extra layer</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_3 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): <span class="co"># note: always make sure forward is spelt correctly!</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Creating a model like this is the same as below, though below</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># generally benefits from speedups where possible.</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># z = self.layer_1(x)</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># z = self.layer_2(z)</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># z = self.layer_3(z)</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return z</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layer_3(<span class="va">self</span>.layer_2(<span class="va">self</span>.layer_1(x)))</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>model_1 <span class="op">=</span> CircleModelV1().to(device)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>model_1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>CircleModelV1(
  (layer_1): Linear(in_features=2, out_features=10, bias=True)
  (layer_2): Linear(in_features=10, out_features=10, bias=True)
  (layer_3): Linear(in_features=10, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<p>Now we’ve got a model, we’ll recreate a loss function and optimizer instance, using the same settings as before.</p>
<div id="cell-60" class="cell" data-execution_count="24">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># loss_fn = nn.BCELoss() # Requires sigmoid on input</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.BCEWithLogitsLoss() <span class="co"># Does not require sigmoid on input</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model_1.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Beautiful, model, optimizer and loss function ready, let’s make a training loop.</p>
<p>This time we’ll train for longer (<code>epochs=1000</code> vs <code>epochs=100</code>) and see if it improves our model.</p>
<div id="cell-62" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="b00e48e9-1075-4f6e-c0c3-e511451d3fe9" data-execution_count="25">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span> <span class="co"># Train for longer</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Put data to target device</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> X_train.to(device), y_train.to(device)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> X_test.to(device), y_test.to(device)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Forward pass</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    y_logits <span class="op">=</span> model_1(X_train).squeeze()</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(y_logits)) <span class="co"># logits -&gt; predicition probabilities -&gt; prediction labels</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate loss/accuracy</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_logits, y_train)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_train, </span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>                      y_pred<span class="op">=</span>y_pred)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Loss backwards</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Optimizer step</span></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Testing</span></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    model_1.<span class="bu">eval</span>()</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Forward pass</span></span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>        test_logits <span class="op">=</span> model_1(X_test).squeeze() </span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>        test_pred <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(test_logits))</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Caculate loss/accuracy</span></span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>        test_loss <span class="op">=</span> loss_fn(test_logits,</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>                            y_test)</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_test,</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>                               y_pred<span class="op">=</span>test_pred)</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out what's happening every 10 epochs</span></span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">% | Test loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">, Test acc: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%
Epoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%
Epoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%
Epoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%
Epoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%
Epoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%
Epoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</code></pre>
</div>
</div>
<p>What? Our model trained for longer and with an extra layer but it still looks like it didn’t learn any patterns better than random guessing.</p>
<p>Let’s visualize.</p>
<div id="cell-64" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:390}}" data-outputid="12d163f9-b602-459e-f34b-54c58abf8d7d" data-execution_count="26">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot decision boundaries for training and test sets</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Train"</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_1, X_train, y_train)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Test"</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_1, X_test, y_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Hmmm.</p>
<p>Our model is still drawing a straight line between the red and blue dots.</p>
<p>If our model is drawing a straight line, could it model linear data? Like we did in <a href="https://www.learnpytorch.io/01_pytorch_workflow/">notebook 01</a>?</p>
<section id="preparing-data-to-see-if-our-model-can-model-a-straight-line" class="level3" data-number="4.9.1">
<h3 data-number="4.9.1" class="anchored" data-anchor-id="preparing-data-to-see-if-our-model-can-model-a-straight-line"><span class="header-section-number">4.9.1</span> 5.1 Preparing data to see if our model can model a straight line</h3>
<p>Let’s create some linear data to see if our model’s able to model it and we’re not just using a model that can’t learn anything.</p>
<div id="cell-67" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="176a4674-3142-420c-bace-97cdbfbf473e" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some data (same as notebook 01)</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>end <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>X_regression <span class="op">=</span> torch.arange(start, end, step).unsqueeze(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>y_regression <span class="op">=</span> weight <span class="op">*</span> X_regression <span class="op">+</span> bias <span class="co"># linear regression formula</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the data</span></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(X_regression))</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>X_regression[:<span class="dv">5</span>], y_regression[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>100</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>(tensor([[0.0000],
         [0.0100],
         [0.0200],
         [0.0300],
         [0.0400]]), tensor([[0.3000],
         [0.3070],
         [0.3140],
         [0.3210],
         [0.3280]]))</code></pre>
</div>
</div>
<p>Wonderful, now let’s split our data into training and test sets.</p>
<div id="cell-69" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2d9144d9-18d9-427a-a898-7a5c920cd9aa" data-execution_count="28">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create train and test splits</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>train_split <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(X_regression)) <span class="co"># 80% of data used for training set</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>X_train_regression, y_train_regression <span class="op">=</span> X_regression[:train_split], y_regression[:train_split]</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>X_test_regression, y_test_regression <span class="op">=</span> X_regression[train_split:], y_regression[train_split:]</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the lengths of each split</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(X_train_regression), </span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">len</span>(y_train_regression), </span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">len</span>(X_test_regression), </span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">len</span>(y_test_regression))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>80 80 20 20</code></pre>
</div>
</div>
<p>Beautiful, let’s see how the data looks.</p>
<p>To do so, we’ll use the <code>plot_predictions()</code> function we created in notebook 01.</p>
<p>It’s contained within the <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"><code>helper_functions.py</code> script</a> on the Learn PyTorch for Deep Learning repo which we downloaded above.</p>
<div id="cell-71" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:428}}" data-outputid="77b50411-c589-4d5f-e7ca-d03b1b9a68cc" data-execution_count="29">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>plot_predictions(train_data<span class="op">=</span>X_train_regression,</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    train_labels<span class="op">=</span>y_train_regression,</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    test_data<span class="op">=</span>X_test_regression,</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    test_labels<span class="op">=</span>y_test_regression</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="adjusting-model_1-to-fit-a-straight-line" class="level3" data-number="4.9.2">
<h3 data-number="4.9.2" class="anchored" data-anchor-id="adjusting-model_1-to-fit-a-straight-line"><span class="header-section-number">4.9.2</span> 5.2 Adjusting <code>model_1</code> to fit a straight line</h3>
<p>Now we’ve got some data, let’s recreate <code>model_1</code> but with a loss function suited to our regression data.</p>
<div id="cell-73" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="7bd16b1f-bd2e-486b-b963-9733aaa757be" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same architecture as model_1 (but using nn.Sequential)</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>model_2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">1</span>, out_features<span class="op">=</span><span class="dv">10</span>),</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">10</span>),</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>).to(device)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>model_2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): Linear(in_features=10, out_features=10, bias=True)
  (2): Linear(in_features=10, out_features=1, bias=True)
)</code></pre>
</div>
</div>
<p>We’ll setup the loss function to be <code>nn.L1Loss()</code> (the same as mean absolute error) and the optimizer to be <code>torch.optim.SGD()</code>.</p>
<div id="cell-75" class="cell" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss and optimizer</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.L1Loss()</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model_2.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now let’s train the model using the regular training loop steps for <code>epochs=1000</code> (just like <code>model_1</code>).</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> We’ve been writing similar training loop code over and over again. I’ve made it that way on purpose though, to keep practicing. However, do you have ideas how we could functionize this? That would save a fair bit of coding in the future. Potentially there could be a function for training and a function for testing.</p>
</blockquote>
<div id="cell-77" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="16da5efa-3c3b-494b-ef4e-e5244f4cf097" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of epochs</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Put data to target device</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>X_train_regression, y_train_regression <span class="op">=</span> X_train_regression.to(device), y_train_regression.to(device)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>X_test_regression, y_test_regression <span class="op">=</span> X_test_regression.to(device), y_test_regression.to(device)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training </span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Forward pass</span></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model_2(X_train_regression)</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate loss (no accuracy since it's a regression problem, not classification)</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_pred, y_train_regression)</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Loss backwards</span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Optimizer step</span></span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Testing</span></span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a>    model_2.<span class="bu">eval</span>()</span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>      <span class="co"># 1. Forward pass</span></span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a>      test_pred <span class="op">=</span> model_2(X_test_regression)</span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>      <span class="co"># 2. Calculate the loss </span></span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>      test_loss <span class="op">=</span> loss_fn(test_pred, y_test_regression)</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out what's happening</span></span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>: </span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> | Train loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, Test loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143
Epoch: 100 | Train loss: 0.09309, Test loss: 0.02901
Epoch: 200 | Train loss: 0.07376, Test loss: 0.02850
Epoch: 300 | Train loss: 0.06745, Test loss: 0.00615
Epoch: 400 | Train loss: 0.06107, Test loss: 0.02004
Epoch: 500 | Train loss: 0.05698, Test loss: 0.01061
Epoch: 600 | Train loss: 0.04857, Test loss: 0.01326
Epoch: 700 | Train loss: 0.06109, Test loss: 0.02127
Epoch: 800 | Train loss: 0.05599, Test loss: 0.01426
Epoch: 900 | Train loss: 0.05571, Test loss: 0.00603</code></pre>
</div>
</div>
<p>Okay, unlike <code>model_1</code> on the classification data, it looks like <code>model_2</code>’s loss is actually going down.</p>
<p>Let’s plot its predictions to see if that’s so.</p>
<p>And remember, since our model and data are using the target <code>device</code>, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can’t handle data on the GPU.</p>
<p>To handle that, we’ll send all of our data to the CPU using <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html"><code>.cpu()</code></a> when we pass it to <code>plot_predictions()</code>.</p>
<div id="cell-79" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:428}}" data-outputid="5887db7d-128b-46f3-c978-c12af112d470" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn on evaluation mode</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model_2.<span class="bu">eval</span>()</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions (inference)</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    y_preds <span class="op">=</span> model_2(X_test_regression)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a><span class="co"># (try removing .cpu() from one of the below and see what happens)</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>plot_predictions(train_data<span class="op">=</span>X_train_regression.cpu(),</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>                 train_labels<span class="op">=</span>y_train_regression.cpu(),</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>                 test_data<span class="op">=</span>X_test_regression.cpu(),</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>                 test_labels<span class="op">=</span>y_test_regression.cpu(),</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>                 predictions<span class="op">=</span>y_preds.cpu())<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Alright, it looks like our model is able to do far better than random guessing on straight lines.</p>
<p>This is a good thing.</p>
<p>It means our model at least has <em>some</em> capacity to learn.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> A helpful troubleshooting step when building deep learning models is to start as small as possible to see if the model works before scaling it up.</p>
<p>This could mean starting with a simple neural network (not many layers, not many hidden neurons) and a small dataset (like the one we’ve made) and then <strong>overfitting</strong> (making the model perform too well) on that small example before increasing the amount data or the model size/design to <em>reduce</em> overfitting.</p>
</blockquote>
<p>So what could it be?</p>
<p>Let’s find out.</p>
</section>
</section>
<section id="the-missing-piece-non-linearity" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="the-missing-piece-non-linearity"><span class="header-section-number">4.10</span> 6. The missing piece: non-linearity</h2>
<p>We’ve seen our model can draw straight (linear) lines, thanks to its linear layers.</p>
<p>But how about we give it the capacity to draw non-straight (non-linear) lines?</p>
<p>How?</p>
<p>Let’s find out.</p>
<section id="recreating-non-linear-data-red-and-blue-circles" class="level3" data-number="4.10.1">
<h3 data-number="4.10.1" class="anchored" data-anchor-id="recreating-non-linear-data-red-and-blue-circles"><span class="header-section-number">4.10.1</span> 6.1 Recreating non-linear data (red and blue circles)</h3>
<p>First, let’s recreate the data to start off fresh. We’ll use the same setup as before.</p>
<div id="cell-83" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:265}}" data-outputid="b8c1d692-6f3e-43ca-9323-98bd40e39a89" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make and plot data</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_circles</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_circles(n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    noise<span class="op">=</span><span class="fl">0.03</span>,</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.RdBu)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-35-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice! Now let’s split it into training and test sets using 80% of the data for training and 20% for testing.</p>
<div id="cell-85" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5619ad2f-ad5d-476a-dcfd-9f8d48036899" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to tensors and split into train and test sets</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn data into tensors</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.from_numpy(X).<span class="bu">type</span>(torch.<span class="bu">float</span>)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.from_numpy(y).<span class="bu">type</span>(torch.<span class="bu">float</span>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train and test sets</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, </span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>                                                    y, </span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>                                                    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>                                                    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>X_train[:<span class="dv">5</span>], y_train[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>(tensor([[ 0.6579, -0.4651],
         [ 0.6319, -0.7347],
         [-1.0086, -0.1240],
         [-0.9666, -0.2256],
         [-0.1666,  0.7994]]), tensor([1., 0., 0., 0., 1.]))</code></pre>
</div>
</div>
</section>
<section id="모델-구축하기-with-non-linearity" class="level3" data-number="4.10.2">
<h3 data-number="4.10.2" class="anchored" data-anchor-id="모델-구축하기-with-non-linearity"><span class="header-section-number">4.10.2</span> 6.2 모델 구축하기 with non-linearity</h3>
<p>Now here comes the fun part.</p>
<p>What kind of pattern do you think you could draw with unlimited straight (linear) and non-straight (non-linear) lines?</p>
<p>I bet you could get pretty creative.</p>
<p>So far our neural networks have only been using linear (straight) line functions.</p>
<p>But the data we’ve been working with is non-linear (circles).</p>
<p>What do you think will happen when we introduce the capability for our model to use <strong>non-linear actviation functions</strong>?</p>
<p>Well let’s see.</p>
<p>PyTorch has a bunch of <a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">ready-made non-linear activation functions</a> that do similiar but different things.</p>
<p>One of the most common and best performing is <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> (rectified linear-unit, <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"><code>torch.nn.ReLU()</code></a>).</p>
<p>Rather than talk about it, let’s put it in our neural network between the hidden layers in the forward pass and see what happens.</p>
<div id="cell-87" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="00a69e38-369f-47fb-b729-fd4c7b5b47de" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build model with non-linear activation function</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CircleModelV2(nn.Module):</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_1 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">2</span>, out_features<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_2 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_3 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">10</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU() <span class="co"># &lt;- add in ReLU activation function</span></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Can also put sigmoid in the model </span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This would mean you don't need to use it on the predictions</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.sigmoid = nn.Sigmoid()</span></span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Intersperse the ReLU activation function between layers</span></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>       <span class="cf">return</span> <span class="va">self</span>.layer_3(<span class="va">self</span>.relu(<span class="va">self</span>.layer_2(<span class="va">self</span>.relu(<span class="va">self</span>.layer_1(x)))))</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>model_3 <span class="op">=</span> CircleModelV2().to(device)</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model_3)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>CircleModelV2(
  (layer_1): Linear(in_features=2, out_features=10, bias=True)
  (layer_2): Linear(in_features=10, out_features=10, bias=True)
  (layer_3): Linear(in_features=10, out_features=1, bias=True)
  (relu): ReLU()
)</code></pre>
</div>
</div>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-tensorflow-playground-relu-activation.png" class="img-fluid" alt="a classification neural network on TensorFlow playground with ReLU activation"> <em>A visual example of what a similar classificiation neural network to the one we’ve just built (suing ReLU activation) looks like. Try create one of your own on the <a href="https://playground.tensorflow.org/">TensorFlow Playground website</a>.</em></p>
<blockquote class="blockquote">
<p><strong>Question:</strong> <em>Where should I put the non-linear activation functions when constructing a neural network?</em></p>
<p>A rule of thumb is to put them in between hidden layers and just after the output layer, however, there is no set in stone option. As you learn more about neural networks and deep learning you’ll find a bunch of different ways of putting things together. In the meantine, best to experiment, experiment, experiment.</p>
</blockquote>
<p>Now we’ve got a model ready to go, let’s create a binary classification loss function as well as an optimizer.</p>
<div id="cell-89" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup loss and optimizer </span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model_3.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Wonderful!</p>
</section>
<section id="training-a-model-with-non-linearity" class="level3" data-number="4.10.3">
<h3 data-number="4.10.3" class="anchored" data-anchor-id="training-a-model-with-non-linearity"><span class="header-section-number">4.10.3</span> 6.3 Training a model with non-linearity</h3>
<p>You know the drill, model, loss function, optimizer ready to go, let’s create a training and testing loop.</p>
<div id="cell-92" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="df71bd5d-cd0d-4e39-f3e0-7fc90bfaaeb0" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Put all data on target device</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> X_train.to(device), y_train.to(device)</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> X_test.to(device), y_test.to(device)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Forward pass</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>    y_logits <span class="op">=</span> model_3(X_train).squeeze()</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(y_logits)) <span class="co"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate loss and accuracy</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_logits, y_train) <span class="co"># BCEWithLogitsLoss calculates loss using logits</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_train, </span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>                      y_pred<span class="op">=</span>y_pred)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Loss backward</span></span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-25"><a href="#cb64-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Optimizer step</span></span>
<span id="cb64-26"><a href="#cb64-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb64-27"><a href="#cb64-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-28"><a href="#cb64-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Testing</span></span>
<span id="cb64-29"><a href="#cb64-29" aria-hidden="true" tabindex="-1"></a>    model_3.<span class="bu">eval</span>()</span>
<span id="cb64-30"><a href="#cb64-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb64-31"><a href="#cb64-31" aria-hidden="true" tabindex="-1"></a>      <span class="co"># 1. Forward pass</span></span>
<span id="cb64-32"><a href="#cb64-32" aria-hidden="true" tabindex="-1"></a>      test_logits <span class="op">=</span> model_3(X_test).squeeze()</span>
<span id="cb64-33"><a href="#cb64-33" aria-hidden="true" tabindex="-1"></a>      test_pred <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(test_logits)) <span class="co"># logits -&gt; prediction probabilities -&gt; prediction labels</span></span>
<span id="cb64-34"><a href="#cb64-34" aria-hidden="true" tabindex="-1"></a>      <span class="co"># 2. Calcuate loss and accuracy</span></span>
<span id="cb64-35"><a href="#cb64-35" aria-hidden="true" tabindex="-1"></a>      test_loss <span class="op">=</span> loss_fn(test_logits, y_test)</span>
<span id="cb64-36"><a href="#cb64-36" aria-hidden="true" tabindex="-1"></a>      test_acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_test,</span>
<span id="cb64-37"><a href="#cb64-37" aria-hidden="true" tabindex="-1"></a>                             y_pred<span class="op">=</span>test_pred)</span>
<span id="cb64-38"><a href="#cb64-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-39"><a href="#cb64-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out what's happening</span></span>
<span id="cb64-40"><a href="#cb64-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb64-41"><a href="#cb64-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, Accuracy: </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">% | Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">, Test Accuracy: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%
Epoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%
Epoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%
Epoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%
Epoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%
Epoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%
Epoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%
Epoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%
Epoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%
Epoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%</code></pre>
</div>
</div>
<p>Ho ho! That’s looking far better!</p>
</section>
<section id="evaluating-a-model-trained-with-non-linear-activation-functions" class="level3" data-number="4.10.4">
<h3 data-number="4.10.4" class="anchored" data-anchor-id="evaluating-a-model-trained-with-non-linear-activation-functions"><span class="header-section-number">4.10.4</span> 6.4 Evaluating a model trained with non-linear activation functions</h3>
<p>Remember how our circle data is non-linear? Well, let’s see how our models predictions look now the model’s been trained with non-linear activation functions.</p>
<div id="cell-95" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e95a412b-bef1-4d3d-b705-1eb75a55449e" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>model_3.<span class="bu">eval</span>()</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    y_preds <span class="op">=</span> torch.<span class="bu">round</span>(torch.sigmoid(model_3(X_test))).squeeze()</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>y_preds[:<span class="dv">10</span>], y[:<span class="dv">10</span>] <span class="co"># want preds in same format as truth labels</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),
 tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</code></pre>
</div>
</div>
<div id="cell-96" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:390}}" data-outputid="3566e274-ef7a-4eb8-ef34-57b97e7781bc" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot decision boundaries for training and test sets</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Train"</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_1, X_train, y_train) <span class="co"># model_1 = no non-linearity</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Test"</span>)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_3, X_test, y_test) <span class="co"># model_3 = has non-linearity</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-41-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice! Not perfect but still far better than before.</p>
<p>Potentially you could try a few tricks to improve the test accuracy of the model? (hint: head back to section 5 for tips on improving the model)</p>
</section>
</section>
<section id="replicating-non-linear-activation-functions" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="replicating-non-linear-activation-functions"><span class="header-section-number">4.11</span> 7. Replicating non-linear activation functions</h2>
<p>We saw before how adding non-linear activation functions to our model can helped it to model non-linear data.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> Much of the data you’ll encounter in the wild is non-linear (or a combination of linear and non-linear). Right now we’ve been working with dots on a 2D plot. But imagine if you had images of plants you’d like to classify, there’s a lot of different plant shapes. Or text from Wikipedia you’d like to summarize, there’s lots of different ways words can be put together (linear and non-linear patterns).</p>
</blockquote>
<p>But what does a non-linear activation <em>look</em> like?</p>
<p>How about we replicate some and what they do?</p>
<p>Let’s start by creating a small amount of data.</p>
<div id="cell-99" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="befa3534-f5b1-48f2-88c5-63836759d834" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a toy tensor (similar to the data going into our model(s))</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>A</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,
          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</code></pre>
</div>
</div>
<p>Wonderful, now let’s plot it.</p>
<div id="cell-101" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:265}}" data-outputid="477fdd3d-ae28-4caf-b9ed-b2a9ec369bee" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the toy tensor</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>plt.plot(A)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-43-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>A straight line, nice.</p>
<p>Now let’s see how the ReLU activation function influences it.</p>
<p>And instead of using PyTorch’s ReLU (<code>torch.nn.ReLU</code>), we’ll recreate it ourselves.</p>
<p>The ReLU function turns all negatives to 0 and leaves the positive values as they are.</p>
<div id="cell-103" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="47c7fa54-6d03-47b7-a745-0bb3599c82fd" data-execution_count="43">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ReLU function by hand </span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> torch.maximum(torch.tensor(<span class="dv">0</span>), x) <span class="co"># inputs must be tensors</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass toy tensor through ReLU function</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>relu(A)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,
        8., 9.])</code></pre>
</div>
</div>
<p>It looks like our ReLU function worked, all of the negative values are zeros.</p>
<p>Let’s plot them.</p>
<div id="cell-105" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:265}}" data-outputid="4ada6523-81e8-450e-89b5-0be3da23f04c" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ReLU activated toy tensor</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>plt.plot(relu(A))<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-45-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice! That looks exactly like the shape of the ReLU function on the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Wikipedia page for ReLU</a>.</p>
<p>How about we try the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> we’ve been using?</p>
<p>The sigmoid function formula goes like so:</p>
<p><span class="math display">\[ out_i = \frac{1}{1+e^{-input_i}} \]</span></p>
<p>Or using <span class="math inline">\(x\)</span> as input:</p>
<p><span class="math display">\[ S(x) = \frac{1}{1+e^{-x_i}} \]</span></p>
<p>Where <span class="math inline">\(S\)</span> stands for sigmoid, <span class="math inline">\(e\)</span> stands for <a href="https://en.wikipedia.org/wiki/Exponential_function">exponential</a> (<a href="https://pytorch.org/docs/stable/generated/torch.exp.html"><code>torch.exp()</code></a>) and <span class="math inline">\(i\)</span> stands for a particular element in a tensor.</p>
<p>Let’s build a function to replicate the sigmoid function with PyTorch.</p>
<div id="cell-107" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="899e7595-5b1c-4182-c1ca-94aadaa097e1" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a custom sigmoid function</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> torch.exp(<span class="op">-</span>x))</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Test custom sigmoid on toy tensor</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>sigmoid(A)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,
        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,
        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,
        9.9966e-01, 9.9988e-01])</code></pre>
</div>
</div>
<p>Woah, those values look a lot like prediction probabilities we’ve seen earlier, let’s see what they look like visualized.</p>
<div id="cell-109" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:265}}" data-outputid="c6a6d3de-e9fb-445d-8d63-3964753a4559" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot sigmoid activated toy tensor</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>plt.plot(sigmoid(A))<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-47-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking good! We’ve gone from a straight line to a curved line.</p>
<p>Now there’s plenty more <a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">non-linear activation functions</a> that exist in PyTorch that we haven’t tried.</p>
<p>But these two are two of the most common.</p>
<p>And the point remains, what patterns could you draw using an unlimited amount of linear (straight) and non-linear (not straight) lines?</p>
<p>Almost anything right?</p>
<p>That’s exactly what our model is doing when we combine linear and non-linear functions.</p>
<p>Instead of telling our model what to do, we give it tools to figure out how to best discover patterns in the data.</p>
<p>And those tools are linear and non-linear functions.</p>
</section>
<section id="putting-things-together-by-building-a-multi-class-pytorch-model" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="putting-things-together-by-building-a-multi-class-pytorch-model"><span class="header-section-number">4.12</span> 8. Putting things together by building a multi-class PyTorch model</h2>
<p>We’ve covered a fair bit.</p>
<p>But now let’s put it all together using a multi-class classification problem.</p>
<p>Recall a <strong>binary classification</strong> problem deals with classifying something as one of two options (e.g.&nbsp;a photo as a cat photo or a dog photo) where as a <strong>multi-class classification</strong> problem deals with classifying something from a list of <em>more than</em> two options (e.g.&nbsp;classifying a photo as a cat a dog or a chicken).</p>
<p><img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-binary-vs-multi-class-classification.png" class="img-fluid" alt="binary vs multi-class classification image with the example of dog vs cat for binary classification and dog vs cat vs chicken for multi-class classification"> <em>Example of binary vs.&nbsp;multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular <a href="https://www.image-net.org/">ImageNet-1k dataset</a> is used as a computer vision benchmark and has 1000 classes.</em></p>
<section id="creating-mutli-class-classification-data" class="level3" data-number="4.12.1">
<h3 data-number="4.12.1" class="anchored" data-anchor-id="creating-mutli-class-classification-data"><span class="header-section-number">4.12.1</span> 8.1 Creating mutli-class classification data</h3>
<p>To begin a multi-class classification problem, let’s create some multi-class data.</p>
<p>To do so, we can leverage Scikit-Learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"><code>make_blobs()</code></a> method.</p>
<p>This method will create however many classes (using the <code>centers</code> parameter) we want.</p>
<p>Specifically, let’s do the following:</p>
<ol type="1">
<li>Create some multi-class data with <code>make_blobs()</code>.</li>
<li>Turn the data into tensors (the default of <code>make_blobs()</code> is to use NumPy arrays).</li>
<li>Split the data into training and test sets using <code>train_test_split()</code>.</li>
<li>Visualize the data.</li>
</ol>
<div id="cell-113" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:515}}" data-outputid="7cd92d66-41e3-4aa0-ab94-f9f9ef40fcce" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import dependencies</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the hyperparameters for data creation</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>NUM_CLASSES <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>NUM_FEATURES <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create multi-class data</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>X_blob, y_blob <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span>NUM_FEATURES, <span class="co"># X features</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    centers<span class="op">=</span>NUM_CLASSES, <span class="co"># y labels </span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    cluster_std<span class="op">=</span><span class="fl">1.5</span>, <span class="co"># give the clusters a little shake up (try changing this to 1.0, the default)</span></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span>RANDOM_SEED</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Turn data into tensors</span></span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>X_blob <span class="op">=</span> torch.from_numpy(X_blob).<span class="bu">type</span>(torch.<span class="bu">float</span>)</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>y_blob <span class="op">=</span> torch.from_numpy(y_blob).<span class="bu">type</span>(torch.LongTensor)</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_blob[:<span class="dv">5</span>], y_blob[:<span class="dv">5</span>])</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Split into train and test sets</span></span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>X_blob_train, X_blob_test, y_blob_train, y_blob_test <span class="op">=</span> train_test_split(X_blob,</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>    y_blob,</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span>RANDOM_SEED</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Plot data</span></span>
<span id="cb78-33"><a href="#cb78-33" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb78-34"><a href="#cb78-34" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_blob[:, <span class="dv">0</span>], X_blob[:, <span class="dv">1</span>], c<span class="op">=</span>y_blob, cmap<span class="op">=</span>plt.cm.RdYlBu)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[-8.4134,  6.9352],
        [-5.7665, -6.4312],
        [-6.0421, -6.7661],
        [ 3.9508,  0.6984],
        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-48-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice! Looks like we’ve got some multi-class data ready to go.</p>
<p>Let’s build a model to separate the coloured blobs.</p>
<blockquote class="blockquote">
<p><strong>Question:</strong> Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?</p>
</blockquote>
</section>
<section id="building-a-multi-class-classification-model-in-pytorch" class="level3" data-number="4.12.2">
<h3 data-number="4.12.2" class="anchored" data-anchor-id="building-a-multi-class-classification-model-in-pytorch"><span class="header-section-number">4.12.2</span> 8.2 Building a multi-class classification model in PyTorch</h3>
<p>We’ve created a few models in PyTorch so far.</p>
<p>You might also be starting to get an idea of how flexible neural networks are.</p>
<p>How about we build one similar to <code>model_3</code> but this still capable of handling multi-class data?</p>
<p>To do so, let’s create a subclass of <code>nn.Module</code> that takes in three hyperparameters: * <code>input_features</code> - the number of <code>X</code> features coming into the model. * <code>output_features</code> - the ideal numbers of output features we’d like (this will be equivalent to <code>NUM_CLASSES</code> or the number of classes in your multi-class classification problem). * <code>hidden_units</code> - the number of hidden neurons we’d like each hidden layer to use.</p>
<p>Since we’re putting things together, let’s setup some device agnostic code (we don’t have to do this again in the same notebook, it’s only a reminder).</p>
<p>Then we’ll create the model class using the hyperparameters above.</p>
<div id="cell-116" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:36}}" data-outputid="64d275af-144c-4b5f-99b7-a2d2ebf235f8" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create device agnostic code</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>device</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>'cuda'</code></pre>
</div>
</div>
<div id="cell-117" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="0f8d1d8e-e578-4bf6-eea6-1cf27bda4764" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Build model</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BlobModel(nn.Module):</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_features, output_features, hidden_units<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Initializes all required hyperparameters for a multi-class classification model.</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="co">            input_features (int): Number of input features to the model.</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="co">            out_features (int): Number of output features of the model</span></span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a><span class="co">              (how many classes there are).</span></span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a><span class="co">            hidden_units (int): Number of hidden units between layers, default 8.</span></span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_layer_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>input_features, out_features<span class="op">=</span>hidden_units),</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units, out_features<span class="op">=</span>hidden_units),</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>            nn.Linear(in_features<span class="op">=</span>hidden_units, out_features<span class="op">=</span>output_features), <span class="co"># how many classes are there?</span></span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_layer_stack(x)</span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of BlobModel and send it to the target device</span></span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a>model_4 <span class="op">=</span> BlobModel(input_features<span class="op">=</span>NUM_FEATURES, </span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>                    output_features<span class="op">=</span>NUM_CLASSES, </span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a>                    hidden_units<span class="op">=</span><span class="dv">8</span>).to(device)</span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>model_4</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>BlobModel(
  (linear_layer_stack): Sequential(
    (0): Linear(in_features=2, out_features=8, bias=True)
    (1): Linear(in_features=8, out_features=8, bias=True)
    (2): Linear(in_features=8, out_features=4, bias=True)
  )
)</code></pre>
</div>
</div>
<p>Excellent! Our multi-class model is ready to go, let’s create a loss function and optimizer for it.</p>
</section>
<section id="creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model" class="level3" data-number="4.12.3">
<h3 data-number="4.12.3" class="anchored" data-anchor-id="creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model"><span class="header-section-number">4.12.3</span> 8.3 Creating a loss function and optimizer for a multi-class PyTorch model</h3>
<p>Since we’re working on a multi-class classification problem, we’ll use the <code>nn.CrossEntropyLoss()</code> method as our loss function.</p>
<p>And we’ll stick with using SGD with a learning rate of 0.1 for optimizing our <code>model_4</code> parameters.</p>
<div id="cell-119" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create loss and optimizer</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model_4.parameters(), </span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>                            lr<span class="op">=</span><span class="fl">0.1</span>) <span class="co"># exercise: try changing the learning rate here and seeing what happens to the model's performance</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="getting-prediction-probabilities-for-a-multi-class-pytorch-model" class="level3" data-number="4.12.4">
<h3 data-number="4.12.4" class="anchored" data-anchor-id="getting-prediction-probabilities-for-a-multi-class-pytorch-model"><span class="header-section-number">4.12.4</span> 8.4 Getting prediction probabilities for a multi-class PyTorch model</h3>
<p>Alright, we’ve got a loss function and optimizer ready, and we’re ready to train our model but before we do let’s do a single forward pass with our model to see if it works.</p>
<div id="cell-121" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2b1fb56f-cf42-49a3-f1ec-7978cfd68b56" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform a single forward pass on the data (we'll need to put it to the target device for it to work)</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>model_4(X_blob_train.to(device))[:<span class="dv">5</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>tensor([[-1.2711, -0.6494, -1.4740, -0.7044],
        [ 0.2210, -1.5439,  0.0420,  1.1531],
        [ 2.8698,  0.9143,  3.3169,  1.4027],
        [ 1.9576,  0.3125,  2.2244,  1.1324],
        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',
       grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>What’s coming out here?</p>
<p>It looks like we get one value per feature of each sample.</p>
<p>Let’s check the shape to confirm.</p>
<div id="cell-123" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="f515c93d-2c57-43fe-f5ce-d6b6aa20cd0f" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># How many elements in a single prediction sample?</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>model_4(X_blob_train.to(device))[<span class="dv">0</span>].shape, NUM_CLASSES </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>(torch.Size([4]), 4)</code></pre>
</div>
</div>
<p>Wonderful, our model is predicting one value for each class that we have.</p>
<p>Do you remember what the raw outputs of our model are called?</p>
<p>Hint: it rhymes with “frog splits” (no animals were harmed in the creation of these materials).</p>
<p>If you guessed <em>logits</em>, you’d be correct.</p>
<p>So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?</p>
<p>As in, how do we go from <code>logits -&gt; prediction probabilities -&gt; prediction labels</code> just like we did with the binary classification problem?</p>
<p>That’s where the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax activation function</a> comes into play.</p>
<p>The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.</p>
<p>If this doesn’t make sense, let’s see in code.</p>
<div id="cell-125" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="c12e6a9f-c80f-466c-aa5c-27e30cfe9963" data-execution_count="53">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make prediction logits with model</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>y_logits <span class="op">=</span> model_4(X_test.to(device))</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform softmax calculation on logits across dimension 1 to get prediction probabilities</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>y_pred_probs <span class="op">=</span> torch.softmax(y_logits, dim<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_logits[:<span class="dv">5</span>])</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_pred_probs[:<span class="dv">5</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 0.2341, -0.3357,  0.2307,  0.2534],
        [ 0.1198, -0.3702,  0.0998,  0.1887],
        [ 0.3790, -0.2037,  0.4095,  0.2689],
        [ 0.1936, -0.3733,  0.1807,  0.2496],
        [ 0.1338, -0.1378,  0.1487,  0.0247]], device='cuda:0',
       grad_fn=&lt;SliceBackward0&gt;)
tensor([[0.2792, 0.1579, 0.2782, 0.2846],
        [0.2729, 0.1672, 0.2675, 0.2924],
        [0.2869, 0.1602, 0.2958, 0.2570],
        [0.2769, 0.1571, 0.2733, 0.2928],
        [0.2722, 0.2075, 0.2763, 0.2441]], device='cuda:0',
       grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Hmm, what’s happened here?</p>
<p>It may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasn’t been trained and is predicting using random patterns) but there’s a very specific thing different about each sample.</p>
<p>After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).</p>
<p>Let’s check.</p>
<div id="cell-127" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="db517fa9-04eb-4efe-a75b-970bdf2a3163" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sum the first sample output of the softmax activation function </span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>torch.<span class="bu">sum</span>(y_pred_probs[<span class="dv">0</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>tensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)</code></pre>
</div>
</div>
<p>These prediction probablities are essentially saying how much the model <em>thinks</em> the target <code>X</code> sample (the input) maps to each class.</p>
<p>Since there’s one value for each class in <code>y_pred_probs</code>, the index of the <em>highest</em> value is the class the model thinks the specific data sample <em>most</em> belongs to.</p>
<p>We can check which index has the highest value using <code>torch.argmax()</code>.</p>
<div id="cell-129" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a7e4db7e-08fd-426c-8b54-dfd7d3943d79" data-execution_count="55">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Which class does the model think is *most* likely at the index 0 sample?</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_pred_probs[<span class="dv">0</span>])</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.argmax(y_pred_probs[<span class="dv">0</span>]))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.2792, 0.1579, 0.2782, 0.2846], device='cuda:0',
       grad_fn=&lt;SelectBackward0&gt;)
tensor(3, device='cuda:0')</code></pre>
</div>
</div>
<p>You can see the output of <code>torch.argmax()</code> returns 3, so for the features (<code>X</code>) of the sample at index 0, the model is predicting that the most likely class value (<code>y</code>) is 3.</p>
<p>Of course, right now this is just random guessing so it’s got a 25% chance of being right (since there’s four classes). But we can improve those chances by training the model.</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> To summarize the above, a model’s raw output is referred to as <strong>logits</strong>.</p>
<p>For a multi-class classification problem, to turn the logits into <strong>prediction probabilities</strong>, you use the softmax activation function (<code>torch.softmax</code>).</p>
<p>The index of the value with the highest <strong>prediction probability</strong> is the class number the model thinks is <em>most</em> likely given the input features for that sample (although this is a prediction, it doesn’t mean it will be correct).</p>
</blockquote>
</section>
<section id="creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model" class="level3" data-number="4.12.5">
<h3 data-number="4.12.5" class="anchored" data-anchor-id="creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model"><span class="header-section-number">4.12.5</span> 8.5 Creating a training and testing loop for a multi-class PyTorch model</h3>
<p>Alright, now we’ve got all of the preparation steps out of the way, let’s write a training and testing loop to improve and evaluation our model.</p>
<p>We’ve done many of these steps before so much of this will be practice.</p>
<p>The only difference is that we’ll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).</p>
<p>Let’s train the model for <code>epochs=100</code> and evaluate it every 10 epochs.</p>
<div id="cell-132" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="4a7b0fd7-8a08-40a4-8694-435178b70832" data-execution_count="56">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set number of epochs</span></span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Put data to target device</span></span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>X_blob_train, y_blob_train <span class="op">=</span> X_blob_train.to(device), y_blob_train.to(device)</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>X_blob_test, y_blob_test <span class="op">=</span> X_blob_test.to(device), y_blob_test.to(device)</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training</span></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>    model_4.train()</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Forward pass</span></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    y_logits <span class="op">=</span> model_4(X_blob_train) <span class="co"># model outputs raw logits </span></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> torch.softmax(y_logits, dim<span class="op">=</span><span class="dv">1</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># go from logits -&gt; prediction probabilities -&gt; prediction labels</span></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(y_logits)</span></span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Calculate loss and accuracy</span></span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_logits, y_blob_train) </span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_blob_train,</span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>                      y_pred<span class="op">=</span>y_pred)</span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Optimizer zero grad</span></span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Loss backwards</span></span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Optimizer step</span></span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Testing</span></span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a>    model_4.<span class="bu">eval</span>()</span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a>      <span class="co"># 1. Forward pass</span></span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>      test_logits <span class="op">=</span> model_4(X_blob_test)</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>      test_pred <span class="op">=</span> torch.softmax(test_logits, dim<span class="op">=</span><span class="dv">1</span>).argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>      <span class="co"># 2. Calculate test loss and accuracy</span></span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a>      test_loss <span class="op">=</span> loss_fn(test_logits, y_blob_test)</span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a>      test_acc <span class="op">=</span> accuracy_fn(y_true<span class="op">=</span>y_blob_test,</span>
<span id="cb95-42"><a href="#cb95-42" aria-hidden="true" tabindex="-1"></a>                             y_pred<span class="op">=</span>test_pred)</span>
<span id="cb95-43"><a href="#cb95-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-44"><a href="#cb95-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print out what's happening</span></span>
<span id="cb95-45"><a href="#cb95-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb95-46"><a href="#cb95-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>loss<span class="sc">:.5f}</span><span class="ss">, Acc: </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">% | Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">:.5f}</span><span class="ss">, Test Acc: </span><span class="sc">{</span>test_acc<span class="sc">:.2f}</span><span class="ss">%"</span>) </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%
Epoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%
Epoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%
Epoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%
Epoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%
Epoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%
Epoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%
Epoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%
Epoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%
Epoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%</code></pre>
</div>
</div>
</section>
<section id="making-and-evaluating-predictions-with-a-pytorch-multi-class-model" class="level3" data-number="4.12.6">
<h3 data-number="4.12.6" class="anchored" data-anchor-id="making-and-evaluating-predictions-with-a-pytorch-multi-class-model"><span class="header-section-number">4.12.6</span> 8.6 Making and evaluating predictions with a PyTorch multi-class model</h3>
<p>It looks like our trained model is performaning pretty well.</p>
<p>But to make sure of this, let’s make some predictions and visualize them.</p>
<div id="cell-134" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="583e10fb-fa1c-463a-fbd0-4d31d76b82ab" data-execution_count="57">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>model_4.<span class="bu">eval</span>()</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    y_logits <span class="op">=</span> model_4(X_blob_test)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a><span class="co"># View the first 10 predictions</span></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>y_logits[:<span class="dv">10</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],
        [  5.0142, -12.0371,   3.3860,  10.6699],
        [ -5.5885, -13.3448,  20.9894,  12.7711],
        [  1.8400,   7.5599,  -8.6016,  -6.9942],
        [  8.0726,   3.2906, -14.5998,  -3.6186],
        [  5.5844, -14.9521,   5.0168,  13.2890],
        [ -5.9739, -10.1913,  18.8655,   9.9179],
        [  7.0755,  -0.7601,  -9.5531,   0.1736],
        [ -5.5918, -18.5990,  25.5309,  17.5799],
        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')</code></pre>
</div>
</div>
<p>Alright, looks like our model’s predictions are still in logit form.</p>
<p>Though to evaluate them, they’ll have to be in the same form as our labels (<code>y_blob_test</code>) which are in integer form.</p>
<p>Let’s convert our model’s prediction logits to prediction probabilities (using <code>torch.softmax()</code>) then to prediction labels (by taking the <code>argmax()</code> of each sample).</p>
<blockquote class="blockquote">
<p><strong>참고:</strong> It’s possible to skip the <code>torch.softmax()</code> function and go straight from <code>predicted logits -&gt; predicted labels</code> by calling <code>torch.argmax()</code> directly on the logits.</p>
<p>For example, <code>y_preds = torch.argmax(y_logits, dim=1)</code>, this saves a computation step (no <code>torch.softmax()</code>) but results in no prediction probabilities being available to use.</p>
</blockquote>
<div id="cell-136" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ca32986f-8dc0-419d-84df-1cc3ba8577e5" data-execution_count="58">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn predicted logits in prediction probabilities</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>y_pred_probs <span class="op">=</span> torch.softmax(y_logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn prediction probabilities into prediction labels</span></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>y_preds <span class="op">=</span> y_pred_probs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare first 10 model preds and test labels</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predictions: </span><span class="sc">{</span>y_preds[:<span class="dv">10</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Labels: </span><span class="sc">{</span>y_blob_test[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>accuracy_fn(y_true<span class="op">=</span>y_blob_test, y_pred<span class="op">=</span>y_preds)<span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')
Labels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')
Test accuracy: 99.5%</code></pre>
</div>
</div>
<p>Nice! Our model predictions are now in the same form as our test labels.</p>
<p>Let’s visualize them with <code>plot_decision_boundary()</code>, remember because our data is on the GPU, we’ll have to move it to the CPU for use with matplotlib (<code>plot_decision_boundary()</code> does this automatically for us).</p>
<div id="cell-138" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:390}}" data-outputid="71dcaf7f-a30e-457d-8949-916cf9c5cc79" data-execution_count="59">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Train"</span>)</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_4, X_blob_train, y_blob_train)</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Test"</span>)</span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(model_4, X_blob_test, y_blob_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_pytorch_classification_files/figure-html/cell-60-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="more-classification-evaluation-metrics" class="level2" data-number="4.13">
<h2 data-number="4.13" class="anchored" data-anchor-id="more-classification-evaluation-metrics"><span class="header-section-number">4.13</span> 9. More classification evaluation metrics</h2>
<p>So far we’ve only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).</p>
<p>These are some of the most common methods you’ll come across and are a good starting point.</p>
<p>However, you may want to evaluate you classification model using more metrics such as the following:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Metric name/Evaluation method</strong></th>
<th><strong>Defintion</strong></th>
<th><strong>Code</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct.</td>
<td><a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#id3"><code>torchmetrics.Accuracy()</code></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"><code>sklearn.metrics.accuracy_score()</code></a></td>
</tr>
<tr class="even">
<td>Precision</td>
<td>Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should’ve been 0).</td>
<td><a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#id4"><code>torchmetrics.Precision()</code></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"><code>sklearn.metrics.precision_score()</code></a></td>
</tr>
<tr class="odd">
<td>Recall</td>
<td>Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should’ve been 1). Higher recall leads to less false negatives.</td>
<td><a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#id5"><code>torchmetrics.Recall()</code></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"><code>sklearn.metrics.recall_score()</code></a></td>
</tr>
<tr class="even">
<td>F1-score</td>
<td>Combines precision and recall into one metric. 1 is best, 0 is worst.</td>
<td><a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#f1score"><code>torchmetrics.F1Score()</code></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"><code>sklearn.metrics.f1_score()</code></a></td>
</tr>
<tr class="odd">
<td><a href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">Confusion matrix</a></td>
<td>Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).</td>
<td><a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#confusionmatrix"><code>torchmetrics.ConfusionMatrix</code></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code>sklearn.metrics.plot_confusion_matrix()</code></a></td>
</tr>
<tr class="even">
<td>Classification report</td>
<td>Collection of some of the main classification metrics such as precision, recall and f1-score.</td>
<td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"><code>sklearn.metrics.classification_report()</code></a></td>
</tr>
</tbody>
</table>
<p>Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you’re looking for a PyTorch-like version, check out <a href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a>, especially the <a href="https://torchmetrics.readthedocs.io/en/latest/references/modules.html#classification">TorchMetrics classification section</a>.</p>
<p>Let’s try the <code>torchmetrics.Accuracy</code> metric out.</p>
<div id="cell-140" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ca9af26f-3d97-4019-fd7d-105f1dc2e68c" data-execution_count="60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip <span class="op">-</span>q install torchmetrics</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics <span class="im">import</span> Accuracy</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup metric and make sure it's on the target device</span></span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>torchmetrics_accuracy <span class="op">=</span> Accuracy().to(device)</span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy</span></span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>torchmetrics_accuracy(y_preds, y_blob_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>     |████████████████████████████████| 409 kB 5.2 MB/s eta 0:00:01</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>tensor(0.9950, device='cuda:0')</code></pre>
</div>
</div>
</section>
<section id="연습-문제" class="level2" data-number="4.14">
<h2 data-number="4.14" class="anchored" data-anchor-id="연습-문제"><span class="header-section-number">4.14</span> 연습 문제</h2>
<p>All of the exercises are focused on practicing the code in the sections above.</p>
<p>You should be able to complete them by referencing each section or by following the resource(s) linked.</p>
<p>All exercises should be completed using <a href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">device-agonistic code</a>.</p>
<p>Resources: * <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/02_pytorch_classification_exercises.ipynb">Exercise template notebook for 02</a> * <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/02_pytorch_classification_exercise_solutions.ipynb">Example solutions notebook for 02</a> (try the exercises <em>before</em> looking at this)</p>
<ol type="1">
<li>Make a binary classification dataset with Scikit-Learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html"><code>make_moons()</code></a> function.</li>
</ol>
<ul>
<li>For consistency, the dataset should have 1000 samples and a <code>random_state=42</code>.</li>
<li>Turn the data into PyTorch tensors. Split the data into training and test sets using <code>train_test_split</code> with 80% training and 20% testing.</li>
</ul>
<ol start="2" type="1">
<li>Build a model by subclassing <code>nn.Module</code> that incorporates non-linear activation functions and is capable of fitting the data you created in 1.</li>
</ol>
<ul>
<li>Feel free to use any combination of PyTorch layers (linear and non-linear) you want.</li>
</ul>
<ol start="3" type="1">
<li>Setup a binary classification compatible loss function and optimizer to use when training the model.</li>
<li>Create a training and testing loop to fit the model you created in 2 to the data you created in 1.</li>
</ol>
<ul>
<li>To measure model accuray, you can create your own accuracy function or use the accuracy function in <a href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a>.</li>
<li>Train the model for long enough for it to reach over 96% accuracy.</li>
<li>The training loop should output progress every 10 epochs of the model’s training and test set loss and accuracy.</li>
</ul>
<ol start="5" type="1">
<li>Make predictions with your trained model and plot them using the <code>plot_decision_boundary()</code> function created in this notebook.</li>
<li>Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.</li>
</ol>
<ul>
<li>Feel free to reference the <a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh">ML cheatsheet website</a> for the formula.</li>
</ul>
<ol start="7" type="1">
<li>Create a multi-class dataset using the <a href="https://cs231n.github.io/neural-networks-case-study/">spirals data creation function from CS231n</a> (see below for the code).</li>
</ol>
<ul>
<li>Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).</li>
<li>Build a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).</li>
<li>Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like).</li>
<li>Plot the decision boundaries on the spirals dataset from your model predictions, the <code>plot_decision_boundary()</code> function should work for this dataset too.</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Code for creating a spiral dataset from CS231n</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span> <span class="co"># number of points per class</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> <span class="dv">2</span> <span class="co"># dimensionality</span></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">3</span> <span class="co"># number of classes</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.zeros((N<span class="op">*</span>K,D)) <span class="co"># data matrix (each row = single example)</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.zeros(N<span class="op">*</span>K, dtype<span class="op">=</span><span class="st">'uint8'</span>) <span class="co"># class labels</span></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>  ix <span class="op">=</span> <span class="bu">range</span>(N<span class="op">*</span>j,N<span class="op">*</span>(j<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>  r <span class="op">=</span> np.linspace(<span class="fl">0.0</span>,<span class="dv">1</span>,N) <span class="co"># radius</span></span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>  t <span class="op">=</span> np.linspace(j<span class="op">*</span><span class="dv">4</span>,(j<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span><span class="dv">4</span>,N) <span class="op">+</span> np.random.randn(N)<span class="op">*</span><span class="fl">0.2</span> <span class="co"># theta</span></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>  X[ix] <span class="op">=</span> np.c_[r<span class="op">*</span>np.sin(t), r<span class="op">*</span>np.cos(t)]</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>  y[ix] <span class="op">=</span> j</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a><span class="co"># lets visualize the data</span></span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="추가-학습-자료" class="level2" data-number="4.15">
<h2 data-number="4.15" class="anchored" data-anchor-id="추가-학습-자료"><span class="header-section-number">4.15</span> 추가 학습 자료</h2>
<ul>
<li>Write down 3 problems where you think machine classification could be useful (these can be anything, get creative as you like, for example, classifying credit card transactions as fraud or not fraud based on the purchase amount and purchase location features).</li>
<li>Research the concept of “momentum” in gradient-based optimizers (like SGD or Adam), what does it mean?</li>
<li>Spend 10-minutes reading the <a href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">Wikipedia page for different activation functions</a>, how many of these can you line up with <a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">PyTorch’s activation functions</a>?</li>
<li>Research when accuracy might be a poor metric to use (hint: read <a href="https://willkoehrsen.github.io/statistics/learning/beyond-accuracy-precision-and-recall/">“Beyond Accuracy” by by Will Koehrsen</a> for ideas).</li>
<li><strong>Watch:</strong> For an idea of what’s happening within our neural networks and what they’re doing to learn, watch <a href="https://youtu.be/7sB052Pz0sQ">MIT’s Introduction to Deep Learning video</a>.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01_pytorch_workflow.html" class="pagination-link" aria-label="01 - PyTorch 워크플로우">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">01 - PyTorch 워크플로우</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03_pytorch_computer_vision.html" class="pagination-link" aria-label="03 - PyTorch 컴퓨터 비전">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">03 - PyTorch 컴퓨터 비전</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>