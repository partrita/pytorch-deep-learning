[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "파이토치 딥러닝 입문",
    "section": "",
    "text": "딥러닝을 위한 PyTorch 배우기\nZero to Mastery 딥러닝을 위한 PyTorch 배우기 과정에 오신 것을 환영합니다. 인터넷에서 PyTorch를 배우기에 두 번째로 좋은 곳입니다(첫 번째는 PyTorch 설명서입니다).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>딥러닝을 위한 PyTorch 배우기</span>"
    ]
  },
  {
    "objectID": "index.html#이-페이지의-내용",
    "href": "index.html#이-페이지의-내용",
    "title": "파이토치 딥러닝 입문",
    "section": "이 페이지의 내용",
    "text": "이 페이지의 내용\n\n과정 자료/개요\n이 과정에 대하여\n상태 (과정 제작 진행 상황)\n로그 (과정 자료 제작 과정 로그)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>딥러닝을 위한 PyTorch 배우기</span>"
    ]
  },
  {
    "objectID": "index.html#과정-자료개요",
    "href": "index.html#과정-자료개요",
    "title": "파이토치 딥러닝 입문",
    "section": "과정 자료/개요",
    "text": "과정 자료/개요\n\n📖 온라인 책 버전: 모든 과정 자료는 learnpytorch.io에서 읽을 수 있는 온라인 책으로 제공됩니다.\n🎥 YouTube의 첫 5개 섹션: 첫 25시간 분량의 자료를 시청하여 하루 만에 Pytorch를 배우십시오.\n🔬 과정 초점: 코드, 코드, 코드, 실험, 실험, 실험.\n🏃‍♂️ 교수 스타일: https://sive.rs/kimo.\n🤔 질문하기: 기존 질문/자신의 질문을 위해 GitHub 토론 페이지를 참조하십시오.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>딥러닝을 위한 PyTorch 배우기</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html",
    "href": "00_pytorch_fundamentals.html",
    "title": "00 - PyTorch 기초",
    "section": "",
    "text": "PyTorch란 무엇인가요?\nPyTorch는 오픈 소스 머신러닝 및 딥러닝 프레임워크입니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#pytorch는-어디에-사용되나요",
    "href": "00_pytorch_fundamentals.html#pytorch는-어디에-사용되나요",
    "title": "00 - PyTorch 기초",
    "section": "PyTorch는 어디에 사용되나요?",
    "text": "PyTorch는 어디에 사용되나요?\nPyTorch를 사용하면 파이썬 코드를 사용하여 데이터를 조작 및 처리하고 머신러닝 알고리즘을 작성할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#누가-pytorch를-사용하나요",
    "href": "00_pytorch_fundamentals.html#누가-pytorch를-사용하나요",
    "title": "00 - PyTorch 기초",
    "section": "누가 PyTorch를 사용하나요?",
    "text": "누가 PyTorch를 사용하나요?\n세계 최대 기술 기업 중 상당수가 Meta (Facebook), Tesla, Microsoft를 비롯하여 OpenAI와 같은 인공지능 연구 기업에서도 연구를 수행하고 제품에 머신러닝을 도입하기 위해 PyTorch를 사용합니다.\n\n\n\n산업 및 연구 분야에서 사용되는 PyTorch\n\n\n예를 들어, Andrej Karpathy(Tesla의 AI 책임자)는 Tesla가 자율 주행 컴퓨터 비전 모델을 구동하기 위해 PyTorch를 어떻게 사용하는지에 대해 여러 차례 강연(PyTorch DevCon 2019, Tesla AI Day 2021)을 한 바 있습니다.\nPyTorch는 농업과 같은 다른 산업 분야에서도 트랙터의 컴퓨터 비전을 구동하는 데 사용됩니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#왜-pytorch를-사용해야-하나요",
    "href": "00_pytorch_fundamentals.html#왜-pytorch를-사용해야-하나요",
    "title": "00 - PyTorch 기초",
    "section": "왜 PyTorch를 사용해야 하나요?",
    "text": "왜 PyTorch를 사용해야 하나요?\n머신러닝 연구자들은 PyTorch 사용을 선호합니다. 2022년 2월 현재, PyTorch는 머신러닝 연구 논문과 그에 부수되는 코드 저장소를 추적하는 웹사이트인 Papers With Code에서 가장 많이 사용되는 딥러닝 프레임워크입니다.\n또한 PyTorch는 백그라운드에서 GPU 가속(코드 실행 속도 향상)과 같은 많은 것들을 처리해 줍니다.\n따라서 여러분은 데이터 조작과 알고리즘 작성에 집중할 수 있으며, PyTorch가 이를 빠르게 실행되도록 보장합니다.\nTesla나 Meta (Facebook) 같은 기업들이 수백 개의 애플리케이션을 구동하고, 수천 대의 자동차를 운전하며, 수십억 명의 사람들에게 콘텐츠를 전달하기 위해 모델을 구축하는 데 PyTorch를 사용한다면, 개발 측면에서도 그 성능은 분명히 입증된 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#이-모듈에서-다룰-내용",
    "href": "00_pytorch_fundamentals.html#이-모듈에서-다룰-내용",
    "title": "00 - PyTorch 기초",
    "section": "이 모듈에서 다룰 내용",
    "text": "이 모듈에서 다룰 내용\n이 과정은 여러 섹션(노트북)으로 나누어져 있습니다.\n각 노트북은 PyTorch 내의 중요한 아이디어와 개념을 다룹니다.\n이후의 노트북은 이전 노트북의 지식을 바탕으로 구성됩니다(번호는 00, 01, 02 순으로 시작하여 끝까지 이어집니다).\n이 노트북은 머신러닝과 딥러닝의 기본 구성 요소인 텐서(tensor)를 다룹니다.\n구체적으로 다음 내용을 다룹니다:\n\n\n\n주제\n내용\n\n\n\n\n텐서 소개\n텐서는 모든 머신러닝 및 딥러닝의 기본 구성 요소입니다.\n\n\n텐서 생성하기\n텐서는 거의 모든 종류의 데이터(이미지, 단어, 숫자 표)를 나타낼 수 있습니다.\n\n\n텐서에서 정보 가져오기\n정보를 텐서에 넣을 수 있다면, 다시 꺼내고 싶을 수도 있습니다.\n\n\n텐서 조작하기\n머신러닝 알고리즘(신경망 등)은 더하기, 곱하기, 결합하기 등 다양한 방식으로 텐서를 조작하는 과정을 포함합니다.\n\n\n텐서 모양(shape) 다루기\n머신러닝에서 가장 흔한 문제 중 하나는 모양 불일치(잘못된 모양의 텐서를 다른 텐서와 혼합하려는 경우)를 다루는 것입니다.\n\n\n텐서 인덱싱\n파이썬 리스트나 NumPy 배열에서 인덱싱을 해보셨다면 텐서와 매우 유사하지만, 훨씬 더 많은 차원을 가질 수 있습니다.\n\n\nPyTorch 텐서와 NumPy 혼합하기\nPyTorch는 텐서(torch.Tensor)를 다루고, NumPy는 배열(np.ndarray)을 선호합니다. 때로는 이 둘을 혼합하여 사용해야 할 때가 있습니다.\n\n\n재현성\n머신러닝은 매우 실험적이며 작동을 위해 많은 무작위성을 사용하기 때문에, 때로는 그 무작위성이 너무 무작위적이지 않기를 원할 때가 있습니다.\n\n\nGPU에서 텐서 실행하기\nGPU(그래픽 처리 장치)는 코드를 더 빠르게 만들어주며, PyTorch는 GPU에서 코드를 쉽게 실행할 수 있게 해줍니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#도움을-받을-수-있는-곳",
    "href": "00_pytorch_fundamentals.html#도움을-받을-수-있는-곳",
    "title": "00 - PyTorch 기초",
    "section": "도움을 받을 수 있는 곳",
    "text": "도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 개발자 포럼도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#pytorch-임포트하기",
    "href": "00_pytorch_fundamentals.html#pytorch-임포트하기",
    "title": "00 - PyTorch 기초",
    "section": "PyTorch 임포트하기",
    "text": "PyTorch 임포트하기\n\n참고: 이 노트북의 코드를 실행하기 전에 PyTorch 설치 단계를 거쳐야 합니다.\n하지만 Google Colab에서 실행 중이라면, 모든 것이 작동할 것입니다(Google Colab에는 PyTorch 및 기타 라이브러리가 이미 설치되어 있습니다).\n\n먼저 PyTorch를 임포트하고 사용 중인 버전을 확인해 보겠습니다.\n\nimport torch\ntorch.__version__\n\n'1.10.0+cu111'\n\n\n좋습니다. PyTorch 1.10.0(2021년 12월 기준)이 설치되어 있는 것 같네요. 즉, 이 자료를 학습할 때 PyTorch 1.10.0과의 호환성이 가장 높을 것이며, 버전 번호가 그보다 훨씬 높으면 약간의 불일치를 느낄 수 있습니다.\n문제가 발생하면 GitHub Discussions 페이지에 게시해 주세요.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#텐서-소개",
    "href": "00_pytorch_fundamentals.html#텐서-소개",
    "title": "00 - PyTorch 기초",
    "section": "텐서 소개",
    "text": "텐서 소개\n이제 PyTorch를 임포트했으니 텐서에 대해 배워볼 시간입니다.\n텐서는 머신러닝의 기본 구성 요소입니다.\n텐서의 역할은 데이터를 수치적으로 나타내는 것입니다.\n예를 들어, 이미지를 [3, 224, 224] 모양의 텐서로 나타낼 수 있는데, 이는 [색상_채널, 높이, 너비]를 의미합니다. 즉, 이미지는 3개의 색상 채널(빨강, 초록, 파랑), 224픽셀의 높이 및 224픽셀의 너비를 가집니다.\n\n\n\n입력 이미지에서 이미지의 텐서 표현으로 전환되는 예시. 이미지는 3개의 색상 채널뿐만 아니라 높이와 너비를 나타내는 숫자로 분해됩니다.\n\n\n텐서 용어(텐서를 설명하는 데 사용되는 언어)로 말하면, 이 텐서는 색상_채널, 높이, 너비에 대해 세 개의 차원을 가집니다.\n하지만 너무 앞서 나갔네요.\n직접 코딩하며 텐서에 대해 더 자세히 알아봅시다.\n\n텐서 생성하기\nPyTorch는 텐서를 사랑합니다. 얼마나 사랑하느냐 하면 torch.Tensor 클래스만을 위한 전용 문서 페이지가 있을 정도입니다.\n첫 번째 숙제는 torch.Tensor 문서를 10분 동안 읽어보는 것입니다. 하지만 나중에 하셔도 됩니다.\n코드를 작성해 봅시다.\n가장 먼저 생성할 것은 스칼라(scalar)입니다.\n스칼라는 단일 숫자이며, 텐서 용어로는 0차원 텐서입니다.\n\n참고: 이것이 이 과정의 트렌드입니다. 구체적인 코드를 작성하는 데 집중할 것입니다. 하지만 종종 PyTorch 문서를 읽고 익숙해지는 연습을 과제로 내드릴 것입니다. 결국 이 과정을 마치고 나면 의심할 여지 없이 더 많은 것을 배우고 싶어질 것이기 때문입니다. 그리고 문서는 여러분이 매우 자주 찾게 될 장소입니다.\n\n\n# 스칼라\nscalar = torch.tensor(7)\nscalar\n\ntensor(7)\n\n\n위의 출력이 tensor(7)로 나오는 것을 보셨나요?\n이는 scalar가 단일 숫자이지만 torch.Tensor 유형임을 의미합니다.\nndim 속성을 사용하여 텐서의 차원을 확인할 수 있습니다.\n\nscalar.ndim\n\n0\n\n\n텐서에서 숫자를 가져오고 싶다면 어떻게 해야 할까요?\n즉, torch.Tensor를 파이썬 정수로 바꾸려면 말이죠.\n그렇게 하려면 item() 메서드를 사용할 수 있습니다.\n\n# 텐서 내의 파이썬 숫자 가져오기 (단일 요소 텐서에서만 작동함)\nscalar.item()\n\n7\n\n\n좋습니다. 이제 벡터(vector)를 살펴봅시다.\n벡터는 단일 차원 텐서이지만 많은 숫자를 포함할 수 있습니다.\n예를 들어, 집의 [침실 수, 욕실 수]를 설명하기 위해 벡터 [3, 2]를 가질 수 있습니다. 또는 집의 [침실 수, 욕실 수, 주차 공간 수]를 설명하기 위해 [3, 2, 2]를 가질 수 있습니다.\n여기서 중요한 경향은 벡터가 나타낼 수 있는 것이 유연하다는 것입니다(텐서도 마찬가지입니다).\n\n# 벡터\nvector = torch.tensor([7, 7])\nvector\n\ntensor([7, 7])\n\n\n멋지네요. vector에는 제가 가장 좋아하는 숫자인 두 개의 7이 들어 있습니다.\n차원이 몇 개일 것 같나요?\n\n# 벡터의 차원 수 확인\nvector.ndim\n\n1\n\n\n음, 이상하네요. vector에는 두 개의 숫자가 들어 있지만 차원은 하나뿐입니다.\n비결을 하나 알려드리죠.\nPyTorch에서 텐서의 차원 수는 바깥쪽 대괄호([)의 개수로 알 수 있으며, 한쪽만 세면 됩니다.\nvector에는 대괄호가 몇 개 있나요?\n텐서의 또 다른 중요한 개념은 shape 속성입니다. 모양(shape)은 내부의 요소가 어떻게 배열되어 있는지를 알려줍니다.\nvector의 모양을 확인해 봅시다.\n\n# 벡터의 모양 확인\nvector.shape\n\ntorch.Size([2])\n\n\n위의 결과는 torch.Size([2])를 반환하는데, 이는 우리 벡터의 모양이 [2]임을 의미합니다. 이는 대괄호 안에 두 개의 요소([7, 7])를 넣었기 때문입니다.\n이제 행렬(matrix)을 살펴봅시다.\n\n# 행렬\nMATRIX = torch.tensor([[7, 8], \n                       [9, 10]])\nMATRIX\n\ntensor([[ 7,  8],\n        [ 9, 10]])\n\n\n와! 더 많은 숫자네요! 행렬은 벡터만큼 유연하지만 차원이 하나 더 추가되었습니다.\n\n# 차원 수 확인\nMATRIX.ndim\n\n2\n\n\nMATRIX는 두 개의 차원을 가집니다(한쪽 면의 바깥쪽 대괄호 개수를 세어보셨나요?).\n어떤 모양(shape)을 가질 것 같나요?\n\nMATRIX.shape\n\ntorch.Size([2, 2])\n\n\nMATRIX가 깊이로 두 개의 요소, 너비로 두 개의 요소를 가지므로 torch.Size([2, 2])라는 출력을 얻습니다.\n이제 텐서(tensor)를 만들어 볼까요?\n\n# 텐서\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n\ntensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\n\n\n우와! 정말 멋진 텐서네요.\n텐서는 거의 무엇이든 나타낼 수 있다는 점을 강조하고 싶습니다.\n방금 만든 것은 스테이크와 아몬드 버터 매장(제가 가장 좋아하는 두 음식입니다)의 판매 수치일 수도 있습니다.\n\n\n\n요일, 스테이크 판매량, 아몬드 버터 판매량을 보여주는 Google 스프레드시트의 간단한 텐서\n\n\n차원이 몇 개일 것 같나요? (힌트: 대괄호 세기 비결을 사용하세요)\n\n# TENSOR의 차원 수 확인\nTENSOR.ndim\n\n3\n\n\n그 모양은 어떨까요?\n\n# TENSOR의 모양 확인\nTENSOR.shape\n\ntorch.Size([1, 3, 3])\n\n\n좋습니다. torch.Size([1, 3, 3])이 출력되네요.\n차원은 바깥쪽에서 안쪽 순서입니다.\n즉, 3x3인 차원이 1개 있다는 뜻입니다.\n\n\n\n다양한 텐서 차원의 예시\n\n\n\n참고: scalar와 vector에는 소문자를 사용하고 MATRIX와 TENSOR에는 대문자를 사용한 것을 눈치채셨을 것입니다. 이는 의도적인 것입니다. 실제로 스칼라와 벡터는 y나 a와 같은 소문자로 표시되는 경우가 많습니다. 그리고 행렬과 텐서는 X나 W와 같은 대문자로 표시됩니다.\n또한 행렬(matrix)과 텐서(tensor)라는 이름이 혼용되어 사용되는 것을 볼 수 있습니다. 이는 흔한 일입니다. PyTorch에서는 종종 torch.Tensor를 다루기 때문입니다(그래서 텐서라는 이름이 붙었습니다). 하지만 내부 내용의 모양과 차원에 따라 실제로 무엇인지가 결정됩니다.\n\n요약해 봅시다.\n\n\n\n\n\n\n\n\n\n이름\n무엇인가요?\n차원 수\n소문자 또는 대문자 (보통/예시)\n\n\n\n\n스칼라(scalar)\n단일 숫자\n0\n소문자 (a)\n\n\n벡터(vector)\n방향이 있는 숫자(예: 방향이 있는 풍속)이지만 다른 많은 숫자도 가질 수 있음\n1\n소문자 (y)\n\n\n행렬(matrix)\n숫자의 2차원 배열\n2\n대문자 (Q)\n\n\n텐서(tensor)\n숫자의 n차원 배열\n어떤 수든 가능, 0차원 텐서는 스칼라, 1차원 텐서는 벡터\n대문자 (X)\n\n\n\n\n\n\n스칼라 벡터 행렬 텐서의 모습\n\n\n\n\n무작위 텐서\n우리는 텐서가 어떤 형태의 데이터를 나타낸다는 것을 확인했습니다.\n그리고 신경망과 같은 머신러닝 모델은 텐서 내의 패턴을 조사하고 찾기 위해 조작합니다.\n하지만 PyTorch로 머신러닝 모델을 구축할 때 (우리가 했던 것처럼) 손으로 텐서를 직접 만드는 경우는 드뭅니다.\n대신, 머신러닝 모델은 종종 대량의 무작위 숫자 텐서로 시작하여 데이터를 처리하면서 이러한 무작위 숫자를 조정하여 데이터를 더 잘 나타내도록 합니다.\n본질적으로:\n무작위 숫자로 시작 -&gt; 데이터 확인 -&gt; 무작위 숫자 업데이트 -&gt; 데이터 확인 -&gt; 무작위 숫자 업데이트...\n데이터 과학자로서 여러분은 머신러닝 모델이 무작위 숫자를 어떻게 시작(초기화)하고, 데이터를 어떻게 확인(표현)하며, 어떻게 업데이트(최적화)할지 정의할 수 있습니다.\n나중에 이러한 단계를 직접 실습해 볼 것입니다.\n지금은 무작위 숫자로 텐서를 생성하는 방법을 알아봅시다.\ntorch.rand()를 사용하고 size 매개변수를 전달하여 이를 수행할 수 있습니다.\n\n# 크기가 (3, 4)인 무작위 텐서 생성\nrandom_tensor = torch.rand(size=(3, 4))\nrandom_tensor, random_tensor.dtype\n\n(tensor([[0.4090, 0.2527, 0.8699, 0.2002],\n         [0.8421, 0.1428, 0.1431, 0.0111],\n         [0.2281, 0.0345, 0.6734, 0.3866]]), torch.float32)\n\n\ntorch.rand()의 유연성은 size를 우리가 원하는 대로 조정할 수 있다는 점입니다.\n예를 들어, 일반적인 이미지 모양인 [224, 224, 3] ([높이, 너비, 색상_채널])의 무작위 텐서를 원한다고 가정해 봅시다.\n\n# 크기가 (224, 224, 3)인 무작위 텐서 생성\nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n\n(torch.Size([224, 224, 3]), 3)\n\n\n\n\n0과 1\n때로는 텐서를 0이나 1로 채우고 싶을 때가 있습니다.\n이는 마스킹(masking) 작업에서 많이 발생합니다(예: 한 텐서의 일부 값을 0으로 마스킹하여 모델이 학습하지 않도록 알림).\ntorch.zeros()를 사용하여 0으로 가득 찬 텐서를 만들어 봅시다.\n여기서도 size 매개변수가 사용됩니다.\n\n# 모두 0인 텐서 생성\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n\n(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]), torch.float32)\n\n\ntorch.ones()를 대신 사용하여 모두 1인 텐서를 만드는 것도 동일하게 수행할 수 있습니다.\n\n# 모두 1인 텐서 생성\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n\n(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]), torch.float32)\n\n\n\n\n범위 및 유사 텐서 생성\n때로는 1에서 10 또는 0에서 100과 같은 숫자 범위가 필요할 수 있습니다.\ntorch.arange(start, end, step)을 사용하여 이를 수행할 수 있습니다.\n여기서: * start = 범위의 시작 (예: 0) * end = 범위의 끝 (예: 10) * step = 각 값 사이의 간격 (예: 1)\n\n참고: 파이썬에서는 range()를 사용하여 범위를 생성할 수 있습니다. 하지만 PyTorch에서 torch.range()는 더 이상 사용되지 않으며(deprecated) 미래에 오류가 발생할 수 있습니다.\n\n\n# torch.arange() 사용, torch.range()는 권장되지 않음\nzero_to_ten_deprecated = torch.range(0, 10) # 참고: 미래에 오류가 발생할 수 있음\n\n# 0에서 10까지의 값 범위 생성\nzero_to_ten = torch.arange(start=0, end=10, step=1)\nzero_to_ten\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  \n\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n때로는 다른 텐서와 동일한 모양을 가진 특정 유형의 텐서가 필요할 수 있습니다.\n예를 들어, 이전 텐서와 모양이 같은 모두 0인 텐서가 있을 수 있습니다.\n그렇게 하려면 각각 input과 동일한 모양의 0 또는 1로 채워진 텐서를 반환하는 torch.zeros_like(input) 또는 torch.ones_like(input)을 사용할 수 있습니다.\n\n# 다른 텐서와 유사한 0으로 된 텐서 생성 가능\nten_zeros = torch.zeros_like(input=zero_to_ten) # 같은 모양을 가짐\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n\n텐서 데이터 타입\nPyTorch에서는 다양한 텐서 데이터 타입을 사용할 수 있습니다.\n어떤 것은 CPU에 특화되어 있고 어떤 것은 GPU에 더 적합합니다.\n어떤 것이 무엇인지 익히는 데는 시간이 좀 걸릴 수 있습니다.\n일반적으로 어디서나 torch.cuda가 보이면, 해당 텐서는 GPU에서 사용되고 있는 것입니다(Nvidia GPU는 CUDA라는 컴퓨팅 툴킷을 사용하기 때문입니다).\n가장 일반적인 유형(및 일반적으로 기본값)은 torch.float32 또는 torch.float입니다.\n이를 “32비트 부동 소수점”이라고 합니다.\n하지만 16비트 부동 소수점(torch.float16 또는 torch.half)과 64비트 부동 소수점(torch.float64 또는 torch.double)도 있습니다.\n더 복잡하게는 8비트, 16비트, 32비트, 64비트 정수도 있습니다.\n그리고 더 많이요!\n\n참고: 정수(integer)는 7과 같이 딱 떨어지는 숫자이고, 부동 소수점(float)은 7.0과 같이 소수점이 있습니다.\n\n이 모든 이유가 있는 것은 컴퓨팅의 정밀도(precision) 때문입니다.\n정밀도는 숫자를 설명하는 데 사용되는 세부 정보의 양입니다.\n정밀도 값(8, 16, 32)이 높을수록 숫자를 표현하는 데 더 많은 세부 정보와 데이터가 사용됩니다.\n이는 딥러닝과 수치 컴퓨팅에서 중요한데, 수많은 연산을 수행하기 때문에 계산해야 할 세부 정보가 많을수록 더 많은 컴퓨팅 파워를 사용해야 하기 때문입니다.\n따라서 정밀도가 낮은 데이터 타입은 일반적으로 계산 속도가 빠르지만 정확도와 같은 평가 지표에서 약간의 성능 손실이 발생합니다(계산은 빠르지만 정확도는 떨어짐).\n\n리소스: * 사용 가능한 모든 텐서 데이터 타입 목록은 PyTorch 문서를 참조하세요. * 컴퓨팅에서의 정밀도에 대한 개요는 Wikipedia 페이지를 읽어보세요.\n\n특정 데이터 타입으로 텐서를 생성하는 방법을 알아봅시다. dtype 매개변수를 사용하여 수행할 수 있습니다.\n\n# 텐서의 기본 데이터 타입은 float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # 기본값은 None이며, torch.float32 또는 전달된 데이터 타입으로 설정됨\n                               device=None, # 기본값은 None이며, 기본 텐서 유형을 사용함\n                               requires_grad=False) # True이면 텐서에서 수행된 연산이 기록됨\n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='cpu'))\n\n\n모양 문제(텐서 모양이 일치하지 않음) 외에도 PyTorch에서 겪게 될 가장 흔한 두 가지 문제는 데이터 타입과 장치(device) 문제입니다.\n예를 들어, 텐서 중 하나는 torch.float32이고 다른 하나는 torch.float16인 경우입니다(PyTorch는 종종 텐서가 동일한 형식인 것을 선호합니다).\n또는 텐서 중 하나는 CPU에 있고 다른 하나는 GPU에 있는 경우입니다(PyTorch는 텐서 간의 계산이 동일한 장치에서 수행되는 것을 선호합니다).\n나중에 이 장치에 대한 이야기를 더 자세히 다룰 것입니다.\n지금은 dtype=torch.float16인 텐서를 만들어 보겠습니다.\n\nfloat_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half도 작동함\n\nfloat_16_tensor.dtype\n\ntorch.float16",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#텐서에서-정보-가져오기",
    "href": "00_pytorch_fundamentals.html#텐서에서-정보-가져오기",
    "title": "00 - PyTorch 기초",
    "section": "텐서에서 정보 가져오기",
    "text": "텐서에서 정보 가져오기\n텐서를 생성한 후에는 (또는 다른 사람이나 PyTorch 모듈이 대신 생성한 후에는) 텐서에서 정보를 얻고 싶을 수 있습니다.\n이전에 살펴보았지만, 텐서에 대해 알아내고 싶은 가장 일반적인 세 가지 속성은 다음과 같습니다. * shape - 텐서의 모양은 무엇인가? (일부 연산에는 특정 모양 규칙이 필요함) * dtype - 텐서 내의 요소는 어떤 데이터 타입으로 저장되어 있는가? * device - 텐서는 어떤 장치에 저장되어 있는가? (보통 GPU 또는 CPU)\n무작위 텐서를 생성하고 세부 정보를 알아봅시다.\n\n# 텐서 생성\nsome_tensor = torch.rand(3, 4)\n\n# 세부 정보 확인\nprint(some_tensor)\nprint(f\"텐서의 모양: {some_tensor.shape}\")\nprint(f\"텐서의 데이터 타입: {some_tensor.dtype}\")\nprint(f\"텐서가 저장된 장치: {some_tensor.device}\") # 기본값은 CPU\n\ntensor([[0.7799, 0.8140, 0.0893, 0.2062],\n        [0.7525, 0.3845, 0.8207, 0.4587],\n        [0.9277, 0.8166, 0.9052, 0.0953]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n참고: PyTorch에서 문제가 발생하면 위의 세 가지 속성 중 하나와 관련된 경우가 매우 많습니다. 따라서 오류 메시지가 나타나면 스스로에게 “뭐, 뭐, 어디”라는 짧은 노래를 불러보세요. * “내 텐서의 모양은 뭐고? 데이터 타입은 뭐고, 어디에 저장되어 있지? 모양은 뭐고, 타입은 뭐고, 어디 어디 어디”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#텐서-조작-텐서-연산",
    "href": "00_pytorch_fundamentals.html#텐서-조작-텐서-연산",
    "title": "00 - PyTorch 기초",
    "section": "텐서 조작 (텐서 연산)",
    "text": "텐서 조작 (텐서 연산)\n딥러닝에서 데이터(이미지, 텍스트, 비디오, 오디오, 단백질 구조 등)는 텐서로 표현됩니다.\n모델은 해당 텐서를 조사하고 입력 데이터의 패턴 표현을 생성하기 위해 텐서에 대해 일련의 연산(수백만 개 이상일 수 있음)을 수행하여 학습합니다.\n이러한 연산은 종종 다음과 같은 멋진 춤과 같습니다. * 덧셈 * 뺄셈 * 곱셈 (요소별) * 나눗셈 * 행렬 곱셈\n그리고 이것이 전부입니다. 물론 여기저기 몇 가지 더 있지만 이것이 신경망의 기본 구성 요소입니다.\n이러한 구성 요소를 올바른 방식으로 쌓으면 (마치 레고처럼!) 가장 정교한 신경망을 만들 수 있습니다.\n\n기본 연산\n덧셈(+), 뺄셈(-), 곱셈(*)과 같은 몇 가지 기본적인 연산부터 시작하겠습니다.\n여러분이 생각하는 대로 작동합니다.\n\n# 값의 텐서를 생성하고 숫자를 더하기\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# 10을 곱하기\ntensor * 10\n\ntensor([10, 20, 30])\n\n\n위의 텐서 값이 tensor([110, 120, 130])으로 끝나지 않은 점에 유의하세요. 이는 텐서 내부의 값이 재할당되지 않는 한 변경되지 않기 때문입니다.\n\n# 텐서는 재할당되지 않는 한 변경되지 않음\ntensor\n\ntensor([1, 2, 3])\n\n\n숫자를 빼보고 이번에는 tensor 변수를 재할당해 보겠습니다.\n\n# 빼고 재할당하기\ntensor = tensor - 10\ntensor\n\ntensor([-9, -8, -7])\n\n\n\n# 더하고 재할당하기\ntensor = tensor + 10\ntensor\n\ntensor([1, 2, 3])\n\n\nPyTorch에는 기본 연산을 수행하기 위한 torch.mul()(곱셈의 약자) 및 torch.add()와 같은 많은 내장 함수도 있습니다.\n\n# torch 함수를 사용할 수도 있음\ntorch.multiply(tensor, 10)\n\ntensor([10, 20, 30])\n\n\n\n# 원본 텐서는 여전히 변경되지 않음\ntensor\n\ntensor([1, 2, 3])\n\n\n하지만 torch.mul() 대신 *와 같은 연산자 기호를 사용하는 것이 더 일반적입니다.\n\n# 요소별 곱셈 (각 요소는 해당 위치의 요소와 곱해짐, 인덱스 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"결과:\", tensor * tensor)\n\ntensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n\n\n\n\n행렬 곱셈 (Matrix Multiplication)\n머신러닝 및 딥러닝 알고리즘(신경망 등)에서 가장 흔한 연산 중 하나는 행렬 곱셈입니다.\nPyTorch는 torch.matmul() 메서드에 행렬 곱셈 기능을 구현합니다.\n기억해야 할 행렬 곱셈의 주요 규칙 두 가지는 다음과 같습니다. 1. 내부 차원(inner dimensions)이 일치해야 합니다. * (3, 2) @ (3, 2)는 작동하지 않음 * (2, 3) @ (3, 2)는 작동함 * (3, 2) @ (2, 3)은 작동함 2. 결과 행렬은 외부 차원(outer dimensions)의 모양을 가집니다. * (2, 3) @ (3, 2) -&gt; (2, 2) * (3, 2) @ (2, 3) -&gt; (3, 3)\n\n참고: 파이썬에서 “@”는 행렬 곱셈 기호입니다.\n\n\n리소스: torch.matmul()을 사용하는 행렬 곱셈의 모든 규칙은 PyTorch 문서에서 확인할 수 있습니다.\n\n텐서를 생성하고 요소별 곱셈과 행렬 곱셈을 수행해 보겠습니다.\n\nimport torch\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n\ntorch.Size([3])\n\n\n요소별 곱셈과 행렬 곱셈의 차이는 값의 합산입니다.\n값이 [1, 2, 3]인 tensor 변수의 경우:\n\n\n\n\n\n\n\n\n연산\n계산\n코드\n\n\n\n\n요소별 곱셈\n[1*1, 2*2, 3*3] = [1, 4, 9]\ntensor * tensor\n\n\n행렬 곱셈\n[1*1 + 2*2 + 3*3] = [14]\ntensor.matmul(tensor)\n\n\n\n\n# 요소별 행렬 곱셈\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\n\n# 행렬 곱셈\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\n\n# 권장되지는 않지만 행렬 곱셈에 \"@\" 기호를 사용할 수도 있음\ntensor @ tensor\n\ntensor(14)\n\n\n행렬 곱셈을 수동으로 할 수도 있지만 권장하지 않습니다.\n내장된 torch.matmul() 메서드가 더 빠릅니다.\n\n%%time\n# 수동 행렬 곱셈\n# (for 루프를 사용한 연산은 계산 비용이 많이 들므로 가급적 피하세요)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n\nCPU times: user 146 µs, sys: 38 µs, total: 184 µs\nWall time: 227 µs\n\n\n\n%%time\ntorch.matmul(tensor, tensor)\n\nCPU times: user 27 µs, sys: 7 µs, total: 34 µs\nWall time: 36.7 µs\n\n\ntensor(14)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#딥러닝에서-가장-흔한-오류-중-하나-모양-오류",
    "href": "00_pytorch_fundamentals.html#딥러닝에서-가장-흔한-오류-중-하나-모양-오류",
    "title": "00 - PyTorch 기초",
    "section": "딥러닝에서 가장 흔한 오류 중 하나 (모양 오류)",
    "text": "딥러닝에서 가장 흔한 오류 중 하나 (모양 오류)\n딥러닝의 대부분은 행렬을 곱하고 연산을 수행하는 것이고, 행렬에는 결합할 수 있는 모양과 크기에 대한 엄격한 규칙이 있기 때문에 딥러닝에서 겪게 될 가장 흔한 오류 중 하나는 모양 불일치(shape mismatch)입니다.\n\n# 모양이 올바른 방식이어야 함\ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (오류 발생)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-37-aceec990e652&gt; in &lt;module&gt;()\n      8                          [9, 12]], dtype=torch.float32)\n      9 \n---&gt; 10 torch.matmul(tensor_A, tensor_B) # (오류 발생)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)\n\n\n\ntensor_A와 tensor_B 사이의 내부 차원을 일치시켜 행렬 곱셈이 작동하도록 만들 수 있습니다.\n이를 수행하는 방법 중 하나는 전치(transpose)(주어진 텐서의 차원을 전환함)를 사용하는 것입니다.\nPyTorch에서는 다음 중 하나를 사용하여 전치를 수행할 수 있습니다. * torch.transpose(input, dim0, dim1) - 여기서 input은 전치할 텐서이고 dim0과 dim1은 교체할 차원입니다. * tensor.T - 여기서 tensor는 전치할 텐서입니다.\n후자를 시도해 봅시다.\n\n# tensor_A와 tensor_B 확인\nprint(tensor_A)\nprint(tensor_B)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7., 10.],\n        [ 8., 11.],\n        [ 9., 12.]])\n\n\n\n# tensor_A와 tensor_B.T 확인\nprint(tensor_A)\nprint(tensor_B.T)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n\n\n\n# tensor_B가 전치되었을 때 연산이 작동함\nprint(f\"원본 모양: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"새로운 모양: tensor_A = {tensor_A.shape} (이전과 동일), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"곱셈 수행: {tensor_A.shape} * {tensor_B.T.shape} &lt;- 내부 차원 일치\\n\")\nprint(\"출력:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\n출력 모양: {output.shape}\")\n\nOriginal shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n\n\ntorch.matmul()의 줄임말인 torch.mm()을 사용할 수도 있습니다.\n\n# torch.mm은 matmul의 줄임말\ntorch.mm(tensor_A, tensor_B.T)\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\n\n전치가 없으면 행렬 곱셈의 규칙이 충족되지 않고 위와 같은 오류가 발생합니다.\n시각화 자료는 어떨까요?\n\n\n\n행렬 곱셈의 시각적 데모\n\n\nhttp://matrixmultiplication.xyz/ 에서 이와 같은 자신만의 행렬 곱셈 시각화 자료를 만들 수 있습니다.\n\n참고: 이와 같은 행렬 곱셈은 두 행렬의 내적(dot product)이라고도 합니다.\n\n신경망은 행렬 곱셈과 내적으로 가득 차 있습니다.\n피드포워드 레이어(feed-forward layer) 또는 완전 연결 레이어(fully connected layer)라고도 하는 torch.nn.Linear() 모듈(나중에 실제 작동하는 모습을 볼 것입니다)은 입력 x와 가중치 행렬 A 사이의 행렬 곱셈을 구현합니다.\n\\[\ny = x\\cdot{A^T} + b\n\\]\n여기서: * x는 레이어의 입력입니다(딥러닝은 torch.nn.Linear() 및 기타 레이어를 서로 겹쳐 쌓은 것입니다). * A는 레이어에 의해 생성된 가중치 행렬입니다. 이것은 무작위 숫자로 시작하여 신경망이 데이터의 패턴을 더 잘 나타내도록 학습함에 따라 조정됩니다(가중치 행렬이 전치되기 때문에 “T”에 유의하세요). * 참고: 가중치 행렬을 나타내기 위해 W나 X와 같은 다른 문자가 사용되는 것을 종종 볼 수 있습니다. * b는 가중치와 입력을 약간 오프셋하는 데 사용되는 편향(bias) 용어입니다. * y는 출력입니다(입력에서 패턴을 발견하기를 바라며 입력을 조작한 결과입니다).\n이것은 선형 함수(고등학교나 다른 곳에서 \\(y = mx+b\\)와 같은 것을 본 적이 있을 것입니다)이며, 직선을 그리는 데 사용될 수 있습니다!\n선형 레이어를 가지고 놀아봅시다.\n아래에서 in_features와 out_features의 값을 변경하고 어떤 일이 일어나는지 확인해 보세요.\n모양과 관련하여 눈에 띄는 점이 있나요?\n\n# 선형 레이어는 무작위 가중치 행렬로 시작하므로 재현 가능하게 만듭시다(나중에 자세히 설명)\ntorch.manual_seed(42)\n# 이것은 행렬 곱셈을 사용함\nlinear = torch.nn.Linear(in_features=2, # in_features = 입력의 내부 차원과 일치해야 함\n                         out_features=6) # out_features = 출력 값을 설명함\nx = tensor_A\noutput = linear(x)\nprint(f\"입력 모양: {x.shape}\\n\")\nprint(f\"출력:\\n{output}\\n\\n출력 모양: {output.shape}\")\n\nInput shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n\n\n\n질문: 위에서 in_features를 2에서 3으로 변경하면 어떻게 되나요? 오류가 발생하나요? 오류에 대응하기 위해 입력(x)의 모양을 어떻게 변경할 수 있을까요? 힌트: 위에서 tensor_B에 무엇을 해야 했나요?\n\n행렬 곱셈을 처음 접한다면 처음에는 혼란스러운 주제일 수 있습니다.\n하지만 몇 번 연습하고 신경망을 직접 분석해 보면 어디에나 있다는 것을 알게 될 것입니다.\n기억하세요, 행렬 곱셈이 여러분에게 필요한 전부입니다.\n\n\n\n행렬 곱셈이 여러분에게 필요한 전부입니다\n\n\n신경망 레이어를 파헤치고 직접 구축하기 시작하면 어디에서나 행렬 곱셈을 발견하게 될 것입니다. 출처: https://marksaroufim.substack.com/p/working-class-deep-learner\n\n최소, 최대, 평균, 합계 등 찾기 (집계)\n텐서를 조작하는 몇 가지 방법을 살펴보았으니, 이제 텐서를 집계하는(많은 값에서 적은 값으로 가는) 몇 가지 방법을 살펴보겠습니다.\n먼저 텐서를 생성한 다음 그 텐서의 최대값, 최소값, 평균 및 합계를 구합니다.\n\n# 텐서 생성\nx = torch.arange(0, 100, 10)\nx\n\ntensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n\n\n이제 집계를 수행해 봅시다.\n\nprint(f\"최소값: {x.min()}\")\nprint(f\"최대값: {x.max()}\")\n# print(f\"평균: {x.mean()}\") # 오류 발생\nprint(f\"평균: {x.type(torch.float32).mean()}\") # float 데이터 타입 없이는 작동하지 않음\nprint(f\"합계: {x.sum()}\")\n\nMinimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n\n\n\n참고: torch.mean()과 같은 일부 메서드는 텐서가 torch.float32(가장 일반적임) 또는 다른 특정 데이터 타입이어야 하며, 그렇지 않으면 연산이 실패한다는 것을 알 수 있습니다.\n\ntorch 메서드를 사용하여 위와 동일하게 수행할 수도 있습니다.\n\ntorch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n\n(tensor(90), tensor(0), tensor(45.), tensor(450))\n\n\n\n\n위치별 최소/최대\n각각 torch.argmax() 및 torch.argmin()을 사용하여 최대값 또는 최소값이 발생하는 텐서의 인덱스를 찾을 수도 있습니다.\n이는 실제 값이 아닌 가장 높은(또는 가장 낮은) 값이 있는 위치만 원할 때 유용합니다(나중에 softmax 활성화 함수를 사용할 때 이 내용을 보게 될 것입니다).\n\n# 텐서 생성\ntensor = torch.arange(10, 100, 10)\nprint(f\"텐서: {tensor}\")\n\n# 최대값 및 최소값의 인덱스 반환\nprint(f\"최대값이 있는 인덱스: {tensor.argmax()}\")\nprint(f\"최소값이 있는 인덱스: {tensor.argmin()}\")\n\nTensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n\n\n\n\n텐서 데이터 타입 변경\n앞서 언급했듯이 딥러닝 연산의 일반적인 문제는 텐서의 데이터 타입이 서로 다른 것입니다.\n하나의 텐서가 torch.float64이고 다른 하나가 torch.float32이면 오류가 발생할 수 있습니다.\n하지만 해결 방법이 있습니다.\ntorch.Tensor.type(dtype=None)을 사용하여 텐서의 데이터 타입을 변경할 수 있습니다. 여기서 dtype 매개변수는 사용하려는 데이터 타입입니다.\n먼저 텐서를 생성하고 데이터 타입을 확인해 보겠습니다(기본값은 torch.float32입니다).\n\n# 텐서 생성 및 데이터 타입 확인\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n\ntorch.float32\n\n\n이제 이전과 동일하게 다른 텐서를 생성하지만 데이터 타입을 torch.float16으로 변경해 보겠습니다.\n\n# float16 텐서 생성\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n\ntensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)\n\n\n그리고 torch.int8 텐서를 만들기 위해 비슷한 작업을 수행할 수 있습니다.\n\n# int8 텐서 생성\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n\ntensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)\n\n\n\n참고: 서로 다른 데이터 타입은 처음에는 혼란스러울 수 있습니다. 하지만 이렇게 생각해 보세요. 숫자(예: 32, 16, 8)가 작을수록 컴퓨터가 값을 덜 정밀하게 저장합니다. 저장 용량이 적으면 일반적으로 계산 속도가 빨라지고 전체 모델 크기가 작아집니다. 모바일 기반 신경망은 종종 8비트 정수로 작동하는데, float32 버전보다 작고 실행 속도가 빠르지만 정확도는 떨어집니다. 이에 대한 자세한 내용은 컴퓨팅 정밀도에 대해 읽어보시기 바랍니다.\n\n\n과제: 지금까지 꽤 많은 텐서 메서드를 다루었지만 torch.Tensor 문서에는 훨씬 더 많은 메서드가 있습니다. 10분 정도 시간을 내어 스크롤하면서 눈에 띄는 것을 찾아보시길 권장합니다. 클릭해 본 다음 직접 코드로 작성하여 어떤 일이 일어나는지 확인해 보세요.\n\n\n\n재구조화, 쌓기, 압축 및 압축 해제 (Reshaping, stacking, squeezing and unsqueezing)\n종종 내부의 값을 실제로 변경하지 않고 텐서의 모양을 바꾸거나 차원을 변경하고 싶을 때가 있습니다.\n이를 위해 인기 있는 메서드는 다음과 같습니다.\n\n\n\n메서드\n한 줄 설명\n\n\n\n\ntorch.reshape(input, shape)\ninput을 shape(호환되는 경우)로 재구조화함. torch.Tensor.reshape()도 사용 가능.\n\n\ntorch.Tensor.view(shape)\n원래 텐서의 다른 shape 뷰(view)를 반환하지만 원래 텐서와 동일한 데이터를 공유함.\n\n\ntorch.stack(tensors, dim=0)\n새로운 차원(dim)을 따라 일련의 tensors를 결합함. 모든 tensors는 크기가 같아야 함.\n\n\ntorch.squeeze(input)\ninput에서 값이 1인 모든 차원을 제거(압축)함.\n\n\ntorch.unsqueeze(input, dim)\ndim 위치에 값이 1인 차원이 추가된 input을 반환함.\n\n\ntorch.permute(input, dims)\n차원이 dims로 순열(재배열)된 원래 input의 뷰를 반환함.\n\n\n\n왜 이런 작업을 할까요?\n딥러닝 모델(신경망)은 어떤 방식으로든 텐서를 조작하는 것이 전부이기 때문입니다. 그리고 행렬 곱셈의 규칙 때문에 모양이 맞지 않으면 오류가 발생합니다. 이러한 메서드는 텐서의 올바른 요소가 다른 텐서의 올바른 요소와 섞이도록 도와줍니다.\n한번 시도해 봅시다.\n먼저 텐서를 생성합니다.\n\n# 텐서 생성\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))\n\n\n이제 torch.reshape()를 사용하여 차원을 추가해 보겠습니다.\n\n# 추가 차원 더하기\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\ntorch.view()를 사용하여 뷰를 변경할 수도 있습니다.\n\n# 뷰 변경 (원본과 동일한 데이터를 유지하면서 뷰만 변경)\n# 자세히 보기: https://stackoverflow.com/a/54507446/7900723\nz = x.view(1, 7)\nz, z.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\ntorch.view()로 텐서의 뷰를 변경하는 것은 실제로는 동일한 텐서의 새로운 뷰를 생성할 뿐이라는 점을 기억하세요.\n따라서 뷰를 변경하면 원본 텐서도 변경됩니다.\n\n# z를 변경하면 x도 변경됨\nz[:, 0] = 5\nz, x\n\n(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))\n\n\n새로운 텐서를 자신 위에 네 번 쌓고 싶다면 torch.stack()을 사용하여 수행할 수 있습니다.\n\n# 텐서를 서로 위에 쌓기\nx_stacked = torch.stack([x, x, x, x], dim=0) # dim을 dim=1로 변경하고 어떤 일이 일어나는지 확인해 보세요\nx_stacked\n\ntensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])\n\n\n텐서에서 모든 단일 차원을 제거하는 것은 어떨까요?\n그렇게 하려면 torch.squeeze()를 사용할 수 있습니다(저는 이를 텐서를 압축하여 1보다 큰 차원만 남게 하는 것으로 기억합니다).\n\nprint(f\"이전 텐서: {x_reshaped}\")\nprint(f\"이전 모양: {x_reshaped.shape}\")\n\n# x_reshaped에서 추가 차원 제거\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\n새로운 텐서: {x_squeezed}\")\nprint(f\"새로운 모양: {x_squeezed.shape}\")\n\nPrevious tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n\n\n그리고 torch.squeeze()의 반대 작업을 하려면 torch.unsqueeze()를 사용하여 특정 인덱스에 값이 1인 차원을 추가할 수 있습니다.\n\nprint(f\"이전 텐서: {x_squeezed}\")\nprint(f\"이전 모양: {x_squeezed.shape}\")\n\n## unsqueeze로 추가 차원 더하기\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"\\n새로운 텐서: {x_unsqueezed}\")\nprint(f\"새로운 모양: {x_unsqueezed.shape}\")\n\nPrevious tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nPrevious shape: torch.Size([7])\n\nNew tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nNew shape: torch.Size([1, 7])\n\n\ntorch.permute(input, dims)를 사용하여 축 값의 순서를 재배열할 수도 있습니다. 여기서 input은 새로운 dims를 가진 뷰로 변환됩니다.\n\n# 특정 모양의 텐서 생성\nx_original = torch.rand(size=(224, 224, 3))\n\n# 원본 텐서를 순열하여 축 순서 재배열\nx_permuted = x_original.permute(2, 0, 1) # 축을 0-&gt;1, 1-&gt;2, 2-&gt;0으로 이동\n\nprint(f\"이전 모양: {x_original.shape}\")\nprint(f\"새로운 모양: {x_permuted.shape}\")\n\nPrevious shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n\n\n\n참고: 순열(permuting)은 뷰(원본과 동일한 데이터를 공유함)를 반환하므로 순열된 텐서의 값은 원본 텐서와 동일하며 뷰에서 값을 변경하면 원본의 값도 변경됩니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#인덱싱-텐서에서-데이터-선택",
    "href": "00_pytorch_fundamentals.html#인덱싱-텐서에서-데이터-선택",
    "title": "00 - PyTorch 기초",
    "section": "인덱싱 (텐서에서 데이터 선택)",
    "text": "인덱싱 (텐서에서 데이터 선택)\n때로는 텐서에서 특정 데이터(예: 첫 번째 열 또는 두 번째 행만)를 선택하고 싶을 때가 있습니다.\n그렇게 하려면 인덱싱을 사용할 수 있습니다.\n파이썬 리스트나 NumPy 배열에서 인덱싱을 해보셨다면 PyTorch 텐서의 인덱싱도 매우 유사합니다.\n\n# 텐서 생성\nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n\n(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]), torch.Size([1, 3, 3]))\n\n\n값 인덱싱은 바깥쪽 차원에서 안쪽 차원 순서로 진행됩니다(대괄호를 확인하세요).\n\n# 대괄호별로 인덱싱해 봅시다\nprint(f\"첫 번째 대괄호:\\n{x[0]}\") \nprint(f\"두 번째 대괄호: {x[0][0]}\") \nprint(f\"세 번째 대괄호: {x[0][0][0]}\")\n\nFirst square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n\n\n:을 사용하여 “이 차원의 모든 값”을 지정한 다음 쉼표(,)를 사용하여 다른 차원을 추가할 수도 있습니다.\n\n# 0번째 차원의 모든 값과 1번째 차원의 0번 인덱스 가져오기\nx[:, 0]\n\ntensor([[1, 2, 3]])\n\n\n\n# 0번째 및 1번째 차원의 모든 값을 가져오되 2번째 차원의 인덱스 1만 가져오기\nx[:, :, 1]\n\ntensor([[2, 5, 8]])\n\n\n\n# 0차원의 모든 값을 가져오되 1차원과 2차원의 인덱스 1 값만 가져오기\nx[:, 1, 1]\n\ntensor([5])\n\n\n\n# 0번째 및 1번째 차원의 인덱스 0과 2번째 차원의 모든 값 가져오기\nx[0, 0, :] # x[0][0]과 동일\n\ntensor([1, 2, 3])\n\n\n인덱싱은 처음에는 상당히 혼란스러울 수 있으며, 특히 텐서가 커질수록 더 그렇습니다(저도 올바르게 인덱싱하기 위해 여러 번 시도하곤 합니다). 하지만 약간의 연습과 데이터 탐험가의 좌우명인 (시각화, 시각화, 시각화)를 따르다 보면 요령을 터득하기 시작할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#pytorch-텐서와-numpy",
    "href": "00_pytorch_fundamentals.html#pytorch-텐서와-numpy",
    "title": "00 - PyTorch 기초",
    "section": "PyTorch 텐서와 NumPy",
    "text": "PyTorch 텐서와 NumPy\nNumPy는 인기 있는 파이썬 수치 컴퓨팅 라이브러리이므로 PyTorch는 이와 잘 상호 작용할 수 있는 기능을 갖추고 있습니다.\nNumPy에서 PyTorch로(그리고 그 반대로) 전환하는 데 사용하려는 두 가지 주요 메서드는 다음과 같습니다. * torch.from_numpy(ndarray) - NumPy 배열 -&gt; PyTorch 텐서. * torch.Tensor.numpy() - PyTorch 텐서 -&gt; NumPy 배열.\n한번 시도해 봅시다.\n\n# NumPy 배열에서 텐서로\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n\n(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\n\n참고: 기본적으로 NumPy 배열은 float64 데이터 타입으로 생성되며 이를 PyTorch 텐서로 변환하면 동일한 데이터 타입이 유지됩니다(위와 같이).\n그러나 많은 PyTorch 계산은 기본적으로 float32를 사용합니다.\n따라서 NumPy 배열(float64) -&gt; PyTorch 텐서(float64) -&gt; PyTorch 텐서(float32)로 변환하려면 tensor = torch.from_numpy(array).type(torch.float32)를 사용할 수 있습니다.\n\n위에서 tensor를 재할당했으므로 텐서를 변경해도 배열은 그대로 유지됩니다.\n\n# 배열을 변경하고 텐서는 유지하기\narray = array + 1\narray, tensor\n\n(array([2., 3., 4., 5., 6., 7., 8.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\nPyTorch 텐서에서 NumPy 배열로 가려면 tensor.numpy()를 호출하면 됩니다.\n\n# 텐서에서 NumPy 배열로\ntensor = torch.ones(7) # dtype=float32인 1로 구성된 텐서 생성\nnumpy_tensor = tensor.numpy() # 변경하지 않는 한 dtype=float32가 됨\ntensor, numpy_tensor\n\n(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))\n\n\n그리고 위와 동일한 규칙이 적용됩니다. 원래 tensor를 변경해도 새로운 numpy_tensor는 그대로 유지됩니다.\n\n# 텐서를 변경하고 배열은 동일하게 유지하기\ntensor = tensor + 1\ntensor, numpy_tensor\n\n(tensor([2., 2., 2., 2., 2., 2., 2.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#재현성-무작위성-제어하기",
    "href": "00_pytorch_fundamentals.html#재현성-무작위성-제어하기",
    "title": "00 - PyTorch 기초",
    "section": "재현성 (무작위성 제어하기)",
    "text": "재현성 (무작위성 제어하기)\n신경망과 머신러닝에 대해 더 배우다 보면 무작위성이 얼마나 큰 역할을 하는지 알게 될 것입니다.\n음, 실제로는 의사 무작위성(pseudorandomness)입니다. 결국 설계된 대로 컴퓨터는 근본적으로 결정론적(각 단계가 예측 가능함)이므로 컴퓨터가 생성하는 무작위성은 시뮬레이션된 무작위성입니다(이에 대해서도 논란이 있지만 저는 컴퓨터 과학자가 아니므로 직접 더 알아보시기 바랍니다).\n그렇다면 이것이 신경망 및 딥러닝과 어떤 관련이 있을까요?\n우리는 신경망이 데이터의 패턴을 설명하기 위해 무작위 숫자로 시작하고(이 숫자는 서투른 설명입니다), 텐서 연산(및 아직 논의하지 않은 몇 가지 다른 것들)을 사용하여 해당 무작위 숫자를 개선하여 데이터의 패턴을 더 잘 설명하려고 시도한다는 것을 논의했습니다.\n요약하자면:\n무작위 숫자로 시작 -&gt; 텐서 연산 -&gt; 더 나아지도록 시도 (반복 반복 반복)\n무작위성은 훌륭하고 강력하지만 때로는 무작위성이 조금 덜했으면 할 때가 있습니다.\n왜일까요?\n반복 가능한 실험을 수행할 수 있기 때문입니다.\n예를 들어, 여러분이 성능 X를 달성할 수 있는 알고리즘을 만들었다고 가정해 봅시다.\n그러면 친구가 여러분이 미치지 않았는지 확인하기 위해 시도해 봅니다.\n그들은 어떻게 그런 일을 할 수 있을까요?\n여기서 재현성(reproducibility)이 등장합니다.\n다시 말해, 여러분이 얻은 것과 동일한 코드를 실행하여 여러분의 컴퓨터에서와 동일한(또는 매우 유사한) 결과를 내 컴퓨터에서도 얻을 수 있습니까?\nPyTorch에서 재현성의 간단한 예를 들어봅시다.\n먼저 두 개의 무작위 텐서를 생성해 보겠습니다. 무작위이기 때문에 서로 다를 것으로 예상하시죠?\n\nimport torch\n\n# 두 개의 무작위 텐서 생성\nrandom_tensor_A = torch.rand(3, 4)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"텐서 A:\\n{random_tensor_A}\\n\")\nprint(f\"텐서 B:\\n{random_tensor_B}\\n\")\nprint(f\"텐서 A와 텐서 B가 같습니까? (어디든)\")\nrandom_tensor_A == random_tensor_B\n\nTensor A:\ntensor([[0.8016, 0.3649, 0.6286, 0.9663],\n        [0.7687, 0.4566, 0.5745, 0.9200],\n        [0.3230, 0.8613, 0.0919, 0.3102]])\n\nTensor B:\ntensor([[0.9536, 0.6002, 0.0351, 0.6826],\n        [0.3743, 0.5220, 0.1336, 0.9666],\n        [0.9754, 0.8474, 0.8988, 0.1105]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n\n\ntensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n\n\n예상했던 대로 텐서는 서로 다른 값으로 나옵니다.\n하지만 동일한 값을 가진 두 개의 무작위 텐서를 만들고 싶다면 어떻게 해야 할까요?\n즉, 텐서에 여전히 무작위 값이 포함되어 있지만 동일한 풍미(flavour)를 갖기를 원하는 것입니다.\n여기서 torch.manual_seed(seed)가 등장합니다. 여기서 seed는 무작위성에 풍미를 더하는 정수(42와 같지만 무엇이든 될 수 있음)입니다.\n좀 더 풍미가 더해진 무작위 텐서를 만들어 시도해 봅시다.\n\nimport torch\nimport random\n\n# 무작위 시드 설정\nRANDOM_SEED=42 # 이 값을 다른 값으로 변경하고 아래 숫자에 어떤 일이 일어나는지 확인해 보세요\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# 새로운 rand()가 호출될 때마다 시드를 재설정해야 함\n# 이것이 없으면 tensor_D는 tensor_C와 달라짐\ntorch.random.manual_seed(seed=RANDOM_SEED) # 이 라인을 주석 처리하고 어떤 일이 일어나는지 확인해 보세요\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"텐서 C:\\n{random_tensor_C}\\n\")\nprint(f\"텐서 D:\\n{random_tensor_D}\\n\")\nprint(f\"텐서 C와 텐서 D가 같습니까? (어디든)\")\nrandom_tensor_C == random_tensor_D\n\nTensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n\n\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n\n\n좋네요!\n시드 설정이 작동한 것 같습니다.\n\n리소스: 우리가 방금 다룬 내용은 PyTorch 재현성의 겉핥기 수준일 뿐입니다. 일반적인 재현성과 무작위 시드에 대한 자세한 내용은 다음을 확인하세요. * PyTorch 재현성 문서 (10분 동안 이 문서를 읽어보는 것이 좋은 연습이 될 것입니다. 지금 당장 이해하지 못하더라도 이를 인지하고 있는 것이 중요합니다). * Wikipedia 무작위 시드 페이지 (무작위 시드 및 의사 무작위성에 대한 좋은 개요를 제공합니다).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#gpu에서-텐서-실행하기-및-계산-가속화",
    "href": "00_pytorch_fundamentals.html#gpu에서-텐서-실행하기-및-계산-가속화",
    "title": "00 - PyTorch 기초",
    "section": "GPU에서 텐서 실행하기 (및 계산 가속화)",
    "text": "GPU에서 텐서 실행하기 (및 계산 가속화)\n딥러닝 알고리즘은 많은 수치 연산을 필요로 합니다.\n그리고 기본적으로 이러한 연산은 종종 CPU(중앙 처리 장치)에서 수행됩니다.\n하지만 GPU(그래픽 처리 장치)라는 또 다른 일반적인 하드웨어가 있는데, 이는 신경망이 필요로 하는 특정 유형의 연산(행렬 곱셈)을 수행하는 데 종종 CPU보다 훨씬 빠릅니다.\n여러분의 컴퓨터에도 하나 있을 수 있습니다.\n그렇다면 신경망을 학습시킬 때마다 가급적 이를 사용해야 합니다. 왜냐하면 학습 시간을 비약적으로 단축할 수 있기 때문입니다.\n먼저 GPU에 액세스하고 다음으로 PyTorch가 GPU를 사용하도록 하는 몇 가지 방법이 있습니다.\n\n참고: 이 과정 전반에서 “GPU”를 언급할 때는 달리 명시되지 않는 한 CUDA가 활성화된 Nvidia GPU를 의미합니다(CUDA는 GPU를 그래픽뿐만 아니라 일반적인 용도의 컴퓨팅에 사용할 수 있도록 도와주는 컴퓨팅 플랫폼 및 API입니다).\n\n\n1. GPU 확보하기\nGPU라고 말할 때 무슨 일이 일어나고 있는지 이미 알고 계실 수도 있습니다. 하지만 그렇지 않다면 GPU에 액세스하는 몇 가지 방법이 있습니다.\n\n\n\n방법\n설정 난이도\n장점\n단점\n설정 방법\n\n\n\n\nGoogle Colab\n쉬움\n무료 사용 가능, 설정 거의 불필요, 링크만으로 작업 공유 가능\n데이터 출력 저장 안 됨, 컴퓨팅 제한적, 타임아웃 발생 가능\nGoogle Colab 가이드 따르기\n\n\n개인 기기 사용\n중간\n자신의 기기에서 모든 것을 로컬로 실행\nGPU는 무료가 아님, 초기 비용 필요\nPyTorch 설치 지침 따르기\n\n\n클라우드 컴퓨팅 (AWS, GCP, Azure)\n중간-어려움\n적은 초기 비용, 거의 무한한 컴퓨팅 액세스\n계속 실행하면 비쌀 수 있음, 올바르게 설정하는 데 시간이 걸림\nPyTorch 클라우드 설치 지침 따르기\n\n\n\nGPU를 사용하기 위한 더 많은 옵션이 있지만 지금은 위의 세 가지면 충분합니다.\n개인적으로 저는 소규모 실험(및 이 과정 제작)에는 Google Colab과 개인 컴퓨터를 혼용해서 사용하고, 더 많은 컴퓨팅 파워가 필요할 때는 클라우드 리소스를 활용합니다.\n\n리소스: 직접 GPU를 구매하고 싶지만 무엇을 사야 할지 모르겠다면 Tim Dettmers의 훌륭한 가이드를 참조하세요.\n\nNvidia GPU에 액세스할 수 있는지 확인하려면 !nvidia-smi를 실행할 수 있습니다. 여기서 !(뱅이라고도 함)는 “이를 명령줄에서 실행하라”는 의미입니다.\n\n!nvidia-smi\n\nThu Feb 10 02:09:18 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n접근 가능한 Nvidia GPU가 없는 경우 위 명령은 다음과 같은 내용을 출력합니다.\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n이 경우 위로 돌아가 설치 단계를 따르세요.\nGPU가 있는 경우 위 라인은 다음과 같은 내용을 출력합니다.\nWed Jan 19 22:09:08 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n2. PyTorch가 GPU에서 실행되도록 하기\n액세스할 준비가 된 GPU가 있으면 다음 단계는 PyTorch가 데이터를 저장(텐서)하고 데이터를 계산(텐서에 대한 연산 수행)하는 데 GPU를 사용하도록 하는 것입니다.\n그렇게 하려면 torch.cuda 패키지를 사용할 수 있습니다.\n말로 설명하기보다 직접 해봅시다.\ntorch.cuda.is_available()을 사용하여 PyTorch가 GPU에 액세스할 수 있는지 테스트할 수 있습니다.\n\n# GPU 확인\nimport torch\ntorch.cuda.is_available()\n\nTrue\n\n\n위 출력이 True이면 PyTorch가 GPU를 보고 사용할 수 있다는 뜻이고, False이면 GPU를 볼 수 없다는 뜻이므로 이 경우 설치 단계를 다시 거쳐야 합니다.\n이제 CPU에서 실행되거나 GPU를 사용할 수 있는 경우 GPU에서 실행되도록 코드를 설정하고 싶다고 가정해 보겠습니다.\n그렇게 하면 여러분이나 누군가가 코드를 실행하기로 결정하더라도 사용 중인 컴퓨팅 장치에 관계없이 코드가 작동할 것입니다.\n사용 가능한 장치 종류를 저장하기 위해 device 변수를 생성해 보겠습니다.\n\n# 장치 타입 설정\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n위 출력이 \"cuda\"이면 사용 가능한 CUDA 장치(GPU)를 사용하도록 모든 PyTorch 코드를 설정할 수 있고, \"cpu\"이면 PyTorch 코드가 CPU를 그대로 사용하게 됩니다.\n\n참고: PyTorch에서는 장치에 구애받지 않는 코드(device agnostic code)를 작성하는 것이 가장 좋은 관행입니다. 이는 CPU(항상 사용 가능) 또는 GPU(사용 가능한 경우)에서 실행될 코드를 의미합니다.\n\n더 빠른 계산을 원하면 GPU를 사용할 수 있지만, 훨씬 더 빠른 계산을 원하면 여러 개의 GPU를 사용할 수 있습니다.\ntorch.cuda.device_count()를 사용하여 PyTorch가 액세스할 수 있는 GPU 수를 셀 수 있습니다.\n\n# 장치 수 세기\ntorch.cuda.device_count()\n\n1\n\n\nPyTorch가 액세스할 수 있는 GPU 수를 아는 것은 하나의 GPU에서 특정 프로세스를 실행하고 다른 GPU에서 다른 프로세스를 실행하려는 경우에 유용합니다(PyTorch에는 모든 GPU에서 프로세스를 실행할 수 있게 해주는 기능도 있습니다).\n\n\n3. GPU에 텐서(및 모델) 넣기\n텐서(및 모델, 나중에 보게 될 것입니다)를 특정 장치에서 실행하려면 해당 텐서(또는 모델)에 대해 to(device)를 호출하면 됩니다. 여기서 device는 텐서(또는 모델)가 이동하려는 대상 장치입니다.\n왜 이렇게 할까요?\nGPU는 CPU보다 훨씬 빠른 수치 계산을 제공하며, GPU를 사용할 수 없는 경우 장치에 구애받지 않는 코드(위 참조) 덕분에 CPU에서 실행될 것이기 때문입니다.\n\n참고: to(device)(예: some_tensor.to(device))를 사용하여 텐서를 GPU에 넣으면 해당 텐서의 복사본이 반환됩니다. 즉, 동일한 텐서가 CPU와 GPU에 모두 있게 됩니다. 텐서를 덮어쓰려면 다음과 같이 재할당하세요.\nsome_tensor = some_tensor.to(device)\n\n텐서를 생성하고 GPU(사용 가능한 경우)에 넣어봅시다.\n\n# 텐서 생성 (기본적으로 CPU에 있음)\ntensor = torch.tensor([1, 2, 3])\n\n# 텐서가 GPU에 있지 않음\nprint(tensor, tensor.device)\n\n# 텐서를 GPU로 이동 (사용 가능한 경우)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n\ntensor([1, 2, 3]) cpu\n\n\ntensor([1, 2, 3], device='cuda:0')\n\n\nGPU를 사용할 수 있는 경우 위 코드는 다음과 같은 내용을 출력합니다.\ntensor([1, 2, 3]) cpu\ntensor([1, 2, 3], device='cuda:0')\n두 번째 텐서에 device='cuda:0'이 있는 것을 확인하세요. 이는 사용 가능한 0번째 GPU에 저장되었음을 의미합니다(GPU는 0부터 인덱싱되며, 두 개의 GPU를 사용할 수 있는 경우 각각 'cuda:0' 및 'cuda:1'부터 'cuda:n'까지입니다).\n\n\n4. 텐서를 다시 CPU로 이동하기\n텐서를 다시 CPU로 이동하고 싶다면 어떻게 해야 할까요?\n예를 들어, NumPy와 텐서를 상호 작용시키고 싶을 때 이 작업이 필요합니다(NumPy는 GPU를 활용하지 않기 때문입니다).\ntensor_on_gpu에 대해 torch.Tensor.numpy() 메서드를 사용해 봅시다.\n\n# 텐서가 GPU에 있으면 NumPy로 변환할 수 없음 (오류 발생)\ntensor_on_gpu.numpy()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-75-53175578f49e&gt; in &lt;module&gt;()\n      1 # 텐서가 GPU에 있으면 NumPy로 변환할 수 없음 (오류 발생)\n----&gt; 2 tensor_on_gpu.numpy()\n\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n\n\n\n대신 텐서를 다시 CPU로 가져와 NumPy와 함께 사용하려면 Tensor.cpu()를 사용할 수 있습니다.\n이는 텐서를 CPU 메모리로 복사하여 CPU에서 사용할 수 있도록 합니다.\n\n# 대신 텐서를 다시 cpu로 복사\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n\narray([1, 2, 3])\n\n\n위 명령은 GPU 텐서의 복사본을 CPU 메모리로 반환하므로 원본 텐서는 여전히 GPU에 있습니다.\n\ntensor_on_gpu\n\ntensor([1, 2, 3], device='cuda:0')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#연습-문제",
    "href": "00_pytorch_fundamentals.html#연습-문제",
    "title": "00 - PyTorch 기초",
    "section": "연습 문제",
    "text": "연습 문제\n\n문서 읽기 - 딥러닝(및 일반적인 코딩 학습)의 큰 부분은 사용 중인 특정 프레임워크의 문서에 익숙해지는 것입니다. 이 과정의 나머지 부분에서 PyTorch 문서를 많이 사용하게 될 것입니다. 따라서 다음 내용을 10분 동안 읽어보시기 바랍니다(지금 당장 이해가 되지 않더라도 괜찮습니다. 핵심은 완전한 이해가 아니라 인지하는 것입니다).\n\n\ntorch.Tensor에 대한 문서.\ntorch.cuda에 대한 문서.\n\n\n모양이 (7, 7)인 무작위 텐서를 만드세요.\n2번의 텐서에 모양이 (1, 7)인 다른 무작위 텐서와 행렬 곱셈을 수행하세요(힌트: 두 번째 텐서를 전치해야 할 수도 있습니다).\n무작위 시드를 0으로 설정하고 2번과 3번을 다시 수행하세요. 출력은 다음과 같아야 합니다.\n\n(tensor([[1.8542],\n         [1.9611],\n         [2.2884],\n         [3.0481],\n         [1.7067],\n         [2.5290],\n         [1.7989]]), torch.Size([7, 1]))\n\n무작위 시드와 관련하여 torch.manual_seed()로 설정하는 방법을 보았는데 GPU에서도 동일한 방법이 있나요? (힌트: 이를 위해 torch.cuda 문서를 찾아봐야 합니다)\n\n\n방법이 있다면 GPU 무작위 시드를 1234로 설정하세요.\n\n\n모양이 (2, 3)인 두 개의 무작위 텐서를 만들고 둘 다 GPU로 보내세요(이를 위해 GPU에 액세스해야 합니다). 텐서를 생성할 때 torch.manual_seed(1234)를 설정하세요(이것이 GPU 무작위 시드일 필요는 없습니다). 출력은 다음과 같아야 합니다.\n\nDevice: cuda\n(tensor([[0.0290, 0.4019, 0.2598],\n         [0.3666, 0.0583, 0.7006]], device='cuda:0'),\n tensor([[0.0518, 0.4681, 0.6738],\n         [0.3315, 0.7837, 0.5631]], device='cuda:0'))\n\n6번에서 만든 텐서에 대해 행렬 곱셈을 수행하세요(여기서도 텐서 중 하나의 모양을 조정해야 할 수도 있습니다).\n7번 출력의 최대값과 최소값을 찾으세요.\n7번 출력의 최대값 및 최소값 인덱스 값을 찾으세요.\n모양이 (1, 1, 1, 10)인 무작위 텐서를 만든 다음 값이 1인 모든 차원이 제거된 모양이 (10)인 새로운 텐서를 만드세요. 생성할 때 시드를 7로 설정하고 첫 번째 텐서와 그 모양, 두 번째 텐서와 그 모양을 출력하세요. 출력은 다음과 같아야 합니다.\n\ntensor([[[[0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297,\n           0.3653, 0.8513]]]]) torch.Size([1, 1, 1, 10])\ntensor([0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297, 0.3653,\n        0.8513]) torch.Size([10])\n\n리소스: 이 연습 문제를 완료하려면 과정 GitHub에서 연습 문제 노트북 템플릿 및 잠재적인 솔루션을 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#추가-학습-자료",
    "href": "00_pytorch_fundamentals.html#추가-학습-자료",
    "title": "00 - PyTorch 기초",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\n1시간 동안 PyTorch 기본 튜토리얼을 훑어보세요(빠른 시작 및 텐서 섹션을 권장합니다).\n텐서가 데이터를 어떻게 표현할 수 있는지 더 자세히 알아보려면 다음 비디오를 시청하세요: What’s a tensor?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>00 - PyTorch 기초</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html",
    "href": "01_pytorch_workflow.html",
    "title": "01 - PyTorch 워크플로우",
    "section": "",
    "text": "이번 장에서 다룰 내용\n머신러닝과 딥러닝의 본질은 과거의 일부 데이터를 가져와서 패턴을 발견하기 위한 알고리즘(신경망 등)을 구축하고, 발견된 패턴을 사용하여 미래를 예측하는 것입니다.\n이를 수행하는 방법은 많으며 항상 새로운 방법이 발견되고 있습니다.\n하지만 작게 시작해 봅시다.\n직선으로 시작하면 어떨까요?\n그리고 그 직선에 맞는 모델을 PyTorch로 구축할 수 있는지 확인해 보겠습니다.\n이 모듈에서는 표준 PyTorch 워크플로우를 다룰 것입니다(필요에 따라 자르고 변경할 수 있지만 주요 단계의 윤곽을 다룹니다).\n지금은 이 워크플로우를 사용하여 간단한 직선을 예측할 것이지만, 워크플로우 단계는 작업 중인 문제에 따라 반복되고 변경될 수 있습니다.\n구체적으로 다음 내용을 다룹니다:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#이번-장에서-다룰-내용",
    "href": "01_pytorch_workflow.html#이번-장에서-다룰-내용",
    "title": "01 - PyTorch 워크플로우",
    "section": "",
    "text": "주제\n내용\n\n\n\n\n1. 데이터 준비하기\n데이터는 거의 무엇이든 될 수 있지만, 시작하기 위해 간단한 직선을 만들 것입니다.\n\n\n2. 모델 구축하기\n여기에서는 데이터의 패턴을 학습할 모델을 만들고, 손실 함수(loss function), 옵티마이저(optimizer)를 선택하고 훈련 루프(training loop)를 구축할 것입니다.\n\n\n3. 데이터에 모델 맞추기 (훈련)\n데이터와 모델이 준비되었으므로, 이제 모델이 (훈련) 데이터에서 패턴을 찾도록 (시도하게) 해봅시다.\n\n\n4. 예측 및 모델 평가 (추론)\n모델이 데이터에서 패턴을 찾았으니, 그 결과를 실제 (테스트) 데이터와 비교해 봅시다.\n\n\n5. 모델 저장 및 불러오기\n모델을 다른 곳에서 사용하거나 나중에 다시 사용하고 싶을 수 있습니다. 여기에서 그 방법을 다룹니다.\n\n\n6. 전체 과정 합치기\n위의 모든 내용을 하나로 합쳐 봅시다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#도움을-받을-수-있는-곳",
    "href": "01_pytorch_workflow.html#도움을-받을-수-있는-곳",
    "title": "01 - PyTorch 워크플로우",
    "section": "도움을 받을 수 있는 곳",
    "text": "도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에서 확인할 수 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 개발자 포럼도 있습니다.\n먼저 다룰 내용을 나중에 참조할 수 있도록 딕셔너리에 넣어 보겠습니다.\n\nwhat_were_covering = {1: \"데이터 (준비 및 로드)\",\n    2: \"모델 구축\",\n    3: \"데이터에 모델 맞추기 (훈련)\",\n    4: \"예측 및 모델 평가 (추론)\",\n    5: \"모델 저장 및 로드\",\n    6: \"전체 과정 합치기\"\n}\n\n이제 이 모듈에 필요한 것들을 임포트해 보겠습니다.\ntorch, torch.nn(nn은 신경망을 뜻하며 이 패키지에는 PyTorch에서 신경망을 구축하기 위한 기본 구성 요소가 들어 있습니다) 및 matplotlib을 가져올 것입니다.\n\nimport torch\nfrom torch import nn # nn에는 신경망을 위한 PyTorch의 모든 구성 요소가 들어 있습니다.\nimport matplotlib.pyplot as plt\n\n# PyTorch 버전 확인\ntorch.__version__\n\n'1.11.0'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#데이터-준비-및-로드",
    "href": "01_pytorch_workflow.html#데이터-준비-및-로드",
    "title": "01 - PyTorch 워크플로우",
    "section": "1. 데이터 (준비 및 로드)",
    "text": "1. 데이터 (준비 및 로드)\n머신러닝에서 “데이터”는 상상할 수 있는 거의 모든 것이 될 수 있다는 점을 강조하고 싶습니다. 숫자 표(큰 Excel 스프레드시트와 같은 것), 모든 종류의 이미지, 비디오(YouTube에는 데이터가 많습니다!), 노래나 팟캐스트와 같은 오디오 파일, 단백질 구조, 텍스트 등입니다.\n\n\n\n머신러닝은 두 부분으로 구성된 게임입니다: 1. 데이터를 대표하는 숫자 집합으로 변환하고 2. 해당 표현을 가능한 한 잘 학습할 모델을 구축하거나 선택합니다.\n\n\n머신러닝은 두 부분으로 나뉩니다: 1. 데이터를 숫자로 변환하여 표현합니다. 2. 해당 표현을 가장 잘 학습할 수 있는 모델을 선택하거나 구축합니다.\n때로는 1번과 2번이 동시에 수행될 수도 있습니다.\n하지만 데이터가 없다면 어떻게 할까요?\n글쎄요, 그것이 지금 우리의 상황입니다.\n데이터가 없습니다.\n하지만 우리는 데이터를 직접 만들 수 있습니다.\n데이터를 직선으로 만들어 봅시다.\n선형 회귀(linear regression)를 사용하여 알려진 파라미터(parameters)(모델이 학습할 수 있는 것들)를 가진 데이터를 생성한 다음, PyTorch를 사용하여 경사 하강법(gradient descent)을 통해 이러한 파라미터를 추정하는 모델을 구축할 수 있는지 확인해 보겠습니다.\n\n# *알려진* 파라미터 생성\nweight = 0.7\nbias = 0.3\n\n# 데이터 생성\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\n멋지네요! 이제 X (특성, features)와 y (레이블, labels) 사이의 관계를 학습할 수 있는 모델을 구축해 보겠습니다.\n\n데이터를 훈련 세트와 테스트 세트로 분할하기\n데이터가 준비되었습니다.\n하지만 모델을 구축하기 전에 데이터를 분할해야 합니다.\n머신러닝 프로젝트에서 가장 중요한 단계 중 하나는 훈련 세트와 테스트 세트(필요한 경우 검증 세트까지)를 만드는 것입니다.\n데이터셋의 각 분할은 특정 목적을 위해 사용됩니다.\n\n\n\n\n\n\n\n\n\n분할\n목적\n전체 데이터의 비율\n얼마나 자주 사용되나요?\n\n\n\n\n훈련 세트(Training set)\n모델이 이 데이터로부터 학습합니다(학기 중에 공부하는 교재와 같음).\n~60-80%\n항상\n\n\n검증 세트(Validation set)\n모델이 이 데이터에서 튜닝됩니다(기말고사 전에 치르는 모의고사와 같음).\n~10-20%\n자주(항상은 아님)\n\n\n테스트 세트(Testing set)\n모델이 학습한 내용을 테스트하기 위해 이 데이터에서 평가됩니다(학기 말에 치르는 기말고사와 같음).\n~10-20%\n항상\n\n\n\n지금은 훈련 세트와 테스트 세트만 사용할 것이며, 이는 우리 모델이 학습하고 평가될 데이터셋을 갖게 됨을 의미합니다.\nX 및 y 텐서를 분할하여 생성할 수 있습니다.\n\n참고: 실제 데이터를 다룰 때 이 단계는 일반적으로 프로젝트 시작 시점에 수행됩니다(테스트 세트는 항상 다른 모든 데이터와 분리되어 보관되어야 합니다). 우리는 모델이 훈련 데이터에서 학습하도록 하고 테스트 데이터에서 평가하여 보지 못한 예제에 대해 얼마나 잘 일반화(generalizes)되는지 확인하고 싶습니다.\n\n\n# 훈련/테스트 분할 생성\ntrain_split = int(0.8 * len(X)) # 데이터의 80%를 훈련 세트로, 20%를 테스트용으로 사용\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\n좋습니다. 훈련용 샘플 40개(X_train & y_train)와 테스트용 샘플 10개(X_test & y_test)를 확보했습니다.\n우리가 만들 모델은 X_train과 y_train 사이의 관계를 학습하려고 노력할 것이며, 그런 다음 X_test와 y_test에서 학습한 내용을 평가할 것입니다.\n하지만 지금 우리 데이터는 단지 페이지 위의 숫자에 불과합니다.\n데이터를 시각화하는 함수를 만들어 보겠습니다.\n\ndef plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n  \"\"\"\n  훈련 데이터와 테스트 데이터를 플롯하고 예측값과 비교합니다.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # 훈련 데이터를 파란색으로 플롯\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"훈련 데이터\")\n  \n  # 테스트 데이터를 초록색으로 플롯\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"테스트 데이터\")\n\n  if predictions is not None:\n    # 예측값을 빨간색으로 플롯 (예측은 테스트 데이터에서 수행됨)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"예측값\")\n\n  # 범례 표시\n  plt.legend(prop={\"size\": 14});\n\n\nplot_predictions();\n\n\n\n\n\n\n\n\n멋지네요!\n이제 데이터는 페이지 위의 단순한 숫자가 아니라 직선이 되었습니다.\n\n참고: 이제 데이터 탐험가의 좌우명인 “시각화, 시각화, 시각화!”를 소개할 좋은 시간입니다.\n데이터를 다루고 숫자로 변환할 때마다 시각화할 수 있는 것이 있다면 이해하는 데 큰 도움이 된다는 점을 기억하세요.\n기계는 숫자를 좋아하고 우리 인간도 숫자를 좋아하지만, 우리는 무언가를 보는 것도 좋아합니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#모델-구축",
    "href": "01_pytorch_workflow.html#모델-구축",
    "title": "01 - PyTorch 워크플로우",
    "section": "2. 모델 구축",
    "text": "2. 모델 구축\n이제 데이터가 준비되었으니 파란색 점을 사용하여 초록색 점을 예측할 모델을 구축해 보겠습니다.\n바로 들어가 보겠습니다.\n먼저 코드를 작성한 다음 모든 것을 설명하겠습니다.\n순수 PyTorch를 사용하여 표준 선형 회귀 모델을 복제해 봅시다.\n\n# 선형 회귀 모델 클래스 생성\nclass LinearRegressionModel(nn.Module): # &lt;- PyTorch의 거의 모든 것은 nn.Module입니다 (이것을 신경망 레고 블록이라고 생각하세요)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- 무작위 가중치로 시작 (모델이 학습함에 따라 조정됨)\n            requires_grad=True, # &lt;- 경사 하강법으로 이 값을 업데이트할 수 있나요?\n            dtype=torch.float # &lt;- PyTorch는 기본적으로 float32를 선호합니다\n        ))\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- 무작위 편향으로 시작 (모델이 학습함에 따라 조정됨)\n            requires_grad=True, # &lt;- 경사 하강법으로 이 값을 업데이트할 수 있나요?\n            dtype=torch.float # &lt;- PyTorch는 기본적으로 float32를 선호합니다\n        ))\n\n    # forward는 모델 내의 계산을 정의합니다\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\"는 입력 데이터입니다 (예: 훈련/테스트 특성)\n        return self.weights * x + self.bias # &lt;- 이것이 선형 회귀 공식입니다 (y = m*x + b)\n\n위 코드에서 꽤 많은 일이 일어나고 있는데, 하나씩 분석해 봅시다.\n\n리소스: 신경망을 구축하기 위해 파이썬 클래스를 사용할 것입니다. 파이썬 클래스 표기법이 낯설다면 Real Python의 파이썬 3 객체 지향 프로그래밍 가이드를 몇 번 읽어보시기를 권장합니다.\n\n\nPyTorch 모델 구축 필수 요소\nPyTorch에는 상상할 수 있는 거의 모든 종류의 신경망을 만드는 데 사용할 수 있는 필수 모듈이 네 가지 정도 있습니다.\ntorch.nn, torch.optim, torch.utils.data.Dataset 및 torch.utils.data.DataLoader입니다. 지금은 처음 두 가지에 집중하고 나중에 다른 두 가지를 다룰 것입니다(그것들이 무엇을 하는지 추측할 수 있을 것입니다).\n\n\n\nPyTorch 모듈\n무엇을 하나요?\n\n\n\n\ntorch.nn\n계산 그래프(본질적으로 특정 방식으로 실행되는 일련의 계산)를 위한 모든 구성 요소를 포함합니다.\n\n\ntorch.nn.Parameter\nnn.Module과 함께 사용할 수 있는 텐서를 저장합니다. requires_grad=True인 경우 경사(경사 하강법을 통해 모델 파라미터를 업데이트하는 데 사용됨)가 자동으로 계산되며, 이를 종종 “autograd”라고 합니다.\n\n\ntorch.nn.Module\n모든 신경망 모듈의 기본 클래스로, 신경망의 모든 구성 요소는 이 클래스의 서브클래스입니다. PyTorch에서 신경망을 구축하는 경우 모델은 nn.Module을 상속해야 합니다. forward() 메서드 구현이 필요합니다.\n\n\ntorch.optim\n다양한 최적화 알고리즘을 포함합니다(이 알고리즘은 nn.Parameter에 저장된 모델 파라미터가 경사 하강법을 개선하고 손실을 줄이기 위해 어떻게 가장 잘 변해야 하는지 알려줍니다).\n\n\ndef forward()\n모든 nn.Module 서브클래스는 forward() 메서드가 필요하며, 이는 특정 nn.Module로 전달된 데이터에 대해 수행될 계산을 정의합니다(예: 위의 선형 회귀 공식).\n\n\n\n위 내용이 복잡하게 들린다면 이렇게 생각해 보세요. PyTorch 신경망의 거의 모든 것은 torch.nn에서 나옵니다. * nn.Module은 더 큰 구성 요소(레이어)를 포함합니다. * nn.Parameter는 가중치 및 편향과 같은 작은 파라미터를 포함합니다(이들을 함께 결합하여 nn.Module을 만듭니다). * forward()는 더 큰 블록이 nn.Module 내에서 입력(데이터가 가득 찬 텐서)에 대해 계산을 수행하는 방법을 알려줍니다. * torch.optim은 입력 데이터를 더 잘 나타내기 위해 nn.Parameter 내의 파라미터를 개선하는 최적화 방법을 포함합니다.\n nn.Module을 상속하여 PyTorch 모델을 만드는 기본 구성 요소. nn.Module을 상속하는 객체의 경우 forward() 메서드가 정의되어야 합니다.\n\n리소스: PyTorch Cheat Sheet에서 이러한 필수 모듈과 그 사용 사례를 더 많이 확인해 보세요.\n\n\n\nPyTorch 모델의 내용 확인하기\n이제 필수 요소를 살펴보았으니, 우리가 만든 클래스로 모델 인스턴스를 생성하고 .parameters()를 사용하여 해당 파라미터를 확인해 보겠습니다.\n\n# nn.Parameter가 무작위로 초기화되므로 수동 시드 설정\ntorch.manual_seed(42)\n\n# 모델의 인스턴스 생성 (이것은 nn.Parameter들을 포함하는 nn.Module의 서브클래스입니다)\nmodel_0 = LinearRegressionModel()\n\n# 우리가 생성한 nn.Module 서브클래스 내의 nn.Parameter들을 확인\nlist(model_0.parameters())\n\n[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]\n\n\n.state_dict()를 사용하여 모델의 상태(모델이 포함하는 내용)를 가져올 수도 있습니다.\n\n# 명명된 파라미터 나열\nmodel_0.state_dict()\n\nOrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n\n\nmodel_0.state_dict()에서 weights 및 bias 값이 무작위 부동 소수점 텐서로 나오는 것을 확인하셨나요?\n이는 위에서 torch.randn()을 사용하여 초기화했기 때문입니다.\n본질적으로 우리는 무작위 파라미터에서 시작하여 모델이 우리 데이터에 가장 잘 맞는 파라미터(직선 데이터를 만들 때 설정한 하드코딩된 weight 및 bias 값)로 업데이트되도록 하기를 원합니다.\n\n연습: 위의 두 셀 위에서 torch.manual_seed() 값을 변경해보고, 가중치와 편향 값에 어떤 일이 일어나는지 확인해 보세요.\n\n우리 모델은 무작위 값으로 시작하기 때문에 지금은 예측 능력이 떨어집니다.\n\n\ntorch.inference_mode()를 사용하여 예측하기\n이를 확인하기 위해 테스트 데이터 X_test를 전달하여 y_test를 얼마나 근접하게 예측하는지 볼 수 있습니다.\n모델에 데이터를 전달하면 모델의 forward() 메서드를 거쳐 정의된 계산을 사용하여 결과가 생성됩니다.\n예측을 해봅시다.\n\n# 모델로 예측하기\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# 참고: 오래된 PyTorch 코드에서는 torch.no_grad()를 볼 수도 있습니다.\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n\n음?\n예측을 수행하기 위해 torch.inference_mode()를 컨텍스트 매니저(with torch.inference_mode(): 부분)로 사용한 것을 눈치채셨을 것입니다.\n이름에서 알 수 있듯이, torch.inference_mode()는 추론(예측 수행)을 위해 모델을 사용할 때 사용됩니다.\ntorch.inference_mode()는 예측을 수행할 때 순전파(forward-passes)(forward() 메서드를 통과하는 데이터)를 더 빠르게 만들기 위해 여러 가지(훈련에는 필요하지만 추론에는 필요하지 않은 경사 추적 등)를 끕니다.\n\n참고: 오래된 PyTorch 코드에서는 추론을 위해 torch.no_grad()가 사용되는 것을 볼 수 있습니다. torch.inference_mode()와 torch.no_grad()는 비슷한 역할을 하지만, torch.inference_mode()가 더 최신 버전이며 잠재적으로 더 빠르고 권장됩니다. 자세한 내용은 PyTorch의 이 트윗을 참조하세요.\n\n예측을 수행했습니다. 어떻게 생겼는지 확인해 봅시다.\n\n# 예측값 확인\nprint(f\"테스트 샘플 수: {len(X_test)}\") \nprint(f\"수행된 예측 수: {len(y_preds)}\")\nprint(f\"예측값:\\n{y_preds}\")\n\nNumber of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n\n\n테스트 샘플당 하나의 예측값이 있는 것을 확인하세요.\n이는 우리가 사용하는 데이터의 종류 때문입니다. 우리의 직선의 경우 하나의 X 값이 하나의 y 값에 매핑됩니다.\n하지만 머신러닝 모델은 매우 유연합니다. 하나의 y 값에 100개의 X 값이 매핑될 수도 있고, 2개, 3개 또는 10개의 y 값에 매핑될 수도 있습니다. 모든 것은 작업 중인 대상에 달려 있습니다.\n우리 예측값은 여전히 페이지 위의 숫자입니다. 위에서 만든 plot_predictions() 함수로 시각화해 봅시다.\n\nplot_predictions(predictions=y_preds.cpu())\n\n\n\n\n\n\n\n\n와! 빨간색 점들을 보세요. 초록색 점들과 거의 완벽하게 일치합니다. 에포크(epochs)를 늘린 것이 도움이 된 것 같네요.\n\n\n6.5 모델 저장 및 로드\n모델의 예측 결과가 만족스러우므로, 나중에 사용할 수 있도록 파일로 저장해 봅시다.\n\nfrom pathlib import Path\n\n# 1. 모델 디렉토리 생성\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. 모델 저장 경로 생성\nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. 모델 state dict 저장\nprint(f\"모델 저장 경로: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # state_dict()만 저장하면 모델의 학습된 파라미터만 저장됩니다.\n           f=MODEL_SAVE_PATH) \n\n모델 저장 경로: models/01_pytorch_workflow_model_1.pth\n\n\n모든 것이 잘 작동했는지 확인하기 위해 모델을 다시 로드해 봅시다.\n다음 단계를 수행합니다: * LinearRegressionModelV2() 클래스의 새로운 인스턴스를 생성합니다. * torch.nn.Module.load_state_dict()를 사용하여 모델의 state dict를 로드합니다. * 코드의 장치 독립성을 보장하기 위해 모델의 새 인스턴스를 대상 장치(target device)로 보냅니다.\n\n# LinearRegressionModelV2의 새로운 인스턴스 생성\nloaded_model_1 = LinearRegressionModelV2()\n\n# 모델 state dict 로드\nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# 모델을 대상 장치로 보냄 (데이터가 GPU에 있으면 예측을 위해 모델도 GPU에 있어야 합니다)\nloaded_model_1.to(device)\n\nprint(f\"로드된 모델:\\n{loaded_model_1}\")\nprint(f\"장치 위의 모델:\\n{next(loaded_model_1.parameters()).device}\")\n\n로드된 모델:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\n장치 위의 모델:\ncuda:0\n\n\n이제 로드된 모델을 평가하여 그 예측이 저장하기 전의 예측과 일치하는지 확인할 수 있습니다.\n\n# 로드된 모델 평가\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='cuda:0')\n\n\n모든 것이 일치합니다! 좋습니다!\n먼 길을 오셨습니다. 이제 PyTorch에서 처음 두 개의 신경망 모델을 직접 구축하고 훈련했습니다!\n이제 실력을 연습해 볼 시간입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#연습-문제",
    "href": "01_pytorch_workflow.html#연습-문제",
    "title": "01 - PyTorch 워크플로우",
    "section": "연습 문제",
    "text": "연습 문제\n모든 연습 문제는 노트북 전체의 코드에서 영감을 얻었습니다.\n주요 섹션당 하나의 연습 문제가 있습니다.\n해당 섹션을 참조하여 완료할 수 있어야 합니다.\n\n참고: 모든 연습 문제에서 코드는 장치 독립적이어야 합니다(즉, 사용 가능한 경우 CPU 또는 GPU에서 실행될 수 있어야 함).\n\n\n선형 회귀 공식(weight * X + bias)을 사용하여 직선 데이터셋을 만듭니다.\n\n\nweight=0.3, bias=0.9로 설정하고 총 데이터 포인트는 100개 이상이어야 합니다.\n데이터를 훈련 80%, 테스트 20%로 분할합니다.\n훈련 및 테스트 데이터를 시각적으로 플롯합니다.\n\n\nnn.Module을 상속하여 PyTorch 모델을 구축합니다.\n\n\n내부에는 무작위로 초기화된 nn.Parameter()가 있어야 하며, 하나는 가중치(weights)용, 하나는 편향(bias)용이고 requires_grad=True여야 합니다.\n1번에서 데이터셋을 만드는 데 사용한 선형 회귀 함수를 계산하도록 forward() 메서드를 구현합니다.\n모델을 구성한 후 인스턴스를 만들고 state_dict()를 확인합니다.\n참고: 원한다면 nn.Parameter() 대신 nn.Linear()를 사용할 수 있습니다.\n\n\n각각 nn.L1Loss() 및 torch.optim.SGD(params, lr)를 사용하여 손실 함수와 옵티마이저를 생성합니다.\n\n\n옵티마이저의 학습률을 0.01로 설정하고, 최적화할 파라미터는 2번에서 만든 모델의 모델 파라미터여야 합니다.\n300 에포크 동안 적절한 훈련 단계를 수행하는 훈련 루프를 작성합니다.\n훈련 루프는 20 에포크마다 테스트 데이터셋에서 모델을 테스트해야 합니다.\n\n\n훈련된 모델로 테스트 데이터에 대해 예측을 수행합니다.\n\n\n이러한 예측값을 원래 훈련 및 테스트 데이터와 함께 시각화합니다 (참고: matplotlib과 같이 CUDA를 지원하지 않는 라이브러리를 사용하여 플롯하려면 예측값이 GPU에 있지 않은지 확인해야 할 수 있습니다).\n\n\n훈련된 모델의 state_dict()를 파일로 저장합니다.\n\n\n2번에서 만든 모델 클래스의 새로운 인스턴스를 생성하고 방금 저장한 state_dict()를 로드합니다.\n로드된 모델로 테스트 데이터에 대해 예측을 수행하고 4번의 원래 모델 예측과 일치하는지 확인합니다.\n\n\n리소스: 이 과정의 GitHub에서 연습 문제 노트북 템플릿 및 솔루션을 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#추가-학습-자료",
    "href": "01_pytorch_workflow.html#추가-학습-자료",
    "title": "01 - PyTorch 워크플로우",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\n비공식 PyTorch 최적화 루프 송(The Unofficial PyTorch Optimization Loop Song)을 들어보세요 (PyTorch 훈련/테스트 루프의 단계를 기억하는 데 도움이 됨).\nPyTorch에서 가장 중요한 모듈 중 하나가 어떻게 작동하는지 더 깊이 이해하려면 Jeremy Howard의 What is torch.nn, really?를 읽어보세요.\n접할 수 있는 다양한 PyTorch 모듈에 대해 PyTorch 문서 치트시트(cheatsheet)를 10분 정도 훑어보세요.\nPyTorch의 다양한 저장 및 로드 옵션에 익숙해지기 위해 PyTorch 웹사이트의 로딩 및 저장 문서를 10분 정도 읽어보세요.\n우리 모델의 학습을 돕기 위해 백그라운드에서 작동해 온 두 가지 주요 알고리즘인 경사 하강법과 역전파의 내부 구조에 대한 개요를 위해 다음 자료를 1~2시간 동안 읽거나 시청하세요.\nWikipedia의 경사 하강법 페이지\nRobert Kwiatkowski의 Gradient Descent Algorithm — a deep dive\n3Blue1Brown의 경사 하강법, 신경망은 어떻게 학습하는가 비디오\n3Blue1Brown의 역전파는 실제로 무엇을 하고 있는가? 비디오\n역전파(Backpropagation) Wikipedia 페이지",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>01 - PyTorch 워크플로우</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html",
    "href": "02_pytorch_classification.html",
    "title": "02 - PyTorch 신경망 분류",
    "section": "",
    "text": "분류 문제란 무엇인가요?\n분류 문제는 어떤 대상이 한 가지인지 아니면 다른 것인지 예측하는 것과 관련이 있습니다.\n예를 들어, 다음과 같은 작업을 원할 수 있습니다:\n분류는 회귀(숫자 예측, 노트북 01에서 다룸)와 함께 머신러닝에서 가장 흔한 문제 유형 중 하나입니다.\n이 노트북에서는 PyTorch를 사용하여 몇 가지 다른 분류 문제를 해결해 볼 것입니다.\n즉, 일련의 입력을 가져와서 해당 입력이 어떤 클래스에 속하는지 예측하는 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#분류-문제란-무엇인가요",
    "href": "02_pytorch_classification.html#분류-문제란-무엇인가요",
    "title": "02 - PyTorch 신경망 분류",
    "section": "",
    "text": "문제 유형\n설명\n예시\n\n\n\n\n이진 분류 (Binary classification)\n타겟이 두 가지 옵션 중 하나임 (예: 예 또는 아니오)\n건강 파라미터를 기반으로 심장병 유무 예측\n\n\n다중 클래스 분류 (Multi-class classification)\n타겟이 세 가지 이상의 옵션 중 하나임\n사진이 음식, 사람, 강아지 중 무엇인지 결정\n\n\n다중 레이블 분류 (Multi-label classification)\n타겟에 하나 이상의 옵션을 할당할 수 있음\n위키피디아 기사에 할당될 카테고리 예측 (예: 수학, 과학, 철학)\n\n\n\n\n\n\n머신러닝에서의 다양한 분류 유형: 이진 분류, 다중 클래스 분류, 다중 레이블 분류",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#이번-장에서-다룰-내용",
    "href": "02_pytorch_classification.html#이번-장에서-다룰-내용",
    "title": "02 - PyTorch 신경망 분류",
    "section": "이번 장에서 다룰 내용",
    "text": "이번 장에서 다룰 내용\n이 노트북에서는 노트북 01에서 다루었던 PyTorch 워크플로우를 반복해서 살펴볼 것입니다.\n\n\n\nPyTorch 워크플로우 순서도\n\n\n다만 직선을 예측하는 대신(회귀 문제), 분류 문제를 다룰 것입니다.\n구체적으로 다음 내용을 다룹니다:\n\n\n\n\n\n\n\n주제\n내용\n\n\n\n\n0. 분류 신경망의 구조\n신경망은 거의 모든 모양이나 크기를 가질 수 있지만, 일반적으로 유사한 기본 설계를 따릅니다.\n\n\n1. 이진 분류 데이터 준비하기\n데이터는 무엇이든 될 수 있지만, 시작하기 위해 간단한 이진 분류 데이터셋을 만들 것입니다.\n\n\n2. PyTorch 분류 모델 구축하기\n데이터의 패턴을 학습할 모델을 만들고, 분류에 특화된 손실 함수, 옵티마이저 및 훈련 루프를 선택할 것입니다.\n\n\n3. 데이터에 모델 맞추기 (훈련)\n데이터와 모델이 준비되었으므로, 이제 모델이 (훈련) 데이터에서 패턴을 찾도록 해봅시다.\n\n\n4. 예측 및 모델 평가 (추론)\n모델이 데이터에서 패턴을 찾았으니, 그 결과를 실제 (테스트) 데이터와 비교해 봅시다.\n\n\n5. 모델 개선하기 (모델 관점에서)\n모델을 훈련하고 평가했지만 성능이 좋지 않다면, 개선을 위해 몇 가지 방법을 시도해 봅니다.\n\n\n6. 비선형성 (Non-linearity)\n지금까지 우리 모델은 직선만 모델링할 수 있었습니다. 곡선(비선형)은 어떻게 처리할까요?\n\n\n7. 비선형 함수 복제하기\n비선형 데이터를 모델링하기 위해 비선형 함수를 사용했는데, 이들은 어떤 모습일까요?\n\n\n8. 전체 과정 합치기 (다중 클래스 분류)\n지금까지 이진 분류를 위해 수행한 모든 작업을 다중 클래스 분류 문제와 하나로 합쳐 봅니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#도움을-받을-수-있는-곳",
    "href": "02_pytorch_classification.html#도움을-받을-수-있는-곳",
    "title": "02 - PyTorch 신경망 분류",
    "section": "도움을 받을 수 있는 곳",
    "text": "도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 개발자 포럼도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#분류-신경망의-구조",
    "href": "02_pytorch_classification.html#분류-신경망의-구조",
    "title": "02 - PyTorch 신경망 분류",
    "section": "0. 분류 신경망의 구조",
    "text": "0. 분류 신경망의 구조\n코드를 작성하기 전에 분류 신경망의 일반적인 구조를 살펴보겠습니다.\n\n\n\n하이퍼파라미터\n이진 분류 (Binary Classification)\n다중 클래스 분류 (Multiclass classification)\n\n\n\n\n입력 레이어 모양 (in_features)\n특성의 수와 동일 (예: 심장병 예측에서 나이, 성별, 키, 몸무게, 흡연 여부 등 5개)\n이진 분류와 동일\n\n\n은닉 레이어 (Hidden layer)\n문제에 따라 다름, 최소 1개, 최대 무제한\n이진 분류와 동일\n\n\n은닉 레이어당 뉴런 수\n문제에 따라 다름, 일반적으로 10 ~ 512개\n이진 분류와 동일\n\n\n출력 레이어 모양 (out_features)\n1 (두 클래스 중 하나)\n클래스당 1개 (예: 음식, 사람, 강아지 사진의 경우 3개)\n\n\n은닉 레이어 활성화 함수\n주로 ReLU (rectified linear unit)를 사용하지만 다른 함수도 가능\n이진 분류와 동일\n\n\n출력 레이어 활성화 함수\nSigmoid (PyTorch에서는 torch.sigmoid)\nSoftmax (PyTorch에서는 torch.softmax)\n\n\n손실 함수 (Loss function)\nBinary crossentropy (PyTorch에서는 torch.nn.BCELoss)\nCross entropy (PyTorch에서는 torch.nn.CrossEntropyLoss)\n\n\n옵티마이저 (Optimizer)\nSGD, Adam (더 많은 옵션은 torch.optim 참조)\n이진 분류와 동일\n\n\n\n물론 분류 신경망 구성 요소의 이 목록은 작업 중인 문제에 따라 달라질 수 있습니다.\n하지만 시작하기에는 충분합니다.\n이 노트북 전체에서 이 설정을 직접 실습해 볼 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#분류-데이터-생성-및-준비",
    "href": "02_pytorch_classification.html#분류-데이터-생성-및-준비",
    "title": "02 - PyTorch 신경망 분류",
    "section": "1. 분류 데이터 생성 및 준비",
    "text": "1. 분류 데이터 생성 및 준비\n먼저 데이터를 생성해 보겠습니다.\nScikit-Learn의 make_circles() 메서드를 사용하여 서로 다른 색상의 점으로 구성된 두 개의 원을 생성합니다.\n\nfrom sklearn.datasets import make_circles\n\n\n# 1000개의 샘플 생성\nn_samples = 1000\n\n# 원 생성\nX, y = make_circles(n_samples,\n                    noise=0.03, # 점들에 약간의 노이즈 추가\n                    random_state=42) # 동일한 값을 얻기 위해 무작위 상태 유지\n\n좋습니다. 이제 처음 5개의 X와 y 값을 확인해 보겠습니다.\n\nprint(f\"처음 5개의 X 특성:\\n{X[:5]}\")\nprint(f\"\\n처음 5개의 y 레이블:\\n{y[:5]}\")\n\nFirst 5 X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst 5 y labels:\n[1 1 1 1 0]\n\n\n하나의 y 값당 두 개의 X 값이 있는 것 같습니다.\n데이터 탐험가의 좌우명인 시각화, 시각화, 시각화를 따라 데이터를 pandas DataFrame에 넣어 보겠습니다.\n\n# 원 데이터의 DataFrame 생성\nimport pandas as pd\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n    \"X2\": X[:, 1],\n    \"label\": y\n})\ncircles.head(10)\n\n\n    \n      \n\n\n\n\n\n\nX1\nX2\nlabel\n\n\n\n\n0\n0.754246\n0.231481\n1\n\n\n1\n-0.756159\n0.153259\n1\n\n\n2\n-0.815392\n0.173282\n1\n\n\n3\n-0.393731\n0.692883\n1\n\n\n4\n0.442208\n-0.896723\n0\n\n\n5\n-0.479646\n0.676435\n1\n\n\n6\n-0.013648\n0.803349\n1\n\n\n7\n0.771513\n0.147760\n1\n\n\n8\n-0.169322\n-0.793456\n1\n\n\n9\n-0.121486\n1.021509\n0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\n각 X 특성 쌍(X1 및 X2)에 대해 레이블(y) 값이 0 또는 1인 것처럼 보입니다.\n옵션이 두 가지(0 또는 1)뿐이므로 이 문제가 이진 분류(binary classification)임을 알 수 있습니다.\n각 클래스의 값은 몇 개입니까?\n\n# 서로 다른 레이블 확인\ncircles.label.value_counts()\n\n1    500\n0    500\nName: label, dtype: int64\n\n\n각각 500개씩 균형이 잘 잡혀 있습니다.\n플롯해 봅시다.\n\n# 플롯으로 시각화\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0], \n            y=X[:, 1], \n            c=y, \n            cmap=plt.cm.RdYlBu);\n\n\n\n\n\n\n\n\n좋습니다. 해결해야 할 문제가 생긴 것 같네요.\n점들을 빨간색(0) 또는 파란색(1)으로 분류하기 위해 PyTorch 신경망을 어떻게 구축할 수 있는지 알아봅시다.\n\n참고: 이 데이터셋은 머신러닝에서 종종 토이 문제(toy problem)(무언가를 시도하고 테스트하는 데 사용되는 문제)로 간주됩니다.\n하지만 이것은 분류의 주요 핵심을 나타냅니다. 수치로 표현된 어떤 데이터가 있고 그것을 분류할 수 있는 모델, 우리의 경우에는 빨간색 또는 파란색 점으로 분리할 수 있는 모델을 만들고자 하는 것입니다.\n\n\n1.1 입력 및 출력 모양\n딥러닝에서 가장 흔한 오류 중 하나는 모양(shape) 오류입니다.\n텐서의 모양과 텐서 연산의 모양이 맞지 않으면 모델에 오류가 발생합니다.\n이 과정 전반에 걸쳐 이러한 사례를 많이 보게 될 것입니다.\n그리고 그것이 발생하지 않도록 보장하는 확실한 방법은 없습니다. 발생할 것입니다.\n대신 할 수 있는 일은 작업 중인 데이터의 모양에 지속적으로 익숙해지는 것입니다.\n저는 이를 입력 및 출력 모양이라고 부르는 것을 좋아합니다.\n스스로에게 물어보세요:\n“내 입력의 모양은 무엇이고 출력의 모양은 무엇인가?”\n알아봅시다.\n\n# 특성과 레이블의 모양 확인\nX.shape, y.shape\n\n((1000, 2), (1000,))\n\n\n각각의 첫 번째 차원이 일치하는 것 같습니다.\n1000개의 X와 1000개의 y가 있습니다.\n하지만 X의 두 번째 차원은 무엇일까요?\n단일 샘플(특성 및 레이블)의 값과 모양을 확인하는 것이 종종 도움이 됩니다.\n그렇게 하면 모델에서 어떤 입력 및 출력 모양을 기대해야 하는지 이해하는 데 도움이 됩니다.\n\n# 특성 및 레이블의 첫 번째 예시 확인\nX_sample = X[0]\ny_sample = y[0]\nprint(f\"X 한 샘플의 값: {X_sample}, y 한 샘플의 값: {y_sample}\")\nprint(f\"X 한 샘플의 모양: {X_sample.shape}, y 한 샘플의 모양: {y_sample.shape}\")\n\nValues for one sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for one sample of X: (2,) and the same for y: ()\n\n\n이는 X의 두 번째 차원이 두 개의 특성(벡터)을 가짐을 의미하며, 반면 y는 단일 특성(스칼라)을 가짐을 알려줍니다.\n하나의 출력에 대해 두 개의 입력이 있습니다.\n\n\n1.2 데이터를 텐서로 변환하고 훈련 및 테스트 분할 생성\n데이터의 입력 및 출력 모양을 조사했으므로, 이제 PyTorch와 모델링에 사용할 수 있도록 준비해 보겠습니다.\n구체적으로 다음 작업이 필요합니다: 1. 데이터를 텐서로 변환합니다 (현재 데이터는 NumPy 배열이며 PyTorch는 PyTorch 텐서로 작업하는 것을 선호합니다). 2. 데이터를 훈련 세트와 테스트 세트로 분할합니다 (훈련 세트에서 모델을 훈련하여 X와 y 사이의 패턴을 학습한 다음, 테스트 데이터셋에서 학습된 패턴을 평가합니다).\n\n# 데이터를 텐서로 변환\n# 그렇지 않으면 나중에 계산할 때 문제가 발생합니다.\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# 처음 5개 샘플 확인\nX[:5], y[:5]\n\n(tensor([[ 0.7542,  0.2315],\n         [-0.7562,  0.1533],\n         [-0.8154,  0.1733],\n         [-0.3937,  0.6929],\n         [ 0.4422, -0.8967]]), tensor([1., 1., 1., 1., 0.]))\n\n\n이제 데이터가 텐서 형식이 되었으므로 훈련 세트와 테스트 세트로 분할해 보겠습니다.\n그렇게 하기 위해 Scikit-Learn의 유용한 함수인 train_test_split()을 사용해 봅시다.\ntest_size=0.2(80% 훈련, 20% 테스트)를 사용하고, 데이터 전체에서 무작위로 분할이 발생하므로 분할을 재현할 수 있도록 random_state=42를 사용합니다.\n\n# 데이터를 훈련 및 테스트 세트로 분할\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, # 20% 테스트, 80% 훈련\n                                                    random_state=42) # 무작위 분할을 재현 가능하게 함\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n\n(800, 200, 800, 200)\n\n\n좋네요! 이제 800개의 훈련 샘플과 200개의 테스트 샘플이 생겼습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#모델-구축하기",
    "href": "02_pytorch_classification.html#모델-구축하기",
    "title": "02 - PyTorch 신경망 분류",
    "section": "2. 모델 구축하기",
    "text": "2. 모델 구축하기\n데이터가 준비되었으니 이제 모델을 구축할 차례입니다.\n몇 가지 부분으로 나누어 보겠습니다.\n\n장치에 구애받지 않는 코드 설정 (사용 가능한 경우 모델이 CPU 또는 GPU에서 실행될 수 있도록 함).\nnn.Module을 상속하여 모델 구성.\n손실 함수 및 옵티마이저 정의.\n훈련 루프 생성 (다음 섹션에 나옵니다).\n\n좋은 소식은 노트북 01에서 위의 모든 단계를 이미 거쳤다는 것입니다.\n다만 이제는 분류 데이터셋에 맞게 조정할 것입니다.\nPyTorch와 torch.nn을 임포트하고 장치에 구애받지 않는 코드를 설정하는 것부터 시작해 봅시다.\n\n# 표준 PyTorch 임포트\nimport torch\nfrom torch import nn\n\n# 장치에 구애받지 않는 코드 만들기\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n좋습니다. 이제 device가 설정되었으므로 생성하는 모든 데이터나 모델에 이를 사용할 수 있으며, PyTorch는 사용 가능한 경우 CPU(기본값) 또는 GPU에서 이를 처리합니다.\n모델을 만들어 볼까요?\n우리는 X 데이터를 입력으로 처리하고 y 데이터 모양의 무언가를 출력으로 생성할 수 있는 모델을 원합니다.\n즉, X(특성)가 주어지면 모델이 y(레이블)를 예측하기를 원합니다.\n특성과 레이블이 있는 이 설정을 지도 학습(supervised learning)이라고 합니다. 데이터가 특정 입력이 주어졌을 때 출력이 무엇이어야 하는지 모델에 알려주기 때문입니다.\n이러한 모델을 만들려면 X와 y의 입력 및 출력 모양을 처리해야 합니다.\n입력 및 출력 모양이 중요하다고 했던 것을 기억하시나요? 여기서 그 이유를 알게 될 것입니다.\n다음과 같은 모델 클래스를 만들어 보겠습니다: 1. nn.Module을 상속합니다 (거의 모든 PyTorch 모델은 nn.Module의 서브클래스입니다). 2. X와 y의 입력 및 출력 모양을 처리할 수 있는 2개의 nn.Linear 레이어를 생성자에 생성합니다. 3. 모델의 순전파 계산을 포함하는 forward() 메서드를 정의합니다. 4. 모델 클래스를 인스턴스화하고 대상 device로 보냅니다.\n\n# 1. nn.Module을 상속하는 모델 클래스 구성\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. X와 y의 입력 및 출력 모양을 처리할 수 있는 2개의 nn.Linear 레이어 생성\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # 2개의 특성(X)을 받아 5개의 특성을 생성\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # 5개의 특성을 받아 1개의 특성(y)을 생성\n    \n    # 3. 순전파 계산을 포함하는 forward 메서드 정의\n    def forward(self, x):\n        # layer_2의 출력을 반환하며, 이는 y와 동일한 모양의 단일 특성입니다.\n        return self.layer_2(self.layer_1(x)) # 계산은 layer_1을 먼저 거친 후 layer_1의 출력이 layer_2를 거칩니다.\n\n# 4. 모델의 인스턴스를 생성하고 대상 장치로 보냄\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n\nCircleModelV0(\n  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n)\n\n\n여기서 무슨 일이 일어나고 있나요?\n우리는 이전에 이러한 단계 중 몇 가지를 보았습니다.\n유일한 주요 변화는 self.layer_1과 self.layer_2 사이에서 일어나는 일입니다.\nself.layer_1은 2개의 입력 특성 in_features=2를 가져와서 5개의 출력 특성 out_features=5를 생성합니다.\n이를 5개의 은닉 유닛(hidden units) 또는 뉴런을 갖는 것이라고 합니다.\n이 레이어는 입력 데이터를 2개의 특성에서 5개의 특성으로 바꿉니다.\n왜 이렇게 할까요?\n이를 통해 모델은 단지 2개의 숫자가 아닌 5개의 숫자로부터 패턴을 학습할 수 있으며, 이는 잠재적으로 더 나은 출력으로 이어질 수 있습니다.\n잠재적이라고 말한 이유는 때때로 작동하지 않기 때문입니다.\n신경망 레이어에서 사용할 수 있는 은닉 유닛의 수는 하이퍼파라미터(직접 설정할 수 있는 값)이며, 정해진 값은 없습니다.\n일반적으로 많을수록 좋지만 너무 많은 것도 좋지 않을 수 있습니다. 선택하는 양은 모델 유형과 작업 중인 데이터셋에 따라 달라집니다.\n우리 데이터셋은 작고 간단하므로 작게 유지하겠습니다.\n은닉 유닛의 유일한 규칙은 다음 레이어(우리의 경우 self.layer_2)가 이전 레이어의 out_features와 동일한 in_features를 가져야 한다는 것입니다.\n그렇기 때문에 self.layer_2는 in_features=5를 가지며, self.layer_1에서 out_features=5를 가져와 선형 계산을 수행하여 out_features=1(y와 동일한 모양)로 변환합니다.\n 방금 구축한 것과 유사한 분류 신경망이 어떻게 생겼는지에 대한 시각적 예시입니다. TensorFlow Playground 웹사이트에서 직접 만들어 보세요.\nnn.Sequential을 사용하여 위와 동일하게 수행할 수도 있습니다.\nnn.Sequential은 입력 데이터를 나타나는 순서대로 레이어를 통해 순전파 계산을 수행합니다.\n\n# nn.Sequential로 CircleModelV0 복제\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),\n    nn.Linear(in_features=5, out_features=1)\n).to(device)\n\nmodel_0\n\nSequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): Linear(in_features=5, out_features=1, bias=True)\n)\n\n\n와, nn.Module을 상속하는 것보다 훨씬 간단해 보이는데, 왜 항상 nn.Sequential을 사용하지 않을까요?\nnn.Sequential은 간단한 계산에는 환상적이지만, 이름에서 알 수 있듯이 항상 순차적으로 실행됩니다.\n따라서 단순히 순차적인 계산이 아닌 다른 일이 일어나길 원한다면 고유한 커스텀 nn.Module 서브클래스를 정의하고 싶을 것입니다.\n이제 모델이 생겼으니 데이터를 통과시키면 어떻게 되는지 봅시다.\n\n# 모델로 예측하기\nuntrained_preds = model_0(X_test.to(device))\nprint(f\"예측값의 길이: {len(untrained_preds)}, 모양: {untrained_preds.shape}\")\nprint(f\"테스트 샘플의 길이: {len(y_test)}, 모양: {y_test.shape}\")\nprint(f\"\\n처음 10개의 예측값:\\n{untrained_preds[:10]}\")\nprint(f\"\\n처음 10개의 테스트 레이블:\\n{y_test[:10]}\")\n\nLength of predictions: 200, Shape: torch.Size([200, 1])\nLength of test samples: 200, Shape: torch.Size([200])\n\nFirst 10 predictions:\ntensor([[0.7311],\n        [0.7607],\n        [0.4336],\n        [0.8163],\n        [0.0846],\n        [0.1053],\n        [0.4653],\n        [0.3110],\n        [0.4488],\n        [0.7589]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nFirst 10 test labels:\ntensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n\n\n음, 예측값의 수는 테스트 레이블의 수와 동일한 것 같지만, 예측값이 테스트 레이블과 동일한 형태나 모양이 아닌 것 같습니다.\n이를 해결하기 위해 몇 가지 단계를 수행할 수 있으며, 나중에 살펴볼 것입니다.\n\n2.1 손실 함수 및 옵티마이저 설정\n우리는 이전에 노트북 01에서 손실(기준 또는 비용 함수라고도 함)과 옵티마이저를 설정했습니다.\n하지만 문제 유형에 따라 서로 다른 손실 함수가 필요합니다.\n예를 들어, 회귀 문제(숫자 예측)의 경우 평균 절대 오차(MAE) 손실을 사용할 수 있습니다.\n그리고 우리와 같은 이진 분류 문제의 경우, 종종 binary cross entropy를 손실 함수로 사용합니다.\n그러나 동일한 옵티마이저 함수는 종종 다른 문제 영역에서도 사용될 수 있습니다.\n예를 들어, 확률적 경사 하강법 옵티마이저(SGD, torch.optim.SGD())는 다양한 문제에 사용될 수 있으며 Adam 옵티마이저(torch.optim.Adam())도 마찬가지입니다.\n\n\n\n손실 함수/옵티마이저\n문제 유형\nPyTorch 코드\n\n\n\n\n확률적 경사 하강법 (SGD) 옵티마이저\n분류, 회귀 등 다수\ntorch.optim.SGD()\n\n\nAdam 옵티마이저\n분류, 회귀 등 다수\ntorch.optim.Adam()\n\n\nBinary cross entropy 손실\n이진 분류\ntorch.nn.BCELossWithLogits 또는 torch.nn.BCELoss\n\n\nCross entropy 손실\n다중 클래스 분류\ntorch.nn.CrossEntropyLoss\n\n\n평균 절대 오차 (MAE) 또는 L1 손실\n회귀\ntorch.nn.L1Loss\n\n\n평균 제곱 오차 (MSE) 또는 L2 손실\n회귀\ntorch.nn.MSELoss\n\n\n\n다양한 손실 함수 및 옵티마이저 표입니다. 더 많이 있지만 이것들이 자주 보게 될 일반적인 것들입니다.\n이진 분류 문제를 다루고 있으므로 binary cross entropy 손실 함수를 사용해 봅시다.\n\n참고: 손실 함수는 모델의 예측이 얼마나 틀렸는지를 측정하는 것이며, 손실이 높을수록 모델의 성능이 좋지 않음을 상기하세요.\n또한 PyTorch 문서는 종종 손실 함수를 “loss criterion” 또는 “criterion”이라고 부르기도 하는데, 이들은 모두 같은 것을 설명하는 다른 방식일 뿐입니다.\n\nPyTorch에는 두 가지 binary cross entropy 구현이 있습니다: 1. torch.nn.BCELoss() - 타겟(레이블)과 입력(특성) 사이의 binary cross entropy를 측정하는 손실 함수를 생성합니다. 2. torch.nn.BCEWithLogitsLoss() - 위와 동일하지만 시그모이드 레이어(nn.Sigmoid)가 내장되어 있습니다 (이것이 무엇을 의미하는지 곧 알게 될 것입니다).\n어떤 것을 사용해야 할까요?\ntorch.nn.BCEWithLogitsLoss() 문서는 nn.Sigmoid 레이어 뒤에 torch.nn.BCELoss()를 사용하는 것보다 수치적으로 더 안정적이라고 명시하고 있습니다.\n따라서 일반적으로 구현 2가 더 나은 옵션입니다. 그러나 고급 사용의 경우 nn.Sigmoid와 torch.nn.BCELoss()의 조합을 분리하고 싶을 수도 있지만 이는 이 노트북의 범위를 벗어납니다.\n이를 바탕으로 손실 함수와 옵티마이저를 생성해 보겠습니다.\n옵티마이저의 경우 학습률 0.1로 모델 파라미터를 최적화하기 위해 torch.optim.SGD()를 사용합니다.\n\n참고: PyTorch 포럼에서 nn.BCELoss와 nn.BCEWithLogitsLoss 사용에 대한 논의가 있습니다. 처음에는 혼란스러울 수 있지만 많은 일들이 그렇듯 연습하면 더 쉬워집니다.\n\n\n# 손실 함수 생성\n# loss_fn = nn.BCELoss() # BCELoss = 시그모이드 내장 안 됨\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = 시그모이드 내장 됨\n\n# 옵티마이저 생성\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n\n이제 평가 지표(evaluation metric)도 만들어 보겠습니다.\n평가 지표는 모델이 어떻게 진행되고 있는지에 대한 또 다른 관점을 제공하는 데 사용될 수 있습니다.\n손실 함수가 모델이 얼마나 틀렸는지를 측정한다면, 평가 지표는 모델이 얼마나 맞았는지를 측정하는 것이라고 생각합니다.\n물론 이 두 가지가 같은 일을 하고 있다고 주장할 수 있지만, 평가 지표는 다른 관점을 제공합니다.\n결국 모델을 평가할 때는 여러 관점에서 사물을 보는 것이 좋습니다.\n분류 문제에 사용할 수 있는 여러 평가 지표가 있지만, 정확도(accuracy)부터 시작해 봅시다.\n정확도는 총 예측 수 대비 정답 예측 수의 비율로 측정할 수 있습니다.\n예를 들어, 100번의 예측 중 99번을 맞게 예측한 모델은 99%의 정확도를 가집니다.\n이를 수행하는 함수를 작성해 봅시다.\n\n# 정확도 계산 (분류 지표)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq()는 두 텐서가 같은 위치를 계산함\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n\n좋습니다! 이제 모델을 훈련하면서 이 함수를 사용하여 손실과 함께 성능을 측정할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#모델-훈련하기",
    "href": "02_pytorch_classification.html#모델-훈련하기",
    "title": "02 - PyTorch 신경망 분류",
    "section": "3. 모델 훈련하기",
    "text": "3. 모델 훈련하기\n자, 이제 손실 함수와 옵티마이저가 준비되었으니 모델을 훈련해 봅시다.\nPyTorch 훈련 루프의 단계를 기억하시나요?\n기억이 안 난다면 여기 리마인더가 있습니다.\n훈련 단계:\n\n\nPyTorch 훈련 루프 단계\n\n\n\n순전파 (Forward pass) - 모델이 모든 훈련 데이터를 한 번 훑으며 forward() 함수 계산을 수행합니다 (model(x_train)).\n\n\n손실 계산 (Calculate the loss) - 모델의 출력(예측)을 정답 레이블과 비교하여 얼마나 틀렸는지 평가합니다 (loss = loss_fn(y_pred, y_train)).\n\n\n옵티마이저 제로 그래디언트 (Zero gradients) - 옵티마이저의 그래디언트를 0으로 설정하여(기본적으로 누적됨) 특정 훈련 단계에 대해 다시 계산할 수 있도록 합니다 (optimizer.zero_grad()).\n\n\n손실에 대한 역전파 수행 (Perform backpropagation on the loss) - 업데이트할 모든 모델 파라미터(requires_grad=True인 각 파라미터)에 대해 손실의 그래디언트를 계산합니다. 이를 역전파라고 하며, 그래서 “backwards”입니다 (loss.backward()).\n\n\n옵티마이저 단계 수행 (경사 하강법) (Step the optimizer) - 손실 그래디언트와 관련하여 requires_grad=True인 파라미터를 업데이트하여 개선합니다 (optimizer.step()).\n\n\n\n\n3.1 원시 모델 출력에서 예측 레이블로 가기 (로짓 -&gt; 예측 확률 -&gt; 예측 레이블)\n훈련 루프 단계를 수행하기 전에, 순전파 동안 모델에서 무엇이 나오는지 확인해 봅시다 (순전파는 forward() 메서드에 의해 정의됩니다).\n이를 위해 모델에 일부 데이터를 전달해 보겠습니다.\n\n# 테스트 데이터에 대한 순전파의 처음 5개 출력 확인\ny_logits = model_0(X_test.to(device))[:5]\ny_logits\n\ntensor([[0.7311],\n        [0.7607],\n        [0.4336],\n        [0.8163],\n        [0.0846]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\n\n우리 모델은 아직 훈련되지 않았기 때문에 이 출력들은 기본적으로 무작위입니다.\n하지만 이것들은 무엇일까요?\n이것들은 우리 forward() 메서드의 출력입니다.\n내부적으로 다음 방정식을 호출하는 두 개의 nn.Linear() 레이어를 구현합니다:\n\\[\n\\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias}\n\\]\n이 방정식의 원시 출력(수정되지 않은)(\\(\\mathbf{y}\\))과 결과적으로 우리 모델의 원시 출력은 종종 로짓(logits)이라고 불립니다.\n모델이 입력 데이터(방정식의 \\(x\\) 또는 코드의 X_test)를 받아 위에서 출력하는 것이 바로 로짓입니다.\n그러나 이러한 숫자는 해석하기 어렵습니다.\n우리는 정답 레이블과 비교할 수 있는 숫자를 원합니다.\n모델의 원시 출력(로짓)을 이러한 형태로 바꾸기 위해 시그모이드 활성화 함수(sigmoid activation function)를 사용할 수 있습니다.\n한번 시도해 봅시다.\n\n# 모델 로짓에 시그모이드 사용\ny_pred_probs = torch.sigmoid(y_logits)\ny_pred_probs\n\ntensor([[0.6751],\n        [0.6815],\n        [0.6067],\n        [0.6935],\n        [0.5211]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n이제 출력에 어떤 일관성이 생긴 것 같습니다 (여전히 무작위이긴 하지만요).\n이제 이들은 예측 확률(prediction probabilities)(저는 주로 y_pred_probs라고 부릅니다) 형태가 되었습니다. 즉, 이 값들은 이제 모델이 해당 데이터 포인트가 특정 클래스에 속한다고 생각하는 정도를 나타냅니다.\n우리의 경우 이진 분류를 다루고 있으므로 이상적인 출력은 0 또는 1입니다.\n따라서 이러한 값은 결정 경계로 볼 수 있습니다.\n0에 가까울수록 모델은 샘플이 클래스 0에 속한다고 생각하고, 1에 가까울수록 모델은 샘플이 클래스 1에 속한다고 생각합니다.\n더 구체적으로: * y_pred_probs &gt;= 0.5 이면, y=1 (클래스 1) * y_pred_probs &lt; 0.5 이면, y=0 (클래스 0)\n예측 확률을 예측 레이블로 바꾸기 위해 시그모이드 활성화 함수의 출력을 반올림할 수 있습니다.\n\n# 예측 레이블 찾기 (예측 확률 반올림)\ny_preds = torch.round(y_pred_probs)\n\n# 전체 과정\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# 동일한지 확인\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# 추가 차원 제거\ny_preds.squeeze()\n\ntensor([True, True, True, True, True], device='cuda:0')\n\n\ntensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)\n\n\n멋지네요! 이제 모델의 예측값이 정답 레이블(y_test)과 동일한 형태가 된 것 같습니다.\n\ny_test[:5]\n\ntensor([1., 0., 1., 0., 1.])\n\n\n이는 모델의 예측값을 테스트 레이블과 비교하여 얼마나 잘 진행되고 있는지 확인할 수 있음을 의미합니다.\n요약하자면, 시그모이드 활성화 함수를 사용하여 모델의 원시 출력(로짓)을 예측 확률로 변환했습니다.\n그런 다음 예측 확률을 반올림하여 예측 레이블로 변환했습니다.\n\n참고: 시그모이드 활성화 함수의 사용은 종종 이진 분류 로짓에만 해당됩니다. 다중 클래스 분류의 경우 소프트맥스 활성화 함수(softmax activation function)의 사용을 살펴볼 것입니다(나중에 나옵니다).\n그리고 모델의 원시 출력을 nn.BCEWithLogitsLoss에 전달할 때는 시그모이드 활성화 함수를 사용할 필요가 없습니다(로짓 손실의 “로짓”은 모델의 원시 로짓 출력에서 작동하기 때문입니다). 이는 시그모이드 함수가 내장되어 있기 때문입니다.\n\n\n\n3.2 훈련 및 테스트 루프 구축하기\n원시 모델 출력을 가져와 예측 레이블로 변환하는 방법을 논의했으므로 이제 훈련 루프를 구축해 보겠습니다.\n100 에포크 동안 훈련하고 10 에포크마다 모델의 진행 상황을 출력하는 것부터 시작해 봅시다.\n\ntorch.manual_seed(42)\n\n# 에포크 수 설정\nepochs = 100\n\n# 데이터를 대상 장치로 이동\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# 훈련 및 평가 루프 구축\nfor epoch in range(epochs):\n    ### 훈련\n    model_0.train()\n\n    # 1. 순전파 (모델은 원시 로짓을 출력함)\n    y_logits = model_0(X_train).squeeze() # 추가 '1' 차원을 제거하기 위해 스퀴즈, 모델과 데이터가 동일한 장치에 있지 않으면 작동하지 않음\n    y_pred = torch.round(torch.sigmoid(y_logits)) # 로짓 -&gt; 예측 확률 -&gt; 예측 레이블로 변환\n  \n    # 2. 손실/정확도 계산\n    # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss를 사용하는 경우 torch.sigmoid()가 필요함\n    #                y_train) \n    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss를 사용하면 원시 로짓에서 작동함\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. 옵티마이저 제로 그래디언트\n    optimizer.zero_grad()\n\n    # 4. 손실 역전파\n    loss.backward()\n\n    # 5. 옵티마이저 단계 수행\n    optimizer.step()\n\n    ### 테스트\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. 순전파\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. 손실/정확도 계산\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # 10 에포크마다 진행 상황 출력\n    if epoch % 10 == 0:\n        print(f\"에포크: {epoch} | 손실: {loss:.5f}, 정확도: {acc:.2f}% | 테스트 손실: {test_loss:.5f}, 테스트 정확도: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 0.72095, Accuracy: 50.00% | Test loss: 0.72767, Test acc: 50.50%\nEpoch: 10 | Loss: 0.70546, Accuracy: 53.87% | Test loss: 0.71203, Test acc: 52.50%\nEpoch: 20 | Loss: 0.70011, Accuracy: 52.25% | Test loss: 0.70601, Test acc: 49.50%\nEpoch: 30 | Loss: 0.69792, Accuracy: 51.50% | Test loss: 0.70315, Test acc: 49.50%\nEpoch: 40 | Loss: 0.69682, Accuracy: 51.12% | Test loss: 0.70148, Test acc: 48.50%\nEpoch: 50 | Loss: 0.69613, Accuracy: 50.75% | Test loss: 0.70034, Test acc: 49.50%\nEpoch: 60 | Loss: 0.69565, Accuracy: 51.00% | Test loss: 0.69948, Test acc: 49.50%\nEpoch: 70 | Loss: 0.69527, Accuracy: 50.75% | Test loss: 0.69880, Test acc: 50.00%\nEpoch: 80 | Loss: 0.69497, Accuracy: 50.62% | Test loss: 0.69825, Test acc: 49.50%\nEpoch: 90 | Loss: 0.69472, Accuracy: 50.62% | Test loss: 0.69779, Test acc: 49.50%\n\n\n음, 우리 모델의 성능에 대해 무엇을 알 수 있나요?\n훈련 및 테스트 단계를 잘 거친 것 같지만 결과가 너무 많이 움직이지 않은 것 같습니다.\n정확도는 각 데이터 분할에서 50%를 간신히 넘습니다.\n그리고 균형 잡힌 이진 분류 문제를 다루고 있기 때문에, 이는 우리 모델이 무작위 추측만큼 성능을 내고 있음을 의미합니다 (클래스 0과 클래스 1의 샘플이 500개씩 있을 때 매번 클래스 1을 예측하는 모델은 50%의 정확도를 달성합니다).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#예측-수행-및-모델-평가",
    "href": "02_pytorch_classification.html#예측-수행-및-모델-평가",
    "title": "02 - PyTorch 신경망 분류",
    "section": "4. 예측 수행 및 모델 평가",
    "text": "4. 예측 수행 및 모델 평가\n지표로 보아 우리 모델은 무작위 추측을 하고 있는 것 같습니다.\n이를 어떻게 더 조사할 수 있을까요?\n좋은 생각이 있습니다.\n데이터 탐험가의 좌우명!\n“시각화, 시각화, 시각화!”\n모델의 예측값, 예측하려는 데이터 및 클래스 0인지 1인지에 대해 생성하는 결정 경계를 플롯해 보겠습니다.\n그렇게 하기 위해 Learn PyTorch for Deep Learning 저장소에서 helper_functions.py 스크립트를 다운로드하여 임포트하는 코드를 작성할 것입니다.\n여기에는 모델이 특정 클래스를 예측하는 서로 다른 점을 시각적으로 플롯하기 위해 NumPy 메시그리드(meshgrid)를 생성하는 plot_decision_boundary()라는 유용한 함수가 포함되어 있습니다.\n또한 나중에 사용하기 위해 노트북 01에서 작성한 plot_predictions()도 임포트할 것입니다.\n\nimport requests\nfrom pathlib import Path \n\n# Learn PyTorch 저장소에서 헬퍼 함수 다운로드 (이미 다운로드되지 않은 경우)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py가 이미 존재합니다. 다운로드를 건너뜁니다.\")\nelse:\n  print(\"helper_functions.py를 다운로드합니다.\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n\nDownloading helper_functions.py\n\n\n\n# 훈련 및 테스트 세트에 대한 결정 경계 플롯\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"훈련\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"테스트\")\nplot_decision_boundary(model_0, X_test, y_test)\n\n\n\n\n\n\n\n\n와, 모델의 성능 문제의 원인을 찾은 것 같습니다.\n현재 직선을 사용하여 빨간색과 파란색 점을 분리하려고 시도하고 있습니다…\n그것이 50% 정확도의 이유입니다. 데이터가 원형이기 때문에 직선을 긋는 것은 기껏해야 중간을 가로지르는 정도일 뿐입니다.\n머신러닝 용어로 우리 모델은 과소적합(underfitting) 상태입니다. 즉, 데이터에서 예측 패턴을 학습하지 못하고 있습니다.\n이를 어떻게 개선할 수 있을까요?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#모델-개선하기-모델-관점에서",
    "href": "02_pytorch_classification.html#모델-개선하기-모델-관점에서",
    "title": "02 - PyTorch 신경망 분류",
    "section": "5. 모델 개선하기 (모델 관점에서)",
    "text": "5. 모델 개선하기 (모델 관점에서)\n모델의 과소적합 문제를 해결해 봅시다.\n데이터가 아닌 모델에 구체적으로 초점을 맞추면 몇 가지 방법이 있을 수 있습니다.\n\n\n\n모델 개선 기술*\n설명\n\n\n\n\n레이어 추가하기\n각 레이어는 모델의 학습 능력을 잠재적으로 증가시키며, 각 레이어는 데이터의 어떤 새로운 패턴을 학습할 수 있습니다. 레이어를 더 많이 추가하는 것을 신경망을 더 깊게 만든다고 합니다.\n\n\n은닉 유닛 추가하기\n위와 유사하게 레이어당 은닉 유닛을 더 많이 추가하면 모델의 학습 능력이 잠재적으로 증가합니다. 은닉 유닛을 더 많이 추가하는 것을 신경망을 더 넓게 만든다고 합니다.\n\n\n더 오래 훈련하기 (에포크 늘리기)\n모델이 데이터를 더 많이 볼 수 있는 기회가 있다면 더 많은 것을 학습할 수 있습니다.\n\n\n활성화 함수 변경하기\n일부 데이터는 우리가 본 것처럼 직선만으로는 적합할 수 없습니다. 비선형 활성화 함수를 사용하는 것이 도움이 될 수 있습니다 (힌트!).\n\n\n학습률 변경하기\n모델 자체보다는 관련이 있지만, 옵티마이저의 학습률은 모델이 매 단계마다 파라미터를 얼마나 변경해야 할지를 결정합니다. 너무 크면 과잉 수정이 발생하고 너무 작으면 충분히 학습하지 못합니다.\n\n\n손실 함수 변경하기\n역시 모델에 따라 다르지만 중요합니다. 문제마다 다른 손실 함수가 필요합니다. 예를 들어 binary cross entropy 손실 함수는 다중 클래스 분류 문제에서 작동하지 않습니다.\n\n\n전이 학습 (Transfer learning) 사용하기\n여러분의 것과 유사한 문제 영역에서 이미 훈련된 모델을 가져와 자신의 문제에 맞게 조정합니다. 전이 학습은 노트북 06에서 다룹니다.\n\n\n\n\n참고: *이 모든 것을 수동으로 조정할 수 있기 때문에 이를 하이퍼파라미터라고 부릅니다.\n그리고 이것이 머신러닝이 절반은 예술이고 절반은 과학인 이유이기도 합니다. 프로젝트에 가장 적합한 값의 조합이 무엇인지 알 수 있는 실질적인 방법은 없으며, 데이터 과학자의 좌우명인 “실험, 실험, 실험”을 따르는 것이 가장 좋습니다.\n\n모델에 레이어를 하나 더 추가하고, 더 오래 훈련하고(epochs=100 대신 epochs=1000), 은닉 유닛의 수를 5에서 10으로 늘리면 어떻게 되는지 봅시다.\n몇 가지 하이퍼파라미터를 변경하여 위와 동일한 단계를 따를 것입니다.\n\nclass CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # 추가 레이어\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        \n    def forward(self, x): # 참고: 항상 forward 스펠링이 올바른지 확인하세요!\n        # 이와 같이 모델을 만드는 것은 아래와 동일하지만, 아래 방식이\n        # 일반적으로 가능한 경우 속도 향상의 이점이 있습니다.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n\nCircleModelV1(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\n이제 모델이 생겼으므로 이전과 동일한 설정을 사용하여 손실 함수와 옵티마이저 인스턴스를 다시 생성합니다.\n\n# loss_fn = nn.BCELoss() # 입력에 시그모이드가 필요함\nloss_fn = nn.BCEWithLogitsLoss() # 입력에 시그모이드가 필요하지 않음\noptimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n\n좋습니다. 모델, 옵티마이저 및 손실 함수가 준비되었으니 훈련 루프를 만들어 보겠습니다.\n이번에는 더 오래(epochs=1000 vs epochs=100) 훈련해보고 모델이 개선되는지 확인해 봅시다.\n\ntorch.manual_seed(42)\n\nepochs = 1000 # 더 오래 훈련\n\n# 데이터를 대상 장치로 이동\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### 훈련\n    # 1. 순전파\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # 로짓 -&gt; 예측 확률 -&gt; 예측 레이블\n\n    # 2. 손실/정확도 계산\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. 옵티마이저 제로 그래디언트\n    optimizer.zero_grad()\n\n    # 4. 손실 역전파\n    loss.backward()\n\n    # 5. 옵티마이저 단계 수행\n    optimizer.step()\n\n    ### 테스트\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. 순전파\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. 손실/정확도 계산\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # 100 에포크마다 진행 상황 출력\n    if epoch % 100 == 0:\n        print(f\"에포크: {epoch} | 손실: {loss:.5f}, 정확도: {acc:.2f}% | 테스트 손실: {test_loss:.5f}, 테스트 정확도: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%\nEpoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%\nEpoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%\nEpoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%\nEpoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%\nEpoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%\nEpoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\n\n\n네? 모델을 더 오래 훈련하고 레이어를 하나 더 추가했지만, 여전히 무작위 추측보다 나은 패턴을 학습하지 못한 것 같습니다.\n시각화해 봅시다.\n\n# 훈련 및 테스트 세트에 대한 결정 경계 플롯\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"훈련\")\nplot_decision_boundary(model_1, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"테스트\")\nplot_decision_boundary(model_1, X_test, y_test)\n\n\n\n\n\n\n\n\n음.\n우리 모델은 여전히 빨간색과 파란색 점 사이에 직선을 긋고 있습니다.\n우리 모델이 직선을 그리고 있다면, 선형 데이터를 모델링할 수 있을까요? 노트북 01에서 했던 것처럼 말이죠.\n\n5.1 모델이 직선을 모델링할 수 있는지 확인하기 위한 데이터 준비\n선형 데이터를 생성하여 우리 모델이 이를 모델링할 수 있는지, 아니면 단순히 아무것도 배울 수 없는 모델을 사용하고 있는지 확인해 봅시다.\n\n# 일부 데이터 생성 (노트북 01과 동일)\nweight = 0.7\nbias = 0.3\nstart = 0\nend = 1\nstep = 0.01\n\n# 데이터 생성\nX_regression = torch.arange(start, end, step).unsqueeze(dim=1)\ny_regression = weight * X_regression + bias # 선형 회귀 공식\n\n# 데이터 확인\nprint(len(X_regression))\nX_regression[:5], y_regression[:5]\n\n100\n\n\n(tensor([[0.0000],\n         [0.0100],\n         [0.0200],\n         [0.0300],\n         [0.0400]]), tensor([[0.3000],\n         [0.3070],\n         [0.3140],\n         [0.3210],\n         [0.3280]]))\n\n\n멋지네요. 이제 데이터를 훈련 세트와 테스트 세트로 분할해 보겠습니다.\n\n# 훈련 및 테스트 분할 생성\ntrain_split = int(0.8 * len(X_regression)) # 데이터의 80%를 훈련 세트로 사용\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# 각 분할의 길이 확인\nprint(len(X_train_regression), \n    len(y_train_regression), \n    len(X_test_regression), \n    len(y_test_regression))\n\n80 80 20 20\n\n\n좋습니다. 데이터가 어떻게 생겼는지 확인해 봅시다.\n이를 위해 노트북 01에서 만든 plot_predictions() 함수를 사용할 것입니다.\n이 함수는 위에서 다운로드한 Learn PyTorch for Deep Learning 저장소의 helper_functions.py 스크립트에 포함되어 있습니다.\n\nplot_predictions(train_data=X_train_regression,\n    train_labels=y_train_regression,\n    test_data=X_test_regression,\n    test_labels=y_test_regression\n);\n\n\n\n\n\n\n\n\n\n\n5.2 직선에 적합하도록 model_1 조정하기\n이제 데이터가 생겼으니, 우리 회귀 데이터에 적합한 손실 함수를 사용하여 model_1을 다시 만들어 보겠습니다.\n\n# model_1과 동일한 구조 (단, nn.Sequential 사용)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n\nSequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\n손실 함수를 nn.L1Loss() (평균 절대 오차와 동일)로, 옵티마이저를 torch.optim.SGD()로 설정하겠습니다.\n\n# 손실 및 옵티마이저\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n\n이제 epochs=1000 (model_1과 동일) 동안 일반적인 훈련 루프 단계를 사용하여 모델을 훈련해 보겠습니다.\n\n참고: 우리는 유사한 훈련 루프 코드를 반복해서 작성해 왔습니다. 하지만 연습을 계속하기 위해 의도적으로 그렇게 만들었습니다. 그러나 이를 함수화할 수 있는 방법이 있을까요? 그렇게 하면 미래에 상당한 코딩 시간을 절약할 수 있을 것입니다. 잠재적으로 훈련을 위한 함수와 테스트를 위한 함수가 있을 수 있습니다.\n\n\n# 모델 훈련\ntorch.manual_seed(42)\n\n# 에포크 수 설정\nepochs = 1000\n\n# 데이터를 대상 장치로 이동\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\nfor epoch in range(epochs):\n    ### 훈련 \n    # 1. 순전파\n    y_pred = model_2(X_train_regression)\n    \n    # 2. 손실 계산 (분류가 아닌 회귀 문제이므로 정확도는 없음)\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. 옵티마이저 제로 그래디언트\n    optimizer.zero_grad()\n\n    # 4. 손실 역전파\n    loss.backward()\n\n    # 5. 옵티마이저 단계 수행\n    optimizer.step()\n\n    ### 테스트\n    model_2.eval()\n    with torch.inference_mode():\n      # 1. 순전파\n      test_pred = model_2(X_test_regression)\n      # 2. 손실 계산 \n      test_loss = loss_fn(test_pred, y_test_regression)\n\n    # 진행 상황 출력\n    if epoch % 100 == 0: \n        print(f\"에포크: {epoch} | 훈련 손실: {loss:.5f}, 테스트 손실: {test_loss:.5f}\")\n\nEpoch: 0 | Train loss: 0.75986, Test loss: 0.54143\nEpoch: 100 | Train loss: 0.09309, Test loss: 0.02901\nEpoch: 200 | Train loss: 0.07376, Test loss: 0.02850\nEpoch: 300 | Train loss: 0.06745, Test loss: 0.00615\nEpoch: 400 | Train loss: 0.06107, Test loss: 0.02004\nEpoch: 500 | Train loss: 0.05698, Test loss: 0.01061\nEpoch: 600 | Train loss: 0.04857, Test loss: 0.01326\nEpoch: 700 | Train loss: 0.06109, Test loss: 0.02127\nEpoch: 800 | Train loss: 0.05599, Test loss: 0.01426\nEpoch: 900 | Train loss: 0.05571, Test loss: 0.00603\n\n\n좋습니다. 분류 데이터에 대한 model_1과는 달리 model_2의 손실은 실제로 감소하고 있는 것 같습니다.\n정말 그런지 확인하기 위해 예측값을 플롯해 봅시다.\n그리고 모델과 데이터가 대상 device를 사용하고 있고 이 장치가 GPU일 수 있지만, 플로팅 함수는 matplotlib을 사용하고 matplotlib은 GPU의 데이터를 처리할 수 없음을 기억하세요.\n이를 처리하기 위해 plot_predictions()에 데이터를 전달할 때 .cpu()를 사용하여 모든 데이터를 CPU로 보낼 것입니다.\n\n# 평가 모드 켜기\nmodel_2.eval()\n\n# 예측 수행 (추론)\nwith torch.inference_mode():\n    y_preds = model_2(X_test_regression)\n\n# CPU의 데이터로 데이터 및 예측값 플롯 (matplotlib은 GPU의 데이터를 처리할 수 없음)\n# (아래 중 하나에서 .cpu()를 제거하고 어떤 일이 일어나는지 확인해 보세요)\nplot_predictions(train_data=X_train_regression.cpu(),\n                 train_labels=y_train_regression.cpu(),\n                 test_data=X_test_regression.cpu(),\n                 test_labels=y_test_regression.cpu(),\n                 predictions=y_preds.cpu());\n\n\n\n\n\n\n\n\n좋습니다. 우리 모델이 직선에 대해 무작위 추측보다 훨씬 더 잘 수행할 수 있는 것 같습니다.\n이것은 좋은 징후입니다.\n이는 우리 모델이 최소한 학습할 수 있는 어느 정도의 능력이 있음을 의미합니다.\n\n참고: 딥러닝 모델을 구축할 때 유용한 문제 해결 단계는 모델을 확장하기 전에 모델이 작동하는지 확인하기 위해 가능한 한 작게 시작하는 것입니다.\n이는 간단한 신경망(레이어와 은닉 뉴런이 많지 않음)과 작은 데이터셋(우리가 만든 것과 같은)으로 시작한 다음, 데이터 양이나 모델 크기/설계를 늘려 과적합을 줄이기 전에 해당 작은 예제에서 과적합(모델의 성능이 너무 좋게 만듦)을 시도하는 것을 의미할 수 있습니다.\n\n그렇다면 무엇이 문제일까요?\n알아봅시다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#누락된-조각-비선형성-non-linearity",
    "href": "02_pytorch_classification.html#누락된-조각-비선형성-non-linearity",
    "title": "02 - PyTorch 신경망 분류",
    "section": "6. 누락된 조각: 비선형성 (Non-linearity)",
    "text": "6. 누락된 조각: 비선형성 (Non-linearity)\n선형 레이어 덕분에 우리 모델이 직선(선형)을 그릴 수 있다는 것을 보았습니다.\n하지만 우리 모델에 직선이 아닌(비선형) 선을 그릴 수 있는 능력을 부여하면 어떨까요?\n어떻게 할까요?\n알아봅시다.\n\n6.1 비선형 데이터 재생성 (빨간색 및 파란색 원)\n먼저 새롭게 시작하기 위해 데이터를 다시 생성해 보겠습니다. 이전과 동일한 설정을 사용합니다.\n\n# 데이터 생성 및 플롯\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\nn_samples = 1000\n\nX, y = make_circles(n_samples=1000,\n    noise=0.03,\n    random_state=42,\n)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);\n\n\n\n\n\n\n\n\n좋습니다! 이제 데이터의 80%를 훈련에, 20%를 테스트에 사용하여 훈련 및 테스트 세트로 분할해 보겠습니다.\n\n# 텐서로 변환하고 훈련 및 테스트 세트로 분할\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n# 데이터를 텐서로 변환\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# 훈련 및 테스트 세트로 분할\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42\n)\n\nX_train[:5], y_train[:5]\n\n(tensor([[ 0.6579, -0.4651],\n         [ 0.6319, -0.7347],\n         [-1.0086, -0.1240],\n         [-0.9666, -0.2256],\n         [-0.1666,  0.7994]]), tensor([1., 0., 0., 0., 1.]))\n\n\n\n\n6.2 비선형성을 사용한 모델 구축하기\n이제 재미있는 부분이 나옵니다.\n제한 없는 직선(선형)과 직선이 아닌(비선형) 선으로 어떤 패턴을 그릴 수 있다고 생각하시나요?\n꽤 창의적이 될 수 있을 것입니다.\n지금까지 우리 신경망은 선형(직선) 함수만 사용해 왔습니다.\n하지만 우리가 작업해 온 데이터는 비선형(원)입니다.\n모델에 비선형 활성화 함수를 사용할 수 있는 기능을 도입하면 어떤 일이 일어날까요?\n한번 봅시다.\nPyTorch에는 유사하지만 다른 작업을 수행하는 기성 비선형 활성화 함수가 많이 있습니다.\n가장 일반적이고 성능이 좋은 것 중 하나는 ReLU (rectified linear-unit, torch.nn.ReLU())입니다.\n말로 설명하기보다 순전파의 은닉 레이어 사이에 이를 넣고 어떤 일이 일어나는지 봅시다.\n\n# 비선형 활성화 함수가 있는 모델 구축\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- ReLU 활성화 함수 추가\n        # 모델에 시그모이드를 넣을 수도 있음\n        # 이는 예측값에 이를 사용할 필요가 없음을 의미함\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # 레이어 사이에 ReLU 활성화 함수 배치\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n\nCircleModelV2(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\n\n 방금 구축한 것과 유사한(ReLU 활성화를 사용하는) 분류 신경망이 어떻게 생겼는지에 대한 시각적 예시입니다. TensorFlow Playground 웹사이트에서 직접 만들어 보세요.\n\n질문: 신경망을 구축할 때 비선형 활성화 함수를 어디에 두어야 하나요?\n경험 법칙은 은닉 레이어 사이와 출력 레이어 바로 뒤에 두는 것이지만, 정해진 옵션은 없습니다. 신경망과 딥러닝에 대해 더 배우다 보면 사물을 결합하는 다양한 방법을 발견하게 될 것입니다. 그동안은 실험, 실험, 실험을 하는 것이 가장 좋습니다.\n\n이제 모델이 준비되었으므로 이진 분류 손실 함수와 옵티마이저를 만들어 보겠습니다.\n\n# 손실 및 옵티마이저 설정\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n\n멋지네요!\n\n\n6.3 비선형성을 사용한 모델 훈련하기\n익숙한 방식대로 모델, 손실 함수, 옵티마이저가 준비되었으니 훈련 및 테스트 루프를 만들어 보겠습니다.\n\n# 모델 적합\ntorch.manual_seed(42)\nepochs = 1000\n\n# 모든 데이터를 대상 장치로 이동\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    # 1. 순전파\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # 로짓 -&gt; 예측 확률 -&gt; 예측 레이블\n    \n    # 2. 손실 및 정확도 계산\n    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss는 로짓을 사용하여 손실을 계산함\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n    \n    # 3. 옵티마이저 제로 그래디언트\n    optimizer.zero_grad()\n\n    # 4. 손실 역전파\n    loss.backward()\n\n    # 5. 옵티마이저 단계 수행\n    optimizer.step()\n\n    ### 테스트\n    model_3.eval()\n    with torch.inference_mode():\n      # 1. 순전파\n      test_logits = model_3(X_test).squeeze()\n      test_pred = torch.round(torch.sigmoid(test_logits)) # 로짓 -&gt; 예측 확률 -&gt; 예측 레이블\n      # 2. 손실 및 정확도 계산\n      test_loss = loss_fn(test_logits, y_test)\n      test_acc = accuracy_fn(y_true=y_test,\n                             y_pred=test_pred)\n\n    # 100 에포크마다 진행 상황 출력\n    if epoch % 100 == 0:\n        print(f\"에포크: {epoch} | 손실: {loss:.5f}, 정확도: {acc:.2f}% | 테스트 손실: {test_loss:.5f}, 테스트 정확도: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\n\n\n허허! 훨씬 좋아 보이네요!\n\n\n6.4 비선형 활성화 함수로 훈련된 모델 평가하기\n우리 원 데이터가 비선형인 것 기억하시나요? 자, 이제 비선형 활성화 함수로 훈련된 모델의 예측이 어떻게 보이는지 봅시다.\n\n# 예측 수행\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # 예측값이 실제 레이블과 동일한 형식인지 확인\n\n(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),\n tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))\n\n\n\n# 훈련 및 테스트 세트에 대한 결정 경계 플롯\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"훈련\")\nplot_decision_boundary(model_1, X_train, y_train) # model_1 = 비선형성 없음\nplt.subplot(1, 2, 2)\nplt.title(\"테스트\")\nplot_decision_boundary(model_3, X_test, y_test) # model_3 = 비선형성 있음\n\n\n\n\n\n\n\n\n멋지네요! 완벽하지는 않지만 이전보다는 훨씬 낫습니다.\n모델의 테스트 정확도를 개선하기 위해 몇 가지 트릭을 시도해 볼 수 있을까요? (힌트: 모델 개선을 위한 팁은 섹션 5로 돌아가 확인하세요)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#비선형-활성화-함수-복제하기",
    "href": "02_pytorch_classification.html#비선형-활성화-함수-복제하기",
    "title": "02 - PyTorch 신경망 분류",
    "section": "7. 비선형 활성화 함수 복제하기",
    "text": "7. 비선형 활성화 함수 복제하기\n모델에 비선형 활성화 함수를 추가하는 것이 비선형 데이터를 모델링하는 데 어떻게 도움이 되는지 보았습니다.\n\n참고: 실생활에서 접하게 될 많은 데이터는 비선형(또는 선형과 비선형의 조합)입니다. 지금 우리는 2D 플롯의 점들을 다루고 있습니다. 하지만 분류하고 싶은 식물 이미지가 있다고 상상해 보세요. 매우 다양한 식물 모양이 있습니다. 또는 요약하고 싶은 위키피디아의 텍스트가 있다면, 단어들이 결합되는 수많은 방식(선형 및 비선형 패턴)이 있습니다.\n\n그런데 비선형 활성화는 어떤 모습일까요?\n일부를 복제하고 그들이 하는 일을 알아볼까요?\n소량의 데이터를 생성하는 것부터 시작하겠습니다.\n\n# 토이 텐서 생성 (우리 모델에 들어가는 데이터와 유사함)\nA = torch.arange(-10, 10, 1, dtype=torch.float32)\nA\n\ntensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])\n\n\n좋습니다. 이제 플롯해 봅시다.\n\n# 토이 텐서 시각화\nplt.plot(A);\n\n\n\n\n\n\n\n\n직선이네요, 좋습니다.\n이제 ReLU 활성화 함수가 여기에 어떻게 영향을 미치는지 봅시다.\n그리고 PyTorch의 ReLU(torch.nn.ReLU)를 사용하는 대신 직접 만들어 보겠습니다.\nReLU 함수는 모든 음수를 0으로 바꾸고 양수 값은 그대로 둡니다.\n\n# 직접 ReLU 함수 만들기\ndef relu(x):\n  return torch.maximum(torch.tensor(0), x) # 입력은 반드시 텐서여야 함\n\n# 토이 텐서를 ReLU 함수에 통과시키기\nrelu(A)\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n        8., 9.])\n\n\n우리 ReLU 함수가 잘 작동한 것 같네요. 모든 음수 값이 0이 되었습니다.\n플롯해 봅시다.\n\n# ReLU가 적용된 토이 텐서 플롯\nplt.plot(relu(A));\n\n\n\n\n\n\n\n\n좋습니다! ReLU Wikipedia 페이지에 있는 ReLU 함수의 모양과 똑같아 보이네요.\n우리가 사용해 온 시그모이드 함수(sigmoid function)는 어떨까요?\n시그모이드 함수 공식은 다음과 같습니다:\n\\[ out_i = \\frac{1}{1+e^{-input_i}} \\]\n또는 \\(x\\)를 입력으로 사용하면:\n\\[ S(x) = \\frac{1}{1+e^{-x_i}} \\]\n여기서 \\(S\\)는 시그모이드를 나타내고, \\(e\\)는 지수(exponential) (torch.exp())를 나타내며, \\(i\\)는 텐서의 특정 요소를 나타냅니다.\nPyTorch로 시그모이드 함수를 복제하는 함수를 구축해 보겠습니다.\n\n# 커스텀 시그모이드 함수 만들기\ndef sigmoid(x):\n  return 1 / (1 + torch.exp(-x))\n\n# 토이 텐서로 커스텀 시그모이드 테스트\nsigmoid(A)\n\ntensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n        9.9966e-01, 9.9988e-01])\n\n\n와, 이 값들은 이전에 보았던 예측 확률과 매우 유사해 보이네요. 시각화하면 어떻게 보이는지 봅시다.\n\n# 시그모이드가 적용된 토이 텐서 플롯\nplt.plot(sigmoid(A));\n\n\n\n\n\n\n\n\n좋아 보이네요! 직선에서 곡선으로 바뀌었습니다.\nPyTorch에는 우리가 시도하지 않은 훨씬 더 많은 비선형 활성화 함수가 존재합니다.\n하지만 이 두 가지가 가장 일반적인 두 가지입니다.\n그리고 핵심은 여전합니다. 무제한의 직선(선형)과 직선이 아닌(비선형) 선을 사용하여 어떤 패턴을 그릴 수 있을까요?\n거의 무엇이든 가능하겠죠?\n그것이 바로 선형 함수와 비선형 함수를 결합할 때 우리 모델이 수행하는 작업입니다.\n모델에 무엇을 해야 할지 지시하는 대신, 데이터에서 패턴을 가장 잘 발견하는 방법을 알아낼 수 있는 도구를 제공합니다.\n그리고 그 도구가 바로 선형 및 비선형 함수입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#다중-클래스-pytorch-모델을-구축하여-전체-과정-합치기",
    "href": "02_pytorch_classification.html#다중-클래스-pytorch-모델을-구축하여-전체-과정-합치기",
    "title": "02 - PyTorch 신경망 분류",
    "section": "8. 다중 클래스 PyTorch 모델을 구축하여 전체 과정 합치기",
    "text": "8. 다중 클래스 PyTorch 모델을 구축하여 전체 과정 합치기\n꽤 많은 내용을 다루었습니다.\n이제 다중 클래스 분류 문제를 사용하여 이 모든 것을 하나로 합쳐 보겠습니다.\n상기해 보면, 이진 분류 문제는 대상을 두 가지 옵션 중 하나(예: 사진이 고양이 사진인지 개 사진인지)로 분류하는 반면, 다중 클래스 분류 문제는 대상을 세 가지 이상의 옵션 목록 중 하나(예: 사진이 고양이, 개, 닭 중 무엇인지)로 분류합니다.\n 이진 분류 vs 다중 클래스 분류의 예시. 이진 분류는 두 개의 클래스(하나 또는 다른 것)를 다루는 반면, 다중 클래스 분류는 두 개 이상의 클래스를 다룰 수 있습니다. 예를 들어, 인기 있는 ImageNet-1k 데이터셋은 컴퓨터 비전 벤치마크로 사용되며 1000개의 클래스를 가집니다.\n\n8.1 다중 클래스 분류 데이터 생성\n다중 클래스 분류 문제를 시작하기 위해 다중 클래스 데이터를 만들어 보겠습니다.\n이를 위해 Scikit-Learn의 make_blobs() 메서드를 활용할 수 있습니다.\n이 메서드는 우리가 원하는 수만큼의 클래스(centers 매개변수 사용)를 생성합니다.\n구체적으로 다음 작업을 수행해 봅시다:\n\nmake_blobs()로 다중 클래스 데이터를 생성합니다.\n데이터를 텐서로 변환합니다 (make_blobs()의 기본값은 NumPy 배열을 사용하는 것입니다).\ntrain_test_split()을 사용하여 데이터를 훈련 및 테스트 세트로 분할합니다.\n데이터를 시각화합니다.\n\n\n# 의존성 임포트\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 생성을 위한 하이퍼파라미터 설정\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. 다중 클래스 데이터 생성\nX_blob, y_blob = make_blobs(n_samples=1000,\n    n_features=NUM_FEATURES, # X 특성\n    centers=NUM_CLASSES, # y 레이블\n    cluster_std=1.5, # 클러스터에 약간의 변화를 줌 (기본값인 1.0으로 변경해 보세요)\n    random_state=RANDOM_SEED\n)\n\n# 2. 데이터를 텐서로 변환\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\nprint(X_blob[:5], y_blob[:5])\n\n# 3. 훈련 및 테스트 세트로 분할\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n    y_blob,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\n\n# 4. 데이터 플롯\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);\n\ntensor([[-8.4134,  6.9352],\n        [-5.7665, -6.4312],\n        [-6.0421, -6.7661],\n        [ 3.9508,  0.6984],\n        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])\n\n\n\n\n\n\n\n\n\n좋네요! 준비된 다중 클래스 데이터가 생긴 것 같습니다.\n색상 블롭(blobs)을 분리할 모델을 구축해 봅시다.\n\n질문: 이 데이터셋에 비선형성이 필요한가요? 아니면 일련의 직선을 그어 분리할 수 있을까요?\n\n\n\n8.2 PyTorch에서 다중 클래스 분류 모델 구축하기\n지금까지 PyTorch에서 몇 가지 모델을 만들었습니다.\n여러분은 또한 신경망이 얼마나 유연한지 감을 잡기 시작했을 수도 있습니다.\nmodel_3과 유사하지만 다중 클래스 데이터를 처리할 수 있는 모델을 만들어 보면 어떨까요?\n그렇게 하기 위해 세 가지 하이퍼파라미터를 받는 nn.Module의 서브클래스를 만들어 봅시다: * input_features - 모델에 들어오는 X 특성의 수. * output_features - 우리가 원하는 이상적인 출력 특성의 수 (이는 NUM_CLASSES 또는 다중 클래스 분류 문제의 클래스 수와 동일합니다). * hidden_units - 각 은닉 레이어에서 사용하려는 은닉 뉴런의 수.\n전체 과정을 합치는 중이므로 장치에 구애받지 않는 코드를 설정하겠습니다 (같은 노트북에서 다시 할 필요는 없지만 리마인더 용도입니다).\n그런 다음 위의 하이퍼파라미터를 사용하여 모델 클래스를 생성하겠습니다.\n\n# 장치에 구애받지 않는 코드 생성\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nfrom torch import nn\n\n# 모델 구축\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n        \"\"\"다중 클래스 분류 모델에 필요한 모든 하이퍼파라미터를 초기화합니다.\n\n        Args:\n            input_features (int): 모델에 입력되는 특성의 수.\n            out_features (int): 모델의 출력 특성 수\n              (클래스가 몇 개인지).\n            hidden_units (int): 레이어 사이의 은닉 유닛 수, 기본값 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- 우리 데이터셋에 비선형 레이어가 필요한가요? (주석을 해제하고 결과가 바뀌는지 확인해 보세요)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- 우리 데이터셋에 비선형 레이어가 필요한가요? (주석을 해제하고 결과가 바뀌는지 확인해 보세요)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # 클래스가 몇 개인가요?\n        )\n    \n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# BlobModel의 인스턴스를 생성하고 대상 장치로 보냄\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n\nBlobModel(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=2, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=8, bias=True)\n    (2): Linear(in_features=8, out_features=4, bias=True)\n  )\n)\n\n\n멋지네요! 다중 클래스 모델이 준비되었으므로 이에 대한 손실 함수와 옵티마이저를 만들어 보겠습니다.\n\n\n8.3 다중 클래스 PyTorch 모델을 위한 손실 함수 및 옵티마이저 생성\n다중 클래스 분류 문제를 다루고 있으므로 nn.CrossEntropyLoss() 메서드를 손실 함수로 사용하겠습니다.\n그리고 model_4 파라미터를 최적화하기 위해 학습률 0.1의 SGD를 계속 사용하겠습니다.\n\n# 손실 및 옵티마이저 생성\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # 연습: 여기서 학습률을 변경하고 모델 성능에 어떤 일이 일어나는지 확인해 보세요\n\n\n\n8.4 다중 클래스 PyTorch 모델에 대한 예측 확률 얻기\n자, 손실 함수와 옵티마이저가 준비되었으며 모델을 훈련할 준비가 되었지만, 그 전에 모델이 작동하는지 확인하기 위해 한 번의 순전파를 수행해 보겠습니다.\n\n# 데이터에 대해 한 번의 순전파를 수행 (작동하려면 대상 장치로 이동시켜야 함)\nmodel_4(X_blob_train.to(device))[:5]\n\ntensor([[-1.2711, -0.6494, -1.4740, -0.7044],\n        [ 0.2210, -1.5439,  0.0420,  1.1531],\n        [ 2.8698,  0.9143,  3.3169,  1.4027],\n        [ 1.9576,  0.3125,  2.2244,  1.1324],\n        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\n여기서 무엇이 나오고 있나요?\n각 샘플의 특성당 하나의 값이 나오는 것 같습니다.\n모양을 확인하여 확인해 봅시다.\n\n# 단일 예측 샘플에 몇 개의 요소가 있나요?\nmodel_4(X_blob_train.to(device))[0].shape, NUM_CLASSES\n\n(torch.Size([4]), 4)\n\n\n멋지네요. 모델이 우리가 가진 각 클래스에 대해 하나의 값을 예측하고 있습니다.\n모델의 원시 출력을 무엇이라고 부르는지 기억하시나요?\n힌트: “로짓”입니다.\n맞습니다, 로짓입니다.\n현재 모델은 로짓을 출력하고 있지만, 샘플에 정확히 어떤 레이블을 부여하고 있는지 알아내고 싶다면 어떻게 해야 할까요?\n즉, 이진 분류 문제에서 했던 것처럼 로짓 -&gt; 예측 확률 -&gt; 예측 레이블로 어떻게 갈 수 있을까요?\n여기서 소프트맥스 활성화 함수(softmax activation function)가 등장합니다.\n소프트맥스 함수는 다른 모든 가능한 클래스와 비교하여 각 예측 클래스가 실제 예측 클래스일 확률을 계산합니다.\n이해가 안 된다면 코드로 확인해 봅시다.\n\n# 모델로 예측 로짓 생성\ny_logits = model_4(X_test.to(device))\n\n# 로짓에 대해 차원 1을 따라 소프트맥스 계산을 수행하여 예측 확률을 얻음\ny_pred_probs = torch.softmax(y_logits, dim=1) \nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n\ntensor([[ 0.2341, -0.3357,  0.2307,  0.2534],\n        [ 0.1198, -0.3702,  0.0998,  0.1887],\n        [ 0.3790, -0.2037,  0.4095,  0.2689],\n        [ 0.1936, -0.3733,  0.1807,  0.2496],\n        [ 0.1338, -0.1378,  0.1487,  0.0247]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\ntensor([[0.2792, 0.1579, 0.2782, 0.2846],\n        [0.2729, 0.1672, 0.2675, 0.2924],\n        [0.2869, 0.1602, 0.2958, 0.2570],\n        [0.2769, 0.1571, 0.2733, 0.2928],\n        [0.2722, 0.2075, 0.2763, 0.2441]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\n음, 여기서 무슨 일이 일어났나요?\n소프트맥스 함수의 출력이 여전히 뒤섞인 숫자처럼 보일 수 있지만 (실제로 그렇습니다. 우리 모델이 훈련되지 않았고 무작위 패턴을 사용하여 예측하고 있기 때문입니다), 각 샘플마다 매우 구체적인 차이점이 있습니다.\n로짓을 소프트맥스 함수에 통과시킨 후, 각 개별 샘플의 합은 이제 1(또는 매우 근접한 값)이 됩니다.\n확인해 봅시다.\n\n# 소프트맥스 활성화 함수의 첫 번째 샘플 출력 합계\ntorch.sum(y_pred_probs[0])\n\ntensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)\n\n\n이러한 예측 확률은 본질적으로 모델이 대상 X 샘플(입력)이 각 클래스에 매핑된다고 생각하는 정도를 말해줍니다.\ny_pred_probs에는 각 클래스당 하나의 값이 있으므로, 가장 높은 값의 인덱스가 모델이 특정 데이터 샘플이 가장 많이 속한다고 생각하는 클래스입니다.\ntorch.argmax()를 사용하여 어떤 인덱스가 가장 높은 값을 갖는지 확인할 수 있습니다.\n\n# 인덱스 0 샘플에서 모델이 가장 가능성이 높다고 생각하는 클래스는 무엇인가요?\nprint(y_pred_probs[0])\nprint(torch.argmax(y_pred_probs[0]))\n\ntensor([0.2792, 0.1579, 0.2782, 0.2846], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\ntensor(3, device='cuda:0')\n\n\ntorch.argmax()의 출력이 3을 반환하는 것을 볼 수 있으므로 인덱스 0 샘플의 특성(X)에 대해 모델은 가장 가능성이 높은 클래스 값(y)이 3이라고 예측하고 있습니다.\n물론 지금은 단지 무작위 추측일 뿐이므로 맞을 확률은 25%입니다(클래스가 4개이므로). 하지만 모델을 훈련하면 그 확률을 높일 수 있습니다.\n\n참고: 위 내용을 요약하면 모델의 원시 출력을 로짓(logits)이라고 합니다.\n다중 클래스 분류 문제에서 로짓을 예측 확률로 바꾸려면 소프트맥스 활성화 함수(torch.softmax)를 사용합니다.\n가장 높은 예측 확률을 가진 값의 인덱스가 모델이 해당 샘플의 입력 특성이 주어졌을 때 가장 가능성이 높다고 생각하는 클래스 번호입니다 (이것은 예측이지만 정답임을 의미하지는 않습니다).\n\n\n\n8.5 다중 클래스 PyTorch 모델을 위한 훈련 및 테스트 루프 생성\n자, 이제 모든 준비 단계가 끝났으므로 모델을 개선하고 평가하기 위한 훈련 및 테스트 루프를 작성해 보겠습니다.\n우리는 이전에 이러한 단계 중 많은 부분을 수행했으므로 대부분 연습이 될 것입니다.\n유일한 차이점은 모델 출력(로짓)을 예측 확률(소프트맥스 활성화 함수 사용)로 변환한 다음 예측 레이블(소프트맥스 활성화 함수의 출력에서 argmax를 취함)로 변환하도록 단계를 조정한다는 것입니다.\nepochs=100 동안 모델을 훈련하고 10 에포크마다 평가해 보겠습니다.\n\n# 모델 적합\ntorch.manual_seed(42)\n\n# 에포크 수 설정\nepochs = 100\n\n# 데이터를 대상 장치로 이동\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### 훈련\n    model_4.train()\n\n    # 1. 순전파\n    y_logits = model_4(X_blob_train) # 모델은 원시 로짓을 출력함 \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # 로짓 -&gt; 예측 확률 -&gt; 예측 레이블로 이동\n    # print(y_logits)\n    # 2. 손실 및 정확도 계산\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. 옵티마이저 제로 그래디언트\n    optimizer.zero_grad()\n\n    # 4. 손실 역전파\n    loss.backward()\n\n    # 5. 옵티마이저 단계 수행\n    optimizer.step()\n\n    ### 테스트\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. 순전파\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. 테스트 손실 및 정확도 계산\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # 10 에포크마다 진행 상황 출력\n    if epoch % 10 == 0:\n        print(f\"에포크: {epoch} | 손실: {loss:.5f}, 정확도: {acc:.2f}% | 테스트 손실: {test_loss:.5f}, 테스트 정확도: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%\nEpoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%\nEpoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%\nEpoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%\nEpoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%\nEpoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%\nEpoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%\nEpoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%\nEpoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%\nEpoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%\n\n\n\n\n8.6 다중 클래스 모델을 사용하여 예측 수행 및 평가\n훈련된 모델이 꽤 잘 작동하는 것 같습니다.\n하지만 이를 확실히 하기 위해 몇 가지 예측을 수행하고 시각화해 보겠습니다.\n\n# 예측 수행\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\n# 처음 10개 예측값 확인\ny_logits[:10]\n\ntensor([[  4.3377,  10.3539, -14.8948,  -9.7642],\n        [  5.0142, -12.0371,   3.3860,  10.6699],\n        [ -5.5885, -13.3448,  20.9894,  12.7711],\n        [  1.8400,   7.5599,  -8.6016,  -6.9942],\n        [  8.0726,   3.2906, -14.5998,  -3.6186],\n        [  5.5844, -14.9521,   5.0168,  13.2890],\n        [ -5.9739, -10.1913,  18.8655,   9.9179],\n        [  7.0755,  -0.7601,  -9.5531,   0.1736],\n        [ -5.5918, -18.5990,  25.5309,  17.5799],\n        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')\n\n\n좋습니다. 모델의 예측값이 여전히 로짓 형태인 것 같습니다.\n하지만 이를 평가하려면 정수 형태인 우리 레이블(y_blob_test)과 동일한 형태여야 합니다.\n모델의 예측 로짓을 예측 확률(torch.softmax() 사용)로 변환한 다음 예측 레이블(각 샘플의 argmax()를 취함)로 변환해 봅시다.\n\n참고: torch.softmax() 함수를 건너뛰고 로짓에서 직접 torch.argmax()를 호출하여 예측 로짓 -&gt; 예측 레이블로 바로 갈 수 있습니다.\n예를 들어, y_preds = torch.argmax(y_logits, dim=1)은 계산 단계 하나를 절약하지만 (torch.softmax() 없음), 사용할 수 있는 예측 확률은 생성되지 않습니다.\n\n\n# 예측 로짓을 예측 확률로 변환\ny_pred_probs = torch.softmax(y_logits, dim=1)\n\n# 예측 확률을 예측 레이블로 변환\ny_preds = y_pred_probs.argmax(dim=1)\n\n# 처음 10개 모델 예측값과 테스트 레이블 비교\nprint(f\"예측값: {y_preds[:10]}\\n레이블: {y_blob_test[:10]}\")\nprint(f\"테스트 정확도: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n\nPredictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nLabels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nTest accuracy: 99.5%\n\n\n멋지네요! 이제 모델 예측값이 테스트 레이블과 동일한 형태입니다.\nplot_decision_boundary()로 시각화해 봅시다. 우리 데이터가 GPU에 있으므로 matplotlib에서 사용하려면 CPU로 옮겨야 한다는 점을 기억하세요 (plot_decision_boundary()가 자동으로 이 작업을 수행해 줍니다).\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"훈련\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\nplt.subplot(1, 2, 2)\nplt.title(\"테스트\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#더-많은-분류-평가-지표",
    "href": "02_pytorch_classification.html#더-많은-분류-평가-지표",
    "title": "02 - PyTorch 신경망 분류",
    "section": "9. 더 많은 분류 평가 지표",
    "text": "9. 더 많은 분류 평가 지표\n지금까지 분류 모델을 평가하는 몇 가지 방법(정확도, 손실 및 예측 시각화)만 다루었습니다.\n이것들은 접하게 될 가장 일반적인 방법 중 일부이며 좋은 시작점입니다.\n그러나 다음과 같은 더 많은 지표를 사용하여 분류 모델을 평가하고 싶을 수 있습니다:\n\n\n\n지표 이름/평가 방법\n정의\n코드\n\n\n\n\n정확도 (Accuracy)\n100번의 예측 중 모델이 몇 개를 맞혔나요? 예: 95% 정확도는 100번 중 95번을 맞혔음을 의미합니다.\ntorchmetrics.Accuracy() 또는 sklearn.metrics.accuracy_score()\n\n\n정밀도 (Precision)\n전체 샘플 중 참 양성(true positive)의 비율. 높은 정밀도는 거짓 양성(실제로는 0인데 모델이 1로 예측)이 적음을 의미합니다.\ntorchmetrics.Precision() 또는 sklearn.metrics.precision_score()\n\n\n재현율 (Recall)\n참 양성과 거짓 음성(실제로는 1인데 모델이 0으로 예측)의 합 중 참 양성의 비율. 높은 재현율은 거짓 음성이 적음을 의미합니다.\ntorchmetrics.Recall() 또는 sklearn.metrics.recall_score()\n\n\nF1-스코어 (F1-score)\n정밀도와 재현율을 하나의 지표로 결합합니다. 1이 최고, 0이 최악입니다.\ntorchmetrics.F1Score() 또는 sklearn.metrics.f1_score()\n\n\n혼동 행렬 (Confusion matrix)\n예측값과 실제값을 표 형식으로 비교합니다. 100% 정확하면 행렬의 모든 값이 왼쪽 위에서 오른쪽 아래 대각선에 위치합니다.\ntorchmetrics.ConfusionMatrix 또는 sklearn.metrics.plot_confusion_matrix()\n\n\n분류 보고서 (Classification report)\n정밀도, 재현율, f1-스코어와 같은 주요 분류 지표들의 모음입니다.\nsklearn.metrics.classification_report()\n\n\n\nScikit-Learn(인기 있고 세계적인 수준의 머신러닝 라이브러리)에는 위 지표들의 많은 구현이 있으며, PyTorch와 유사한 버전을 찾고 있다면 TorchMetrics, 특히 TorchMetrics 분류 섹션을 확인해 보세요.\ntorchmetrics.Accuracy 지표를 사용해 봅시다.\n\n!pip -q install torchmetrics\n\nfrom torchmetrics import Accuracy\n\n# 지표 설정 및 대상 장치에 있는지 확인\ntorchmetrics_accuracy = Accuracy().to(device)\n\n# 정확도 계산\ntorchmetrics_accuracy(y_preds, y_blob_test)\n\n     |████████████████████████████████| 409 kB 5.2 MB/s eta 0:00:01\n\n\ntensor(0.9950, device='cuda:0')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#연습-문제",
    "href": "02_pytorch_classification.html#연습-문제",
    "title": "02 - PyTorch 신경망 분류",
    "section": "연습 문제",
    "text": "연습 문제\n모든 연습 문제는 위의 섹션들에 있는 코드를 연습하는 데 중점을 둡니다.\n각 섹션을 참조하거나 링크된 리소스를 따라 완료할 수 있어야 합니다.\n모든 연습 문제는 장치에 구애받지 않는 코드(device-agonistic code)를 사용하여 완료해야 합니다.\n리소스: * 02 연습 문제 노트북 템플릿 * 02 예시 솔루션 노트북 (솔루션을 보기 전에 먼저 연습 문제를 시도해 보세요)\n\nScikit-Learn의 make_moons() 함수를 사용하여 이진 분류 데이터셋을 만듭니다.\n\n\n일관성을 위해 데이터셋은 1000개의 샘플을 가져야 하며 random_state=42여야 합니다.\n데이터를 PyTorch 텐서로 변환합니다. train_test_split을 사용하여 데이터를 훈련 80%, 테스트 20%로 분할합니다.\n\n\n비선형 활성화 함수를 포함하고 1번에서 만든 데이터에 적합할 수 있는 nn.Module 상속 모델을 구축합니다.\n\n\n원하는 PyTorch 레이어(선형 및 비선형)의 조합을 자유롭게 사용하세요.\n\n\n모델 훈련 시 사용할 이진 분류 호환 손실 함수와 옵티마이저를 설정합니다.\n2번에서 만든 모델을 1번에서 만든 데이터에 적합시키기 위한 훈련 및 테스트 루프를 생성합니다.\n\n\n모델 정확도를 측정하기 위해 자신만의 정확도 함수를 만들거나 TorchMetrics의 정확도 함수를 사용할 수 있습니다.\n정확도가 96% 이상이 될 때까지 충분히 오래 모델을 훈련합니다.\n훈련 루프는 매 10 에포크마다 모델의 훈련 및 테스트 세트 손실과 정확도 진행 상황을 출력해야 합니다.\n\n\n훈련된 모델로 예측을 수행하고 이 노트북에서 만든 plot_decision_boundary() 함수를 사용하여 플롯합니다.\n순수 PyTorch로 Tanh(쌍곡 탄젠트) 활성화 함수를 복제합니다.\n\n\n공식은 ML cheatsheet 웹사이트를 참조하세요.\n\n\nCS231n의 나선형(spirals) 데이터 생성 함수를 사용하여 다중 클래스 데이터셋을 생성합니다 (아래 코드 참조).\n\n\n데이터에 적합할 수 있는 모델을 구축합니다 (선형 및 비선형 레이어의 조합이 필요할 수 있습니다).\n다중 클래스 데이터를 처리할 수 있는 손실 함수와 옵티마이저를 구축합니다 (선택적 확장: SGD 대신 Adam 옵티마이저를 사용해 보세요. 작동시키려면 학습률의 다양한 값을 실험해야 할 수도 있습니다).\n다중 클래스 데이터에 대한 훈련 및 테스트 루프를 만들고 모델을 훈련하여 테스트 정확도 95% 이상을 달성합니다 (여기서는 원하는 어떤 정확도 측정 함수든 사용할 수 있습니다).\n모델 예측에서 나선형 데이터셋의 결정 경계를 플롯합니다. plot_decision_boundary() 함수가 이 데이터셋에서도 작동해야 합니다.\n\n# CS231n의 나선형 데이터셋 생성을 위한 코드\nimport numpy as np\nN = 100 # 클래스당 점의 수\nD = 2 # 차원\nK = 3 # 클래스 수\nX = np.zeros((N*K,D)) # 데이터 행렬 (각 행 = 단일 샘플)\ny = np.zeros(N*K, dtype='uint8') # 클래스 레이블\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # 반지름\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # 세타\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# 데이터 시각화\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#추가-학습-자료",
    "href": "02_pytorch_classification.html#추가-학습-자료",
    "title": "02 - PyTorch 신경망 분류",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\n머신 분류가 유용할 것 같은 3가지 문제를 적어보세요 (무엇이든 가능하며 창의력을 발휘해 보세요. 예를 들어 구매 금액과 구매 위치 특성을 기반으로 신용카드 거래를 사기 또는 사기가 아닌 것으로 분류하기).\n경사 기반 옵티마이저(SGD 또는 Adam)에서 “모멘텀(momentum)”의 개념을 조사해 보세요. 무엇을 의미하나요?\n다양한 활성화 함수에 대한 Wikipedia 페이지를 10분 동안 읽어보고, 이 중 몇 개가 PyTorch의 활성화 함수와 일치하는지 확인해 보세요.\n정확도가 부적절한 지표가 될 수 있는 경우를 조사해 보세요 (힌트: 아이디어를 위해 Will Koehrsen의 [“Beyond Accuracy”] (https://willkoehrsen.github.io/statistics/learning/beyond-accuracy-precision-and-recall/)를 읽어보세요).\n시청: 우리 신경망 내부에서 어떤 일이 일어나고 있고 학습을 위해 무엇을 하고 있는지에 대한 아이디어를 얻으려면 MIT의 Introduction to Deep Learning 비디오를 시청하세요.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>02 - PyTorch 신경망 분류</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html",
    "href": "03_pytorch_computer_vision.html",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "",
    "text": "컴퓨터 비전은 어디에 사용되나요?\n컴퓨터 비전은 컴퓨터가 볼 수 있도록 가르치는 예술입니다.\n예를 들어, 사진이 고양이인지 강아지인지 분류하는 모델을 구축하는 것(이진 분류)이 포함될 수 있습니다.\n또는 사진이 고양이, 강아지, 닭 중 무엇인지 분류하는 것(다중 클래스 분류)도 포함됩니다.\n비디오 프레임에서 자동차가 어디에 나타나는지 식별하는 것(객체 탐지)이나,\n이미지에서 서로 다른 물체가 어디에서 분리되는지 알아내는 것(전경 분할(Panoptic Segmentation))도 컴퓨터 비전의 영역입니다.\n이진 분류, 다중 클래스 분류, 객체 탐지 및 분할에 대한 컴퓨터 비전 문제 예시입니다.\n스마트폰을 사용한다면 이미 컴퓨터 비전을 사용하고 있는 것입니다.\n카메라 및 사진 앱은 컴퓨터 비전을 사용하여 이미지를 개선하고 정렬합니다.\n현대 자동차는 다른 자동차를 피하고 차선을 유지하기 위해 컴퓨터 비전을 사용합니다.\n제조업체는 컴퓨터 비전을 사용하여 다양한 제품의 결함을 식별합니다.\n보안 카메라는 컴퓨터 비전을 사용하여 잠재적인 침입자를 감지합니다.\n본질적으로 시각적으로 설명할 수 있는 모든 것은 잠재적인 컴퓨터 비전 문제가 될 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#이번-장에서-다룰-내용",
    "href": "03_pytorch_computer_vision.html#이번-장에서-다룰-내용",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "이번 장에서 다룰 내용",
    "text": "이번 장에서 다룰 내용\n지난 몇 개 섹션에서 배웠던 PyTorch 워크플로우를 컴퓨터 비전에 적용해 보겠습니다.\n\n\n\n컴퓨터 비전에 중점을 둔 PyTorch 워크플로우\n\n\n구체적으로 다음 내용을 다룹니다:\n\n\n\n주제\n내용\n\n\n\n\n0. PyTorch의 컴퓨터 비전 라이브러리\nPyTorch에는 내장된 유용한 컴퓨터 비전 라이브러리가 많이 있습니다. 하나씩 살펴보겠습니다.\n\n\n1. 데이터 로드\n컴퓨터 비전 연습을 위해 FashionMNIST의 다양한 의류 이미지로 시작해 보겠습니다.\n\n\n2. 데이터 준비\n이미지를 가져와서 PyTorch DataLoader로 로드하여 훈련 루프에서 사용할 수 있도록 준비합니다.\n\n\n3. 모델 0: 베이스라인 모델 구축\n데이터에서 패턴을 학습하기 위한 다중 클래스 분류 모델을 만들고, 손실 함수, 옵티마이저를 선택하고 훈련 루프를 구축합니다.\n\n\n4. 모델 0의 예측 및 평가\n베이스라인 모델로 예측을 수행하고 평가해 봅니다.\n\n\n5. 향후 모델을 위한 장치 중립적 코드 설정\n장치 중립적(device-agnostic) 코드를 작성하는 것이 가장 좋으므로, 이를 설정해 봅니다.\n\n\n6. 모델 1: 비선형성 추가\n실험은 머신러닝의 큰 부분입니다. 비선형 레이어를 추가하여 베이스라인 모델을 개선해 봅니다.\n\n\n7. 모델 2: 합성곱 신경망 (CNN)\n컴퓨터 비전에 특화된 강력한 합성곱 신경망 아키텍처를 소개합니다.\n\n\n8. 모델 비교\n세 가지 서로 다른 모델을 구축했으므로, 이들을 비교해 봅니다.\n\n\n9. 최적의 모델 평가\n무작위 이미지에 대해 예측을 수행하고 최적의 모델을 평가해 봅니다.\n\n\n10. 혼동 행렬 만들기\n혼동 행렬은 분류 모델을 평가하는 좋은 방법입니다. 어떻게 만드는지 살펴보겠습니다.\n\n\n11. 가장 성능이 좋은 모델 저장 및 불러오기\n나중에 모델을 사용하고 싶을 수 있으므로 저장하고 올바르게 불러와지는지 확인합니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#도움을-받을-수-있는-곳",
    "href": "03_pytorch_computer_vision.html#도움을-받을-수-있는-곳",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "도움을 받을 수 있는 곳",
    "text": "도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 문서와 PyTorch 개발자 포럼도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#pytorch의-컴퓨터-비전-라이브러리",
    "href": "03_pytorch_computer_vision.html#pytorch의-컴퓨터-비전-라이브러리",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "0. PyTorch의 컴퓨터 비전 라이브러리",
    "text": "0. PyTorch의 컴퓨터 비전 라이브러리\n코드를 작성하기 전에 알아야 할 몇 가지 PyTorch 컴퓨터 비전 라이브러리에 대해 이야기해 보겠습니다.\n\n\n\nPyTorch 모듈\n역할\n\n\n\n\ntorchvision\n컴퓨터 비전 문제에 자주 사용되는 데이터셋, 모델 아키텍처 및 이미지 변환이 포함되어 있습니다.\n\n\ntorchvision.datasets\n이미지 분류, 객체 탐지, 이미지 캡셔닝, 비디오 분류 등 다양한 문제에 대한 많은 예시 컴퓨터 비전 데이터셋이 있습니다. 또한 커스텀 데이터셋을 만들기 위한 일련의 기본 클래스도 포함되어 있습니다.\n\n\ntorchvision.models\nPyTorch로 구현된 성능이 뛰어나고 일반적으로 사용되는 컴퓨터 비전 모델 아키텍처가 포함되어 있어 자신의 문제에 사용할 수 있습니다.\n\n\ntorchvision.transforms\n모델에 사용하기 전에 이미지를 변환(숫자로 변환/처리/증강)해야 하는 경우가 많으며, 일반적인 이미지 변환 기능이 여기에 있습니다.\n\n\ntorch.utils.data.Dataset\nPyTorch를 위한 기본 데이터셋 클래스입니다.\n\n\ntorch.utils.data.DataLoader\n데이터셋(torch.utils.data.Dataset으로 생성)에 대해 파이썬 이터러블(iterable)을 생성합니다.\n\n\n\n\n참고: torch.utils.data.Dataset 및 torch.utils.data.DataLoader 클래스는 PyTorch에서 컴퓨터 비전 전용이 아니며, 다양한 유형의 데이터를 처리할 수 있습니다.\n\n이제 가장 중요한 PyTorch 컴퓨터 비전 라이브러리 몇 가지를 살펴보았으니 관련 종속성을 임포트해 보겠습니다.\n\n# PyTorch 임포트\nimport torch\nfrom torch import nn\n\n# torchvision 임포트\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# 시각화를 위한 matplotlib 임포트\nimport matplotlib.pyplot as plt\n\n# 버전 확인\n# 참고: PyTorch 버전은 1.10.0 이상, torchvision 버전은 0.11 이상이어야 합니다.\nprint(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")\n\nPyTorch version: 1.11.0\ntorchvision version: 0.12.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#데이터셋-가져오기",
    "href": "03_pytorch_computer_vision.html#데이터셋-가져오기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "1. 데이터셋 가져오기",
    "text": "1. 데이터셋 가져오기\n컴퓨터 비전 문제를 시작하기 위해 컴퓨터 비전 데이터셋을 가져와 보겠습니다.\n먼저 FashionMNIST로 시작합니다.\nMNIST는 Modified National Institute of Standards and Technology의 약자입니다.\n오리지널 MNIST 데이터셋에는 수천 개의 손글씨 숫자(0~9) 예시가 포함되어 있으며 우편 서비스를 위한 숫자 식별 컴퓨터 비전 모델을 구축하는 데 사용되었습니다.\nZalando Research에서 만든 FashionMNIST는 비슷한 설정입니다.\n다만 10가지 서로 다른 종류의 의류를 나타내는 회색조 이미지가 포함되어 있습니다.\n torchvision.datasets에는 컴퓨터 비전 코드 작성을 연습할 수 있는 많은 예시 데이터셋이 포함되어 있습니다. FashionMNIST는 그중 하나입니다. 10개의 서로 다른 이미지 클래스(서로 다른 유형의 의류)가 있으므로 다중 클래스 분류 문제입니다.\n나중에 이 이미지들에서 서로 다른 스타일의 의류를 식별하는 컴퓨터 비전 신경망을 구축할 것입니다.\nPyTorch에는 torchvision.datasets에 저장된 많은 공통 컴퓨터 비전 데이터셋이 있습니다.\ntorchvision.datasets.FashionMNIST()에 있는 FashionMNIST를 포함해서요.\n이를 다운로드하기 위해 다음 매개변수를 제공합니다: * root: str - 데이터를 어느 폴더에 다운로드할 것인가요? * train: Bool - 훈련 세트 또는 테스트 세트 중 무엇을 원하나요? * download: Bool - 데이터를 다운로드해야 하나요? * transform: torchvision.transforms - 데이터에 어떤 변환을 적용하고 싶나요? * target_transform - 원하는 경우 타겟(레이블)도 변환할 수 있습니다.\ntorchvision의 다른 많은 데이터셋에도 이러한 매개변수 옵션이 있습니다.\n\n# 훈련 데이터 설정\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # 데이터를 어디에 다운로드할까요?\n    train=True, # 훈련 데이터를 가져옵니다.\n    download=True, # 디스크에 데이터가 없으면 다운로드합니다.\n    transform=ToTensor(), # 이미지는 PIL 형식으로 제공되므로 Torch 텐서로 변환하려고 합니다.\n    target_transform=None # 레이블도 변환할 수 있습니다.\n)\n\n# 테스트 데이터 설정\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # 테스트 데이터를 가져옵니다.\n    download=True,\n    transform=ToTensor()\n)\n\n훈련 데이터의 첫 번째 샘플을 확인해 보겠습니다.\n\n# 첫 번째 훈련 샘플 확인\nimage, label = train_data[0]\nimage, label\n\n(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n           0.0157, 0.0000, 0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0471, 0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n           0.3020, 0.5098, 0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n           0.5529, 0.3451, 0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n           0.4824, 0.7686, 0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n           0.8745, 0.9608, 0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n           0.8627, 0.9529, 0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n           0.8863, 0.7725, 0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n           0.9608, 0.4667, 0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n           0.8510, 0.8196, 0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n           0.8549, 1.0000, 0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n           0.8784, 0.9569, 0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n           0.9137, 0.9333, 0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n           0.8627, 0.9098, 0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n           0.8706, 0.8941, 0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n           0.8745, 0.8784, 0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n           0.8627, 0.8667, 0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n           0.7098, 0.8039, 0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n           0.6549, 0.6941, 0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n           0.7529, 0.8471, 0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n           0.3882, 0.2275, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 9)\n\n\n\n1.1 컴퓨터 비전 모델의 입력 및 출력 모양\n이미지를 나타내는 큰 값의 텐서가 있고, 타겟(레이블)을 나타내는 단일 값이 있습니다.\n이미지 모양을 확인해 보겠습니다.\n\n# 이미지의 모양은 무엇인가요?\nimage.shape\n\ntorch.Size([1, 28, 28])\n\n\n이미지 텐서의 모양은 [1, 28, 28]이며 더 구체적으로는 다음과 같습니다:\n[색상_채널=1, 높이=28, 너비=28]\n색상_채널=1은 이미지가 회색조임을 의미합니다.\n 다양한 문제에는 다양한 입력 및 출력 모양이 있습니다. 하지만 전제는 동일합니다. 데이터를 숫자로 인코딩하고, 해당 숫자에서 패턴을 찾기 위한 모델을 구축하고, 그 패턴을 의미 있는 것으로 변환하는 것입니다.\n색상_채널=3인 경우 이미지는 빨간색, 녹색, 파란색의 픽셀 값으로 제공됩니다(이를 RGB 색상 모델이라고도 함).\n현재 텐서의 순서는 종종 CHW(색상 채널, 높이, 너비)라고 불립니다.\n이미지를 CHW(색상 채널 우선)로 표시해야 하는지 아니면 HWC(색상 채널 마지막)로 표시해야 하는지에 대한 논의가 있습니다.\n\n참고: N이 이미지 수를 나타내는 NCHW 및 NHWC 형식도 볼 수 있습니다. 예를 들어 batch_size=32인 경우 텐서 모양은 [32, 1, 28, 28]이 될 수 있습니다. 배치 크기는 나중에 다루겠습니다.\n\nPyTorch는 일반적으로 많은 연산자에서 NCHW(채널 우선)를 기본값으로 허용합니다.\n그러나 PyTorch는 NHWC(채널 마지막)가 성능이 더 좋으며 권장 사항(best practice)으로 간주된다고 설명합니다.\n지금은 데이터셋과 모델이 상대적으로 작기 때문에 큰 차이가 없을 것입니다.\n하지만 나중에 더 큰 이미지 데이터셋을 다루고 합성곱 신경망을 사용할 때 이를 염두에 두세요.\n데이터의 모양을 더 확인해 보겠습니다.\n\n# 샘플은 몇 개인가요?\nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n\n(60000, 60000, 10000, 10000)\n\n\n따라서 60,000개의 훈련 샘플과 10,000개의 테스트 샘플이 있습니다.\n어떤 클래스가 있나요?\n.classes 속성을 통해 이를 찾을 수 있습니다.\n\n# 클래스 확인\nclass_names = train_data.classes\nclass_names\n\n['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\n좋습니다! 10가지 서로 다른 종류의 옷을 다루고 있는 것 같네요.\n10개의 서로 다른 클래스를 다루고 있기 때문에 우리의 문제는 다중 클래스 분류입니다.\n이제 시각화를 해보겠습니다.\n\n\n1.2 데이터 시각화하기\n\nimport matplotlib.pyplot as plt\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\nplt.title(label);\n\nImage shape: torch.Size([1, 28, 28])\n\n\n\n\n\n\n\n\n\nWe can turn the image into grayscale using the cmap parameter of plt.imshow().\n\nplt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(class_names[label]);\n\n\n\n\n\n\n\n\nBeautiful, well as beautiful as a pixelated grayscale ankle boot can get.\nLet’s view a few more.\n\n# Plot more images\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows * cols + 1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n\n\n\n\n\n\n\n\nHmmm, this dataset doesn’t look too aesthetic.\nBut the principles we’re going to learn on how to build a model for it will be similar across a wide range of computer vision problems.\nIn essence, taking pixel values and building a model to find patterns in them to use on future pixel values.\nPlus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?\nYou probably could.\nBut I think coding a model in PyTorch would be faster.\n\nQuestion: Do you think the above data can be model with only straight (linear) lines? Or do you think you’d also need non-straight (non-linear) lines?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#dataloader-준비하기",
    "href": "03_pytorch_computer_vision.html#dataloader-준비하기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "2. DataLoader 준비하기",
    "text": "2. DataLoader 준비하기\n이제 데이터셋이 준비되었습니다.\n다음 단계는 torch.utils.data.DataLoader 또는 줄여서 DataLoader를 사용하여 준비하는 것입니다.\nDataLoader는 이름에서 짐작할 수 있는 역할을 합니다.\n모델에 데이터를 로드하는 것을 돕습니다.\n훈련과 추론을 위해서요.\n큰 Dataset을 작은 덩어리의 파이썬 이터러블로 변환합니다.\n이러한 작은 덩어리를 배치(batch) 또는 미니 배치(mini-batch)라고 하며 batch_size 매개변수로 설정할 수 있습니다.\n왜 이렇게 할까요?\n컴퓨팅 효율성이 더 높기 때문입니다.\n이상적인 세상에서는 모든 데이터를 한 번에 순전파 및 역전파할 수 있을 것입니다.\n하지만 정말 큰 데이터셋을 사용하기 시작하면 무한한 컴퓨팅 파워가 없는 한 데이터를 배치로 나누는 것이 더 쉽습니다.\n또한 모델이 개선될 기회를 더 많이 제공합니다.\n미니 배치(데이터의 작은 부분)를 사용하면 에포크(epoch)당 한 번이 아니라 미니 배치당 한 번씩 경사 하강법이 더 자주 수행됩니다.\n좋은 배치 크기는 얼마일까요?\n32는 시작하기 좋은 지점입니다.\n하지만 이는 사용자가 설정할 수 있는 값(하이퍼파라미터)이므로 모든 종류의 값을 시도해 볼 수 있습니다. 다만 일반적으로 2의 거듭제곱(예: 32, 64, 128, 256, 512)이 가장 자주 사용됩니다.\n FashionMNIST를 배치 크기 32로 배치하고 셔플을 켠 모습입니다. 다른 데이터셋에 대해서도 유사한 배치 프로세스가 발생하지만 배치 크기에 따라 달라집니다.\n훈련 세트와 테스트 세트를 위한 DataLoader를 만들어 보겠습니다.\n\nfrom torch.utils.data import DataLoader\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 32\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True # shuffle data every epoch?\n)\n\ntest_dataloader = DataLoader(test_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False # don't necessarily have to shuffle the testing data\n)\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n\nDataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193a8a90&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193b0700&gt;)\nLength of train dataloader: 1875 batches of 32\nLength of test dataloader: 313 batches of 32\n\n\n\n# Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n\n(torch.Size([32, 1, 28, 28]), torch.Size([32]))\n\n\nAnd we can see that the data remains unchanged by checking a single sample.\n\n# Show a sample\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n\nImage size: torch.Size([1, 28, 28])\nLabel: 6, label size: torch.Size([])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#모델-0-베이스라인-모델-구축하기",
    "href": "03_pytorch_computer_vision.html#모델-0-베이스라인-모델-구축하기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "3. 모델 0: 베이스라인 모델 구축하기",
    "text": "3. 모델 0: 베이스라인 모델 구축하기\n데이터 로드 및 준비 완료!\n이제 nn.Module을 상속받아 베이스라인 모델을 구축할 시간입니다.\n베이스라인 모델은 상상할 수 있는 가장 간단한 모델 중 하나입니다.\n베이스라인을 시작점으로 사용하고 이후의 더 복잡한 모델로 이를 개선하려고 노력합니다.\n우리의 베이스라인은 두 개의 nn.Linear() 레이어로 구성됩니다.\n이전 섹션에서 이 작업을 수행했지만 한 가지 약간의 차이점이 있습니다.\n이미지 데이터를 다루고 있기 때문에 시작을 위해 다른 레이어를 사용할 것입니다.\n바로 nn.Flatten() 레이어입니다.\nnn.Flatten()은 텐서의 차원을 단일 벡터로 압축합니다.\n이것은 직접 보면 이해하기 더 쉽습니다.\n\n# Flatten 레이어 생성\nflatten_model = nn.Flatten() # 모든 nn 모듈은 모델로 작동합니다(순전파를 수행할 수 있음)\n\n# 단일 샘플 가져오기\nx = train_features_batch[0]\n\n# 샘플을 평탄화(flatten)\noutput = flatten_model(x) # 순전파 수행\n\n# 결과 출력\nprint(f\"평탄화 전 모양: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"평탄화 후 모양: {output.shape} -&gt; [color_channels, height*width]\")\n\n# 아래 주석을 해제하고 어떤 일이 일어나는지 확인해 보세요\n#print(x)\n#print(output)\n\nShape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]\nShape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]\n\n\nnn.Flatten() 레이어는 모양을 [color_channels, height, width]에서 [color_channels, height*width]로 바꿨습니다.\n왜 이렇게 할까요?\n높이와 너비 차원의 픽셀 데이터를 하나의 긴 특성 벡터(feature vector)로 변환했기 때문입니다.\n그리고 nn.Linear() 레이어는 입력이 특성 벡터 형태인 것을 선호합니다.\n첫 번째 레이어로 nn.Flatten()을 사용하여 첫 번째 모델을 만들어 보겠습니다.\n\nfrom torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # 신경망은 입력이 벡터 형태인 것을 선호합니다.\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = 데이터 샘플의 특성 수(784픽셀)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n\n멋지네요!\n이제 사용할 수 있는 베이스라인 모델 클래스가 생겼으니 모델을 인스턴스화해 보겠습니다.\n다음 매개변수를 설정해야 합니다: * input_shape=784 - 모델에 들어가는 특성 수입니다. 우리의 경우 대상 이미지의 각 픽셀에 대해 하나씩입니다(높이 28픽셀 x 너비 28픽셀 = 784개의 특성). * hidden_units=10 - 은닉 레이어의 유닛/뉴런 수입니다. 이 숫자는 원하는 대로 설정할 수 있지만 모델을 작게 유지하기 위해 10으로 시작합니다. * output_shape=len(class_names) - 다중 클래스 분류 문제를 다루고 있으므로 데이터셋의 각 클래스당 하나의 출력 뉴런이 필요합니다.\n이제 모델 인스턴스를 만들고 지금은 CPU로 보냅니다(곧 CPU에서 model_0을 실행하는 것과 GPU에서 유사한 모델을 실행하는 것에 대한 작은 테스트를 수행할 것입니다).\n\ntorch.manual_seed(42)\n\n# 입력 매개변수로 모델 설정\nmodel_0 = FashionMNISTModelV0(input_shape=784, # 모든 픽셀에 대해 하나씩 (28x28)\n    hidden_units=10, # 은닉 레이어의 유닛 수\n    output_shape=len(class_names) # 각 클래스에 대해 하나씩\n)\nmodel_0.to(\"cpu\") # 우선 모델을 CPU에 둡니다. \n\nFashionMNISTModelV0(\n  (layer_stack): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n\n\n\n3.1 손실 함수, 옵티마이저 및 평가 지표 설정\n분류 문제를 다루고 있으므로 helper_functions.py 스크립트를 가져오고, 이어서 노트북 02에서 정의한 accuracy_fn()을 가져오겠습니다.\n\n참고: 자체 정확도 함수나 평가 지표를 임포트하여 사용하는 대신 TorchMetrics 패키지에서 다양한 평가 지표를 임포트할 수도 있습니다.\n\n\nimport requests\nfrom pathlib import Path \n\n# Learn PyTorch 저장소에서 helper functions 다운로드 (이미 다운로드되지 않은 경우)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # 참고: 이것이 작동하려면 \"raw\" GitHub URL이 필요합니다.\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nhelper_functions.py already exists, skipping download\n\n\n\n# 정확도 지표 임포트\nfrom helper_functions import accuracy_fn # 참고: torchmetrics.Accuracy()를 사용할 수도 있습니다.\n\n# 손실 함수 및 옵티마이저 설정\nloss_fn = nn.CrossEntropyLoss() # 이것은 일부 장소에서 \"criterion\" 또는 \"cost function\"으로도 불립니다.\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n\n\n\n3.2 실험 시간을 측정하는 함수 만들기\n손실 함수와 옵티마이저가 준비되었습니다!\n이제 모델 훈련을 시작할 시간입니다.\n하지만 훈련하는 동안 작은 실험을 하나 해보면 어떨까요?\n모델이 CPU에서 훈련되는 시간과 GPU를 사용하여 훈련되는 시간을 측정하는 타이밍 함수를 만들어 보겠습니다.\n이 모델은 CPU에서 훈련하지만 다음 모델은 GPU에서 훈련하고 어떤 일이 일어나는지 살펴보겠습니다.\n우리의 타이밍 함수는 파이썬 timeit 모듈에서 timeit.default_timer() 함수를 임포트할 것입니다.\n\nfrom timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n    \"\"\"시작 시간과 종료 시간 사이의 차이를 출력합니다.\n\n    Args:\n        start (float): 계산 시작 시간 (timeit 형식 선호). \n        end (float): 계산 종료 시간.\n        device ([type], optional): 계산이 실행되는 장치. 기본값은 None.\n\n    Returns:\n        float: 시작 시간과 종료 시간 사이의 초 단위 시간 (높을수록 더 긺).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n\n\n\n3.3 훈련 루프 생성 및 데이터 배치로 모델 훈련하기\n아름답네요!\n타이머, 손실 함수, 옵티마이저, 모델, 그리고 가장 중요한 데이터까지 모든 퍼즐 조각이 준비된 것 같습니다.\n이제 모델을 훈련하고 평가하기 위해 훈련 루프와 테스트 루프를 만들어 보겠습니다.\n이전 노트북과 동일한 단계를 사용하겠지만, 데이터가 이제 배치 형태이므로 데이터 배치를 반복하기 위한 또 다른 루프를 추가할 것입니다.\n데이터 배치는 각각 훈련 및 테스트 데이터 분할을 위한 DataLoader인 train_dataloader와 test_dataloader에 포함되어 있습니다.\n하나의 배치는 BATCH_SIZE개의 X(특성)와 y(레이블) 샘플이며, BATCH_SIZE=32를 사용하고 있으므로 배치는 32개의 이미지와 타겟 샘플을 가집니다.\n그리고 데이터 배치에 대해 계산을 수행하므로 손실 및 평가 지표는 전체 데이터셋이 아니라 배치당 계산됩니다.\n즉, 손실 및 정확도 값을 각 데이터셋의 해당 데이터로더에 있는 배치 수로 나누어야 함을 의미합니다.\n단계별로 살펴보겠습니다: 1. 에포크를 반복합니다. 2. 훈련 배치를 반복하고, 훈련 단계를 수행하며, 배치당 훈련 손실을 계산합니다. 3. 테스트 배치를 반복하고, 테스트 단계를 수행하며, 배치당 테스트 손실을 계산합니다. 4. 진행 상황을 출력합니다. 5. 전체 시간을 측정합니다(재미로요).\n몇 가지 단계가 있지만…\n…의심스러우면 코드로 구현해 보세요.\n\n# 진행률 표시줄을 위한 tqdm 임포트\nfrom tqdm.auto import tqdm\n\n# 시드 설정 및 타이머 시작\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# 에포크 수 설정 (빠른 훈련 시간을 위해 작게 유지)\nepochs = 3\n\n# 훈련 및 테스트 루프 생성\nfor epoch in tqdm(range(epochs)):\n    print(f\"에포크: {epoch}\\n-------\")\n    ### 훈련\n    train_loss = 0\n    # 훈련 배치를 반복하기 위한 루프 추가\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. 순전파\n        y_pred = model_0(X)\n\n        # 2. 손실 계산 (배치당)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # 에포크당 손실을 누적해서 더함\n\n        # 3. 옵티마이저 제로 그라디언트\n        optimizer.zero_grad()\n\n        # 4. 손실 역전파\n        loss.backward()\n\n        # 5. 옵티마이저 단계 수행\n        optimizer.step()\n\n        # 지금까지 본 샘플 수 출력\n        if batch % 400 == 0:\n            print(f\"{batch * len(X)}/{len(train_dataloader.dataset)} 샘플 확인\")\n\n    # 전체 훈련 손실을 훈련 데이터로더의 길이로 나눔 (에포크당 배치당 평균 손실)\n    train_loss /= len(train_dataloader)\n    \n    ### 테스트\n    # 손실 및 정확도를 누적해서 더하기 위한 변수 설정\n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. 순전파\n            test_pred = model_0(X)\n           \n            # 2. 손실 계산 (누적)\n            test_loss += loss_fn(test_pred, y) # 에포크당 손실을 누적해서 더함\n\n            # 3. 정확도 계산 (예측값이 y_true와 같아야 함)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # 테스트 지표 계산은 torch.inference_mode() 내부에서 이루어져야 합니다.\n        # 전체 테스트 손실을 테스트 데이터로더의 길이로 나눔 (배치당)\n        test_loss /= len(test_dataloader)\n\n        # 전체 정확도를 테스트 데이터로더의 길이로 나눔 (배치당)\n        test_acc /= len(test_dataloader)\n\n    ## 진행 상황 출력\n    print(f\"\\n훈련 손실: {train_loss:.5f} | 테스트 손실: {test_loss:.5f}, 테스트 정확도: {test_acc:.2f}%\\n\")\n\n# 훈련 시간 계산\ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n\n\n\n\nEpoch: 0\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n\nEpoch: 1\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n\nEpoch: 2\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n\nTrain time on cpu: 14.975 seconds\n\n\n좋습니다! 베이스라인 모델이 꽤 잘 작동하는 것 같네요.\nCPU에서도 훈련하는 데 그리 오래 걸리지 않았습니다. GPU에서는 더 빨라질까요?\n이제 모델을 평가하는 코드를 작성해 보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#예측-수행-및-모델-0-결과-가져오기",
    "href": "03_pytorch_computer_vision.html#예측-수행-및-모델-0-결과-가져오기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "4. 예측 수행 및 모델 0 결과 가져오기",
    "text": "4. 예측 수행 및 모델 0 결과 가져오기\n앞으로 몇 가지 모델을 구축할 것이므로, 모두 유사한 방식으로 평가하는 코드를 작성하는 것이 좋습니다.\n구체적으로, 훈련된 모델, DataLoader, 손실 함수 및 정확도 함수를 입력으로 받는 함수를 만들어 보겠습니다.\n이 함수는 모델을 사용하여 DataLoader의 데이터에 대해 예측을 수행한 다음, 손실 함수와 정확도 함수를 사용하여 해당 예측을 평가합니다.\n\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n    \"\"\"data_loader에 대한 모델의 예측 결과를 딕셔너리로 반환합니다.\n\n    Args:\n        model (torch.nn.Module): data_loader에 대해 예측을 수행할 수 있는 PyTorch 모델.\n        data_loader (torch.utils.data.DataLoader): 예측 대상 데이터셋.\n        loss_fn (torch.nn.Module): 모델의 손실 함수.\n        accuracy_fn: 모델의 예측을 실제 레이블과 비교하는 정확도 함수.\n\n    Returns:\n        (dict): data_loader에 대한 모델의 예측 결과.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # 모델로 예측 수행\n            y_pred = model(X)\n            \n            # 배치당 손실 및 정확도 값 누적\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # 정확도를 위해 예측 레이블이 필요함 (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # 배치당 평균 손실/정확도를 찾기 위해 손실 및 정확도 스케일링\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # 모델이 클래스로 생성된 경우에만 작동함\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# 테스트 데이터셋에 대해 모델 0 결과 계산\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n\n{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}\n\n\n좋아 보이네요!\n이 딕셔너리를 사용하여 나중에 베이스라인 모델 결과를 다른 모델과 비교할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시",
    "href": "03_pytorch_computer_vision.html#장치-중립적device-agnostic-코드-설정-gpu-사용-가능-시",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "5. 장치 중립적(device-agnostic) 코드 설정 (GPU 사용 가능 시)",
    "text": "5. 장치 중립적(device-agnostic) 코드 설정 (GPU 사용 가능 시)\nCPU에서 60,000개 샘플에 대해 PyTorch 모델을 훈련하는 데 얼마나 걸리는지 확인했습니다.\n\n참고: 모델 훈련 시간은 사용된 하드웨어에 따라 다릅니다. 일반적으로 프로세서가 많을수록 훈련 속도가 빨라지며, 작은 데이터셋의 작은 모델은 종종 큰 데이터셋의 큰 모델보다 빠르게 훈련됩니다.\n\n이제 모델과 데이터를 GPU(사용 가능한 경우)에서 실행할 수 있도록 장치 중립적 코드를 설정해 보겠습니다.\nGoogle Colab에서 이 노트북을 실행 중이고 아직 GPU를 켜지 않았다면, 지금 런타임 -&gt; 런타임 유형 변경 -&gt; 하드웨어 가속기 -&gt; GPU를 통해 켜야 할 때입니다. 이 작업을 수행하면 런타임이 재설정될 가능성이 높으므로 런타임 -&gt; 이전 셀 실행을 통해 위의 모든 셀을 다시 실행해야 합니다.\n\n# 장치 중립적 코드 설정\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n멋지네요!\n이제 다른 모델을 구축해 보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#모델-1-비선형성을-이용한-더-나은-모델-구축하기",
    "href": "03_pytorch_computer_vision.html#모델-1-비선형성을-이용한-더-나은-모델-구축하기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "6. 모델 1: 비선형성을 이용한 더 나은 모델 구축하기",
    "text": "6. 모델 1: 비선형성을 이용한 더 나은 모델 구축하기\n노트북 02에서 비선형성의 힘에 대해 배웠습니다.\n우리가 다루고 있는 데이터를 보았을 때, 비선형 함수가 필요하다고 생각하시나요?\n선형은 직선을, 비선형은 직선이 아님을 의미한다는 것을 기억하세요.\n한번 확인해 봅시다.\n이전과 유사한 모델을 만들되, 이번에는 각 선형 레이어 사이에 비선형 함수(nn.ReLU())를 추가해 보겠습니다.\n\n# 비선형 및 선형 레이어가 있는 모델 생성\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # 입력을 단일 벡터로 평탄화\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n\n좋아 보이네요.\n이제 이전과 동일한 설정으로 인스턴스화해 보겠습니다.\ninput_shape=784(이미지 데이터의 특성 수와 동일), hidden_units=10(작게 시작하며 베이스라인 모델과 동일), output_shape=len(class_names)(클래스당 하나의 출력 유닛)가 필요합니다.\n\n참고: 한 가지 변화(비선형 레이어 추가)를 제외하고 모델의 대부분의 설정을 동일하게 유지한 것에 주목하세요. 이것은 일련의 머신러닝 실험을 수행할 때 표준적인 관행입니다. 한 가지를 바꾸고 어떤 일이 일어나는지 확인한 다음, 다시 반복하는 것이죠.\n\n\ntorch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # 입력 특성 수\n    hidden_units=10,\n    output_shape=len(class_names) # 원하는 출력 클래스 수\n).to(device) # 사용 가능한 경우 모델을 GPU로 보냅니다.\nnext(model_1.parameters()).device # 모델 장치 확인\n\ndevice(type='cuda', index=0)\n\n\n\n6.1 손실 함수, 옵티마이저 및 평가 지표 설정\n평소와 같이 손실 함수, 옵티마이저 및 평가 지표를 설정하겠습니다(여러 평가 지표를 사용할 수 있지만 지금은 정확도를 고수하겠습니다).\n\nfrom helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n\n\n\n6.2 훈련 및 테스트 루프 기능화하기\n지금까지는 훈련 및 테스트 루프를 반복해서 작성했습니다.\n이제 이를 다시 작성하되, 반복해서 호출할 수 있도록 함수에 넣어 보겠습니다.\n그리고 이제 장치 중립적 코드를 사용하고 있으므로 특성(X) 및 타겟(y) 텐서에 대해 .to(device)를 호출해야 합니다.\n훈련 루프를 위해 모델, DataLoader, 손실 함수 및 옵티마이저를 입력으로 받는 train_step() 함수를 만들겠습니다.\n테스트 루프도 비슷하게 만들되 test_step()이라고 부르고 모델, DataLoader, 손실 함수 및 평가 함수를 입력으로 받겠습니다.\n\n참고: 함수이기 때문에 원하는 방식으로 커스터마이징할 수 있습니다. 여기서 만드는 것은 특정 분류 사용 사례를 위한 기본적인 훈련 및 테스트 함수라고 볼 수 있습니다.\n\n\ndef train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    for batch, (X, y) in enumerate(data_loader):\n        # 데이터를 GPU로 보냄\n        X, y = X.to(device), y.to(device)\n\n        # 1. 순전파\n        y_pred = model(X)\n\n        # 2. 손실 계산\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # logits -&gt; pred labels로 변환\n\n        # 3. 옵티마이저 제로 그라디언트\n        optimizer.zero_grad()\n\n        # 4. 손실 역전파\n        loss.backward()\n\n        # 5. 옵티마이저 단계 수행\n        optimizer.step()\n\n    # 에포크당 손실 및 정확도 계산 및 출력\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"훈련 손실: {train_loss:.5f} | 훈련 정확도: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.eval() # 모델을 평가 모드로 설정\n    # 추론 모드 컨텍스트 매니저 켜기\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # 데이터를 GPU로 보냄\n            X, y = X.to(device), y.to(device)\n            \n            # 1. 순전파\n            test_pred = model(X)\n            \n            # 2. 손실 및 정확도 계산\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # logits -&gt; pred labels로 변환\n            )\n        \n        # 지표 조정 및 출력\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"테스트 손실: {test_loss:.5f} | 테스트 정확도: {test_acc:.2f}%\\n\")\n\n야호!\n이제 모델을 훈련하고 테스트하기 위한 함수가 생겼으니 실행해 보겠습니다.\n각 에포크에 대해 또 다른 루프 내부에서 이를 수행할 것입니다.\n그렇게 하면 각 에포크마다 훈련 및 테스트 단계를 거치게 됩니다.\n\n참고: 테스트 단계를 얼마나 자주 수행할지 커스텀할 수 있습니다. 때로는 5에포크 또는 10에포크마다 수행하기도 하지만, 여기서는 매 에포크마다 수행합니다.\n\nGPU에서 코드를 실행하는 데 걸리는 시간도 측정해 보겠습니다.\n\ntorch.manual_seed(42)\n\n# 시간 측정\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"에포크: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n\n\n\n\nEpoch: 0\n---------\nTrain loss: 1.09199 | Train accuracy: 61.34%\nTest loss: 0.95636 | Test accuracy: 65.00%\n\nEpoch: 1\n---------\nTrain loss: 0.78101 | Train accuracy: 71.93%\nTest loss: 0.72227 | Test accuracy: 73.91%\n\nEpoch: 2\n---------\nTrain loss: 0.67027 | Train accuracy: 75.94%\nTest loss: 0.68500 | Test accuracy: 75.02%\n\nTrain time on cuda: 16.943 seconds\n\n\n훌륭합니다!\n모델이 훈련되었지만 훈련 시간이 더 오래 걸렸나요?\n\n참고: CUDA 대 CPU 훈련 시간은 주로 사용하는 CPU/GPU의 품질에 따라 달라집니다. 더 자세한 설명은 아래를 읽어보세요.\n\n\n질문: “GPU를 사용했는데 모델 훈련 속도가 빨라지지 않았습니다. 이유가 무엇일까요?”\n답변: 한 가지 이유는 데이터셋과 모델이 모두 매우 작기 때문에(우리가 다루는 데이터셋과 모델처럼), GPU를 사용함으로써 얻는 이점보다 실제로 데이터를 GPU로 전송하는 데 걸리는 시간이 더 크기 때문일 수 있습니다.\nCPU 메모리(기본값)에서 GPU 메모리로 데이터를 복사하는 사이에 작은 병목 현상이 발생합니다.\n따라서 작은 모델과 데이터셋의 경우 실제로는 CPU가 계산하기에 최적의 장소일 수 있습니다.\n하지만 큰 데이터셋과 모델의 경우 GPU가 제공할 수 있는 계산 속도는 대개 데이터를 전송하는 비용보다 훨씬 큽니다.\n그러나 이것은 주로 사용 중인 하드웨어에 따라 달라집니다. 연습을 통해 모델을 훈련하기에 가장 좋은 장소가 어디인지 익숙해질 것입니다.\n\n이제 eval_model() 함수를 사용하여 훈련된 model_1을 평가하고 어떻게 되었는지 확인해 보겠습니다.\n\ntorch.manual_seed(42)\n\n# 참고: This will error due to `eval_model()` not using device agnostic code \nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_1084458/2906876561.py in &lt;module&gt;\n      2 \n      3 # Note: This will error due to `eval_model()` not using device agnostic code\n----&gt; 4 model_1_results = eval_model(model=model_1, \n      5     data_loader=test_dataloader,\n      6     loss_fn=loss_fn,\n\n/tmp/ipykernel_1084458/2300884397.py in eval_model(model, data_loader, loss_fn, accuracy_fn)\n     20         for X, y in data_loader:\n     21             # Make predictions with the model\n---&gt; 22             y_pred = model(X)\n     23 \n     24             # Accumulate the loss and accuracy values per batch\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n/tmp/ipykernel_1084458/3744982926.py in forward(self, x)\n     12 \n     13     def forward(self, x: torch.Tensor):\n---&gt; 14         return self.layer_stack(x)\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/container.py in forward(self, input)\n    139     def forward(self, input):\n    140         for module in self:\n--&gt; 141             input = module(input)\n    142         return input\n    143 \n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/linear.py in forward(self, input)\n    101 \n    102     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 103         return F.linear(input, self.weight, self.bias)\n    104 \n    105     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n\n안돼요!\neval_model() 함수에서 다음과 같은 오류가 발생합니다.\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\n그 이유는 데이터와 모델은 장치 중립적 코드를 사용하도록 설정했지만 평가 함수는 그렇지 않았기 때문입니다.\neval_model() 함수에 타겟 device 매개변수를 전달하여 이를 해결해 볼까요?\n그런 다음 결과를 다시 계산해 보겠습니다.\n\n# Move values to device\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n    \"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 1 results with device-agnostic code \nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n\n{'model_name': 'FashionMNISTModelV1',\n 'model_loss': 0.6850008964538574,\n 'model_acc': 75.01996805111821}\n\n\n\n# Check baseline results\nmodel_0_results\n\n{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}\n\n\n이런, 이 경우에는 모델에 비선형성을 추가했음에도 베이스라인보다 성능이 떨어졌습니다.\n머신러닝에서 주의해야 할 점은, 효과가 있을 것이라고 생각했던 것이 그렇지 않을 때가 있고, 그 반대의 경우도 있다는 것입니다.\n이것은 과학이기도 하지만 예술이기도 합니다.\n겉보기에는 우리 모델이 훈련 데이터에 과적합(overfitting)된 것으로 보입니다.\n과적합은 모델이 훈련 데이터는 잘 학습하지만 그 패턴이 테스트 데이터로 일반화되지 않는 것을 의미합니다.\n과적합을 해결하는 두 가지 주요 방법은 다음과 같습니다. 1. 더 작거나 다른 모델을 사용합니다(일부 모델은 특정 종류의 데이터에 다른 모델보다 더 잘 맞습니다). 2. 더 큰 데이터셋을 사용합니다(데이터가 많을수록 모델이 일반화 가능한 패턴을 학습할 기회가 더 많아집니다).\n더 많은 방법이 있지만, 그것은 여러분이 탐구해 볼 과제로 남겨두겠습니다.\n온라인에서 “머신러닝에서 과적합을 방지하는 방법”을 검색하여 무엇이 나오는지 확인해 보세요.\n그동안 우리는 1번 방법인 다른 모델 사용하기를 살펴보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#모델-2-합성곱-신경망-cnn-구축하기",
    "href": "03_pytorch_computer_vision.html#모델-2-합성곱-신경망-cnn-구축하기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "7. 모델 2: 합성곱 신경망 (CNN) 구축하기",
    "text": "7. 모델 2: 합성곱 신경망 (CNN) 구축하기\n좋습니다. 이제 한 단계 더 나아가 볼 시간입니다.\n이제 합성곱 신경망(Convolutional Neural Network)(CNN 또는 ConvNet)을 만들 차례입니다.\nCNN은 시각적 데이터에서 패턴을 찾는 능력으로 잘 알려져 있습니다.\n우리는 시각적 데이터를 다루고 있으므로 CNN 모델을 사용하여 베이스라인을 개선할 수 있는지 확인해 보겠습니다.\n우리가 사용할 CNN 모델은 CNN Explainer 웹사이트의 TinyVGG로 알려진 모델입니다.\n이 모델은 합성곱 신경망의 전형적인 구조를 따릅니다.\n입력 레이어 -&gt; [합성곱 레이어 -&gt; 활성화 레이어 -&gt; 풀링 레이어] -&gt; 출력 레이어\n[합성곱 레이어 -&gt; 활성화 레이어 -&gt; 풀링 레이어]의 내용은 요구 사항에 따라 여러 번 반복되고 확장될 수 있습니다.\n\n어떤 모델을 사용해야 하나요?\n\n질문: 잠깐만요, CNN이 이미지에 좋다고 하셨는데, 제가 알아야 할 다른 모델 유형이 있나요?\n\n좋은 질문입니다.\n이 표는 어떤 모델을 사용할지에 대한 일반적인 가이드입니다(예외는 있습니다).\n\n\n\n문제 유형\n사용할 모델 (일반적으로)\n코드 예시\n\n\n\n\n정형 데이터 (Excel 스프레드시트, 행 및 열 데이터)\n그라디언트 부스팅 모델, 랜덤 포레스트, XGBoost\nsklearn.ensemble, XGBoost 라이브러리\n\n\n비정형 데이터 (이미지, 오디오, 언어)\n합성곱 신경망, 트랜스포머\ntorchvision.models, HuggingFace Transformers\n\n\n\n\n참고: 위의 표는 참고용일 뿐이며, 결국 사용하게 될 모델은 작업 중인 문제와 제약 조건(데이터 양, 지연 시간 요구 사항)에 따라 크게 달라집니다.\n\n모델에 대한 이야기는 이쯤 하고, 이제 CNN Explainer 웹사이트의 모델을 복제하는 CNN을 구축해 보겠습니다.\n\n\n\nTinyVGG 아키텍처, CNN Explainer 웹사이트의 설정\n\n\n이를 위해 torch.nn의 nn.Conv2d() 및 nn.MaxPool2d() 레이어를 활용할 것입니다.\n\n# Create a convolutional neural network \nclass FashionMNISTModelV2(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n\nFashionMNISTModelV2(\n  (block_1): Sequential(\n    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=490, out_features=10, bias=True)\n  )\n)\n\n\nNice!\nOur biggest model yet!\nWhat we’ve done is a common practice in machine learning.\nFind a model architecture somewhere and replicate it with code.\n\n\n7.1 nn.Conv2d() 단계별로 살펴보기\n위에서 만든 모델을 바로 사용할 수도 있지만, 먼저 새로 추가한 두 개의 레이어를 단계별로 살펴보겠습니다. * nn.Conv2d(): 합성곱 레이어(convolutional layer)라고도 합니다. * nn.MaxPool2d(): 최대 풀링 레이어(max pooling layer)라고도 합니다.\n\n질문: nn.Conv2d()에서 “2d”는 무엇을 의미하나요?\n2d는 2차원(2-dimensional) 데이터를 의미합니다. 우리의 이미지는 높이와 너비라는 두 개의 차원을 가집니다. 색상 채널 차원도 있지만, 각 색상 채널 차원 자체도 높이와 너비라는 두 개의 차원을 가집니다.\n다른 차원의 데이터(텍스트의 경우 1D, 3D 객체의 경우 3D)를 위해 nn.Conv1d() 및 nn.Conv3d()도 존재합니다.\n\n레이어를 테스트하기 위해 CNN Explainer에서 사용된 데이터와 유사한 장난감 데이터(toy data)를 만들어 보겠습니다.\n\ntorch.manual_seed(42)\n\n# 이미지 배치와 동일한 크기의 무작위 숫자 샘플 배치 생성\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # 테스트를 위한 단일 이미지 가져오기\nprint(f\"이미지 배치 모양: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"단일 이미지 모양: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"단일 이미지 픽셀 값:\\n{test_image}\")\n\n다양한 매개변수를 사용하여 nn.Conv2d() 예시를 만들어 보겠습니다. * in_channels (int) - 입력 이미지의 채널 수. * out_channels (int) - 합성곱에 의해 생성된 채널 수. * kernel_size (int 또는 tuple) - 합성곱 커널/필터의 크기. * stride (int 또는 tuple, 선택 사항) - 합성곱 커널이 한 번에 이동하는 단계의 크기. 기본값: 1. * padding (int, tuple, str) - 입력의 네 면 모두에 추가되는 패딩. 기본값: 0.\n\n\n\nConv2d 레이어의 다양한 매개변수를 살펴보는 예시\n\n\nnn.Conv2d() 레이어의 하이퍼파라미터를 변경할 때 일어나는 일의 예시입니다.\n\ntorch.manual_seed(42)\n\n# TinyVGG와 동일한 차원의 합성곱 레이어 생성\n# (매개변수를 변경해 보며 어떤 일이 일어나는지 확인해 보세요)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # 여기서 \"valid\" 또는 \"same\"도 사용해 보세요 \n\n# 합성곱 레이어에 데이터 통과\nconv_layer(test_image) # 참고: PyTorch &lt;1.11.0 버전을 실행 중인 경우 모양 문제로 인해 오류가 발생합니다(nn.Conv2d()는 4D 텐서를 입력으로 기대함) \n\n단일 이미지를 통과시키려고 하면 모양 불일치 오류가 발생합니다.\n\nRuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead\n참고: PyTorch 1.11.0 이상 버전을 실행 중이라면 이 오류는 발생하지 않습니다.\n\n이는 nn.Conv2d() 레이어가 (N, C, H, W) 또는 [batch_size, color_channels, height, width] 크기의 4차원 텐서를 입력으로 기대하기 때문입니다.\n현재 단일 이미지 test_image는 [color_channels, height, width] 또는 [3, 64, 64] 모양만 가지고 있습니다.\ntest_image.unsqueeze(dim=0)을 사용하여 N에 대한 추가 차원을 더함으로써 이를 해결할 수 있습니다.\n\n# 테스트 이미지에 추가 차원 더하기\ntest_image.unsqueeze(dim=0).shape\n\n\n# 추가 차원이 있는 테스트 이미지를 conv_layer에 통과시키기\nconv_layer(test_image.unsqueeze(dim=0)).shape\n\nHmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on CNN Explainer), we get different channel sizes as well as different pixel sizes.\nWhat if we changed the values of conv_layer?\n\ntorch.manual_seed(42)\n# Create a new conv_layer with different values (try setting these to whatever you like)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n\ntorch.Size([1, 10, 30, 30])\n\n\nWoah, we get another shape change.\nNow our image is of shape [1, 10, 30, 30] (it will be different if you use different values) or [batch_size=1, color_channels=10, height=30, width=30].\nWhat’s going on here?\nBehind the scenes, our nn.Conv2d() is compressing the information stored in the image.\nIt does this by performing operations on the input (our test image) against its internal parameters.\nThe goal of this is similar to all of the other neural networks we’ve been building.\nData goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.\nThe only difference is how the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer forward() method.\nIf we check out our conv_layer_2.state_dict() we’ll find a similar weight and bias setup as we’ve seen before.\n\n# Check out the conv_layer_2 internal parameters\nprint(conv_layer_2.state_dict())\n\nOrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n\n         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n\n         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n\n\n        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n\n         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n\n         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n\n\n        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n\n         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n\n         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n\n\n        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n\n         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n\n         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n\n\n        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n\n         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n\n         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n\n\n        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n\n         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n\n         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n\n\n        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n\n         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n\n         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n\n\n        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n\n         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n\n         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n\n\n        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n\n         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n\n         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n\n\n        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n\n         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n\n         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n        -0.0797,  0.1121]))])\n\n\nLook at that! A bunch of random numbers for a weight and bias tensor.\nThe shapes of these are manipulated by the inputs we passed to nn.Conv2d() when we set it up.\nLet’s check them out.\n\n# Get shapes of weight and bias tensors within conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n\nconv_layer_2 weight shape: \ntorch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n\nconv_layer_2 bias shape: \ntorch.Size([10]) -&gt; [out_channels=10]\n\n\n\nQuestion: What should we set the parameters of our nn.Conv2d() layers?\nThat’s a good one. But similar to many other things in machine learning, the values of these aren’t set in stone (and recall, because these values are ones we can set ourselves, they’re referred to as “hyperparameters”).\nThe best way to find out is to try out different values and see how they effect your model’s performance.\nOr even better, find a working example on a problem similar to yours (like we’ve done with TinyVGG) and copy it.\n\nWe’re working with a different of layer here to what we’ve seen before.\nBut the premise remains the same: start with random numbers and update them to better represent the data.\n\n\n7.2 Stepping through nn.MaxPool2d()\nNow let’s check out what happens when we move data through nn.MaxPool2d().\n\n# Print out original image shape without and with unsqueezed dimension\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Create a sample nn.MaxPoo2d() layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass data through just the conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pass data through the max pool layer\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n\nTest image original shape: torch.Size([3, 64, 64])\nTest image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\nShape after going through conv_layer(): torch.Size([1, 10, 62, 62])\nShape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n\n\nNotice the change in the shapes of what’s happening in and out of a nn.MaxPool2d() layer.\nThe kernel_size of the nn.MaxPool2d() layer will effects the size of the output shape.\nIn our case, the shape halves from a 62x62 image to 31x31 image.\nLet’s see this work with a smaller tensor.\n\ntorch.manual_seed(42)\n# Create a random tensor with a similiar number of dimensions to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n\nRandom tensor:\ntensor([[[[0.3367, 0.1288],\n          [0.2345, 0.2303]]]])\nRandom tensor shape: torch.Size([1, 1, 2, 2])\n\nMax pool tensor:\ntensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor\nMax pool tensor shape: torch.Size([1, 1, 1, 1])\n\n\nrandom_tensor와 max_pool_tensor 사이의 마지막 두 차원을 주목해 보세요. [2, 2]에서 [1, 1]로 바뀌었습니다.\n본질적으로 절반으로 줄어든 것입니다.\n그리고 이 변화는 nn.MaxPool2d()의 kernel_size 값에 따라 달라질 것입니다.\n또한 max_pool_tensor에 남은 값은 random_tensor에서 최댓값(maximum)이라는 점도 주목하세요.\n여기서 무슨 일이 일어나고 있는 걸까요?\n이것은 신경망 퍼즐의 또 다른 중요한 조각입니다.\n기본적으로 신경망의 모든 레이어는 고차원 공간에서 저차원 공간으로 데이터를 압축하려고 시도합니다.\n즉, 많은 숫자(원시 데이터)를 가져와서 해당 숫자들에서 패턴을 학습하는 것입니다. 이 패턴은 예측 능력을 갖추면서도 원래 값보다 크기가 작은 패턴입니다.\n인공지능의 관점에서 보면 신경망의 전체 목표를 정보의 압축이라고 볼 수 있습니다.\n\n\n\n신경망의 각 레이어는 원래 입력 데이터를 더 작은 표현으로 압축하며, 이는 (바라건대) 미래의 입력 데이터에 대해 예측을 수행할 수 있는 능력을 갖춥니다\n\n\n즉, 신경망의 관점에서 지능은 압축입니다.\n이것이 nn.MaxPool2d() 레이어를 사용하는 아이디어입니다. 텐서의 일부에서 최댓값을 가져오고 나머지는 무시하는 것이죠.\n본질적으로 정보의 상당 부분을 (바라건대) 유지하면서 텐서의 차원을 낮추는 것입니다.\nnn.Conv2d() 레이어의 경우도 마찬가지입니다.\n다만 최댓값만 가져오는 대신, nn.Conv2d()는 데이터에 대해 합성곱 연산을 수행합니다(CNN Explainer 웹페이지에서 이를 실제로 확인해 보세요).\n\n과제: nn.AvgPool2d() 레이어는 무엇을 한다고 생각하시나요? 위에서 했던 것처럼 무작위 텐서를 만들어 통과시켜 보세요. 입력 및 출력 모양과 입력 및 출력 값을 확인해 보세요.\n\n\n추가 학습 자료: “가장 흔한 합성곱 신경망”을 검색해 보세요. 어떤 아키텍처를 찾았나요? 그중 torchvision.models 라이브러리에 포함된 것이 있나요? 이것들로 무엇을 할 수 있을 것 같나요?\n\n\n\n7.3 model_2를 위한 손실 함수 및 옵티마이저 설정\n첫 번째 CNN의 레이어들을 충분히 살펴보았습니다.\n하지만 여전히 명확하지 않은 부분이 있다면 작게 시작해 보세요.\n모델의 단일 레이어를 선택하고 일부 데이터를 통과시켜 어떤 일이 일어나는지 확인해 보세요.\n이제 앞으로 나아가 훈련을 시작할 시간입니다!\n손실 함수와 옵티마이저를 설정해 보겠습니다.\n이전과 동일하게 다중 클래스 분류 데이터를 다루고 있으므로 nn.CrossEntropyLoss()를 손실 함수로 사용합니다.\n그리고 model_2.parameters()를 학습률 0.1로 최적화하기 위해 torch.optim.SGD()를 옵티마이저로 사용합니다.\n\n# 손실 함수 및 옵티마이저 설정\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n\n\n\n7.4 훈련 및 테스트 함수를 사용하여 model_2 훈련 및 테스트하기\n손실 함수와 옵티마이저가 준비되었습니다!\n이제 훈련하고 테스트할 시간입니다.\n이전에 만든 train_step() 및 test_step() 함수를 사용하겠습니다.\n또한 다른 모델과 비교하기 위해 시간을 측정하겠습니다.\n\ntorch.manual_seed(42)\n\n# 시간 측정\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# 모델 훈련 및 테스트\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"에포크: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n\n\n\n\nEpoch: 0\n---------\nTrain loss: 0.59411 | Train accuracy: 78.41%\nTest loss: 0.39967 | Test accuracy: 85.70%\n\nEpoch: 1\n---------\nTrain loss: 0.36450 | Train accuracy: 86.81%\nTest loss: 0.34607 | Test accuracy: 87.48%\n\nEpoch: 2\n---------\nTrain loss: 0.32553 | Train accuracy: 88.33%\nTest loss: 0.32664 | Test accuracy: 88.23%\n\nTrain time on cuda: 21.099 seconds\n\n\n와! 합성곱 레이어와 최대 풀링 레이어가 성능을 약간 향상시킨 것 같네요.\neval_model() 함수를 사용하여 model_2의 결과를 평가해 보겠습니다.\n\n# model_2 결과 가져오기\nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#모델-결과-및-훈련-시간-비교하기",
    "href": "03_pytorch_computer_vision.html#모델-결과-및-훈련-시간-비교하기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "8. 모델 결과 및 훈련 시간 비교하기",
    "text": "8. 모델 결과 및 훈련 시간 비교하기\n우리는 세 가지 다른 모델을 훈련했습니다.\n\nmodel_0 - 두 개의 nn.Linear() 레이어가 있는 베이스라인 모델.\nmodel_1 - 베이스라인 모델과 동일한 설정이지만 nn.Linear() 레이어 사이에 nn.ReLU() 레이어가 추가된 모델.\nmodel_2 - CNN Explainer 웹사이트의 TinyVGG 아키텍처를 모방한 첫 번째 CNN 모델.\n\n이것은 머신러닝에서 일반적인 관행입니다.\n여러 모델을 구축하고 여러 번의 훈련 실험을 수행하여 어느 모델이 가장 좋은 성능을 내는지 확인하는 것이죠.\n모델 결과 딕셔너리를 DataFrame으로 결합하여 확인해 보겠습니다.\n\nimport pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n\n\n\n\n\n\n\n\nmodel_name\nmodel_loss\nmodel_acc\n\n\n\n\n0\nFashionMNISTModelV0\n0.476639\n83.426518\n\n\n1\nFashionMNISTModelV1\n0.685001\n75.019968\n\n\n2\nFashionMNISTModelV2\n0.326644\n88.228834\n\n\n\n\n\n\n\n좋네요!\n훈련 시간 값도 추가할 수 있습니다.\n\n# 결과 비교에 훈련 시간 추가\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n\n우리 CNN(FashionMNISTModelV2) 모델이 가장 성능이 좋았지만(가장 낮은 손실, 가장 높은 정확도), 훈련 시간은 가장 길었습니다.\n그리고 베이스라인 모델(FashionMNISTModelV0)은 model_1(FashionMNISTModelV1)보다 성능이 좋았지만 훈련 시간이 더 오래 걸렸습니다(이는 model_0 훈련에는 CPU를, model_1 훈련에는 GPU를 사용했기 때문일 가능성이 큽니다).\n여기서 발생하는 상충 관계를 성능-속도(performance-speed) 상충 관계라고 합니다.\n일반적으로 더 크고 복잡한 모델(우리가 model_2로 했던 것처럼)에서 더 나은 성능을 얻을 수 있습니다.\n하지만 이러한 성능 향상은 종종 훈련 속도와 추론 속도의 희생을 수반합니다.\n\n참고: 훈련 시간은 사용 중인 하드웨어에 따라 크게 달라집니다.\n일반적으로 CPU 코어가 많을수록 CPU에서의 모델 훈련 속도가 빨라집니다. GPU의 경우도 마찬가지입니다.\n최신 하드웨어는 기술 발전을 통합하기 때문에 대개 모델 훈련 속도가 더 빠릅니다.\n\n이제 시각화를 해볼까요?\n\n# Visualize our model results\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#make-and-evaluate-random-predictions-with-best-model",
    "href": "03_pytorch_computer_vision.html#make-and-evaluate-random-predictions-with-best-model",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "9. Make and evaluate random predictions with best model",
    "text": "9. Make and evaluate random predictions with best model\nAlright, we’ve compared our models to each other, let’s further evaluate our best performing model, model_2.\nTo do so, let’s create a function make_predictions() where we can pass the model and some data for it to predict on.\n\ndef make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n\n\nimport random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# View the first test sample shape and label\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n\nTest sample image shape: torch.Size([1, 28, 28])\nTest sample label: 5 (Sandal)\n\n\nAnd now we can use our make_predictions() function to predict on test_samples.\n\n# Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n\ntensor([[2.3550e-07, 1.7185e-08, 4.6618e-07, 6.1371e-08, 5.1185e-08, 9.9957e-01,\n         3.7702e-07, 1.5924e-05, 3.7681e-05, 3.7831e-04],\n        [7.3275e-02, 6.7410e-01, 3.7231e-03, 8.8129e-02, 1.0114e-01, 6.9186e-05,\n         5.8674e-02, 4.2595e-04, 3.8635e-04, 7.1354e-05]])\n\n\nExcellent!\nAnd now we can go from prediction probabilities to prediction labels by taking the torch.argmax() of the output of the torch.softmax() activation function.\n\n# Turn the prediction probabilities into prediction labels by taking the argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n\ntensor([5, 1, 7, 4, 3, 0, 4, 7, 1])\n\n\n\n# Are our predictions in the same form as our test labels? \ntest_labels, pred_classes\n\n([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))\n\n\nNow our predicted classes are in the same format as our test labels, we can compare.\nSince we’re dealing with image data, let’s stay true to the data explorer’s motto.\n“Visualize, visualize, visualize!”\n\n# Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n\n\n\n\n\n\n\n\nWell, well, well, doesn’t that look good!\nNot bad for a couple dozen lines of PyTorch code!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#making-a-confusion-matrix-for-further-prediction-evaluation",
    "href": "03_pytorch_computer_vision.html#making-a-confusion-matrix-for-further-prediction-evaluation",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "10. Making a confusion matrix for further prediction evaluation",
    "text": "10. Making a confusion matrix for further prediction evaluation\nThere are many different evaluation metrics we can use for classification problems.\nOne of the most visual is a confusion matrix.\nA confusion matrix shows you where your classification model got confused between predicitons and true labels.\nTo make a confusion matrix, we’ll go through three steps: 1. Make predictions with our trained model, model_2 (a confusion matrix compares predictions to true labels). 2. Make a confusion matrix using torch.ConfusionMatrix. 3. Plot the confusion matrix using mlxtend.plotting.plot_confusion_matrix().\nLet’s start by making predictions with our trained model.\n\n# Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# 1. Make predictions with trained model\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenate list of predictions into a tensor\ny_pred_tensor = torch.cat(y_preds)\n\n\n\n\nWonderful!\nNow we’ve got predictions, let’s go through steps 2 & 3: 2. Make a confusion matrix using torchmetrics.ConfusionMatrix. 3. Plot the confusion matrix using mlxtend.plotting.plot_confusion_matrix().\nFirst we’ll need to make sure we’ve got torchmetrics and mlxtend installed (these two libraries will help us make and visual a confusion matrix).\n\n참고: If you’re using Google Colab, the default version of mlxtend installed is 0.14.0 (as of March 2022), however, for the parameters of the plot_confusion_matrix() function we’d like use, we need 0.19.0 or higher.\n\n\n# See if torchmetrics exists, if not, install it\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- 참고: If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n\nmlxtend version: 0.19.0\n\n\nTo plot the confusion matrix, we need to make sure we’ve got and mlxtend version of 0.19.0 or higher.\n\n# Import mlxtend upgraded version\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n\n0.19.0\n\n\ntorchmetrics and mlxtend installed, let’s make a confusion matrix!\nFirst we’ll create a torchmetrics.ConfusionMatrix instance telling it how many classes we’re dealing with by setting num_classes=len(class_names).\nThen we’ll create a confusion matrix (in tensor format) by passing our instance our model’s predictions (preds=y_pred_tensor) and targets (target=test_data.targets).\nFinally we can plot our confision matrix using the plot_confusion_matrix() function from mlxtend.plotting.\n\nfrom torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Setup confusion matrix instance and compare predictions to targets\nconfmat = ConfusionMatrix(num_classes=len(class_names))\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Plot the confusion matrix\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#최적의-모델로-무작위-예측-수행-및-평가하기",
    "href": "03_pytorch_computer_vision.html#최적의-모델로-무작위-예측-수행-및-평가하기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "9. 최적의 모델로 무작위 예측 수행 및 평가하기",
    "text": "9. 최적의 모델로 무작위 예측 수행 및 평가하기\n좋습니다. 모델들을 서로 비교해 보았으니, 이제 가장 성능이 좋은 모델인 model_2를 더 평가해 보겠습니다.\n이를 위해 모델과 예측할 데이터를 전달할 수 있는 make_predictions() 함수를 만들어 보겠습니다.\n\ndef make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # 샘플 준비\n            sample = torch.unsqueeze(sample, dim=0).to(device) # 추가 차원을 더하고 샘플을 장치로 보냄\n\n            # 순전파 (모델은 가공되지 않은 로짓을 출력함)\n            pred_logit = model(sample)\n\n            # 예측 확률 가져오기 (로짓 -&gt; 예측 확률)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n\n            # 후속 계산을 위해 pred_prob를 GPU에서 내림\n            pred_probs.append(pred_prob.cpu())\n            \n    # pred_probs 리스트를 텐서로 변환하기 위해 스택(stack) 수행\n    return torch.stack(pred_probs)\n\n\nimport random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# 첫 번째 테스트 샘플의 모양과 레이블 확인\nprint(f\"테스트 샘플 이미지 모양: {test_samples[0].shape}\\n테스트 샘플 레이블: {test_labels[0]} ({class_names[test_labels[0]]})\")\n\n이제 make_predictions() 함수를 사용하여 test_samples에 대해 예측을 수행할 수 있습니다.\n\n# 모델 2로 테스트 샘플에 대해 예측 수행\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# 처음 두 개의 예측 확률 리스트 확인\npred_probs[:2]\n\n훌륭합니다!\n이제 torch.softmax() 활성화 함수의 출력값에 torch.argmax()를 취하여 예측 확률에서 예측 레이블로 변환할 수 있습니다.\n\n# argmax()를 사용하여 예측 확률을 예측 레이블로 변환\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n\n\n# 예측값이 테스트 레이블과 동일한 형식인가요? \ntest_labels, pred_classes\n\n이제 예측 클래스가 테스트 레이블과 동일한 형식이 되었으므로 비교할 수 있습니다.\n이미지 데이터를 다루고 있으니 데이터 탐색가의 모토를 따릅시다.\n“시각화, 시각화, 시각화!”\n\n# 예측 시각화\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # 서브플롯 생성\n  plt.subplot(nrows, ncols, i+1)\n\n  # 대상 이미지 그리기\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # 예측 레이블 찾기 (텍스트 형식, 예: \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # 실제 레이블 가져오기 (텍스트 형식, 예: \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # 플롯의 제목 텍스트 생성\n  title_text = f\"예측: {pred_label} | 실제: {truth_label}\"\n  \n  # 일치 여부를 확인하고 그에 따라 제목 색상 변경\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # 맞으면 초록색 텍스트\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # 틀리면 빨간색 텍스트\n  plt.axis(False);\n\n와, 정말 좋아 보이네요!\nPyTorch 코드 수십 줄 치고는 나쁘지 않죠!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#추가-예측-평가를-위해-혼동-행렬-만들기",
    "href": "03_pytorch_computer_vision.html#추가-예측-평가를-위해-혼동-행렬-만들기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "10. 추가 예측 평가를 위해 혼동 행렬 만들기",
    "text": "10. 추가 예측 평가를 위해 혼동 행렬 만들기\n분류 문제에 사용할 수 있는 다양한 평가 지표가 많이 있습니다.\n가장 시각적인 것 중 하나는 혼동 행렬(confusion matrix)입니다.\n혼동 행렬은 분류 모델이 예측값과 실제 레이블 사이에서 어디서 혼동을 일으켰는지 보여줍니다.\n혼동 행렬을 만들기 위해 세 단계를 거칩니다. 1. 훈련된 모델인 model_2로 예측을 수행합니다(혼동 행렬은 예측값을 실제 레이블과 비교합니다). 2. torchmetrics.ConfusionMatrix를 사용하여 혼동 행렬을 만듭니다. 3. mlxtend.plotting.plot_confusion_matrix()를 사용하여 혼동 행렬을 그립니다.\n먼저 훈련된 모델로 예측을 수행해 보겠습니다.\n\n# 진행률 표시줄을 위한 tqdm 임포트\nfrom tqdm.auto import tqdm\n\n# 1. 훈련된 모델로 예측 수행\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"예측 수행 중\"):\n    # 데이터와 타겟을 타겟 장치로 보냄\n    X, y = X.to(device), y.to(device)\n    # 순전파 수행\n    y_logit = model_2(X)\n    # 예측값을 로짓 -&gt; 예측 확률 -&gt; 예측 레이블로 변환\n    y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n    # 평가를 위해 예측값을 CPU에 둠\n    y_preds.append(y_pred.cpu())\n# 예측 리스트를 텐서로 결합\ny_pred_tensor = torch.cat(y_preds)\n\n멋지네요!\n이제 예측값이 생겼으니 2단계와 3단계를 진행해 보겠습니다. 2. torchmetrics.ConfusionMatrix를 사용하여 혼동 행렬을 만듭니다. 3. mlxtend.plotting.plot_confusion_matrix()를 사용하여 혼동 행렬을 그립니다.\n먼저 torchmetrics와 mlxtend가 설치되어 있는지 확인해야 합니다(이 두 라이브러리는 혼동 행렬을 만들고 시각화하는 데 도움을 줍니다).\n\n참고: Google Colab을 사용 중이라면 mlxtend의 기본 설치 버전은 0.14.0(2022년 3월 기준)입니다. 하지만 우리가 사용하려는 plot_confusion_matrix() 함수의 매개변수를 위해서는 0.19.0 이상의 버전이 필요합니다.\n\n\n# torchmetrics가 있는지 확인하고, 없으면 설치합니다.\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend 버전: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend 버전은 0.19.0 이상이어야 합니다.\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- 참고: Google Colab을 사용하는 경우 런타임을 다시 시작해야 할 수도 있습니다.\n    import torchmetrics, mlxtend\n    print(f\"mlxtend 버전: {mlxtend.__version__}\")\n\n혼동 행렬을 그리려면 mlxtend 버전이 0.19.0 이상이어야 합니다.\n\n# 업그레이드된 mlxtend 버전 임포트\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # 0.19.0 이상 버전이어야 함\n\ntorchmetrics와 mlxtend가 설치되었으니 혼동 행렬을 만들어 봅시다!\n먼저 num_classes=len(class_names)로 설정하여 우리가 다루는 클래스 수를 알려주는 torchmetrics.ConfusionMatrix 인스턴스를 생성합니다.\n그런 다음 모델의 예측값(preds=y_pred_tensor)과 실제 타겟(target=test_data.targets)을 인스턴스에 전달하여 텐서 형식의 혼동 행렬을 생성합니다.\n마지막으로 mlxtend.plotting의 plot_confusion_matrix() 함수를 사용하여 혼동 행렬을 시각화할 수 있습니다.\n\nfrom torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. 혼동 행렬 인스턴스를 설정하고 예측값과 타겟을 비교합니다.\nconfmat = ConfusionMatrix(num_classes=len(class_names))\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. 혼동 행렬을 그립니다.\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib은 NumPy와 함께 작동하는 것을 선호합니다. \n    class_names=class_names, # 행과 열 레이블을 클래스 이름으로 바꿉니다.\n    figsize=(10, 7)\n);\n\n와! 정말 좋아 보이지 않나요?\n대부분의 어두운 사각형이 왼쪽 위에서 오른쪽 아래로 이어지는 대각선에 몰려 있는 것을 통해 우리 모델이 꽤 잘 작동하고 있음을 알 수 있습니다(이상적인 모델은 이 대각선 사각형에만 값이 있고 나머지는 모두 0일 것입니다).\n모델은 서로 비슷한 클래스에서 가장 많이 “혼동”을 일으킵니다. 예를 들어 실제로는 “Shirt”로 레이블이 지정된 이미지에 대해 “Pullover”라고 예측하는 경우입니다.\n실제로 “T-shirt/top”으로 레이블이 지정된 클래스에 대해 “Shirt”라고 예측하는 경우도 마찬가지입니다.\n이러한 정보는 단일 정확도 지표보다 훨씬 더 유용할 때가 많습니다. 모델이 어디서 틀리고 있는지 알려주기 때문입니다.\n또한 모델이 왜 특정한 실수를 하는지에 대한 힌트도 제공합니다.\n모델이 “T-shirt/top”으로 레이블이 지정된 이미지를 가끔 “Shirt”라고 예측하는 것은 충분히 이해할 수 있는 일입니다.\n이러한 정보를 사용하여 모델과 데이터를 더 자세히 조사하고 어떻게 개선할 수 있을지 파악할 수 있습니다.\n\n과제: 훈련된 model_2를 사용하여 테스트용 FashionMNIST 데이터셋에 대해 예측을 수행해 보세요. 그런 다음 모델이 틀린 몇 가지 예측을 해당 이미지의 실제 레이블과 함께 시각화해 보세요. 이러한 예측을 시각화한 후, 이것이 모델링 오류에 가까운지 아니면 데이터 오류에 가까운지 생각해 보세요. 즉, 모델이 더 잘할 수 있었을까요, 아니면 데이터의 레이블이 서로 너무 비슷했나요(예: “Shirt” 레이블과 “T-shirt/top”이 너무 비슷함)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#가장-성능이-좋은-모델-저장-및-불러오기",
    "href": "03_pytorch_computer_vision.html#가장-성능이-좋은-모델-저장-및-불러오기",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "11. 가장 성능이 좋은 모델 저장 및 불러오기",
    "text": "11. 가장 성능이 좋은 모델 저장 및 불러오기\n가장 성능이 좋은 모델을 저장하고 불러오는 것으로 이 섹션을 마무리하겠습니다.\n노트북 01에서 보았듯이 다음 함수들을 조합하여 PyTorch 모델을 저장하고 불러올 수 있습니다. * torch.save: 전체 PyTorch 모델 또는 모델의 state_dict()를 저장하는 함수입니다. * torch.load: 저장된 PyTorch 객체를 불러오는 함수입니다. * torch.nn.Module.load_state_dict(): 저장된 state_dict()를 기존 모델 인스턴스로 불러오는 함수입니다.\n이 세 가지에 대한 자세한 내용은 PyTorch 모델 저장 및 불러오기 문서에서 확인할 수 있습니다.\n이제 model_2의 state_dict()를 저장한 다음, 다시 불러와서 평가하여 저장과 불러오기가 올바르게 수행되었는지 확인해 보겠습니다.\n\nfrom pathlib import Path\n\n# 모델 디렉토리 생성 (이미 존재하지 않는 경우), 참고: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # 필요한 경우 부모 디렉토리 생성\n                 exist_ok=True # 모델 디렉토리가 이미 존재해도 오류를 발생시키지 않음\n)\n\n# 모델 저장 경로 생성\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 모델 state dict 저장\nprint(f\"모델 저장 중: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # state_dict()만 저장하면 학습된 매개변수만 저장됩니다.\n           f=MODEL_SAVE_PATH)\n\n저장된 모델 state_dict()가 있으므로 load_state_dict()와 torch.load()를 조합하여 다시 불러올 수 있습니다.\nload_state_dict()를 사용하므로 저장된 모델 state_dict()와 동일한 입력 매개변수를 사용하여 FashionMNISTModelV2()의 새 인스턴스를 만들어야 합니다.\n\n# FashionMNISTModelV2의 새 인스턴스 생성 (저장된 state_dict()와 동일한 클래스)\n# 참고: 여기서의 모양이 저장된 버전과 같지 않으면 모델 로드 시 오류가 발생합니다.\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # 이것을 128로 변경하고 어떤 일이 일어나는지 확인해 보세요 \n                                    output_shape=10) \n\n# 저장된 state_dict() 불러오기\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# 모델을 GPU로 보냄\nloaded_model_2 = loaded_model_2.to(device)\n\n이제 불러온 모델이 있으므로 eval_model()로 평가하여 해당 매개변수가 저장 전의 model_2와 유사하게 작동하는지 확인해 보겠습니다.\n\n# 불러온 모델 평가\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n\n이 결과가 model_2_results와 동일하게 보이나요?\n\nmodel_2_results\n\ntorch.isclose()를 사용하고 atol(절대 허용 오차) 및 rtol(상대 허용 오차) 매개변수를 통해 근접도 허용 수준을 전달하여 두 텐서가 서로 가까운지 확인할 수 있습니다.\n모델의 결과가 가깝다면 torch.isclose()의 출력은 True여야 합니다.\n\n# 결과가 서로 가까운지 확인 (너무 멀리 떨어져 있으면 오류가 있을 수 있음)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # 절대 허용 오차\n              rtol=0.0001) # 상대 허용 오차",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#연습-문제",
    "href": "03_pytorch_computer_vision.html#연습-문제",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "연습 문제",
    "text": "연습 문제\n모든 연습 문제는 위 섹션의 코드를 연습하는 데 중점을 둡니다.\n각 섹션을 참조하거나 링크된 리소스를 따라가며 완료할 수 있어야 합니다.\n모든 연습 문제는 장치 중립적 코드를 사용하여 완료해야 합니다.\n리소스: * 03 연습 문제 템플릿 노트북 * 03 연습 문제 예시 솔루션 노트북 (솔루션을 보기 전에 직접 풀어보세요)\n\n현재 컴퓨터 비전이 사용되고 있는 산업 분야 3가지는 무엇인가요?\n“머신러닝에서 과적합(overfitting)이란 무엇인가”를 검색하고 찾은 내용에 대해 한 문장으로 적어보세요.\n“머신러닝에서 과적합을 방지하는 방법”을 검색하여 3가지를 적고 각각에 대해 한 문장으로 설명하세요. 참고: 방법이 아주 많으므로 너무 걱정하지 말고 3가지만 골라 시작해 보세요.\nCNN Explainer 웹사이트를 20분 동안 읽고 클릭해 보세요.\n\n“upload” 버튼을 사용하여 자신의 예시 이미지를 업로드하고 이미지가 CNN의 각 레이어를 통과할 때 어떤 일이 일어나는지 확인해 보세요.\n\ntorchvision.datasets.MNIST() 훈련 및 테스트 데이터셋을 로드하세요.\nMNIST 훈련 데이터셋에서 적어도 5개의 서로 다른 샘플을 시각화하세요.\ntorch.utils.data.DataLoader를 사용하여 MNIST 훈련 및 테스트 데이터셋을 데이터로더로 변환하고 batch_size=32로 설정하세요.\nMNIST 데이터셋에 적합한, 이 노트북에서 사용된 model_2(CNN Explainer 웹사이트의 모델, TinyVGG로도 알려짐)를 재현하세요.\n8번 연습 문제에서 구축한 모델을 CPU와 GPU에서 훈련하고 각각 얼마나 걸리는지 확인해 보세요.\n훈련된 모델을 사용하여 예측을 수행하고 적어도 5개를 시각화하여 예측값과 실제 레이블을 비교해 보세요.\n모델의 예측값을 실제 레이블과 비교하는 혼동 행렬을 그리세요.\n모양이 [1, 3, 64, 64]인 무작위 텐서를 만들고 다양한 하이퍼파라미터 설정(원하는 설정 가능)으로 nn.Conv2d() 레이어에 통과시키세요. kernel_size 매개변수가 커지거나 작아지면 어떤 변화가 나타나나요?\n이 노트북의 훈련된 model_2와 유사한 모델을 사용하여 테스트용 torchvision.datasets.FashionMNIST 데이터셋에 대해 예측을 수행하세요.\n\n그런 다음 모델이 틀린 몇 가지 예측을 해당 이미지의 실제 레이블과 함께 시각화하세요.\n이러한 예측을 시각화한 후, 이것이 모델링 오류에 가까운지 아니면 데이터 오류에 가까운지 생각해 보세요.\n즉, 모델이 더 잘할 수 있었을까요, 아니면 데이터의 레이블이 서로 너무 비슷했나요(예: “Shirt” 레이블과 “T-shirt/top”이 너무 비슷함)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#추가-학습-자료",
    "href": "03_pytorch_computer_vision.html#추가-학습-자료",
    "title": "03 - PyTorch 컴퓨터 비전",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\n시청: MIT의 딥 컴퓨터 비전 입문 강의. 합성곱 신경망 뒤에 숨겨진 훌륭한 직관을 제공할 것입니다.\nPyTorch vision 라이브러리의 다양한 옵션을 10분 동안 클릭해 보세요. 어떤 모듈들을 사용할 수 있나요?\n“가장 흔한 합성곱 신경망”을 검색해 보세요. 어떤 아키텍처를 찾았나요? 그중 torchvision.models 라이브러리에 포함된 것이 있나요? 이것들로 무엇을 할 수 있을 것 같나요?\n수많은 사전 훈련된 PyTorch 컴퓨터 비전 모델과 PyTorch의 컴퓨터 비전 기능에 대한 다양한 확장 기능은 Ross Wightman의 PyTorch Image Models 라이브러리 timm(Torch Image Models)을 확인해 보세요.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>03 - PyTorch 컴퓨터 비전</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html",
    "href": "04_pytorch_custom_datasets.html",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "",
    "text": "사용자 정의 데이터셋이란 무엇인가요?\n이전 노트북인 노트북 03에서는 PyTorch의 내장 데이터셋(FashionMNIST)을 사용하여 컴퓨터 비전 모델을 구축하는 방법을 살펴보았습니다.\n우리가 수행한 단계는 머신러닝의 다양한 문제에 걸쳐 유사합니다.\n데이터셋을 찾고, 데이터를 숫자로 변환하고, 모델을 구축(또는 기존 모델을 탐색)하여 예측에 사용할 수 있는 패턴을 해당 숫자에서 찾는 것입니다.\nPyTorch에는 많은 머신러닝 벤치마크에 사용되는 많은 내장 데이터셋이 있지만, 종종 자신만의 사용자 정의 데이터셋(custom dataset)을 사용하고 싶을 것입니다.\n사용자 정의 데이터셋은 작업 중인 특정 문제와 관련된 데이터 모음입니다.\n본질적으로 사용자 정의 데이터셋은 거의 모든 것으로 구성될 수 있습니다.\n예를 들어, Nutrify와 같은 음식 이미지 분류 앱을 구축한다면 사용자 정의 데이터셋은 음식 이미지가 될 것입니다.\n또는 웹사이트의 텍스트 기반 리뷰가 긍정적인지 부정적인지 분류하는 모델을 구축하려는 경우, 사용자 정의 데이터셋은 기존 고객 리뷰와 해당 평점의 예시가 될 것입니다.\n또는 소리 분류 앱을 구축하려는 경우, 사용자 정의 데이터셋은 소리 샘플과 해당 샘플 레이블이 될 것입니다.\n또는 웹사이트에서 물건을 구매하는 고객을 위한 추천 시스템을 구축하려는 경우, 사용자 정의 데이터셋은 다른 사람들이 구매한 제품의 예시가 될 것입니다.\nPyTorch에는 TorchVision, TorchText, TorchAudio 및 TorchRec 도메인 라이브러리에 다양한 사용자 정의 데이터셋을 로드하기 위한 많은 기존 함수가 포함되어 있습니다.\n하지만 때로는 이러한 기존 함수만으로는 충분하지 않을 수 있습니다.\n이러한 경우 항상 torch.utils.data.Dataset을 상속받아 원하는 대로 커스터마이징할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#이번-장에서-다룰-내용",
    "href": "04_pytorch_custom_datasets.html#이번-장에서-다룰-내용",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "이번 장에서 다룰 내용",
    "text": "이번 장에서 다룰 내용\n노트북 01과 노트북 02에서 다루었던 PyTorch 워크플로우를 컴퓨터 비전 문제에 적용해 보겠습니다.\n하지만 PyTorch 내장 데이터셋을 사용하는 대신, 피자, 스테이크, 스시 이미지로 구성된 우리만의 데이터셋을 사용할 것입니다.\n목표는 이러한 이미지를 로드한 다음 훈련 및 예측을 위한 모델을 구축하는 것입니다.\n\n우리가 구축할 내용입니다. torchvision.datasets 뿐만 아니라 자체 사용자 정의 Dataset 클래스를 사용하여 음식 이미지를 로드한 다음, 이를 분류할 수 있는 PyTorch 컴퓨터 비전 모델을 구축할 것입니다.\n구체적으로 다음 내용을 다룹니다:\n\n\n\n\n\n\n\n주제\n내용\n\n\n\n\n0. PyTorch 임포트 및 장치 중립적 코드 설정\nPyTorch를 로드하고 장치 중립적인 코드를 설정하기 위한 권장 사항을 따릅니다.\n\n\n1. 데이터 가져오기\n피자, 스테이크, 스시 이미지로 구성된 우리만의 사용자 정의 데이터셋을 사용할 것입니다.\n\n\n2. 데이터와 하나 되기 (데이터 준비)\n새로운 머신러닝 문제를 시작할 때, 다루고 있는 데이터를 이해하는 것이 가장 중요합니다. 여기서는 우리가 가진 데이터를 파악하기 위한 몇 가지 단계를 거칩니다.\n\n\n3. 데이터 변환\n가져온 데이터가 머신러닝 모델에 바로 사용할 수 있는 상태가 아닌 경우가 많습니다. 여기서는 이미지를 모델에 사용할 수 있도록 변환하는 몇 가지 단계를 살펴봅니다.\n\n\n4. ImageFolder를 사용하여 데이터 로드 (옵션 1)\nPyTorch에는 일반적인 데이터 유형을 위한 많은 내장 데이터 로딩 함수가 있습니다. ImageFolder는 이미지가 표준 이미지 분류 형식으로 되어 있을 때 유용합니다.\n\n\n5. 사용자 정의 Dataset으로 이미지 데이터 로드\nPyTorch에 내장된 데이터 로딩 함수가 없다면 어떻게 해야 할까요? 여기서 torch.utils.data.Dataset의 자체 사용자 정의 서브클래스를 구축할 수 있습니다.\n\n\n6. 다른 형태의 변환 (데이터 증강)\n데이터 증강(Data augmentation)은 훈련 데이터의 다양성을 확장하기 위한 일반적인 기술입니다. 여기서는 torchvision의 내장 데이터 증강 함수 중 일부를 살펴봅니다.\n\n\n7. 모델 0: 데이터 증강이 없는 TinyVGG\n이 단계에서는 데이터가 준비될 것이며, 이를 학습할 수 있는 모델을 구축해 봅니다. 또한 모델을 훈련하고 평가하기 위한 훈련 및 테스트 함수를 만듭니다.\n\n\n8. 손실 곡선 탐색\n손실 곡선은 모델이 시간이 지남에 따라 어떻게 훈련/개선되고 있는지 확인하는 좋은 방법입니다. 또한 모델이 과소적합(underfitting) 또는 과적합(overfitting) 상태인지 확인하는 좋은 방법이기도 합니다.\n\n\n9. 모델 1: 데이터 증강이 있는 TinyVGG\n이제 데이터 증강 없이 모델을 시도해 보았으니, 데이터 증강이 있는 모델을 시도해 보면 어떨까요?\n\n\n10. 모델 결과 비교\n서로 다른 모델의 손실 곡선을 비교하여 어떤 모델이 더 성능이 좋은지 확인하고 성능 향상을 위한 몇 가지 옵션에 대해 논의합니다.\n\n\n11. 사용자 정의 이미지에 대해 예측 수행\n우리 모델은 피자, 스테이크, 스시 이미지 데이터셋으로 훈련되었습니다. 이 섹션에서는 훈련된 모델을 사용하여 기존 데이터셋 외부의 이미지에 대해 예측하는 방법을 다룹니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#도움을-받을-수-있는-곳",
    "href": "04_pytorch_custom_datasets.html#도움을-받을-수-있는-곳",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "도움을 받을 수 있는 곳",
    "text": "도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 문서와 PyTorch 개발자 포럼도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#importing-pytorch-and-setting-up-device-agnostic-code",
    "href": "04_pytorch_custom_datasets.html#importing-pytorch-and-setting-up-device-agnostic-code",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "0. Importing PyTorch and setting up device-agnostic code",
    "text": "0. Importing PyTorch and setting up device-agnostic code\n\nimport torch\nfrom torch import nn\n\n# 참고: this notebook requires torch &gt;= 1.10.0\ntorch.__version__\n\n'1.11.0'\n\n\nAnd now let’s follow best practice and setup device-agnostic code.\n\n참고: If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU. If you do this, your runtime will likely reset and you’ll have to run all of the cells above by going Runtime -&gt; Run before.\n\n\n# Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#get-data",
    "href": "04_pytorch_custom_datasets.html#get-data",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "1. Get data",
    "text": "1. Get data\nFirst thing’s first we need some data.\nAnd like any good cooking show, some data has already been prepared for us.\nWe’re going to start small.\nBecause we’re not looking to train the biggest model or use the biggest dataset yet.\nMachine learning is an iterative process, start small, get something working and increase when necessary.\nThe data we’re going to be using is a subset of the Food101 dataset.\nFood101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).\nCan you think of 101 different foods?\nCan you think of a computer program to classify 101 foods?\nI can.\nA machine learning model!\nSpecifically, a PyTorch computer vision model like we covered in notebook 03.\nInstead of 101 food classes though, we’re going to start with 3: pizza, steak and sushi.\nAnd instead of 1,000 images per class, we’re going to start with a random 10% (start small, increase when necessary).\nIf you’d like to see where the data came from you see the following resources: * Original Food101 dataset and paper website. * torchvision.datasets.Food101 - the version of the data I downloaded for this notebook. * extras/04_custom_data_creation.ipynb - a notebook I used to format the Food101 dataset to use for this notebook. * data/pizza_steak_sushi.zip - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.\nLet’s write some code to download the formatted data from GitHub.\n\n참고: The dataset we’re about to use has been pre-formatted for what we’d like to use it for. However, you’ll often have to format your own datasets for whatever problem you’re working on. This is a regular practice in the machine learning world.\n\n\nimport requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\ndata/pizza_steak_sushi directory exists.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#become-one-with-the-data-data-preparation",
    "href": "04_pytorch_custom_datasets.html#become-one-with-the-data-data-preparation",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "2. Become one with the data (data preparation)",
    "text": "2. Become one with the data (data preparation)\nDataset downloaded!\nTime to become one with it.\nThis is another important step before building a model.\nAs Abraham Lossfunction said…\n\nData preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here? Source: @mrdbourke Twitter.\nWhat’s inspecting the data and becoming one with it?\nBefore starting a project or building any kind of model, it’s important to know what data you’re working with.\nIn our case, we have images of pizza, steak and sushi in standard image classification format.\nImage classification format contains separate classes of images in seperate directories titled with a particular class name.\nFor example, all images of pizza are contained in the pizza/ directory.\nThis format is popular across many different image classification benchmarks, including ImageNet (of the most popular computer vision benchmark datasets).\nYou can see an example of the storage format below, the images numbers are arbitrary.\npizza_steak_sushi/ &lt;- overall dataset folder\n    train/ &lt;- training images\n        pizza/ &lt;- class name as folder name\n            image01.jpeg\n            image02.jpeg\n            ...\n        steak/\n            image24.jpeg\n            image25.jpeg\n            ...\n        sushi/\n            image37.jpeg\n            ...\n    test/ &lt;- testing images\n        pizza/\n            image101.jpeg\n            image102.jpeg\n            ...\n        steak/\n            image154.jpeg\n            image155.jpeg\n            ...\n        sushi/\n            image167.jpeg\n            ...\nThe goal will be to take this data storage structure and turn it into a dataset usable with PyTorch.\n\n참고: The structure of the data you work with will vary depending on the problem you’re working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.\n\nWe can inspect what’s in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.\nTo do so, we’ll use Python’s in-built os.walk().\n\nimport os\ndef walk_through_dir(dir_path):\n  \"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  \n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n\n\nwalk_through_dir(image_path)\n\nThere are 2 directories and 0 images in 'data\\pizza_steak_sushi'.\nThere are 3 directories and 0 images in 'data\\pizza_steak_sushi\\test'.\nThere are 0 directories and 25 images in 'data\\pizza_steak_sushi\\test\\pizza'.\nThere are 0 directories and 19 images in 'data\\pizza_steak_sushi\\test\\steak'.\nThere are 0 directories and 31 images in 'data\\pizza_steak_sushi\\test\\sushi'.\nThere are 3 directories and 0 images in 'data\\pizza_steak_sushi\\train'.\nThere are 0 directories and 78 images in 'data\\pizza_steak_sushi\\train\\pizza'.\nThere are 0 directories and 75 images in 'data\\pizza_steak_sushi\\train\\steak'.\nThere are 0 directories and 72 images in 'data\\pizza_steak_sushi\\train\\sushi'.\n\n\nExcellent!\nIt looks like we’ve got about 75 images per training class and 25 images per testing class.\nThat should be enough to get started.\nRemember, these images are subsets of the original Food101 dataset.\nYou can see how they were created in the data creation notebook.\nWhile we’re at it, let’s setup our training and testing paths.\n\n# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n\n(WindowsPath('data/pizza_steak_sushi/train'),\n WindowsPath('data/pizza_steak_sushi/test'))\n\n\n\n2.1 Visualize an image\nOkay, we’ve seen how our directory structure is formatted.\nNow in the spirit of the data explorer, it’s time to visualize, visualize, visualize!\nLet’s write some code to: 1. Get all of the image paths using pathlib.Path.glob() to find all of the files ending in .jpg. 2. Pick a random image path using Python’s random.choice(). 3. Get the image class name using pathlib.Path.parent.stem. 4. And since we’re working with images, we’ll open the random image path using PIL.Image.open() (PIL stands for Python Image Library). 5. We’ll then show the image and print some metadata.\n\nimport random\nfrom PIL import Image\n\n# Set seed\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n\nRandom image path: data\\pizza_steak_sushi\\test\\sushi\\2394442.jpg\nImage class: sushi\nImage height: 408\nImage width: 512\n\n\n\n\n\n\n\n\n\nWe can do the same with matplotlib.pyplot.imshow(), except we have to convert the image to a NumPy array first.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#transforming-data",
    "href": "04_pytorch_custom_datasets.html#transforming-data",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "3. Transforming data",
    "text": "3. Transforming data\nNow what if we wanted to load our image data into PyTorch?\nBefore we can use our image data with PyTorch we need to:\n\nTurn it into tensors (numerical representations of our images).\nTurn it into a torch.utils.data.Dataset and subsequently a torch.utils.data.DataLoader, we’ll call these Dataset and DataLoader for short.\n\nThere are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem you’re working on.\n\n\n\nProblem space\nPre-built Datasets and Functions\n\n\n\n\nVision\ntorchvision.datasets\n\n\nAudio\ntorchaudio.datasets\n\n\nText\ntorchtext.datasets\n\n\nRecommendation system\ntorchrec.datasets\n\n\n\nSince we’re working with a vision problem, we’ll be looking at torchvision.datasets for our data loading functions as well as torchvision.transforms for preparing our data.\nLet’s import some base libraries.\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\n3.1 Transforming data with torchvision.transforms\nWe’ve got folders of images but before we can use them with PyTorch, we need to convert them into tensors.\nOne of the ways we can do this is by using the torchvision.transforms module.\ntorchvision.transforms contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, we’ll see this later on) purposes .\nTo get experience with torchvision.transforms, let’s write a series of transform steps that: 1. Resize the images using transforms.Resize() (from about 512x512 to 64x64, the same shape as the images on the CNN Explainer website). 2. Flip our images randomly on the horizontal using transforms.RandomHorizontalFlip() (this could be considered a form of data augmentation because it will artificially change our image data). 3. Turn our images from a PIL image to a PyTorch tensor using transforms.ToTensor().\nWe can compile all of these steps using torchvision.transforms.Compose().\n\n# Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n\nNow we’ve got a composition of transforms, let’s write a function to try them out on various images.\n\ndef plot_transformed_images(image_paths, transform, n=3, seed=42):\n    \"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # 참고: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNice!\nWe’ve now got a way to convert our images to tensors using torchvision.transforms.\nWe also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).\nGenerally, the larger the shape of the image, the more information a model can recover.\nFor example, an image of size [256, 256, 3] will have 16x more pixels than an image of size [64, 64, 3] ((256*256*3)/(64*64*3)=16).\nHowever, the tradeoff is that more pixels requires more computations.\n\nExercise: Try commenting out one of the transforms in data_transform and running the plotting function plot_transformed_images() again, what happens?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#option-1-loading-image-data-using-imagefolder",
    "href": "04_pytorch_custom_datasets.html#option-1-loading-image-data-using-imagefolder",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "4. Option 1: Loading Image Data Using ImageFolder",
    "text": "4. Option 1: Loading Image Data Using ImageFolder\nAlright, time to turn our image data into a Dataset capable of being used with PyTorch.\nSince our data is in standard image classification format, we can use the class torchvision.datasets.ImageFolder.\nWhere we can pass it the file path of a target image directory as well as a series of transforms we’d like to perform on our images.\nLet’s test it out on our data folders train_dir and test_dir passing in transform=data_transform to turn our images into tensors.\n\n# Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n\nTrain data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data\\pizza_steak_sushi\\train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data\\pizza_steak_sushi\\test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\n\n\nBeautiful!\nIt looks like PyTorch has registered our Dataset’s.\nLet’s inspect them by checking out the classes and class_to_idx attributes as well as the lengths of our training and test sets.\n\n# Get class names as a list\nclass_names = train_data.classes\nclass_names\n\n['pizza', 'steak', 'sushi']\n\n\n\n# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n\n{'pizza': 0, 'steak': 1, 'sushi': 2}\n\n\n\n# Check the lengths\nlen(train_data), len(test_data)\n\n(225, 75)\n\n\nNice! Looks like we’ll be able to use these to reference for later.\nHow about our images and labels?\nHow do they look?\nWe can index on our train_data and test_data Dataset’s to find samples and their target labels.\n\nimg, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n\nImage tensor:\ntensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],\n         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],\n         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],\n         ...,\n         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],\n\n        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],\n         [0.0706, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],\n         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],\n         ...,\n         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],\n         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],\n         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],\n\n        [[0.0941, 0.0902, 0.0902,  ..., 0.0196, 0.0196, 0.0196],\n         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],\n         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],\n         ...,\n         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1804],\n         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],\n         [0.1059, 0.1020, 0.1059,  ..., 0.1843, 0.1804, 0.1765]]])\nImage shape: torch.Size([3, 64, 64])\nImage datatype: torch.float32\nImage label: 0\nLabel datatype: &lt;class 'int'&gt;\n\n\nOur images are now in the form of a tensor (with shape [3, 64, 64]) and the labels are in the form of an integer relating to a specific class (as referenced by the class_to_idx attribute).\nHow about we plot a single image tensor using matplotlib?\nWe’ll first have to to permute (rearrange the order of its dimensions) so it’s compatible.\nRight now our image dimensions are in the format CHW (color channels, height, width) but matplotlib prefers HWC (height, width, color channels).\n\n# Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n\nOriginal shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nImage permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]\n\n\n\n\n\n\n\n\n\nNotice the image is now more pixelated (less quality).\nThis is due to it being resized from 512x512 to 64x64 pixels.\nThe intuition here is that if you think the image is harder to recognize what’s going on, chances are a model will find it harder to understand too.\n\n4.1 Turn loaded images into DataLoader’s\nWe’ve got our images as PyTorch Dataset’s but now let’s turn them into DataLoader’s.\nWe’ll do so using torch.utils.data.DataLoader.\nTurning our Dataset’s into DataLoader’s makes them iterable so a model can go through learn the relationships between samples and targets (features and labels).\nTo keep things simple, we’ll use a batch_size=1 and num_workers=1.\nWhat’s num_workers?\nGood question.\nIt defines how many subprocesses will be created to load your data.\nThink of it like this, the higher value num_workers is set to, the more compute power PyTorch will use to load your data.\nPersonally, I usually set it to the total number of CPUs on my machine via Python’s os.cpu_count().\nThis ensures the DataLoader recruits as many cores as possible to load data.\n\n참고: There are more parameters you can get familiar with using torch.utils.data.DataLoader in the PyTorch documentation.\n\n\n# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94fa0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94dc0&gt;)\n\n\nWonderful!\nNow our data is iterable.\nLet’s try it out and check the shapes.\n\nimg, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n\nImage shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n\n\nWe could now use these DataLoader’s with a training and testing loop to train a model.\nBut before we do, let’s look at another option to load images (or almost any other kind of data).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#option-2-loading-image-data-with-a-custom-dataset",
    "href": "04_pytorch_custom_datasets.html#option-2-loading-image-data-with-a-custom-dataset",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "5. Option 2: Loading Image Data with a Custom Dataset",
    "text": "5. Option 2: Loading Image Data with a Custom Dataset\nWhat if a pre-built Dataset creator like torchvision.datasets.ImageFolder() didn’t exist?\nOr one for your specific problem didn’t exist?\nWell, you could build your own.\nBut wait, what are the pros and cons of creating your own custom way to load Dataset’s?\n\n\n\n\n\n\n\nPros of creating a custom Dataset\nCons of creating a custom Dataset\n\n\n\n\nCan create a Dataset out of almost anything.\nEven though you could create a Dataset out of almost anything, it doesn’t mean it will work.\n\n\nNot limited to PyTorch pre-built Dataset functions.\nUsing a custom Dataset often results in writing more code, which could be prone to errors or performance issues.\n\n\n\nTo see this in action, let’s work towards replicating torchvision.datasets.ImageFolder() by subclassing torch.utils.data.Dataset (the base class for all Dataset’s in PyTorch).\nWe’ll start by importing the modules we need: * Python’s os for dealing with directories (our data is stored in directories). * Python’s pathlib for dealing with filepaths (each of our images has a unique filepath). * torch for all things PyTorch. * PIL’s Image class for loading images. * torch.utils.data.Dataset to subclass and create our own custom Dataset. * torchvision.transforms to turn our images into tensors. * Various types from Python’s typing module to add type hints to our code.\n\n참고: You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format you’d like it.\n\n\nimport os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n\nRemember how our instances of torchvision.datasets.ImageFolder() allowed us to use the classes and class_to_idx attributes?\n\n# Instance of torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n\n(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})\n\n\n\n5.1 Creating a helper function to get class names\nLet’s write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.\nTo do so, we’ll: 1. Get the class names using os.scandir() to traverse a target directory (ideally the directory is in standard image classification format). 2. Raise an error if the class names aren’t found (if this happens, there might be something wrong with the directory structure). 3. Turn the class names into a dictionary of numerical labels, one for each class.\nLet’s see a small example of step 1 before we write the full function.\n\n# Setup path for target directory\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Get the class names from the target directory\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n\nTarget directory: data\\pizza_steak_sushi\\train\nClass names found: ['pizza', 'steak', 'sushi']\n\n\nExcellent!\nHow about we turn it into a full function?\n\n# Make function to find classes in target directory\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folder names in a target directory.\n    \n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    \n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n\nLooking good!\nNow let’s test out our find_classes() function.\n\nfind_classes(train_dir)\n\n(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})\n\n\nWoohoo! Looking good!\n\n\n5.2 Create a custom Dataset to replicate ImageFolder\nNow we’re ready to build our own custom Dataset.\nWe’ll build one to replicate the functionality of torchvision.datasets.ImageFolder().\nThis will be good practice, plus, it’ll reveal a few of the required steps to make your own custom Dataset.\nIt’ll be a fair bit of a code… but nothing we can’t handle!\nLet’s break it down: 1. Subclass torch.utils.data.Dataset. 2. Initialize our subclass with a targ_dir parameter (the target data directory) and transform parameter (so we have the option to transform our data if needed). 3. Create several attributes for paths (the paths of our target images), transform (the transforms we might like to use, this can be None), classes and class_to_idx (from our find_classes() function). 4. Create a function to load images from file and return them, this could be using PIL or torchvision.io (for input/output of vision data). 5. Overwrite the __len__ method of torch.utils.data.Dataset to return the number of samples in the Dataset, this is recommended but not required. This is so you can call len(Dataset). 6. Overwrite the __getitem__ method of torch.utils.data.Dataset to return a single sample from the Dataset, this is required.\nLet’s do it!\n\n# Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n\nWoah! A whole bunch of code to load in our images.\nThis is one of the downsides of creating your own custom Dataset’s.\nHowever, now we’ve written it once, we could move it into a .py file such as data_loader.py along with some other helpful data functions and reuse it later on.\nBefore we test out our new ImageFolderCustom class, let’s create some transforms to prepare our images.\n\n# Augment train data\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n\nNow comes the moment of truth!\nLet’s turn our training images (contained in train_dir) and our testing images (contained in test_dir) into Dataset’s using our own ImageFolderCustom class.\n\ntrain_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n\n(&lt;__main__.ImageFolderCustom at 0x1fd2cf886d0&gt;,\n &lt;__main__.ImageFolderCustom at 0x1fd2cf5a0d0&gt;)\n\n\nHmm… no errors, did it work?\nLet’s try calling len() on our new Dataset’s and find the classes and class_to_idx attributes.\n\nlen(train_data_custom), len(test_data_custom)\n\n(225, 75)\n\n\n\ntrain_data_custom.classes\n\n['pizza', 'steak', 'sushi']\n\n\n\ntrain_data_custom.class_to_idx\n\n{'pizza': 0, 'steak': 1, 'sushi': 2}\n\n\nlen(test_data_custom) == len(test_data) and len(test_data_custom) == len(test_data) Yes!!!\nIt looks like it worked.\nWe could check for equality with the Dataset’s made by the torchvision.datasets.ImageFolder() class too.\n\n# Check for equality amongst our custom Dataset and ImageFolder Dataset\nprint((len(train_data_custom) == len(train_data)) & (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n\nTrue\nTrue\nTrue\n\n\nHo ho!\nLook at us go!\nThree True’s!\nYou can’t get much better than that.\nHow about we take it up a notch and plot some random images to test our __getitem__ override?\n\n\n5.3 Create a function to display random images\nYou know what time it is!\nTime to put on our data explorer’s hat and visualize, visualize, visualize!\nLet’s create a helper function called display_random_images() that helps us visualize images in our Dataset's.\nSpecifically, it’ll: 1. Take in a Dataset and a number of other parameters such as classes (the names of our target classes), the number of images to display (n) and a random seed. 2. To prevent the display getting out of hand, we’ll cap n at 10 images. 3. Set the random seed for reproducible plots (if seed is set). 4. Get a list of random sample indexes (we can use Python’s random.sample() for this) to plot. 5. Setup a matplotlib plot. 6. Loop through the random sample indexes found in step 4 and plot them with matplotlib. 7. Make sure the sample images are of shape HWC (height, width, color channels) so we can plot them.\n\n# 1. Take in a Dataset as well as a list of class names\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n\nWhat a good looking function!\nLet’s test it out first with the Dataset we created with torchvision.datasets.ImageFolder().\n\n# Display random images from ImageFolder created Dataset\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n\n\n\n\n\n\n\n\nAnd now with the Dataset we created with our own ImageFolderCustom.\n\n# Display random images from ImageFolderCustom Dataset\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n\nFor display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\n\n\n\n\n\n\n\n\n\nNice!!!\nLooks like our ImageFolderCustom is working just as we’d like it to.\n\n\n5.4 Turn custom loaded images into DataLoader’s\nWe’ve got a way to turn our raw images into Dataset’s (features mapped to labels or X’s mapped to y’s) through our ImageFolderCustom class.\nNow how could we turn our custom Dataset’s into DataLoader’s?\nIf you guessed by using torch.utils.data.DataLoader(), you’d be right!\nBecause our custom Dataset’s subclass torch.utils.data.Dataset, we can use them directly with torch.utils.data.DataLoader().\nAnd we can do using very similar steps to before except this time we’ll be using our custom created Dataset’s.\n\n# Turn train and test custom Dataset's into DataLoader's\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cfabf10&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cfabb80&gt;)\n\n\nDo the shapes of the samples look the same?\n\n# Get image and label from custom DataLoader\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n\nImage shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n\n\nThey sure do!\nLet’s now take a lot at some other forms of data transforms.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#other-forms-of-transforms-data-augmentation",
    "href": "04_pytorch_custom_datasets.html#other-forms-of-transforms-data-augmentation",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "6. Other forms of transforms (data augmentation)",
    "text": "6. Other forms of transforms (data augmentation)\nWe’ve seen a couple of transforms on our data already but there’s plenty more.\nYou can see them all in the torchvision.transforms documentation.\nThe purpose of tranforms is to alter your images in some way.\nThat may be turning your images into a tensor (as we’ve seen before).\nOr cropping it or randomly erasing a portion or randomly rotating them.\nDoing this kinds of transforms is often referred to as data augmentation.\nData augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.\nTraining a model on this artificially altered dataset hopefully results in a model that is capable of better generalization (the patterns it learns are more robust to future unseen examples).\nYou can see many different examples of data augmentation performed on images using torchvision.transforms in PyTorch’s Illustration of Transforms example.\nBut let’s try one out ourselves.\nMachine learning is all about harnessing the power of randomness and research shows that random transforms (like transforms.RandAugment() and transforms.TrivialAugmentWide()) generally perform better than hand-picked transforms.\nThe idea behind TrivialAugment is… well, trivial.\nYou have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).\nThe PyTorch team even used TrivialAugment it to train their latest state-of-the-art vision models.\n\n\n\ntrivial augment data augmentation being used for PyTorch state of the art training\n\n\nTrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.\nHow about we test it out on some of our own images?\nThe main parameter to pay attention to in transforms.TrivialAugmentWide() is num_magnitude_bins=31.\nIt defines how much of a range an intensity value will be picked to apply a certain transform, 0 being no range and 31 being maximum range (highest chance for highest intensity).\nWe can incorporate transforms.TrivialAugmentWide() into transforms.Compose().\n\nfrom torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 & 1\n])\n\n# Don't need to perform augmentation on the test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n\n\n참고: You usually don’t perform data augmentation on the test set. The idea of data augmentation is to to artificially increase the diversity of the training set to better predict on the testing set.\nHowever, you do need to make sure your test set images are transformed to tensors. We size the test images to the same size as our training images too, however, inference can be done on different size images if necessary (though this may alter performance).\n\nBeautiful, now we’ve got a training transform (with data augmentation) and test transform (without data augmentation).\nLet’s test our data augmentation out!\n\n# Get all image paths\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Plot random images\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry running the cell above a few times and seeing how the original image changes as it goes through the transform.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#model-0-tinyvgg-without-data-augmentation",
    "href": "04_pytorch_custom_datasets.html#model-0-tinyvgg-without-data-augmentation",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "7. Model 0: TinyVGG without data augmentation",
    "text": "7. Model 0: TinyVGG without data augmentation\nAlright, we’ve seen how to turn our data from images in folders to transformed tensors.\nNow let’s construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.\nTo begin, we’ll start with a simple transform, only resizing the images to (64, 64) and turning them into tensors.\n\n7.1 Creating transforms and loading data for Model 0\n\n# Create simple transform\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n\nExcellent, now we’ve got a simple transform, let’s: 1. Load the data, turning each of our training and test folders first into a Dataset with torchvision.datasets.ImageFolder() 2. Then into a DataLoader using torch.utils.data.DataLoader(). * We’ll set the batch_size=32 and num_workers to as many CPUs on our machine (this will depend on what machine you’re using).\n\n# 1. Load and transform data\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Turn data into DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Setup batch size and number of workers \nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Create DataLoader's\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n\nCreating DataLoader's with batch size 32 and 16 workers.\n\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2ce5c4f0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd1d0e10d0&gt;)\n\n\nDataLoader’s created!\nLet’s build a model.\n\n\n7.2 Create TinyVGG model class\nIn notebook 03, we used the TinyVGG model from the CNN Explainer website.\nLet’s recreate the same model, except this time we’ll be using color images instead of grayscale (in_channels=3 instead of in_channels=1 for RGB pixels).\n\nclass TinyVGG(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)\n\n\n\n참고: One of the ways to speed up deep learning models computing on a GPU is to leverage operator fusion.\nThis means in the forward() method in our model above, instead of calling a layer block and reassigning x every time, we call each block in succession (see the final line of the forward() method in the model above for an example).\nThis saves the time spent reassigning x (memory heavy) and focuses on only computing on x.\nSee Making Deep Learning Go Brrrr From First Principles by Horace He for more ways on how to speed up machine learning models.\n\nNow that’s a nice looking model!\nHow about we test it out with a forward pass on a single image?\n\n\n7.3 Try a forward pass on a single image (to test the model)\nA good way to test a model is to do a forward pass on a single piece of data.\nIt’s also handy way to test the input and output shapes of our different layers.\nTo do a forward pass on a single image, let’s: 1. Get a batch of images and labels from the DataLoader. 2. Get a single image from the batch and unsqueeze() the image so it has a batch size of 1 (so its shape fits the model). 3. Perform inference on a single image (making sure to send the image to the target device). 4. Print out what’s happening and convert the model’s raw output logits to prediction probabilities with torch.softmax() (since we’re working with multi-class data) and convert the prediction probabilities to prediction labels with torch.argmax().\n\n# 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n\nSingle image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[0.0578, 0.0635, 0.0352]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3352, 0.3371, 0.3277]], device='cuda:0')\n\nOutput prediction label:\ntensor([1], device='cuda:0')\n\nActual label:\n2\n\n\nWonderful, it looks like our model is outputting what we’d expect it to output.\nYou can run the cell above a few times and each time have a different image be predicted on.\nAnd you’ll probably notice the predictions are often wrong.\nThis is to be expected because the model hasn’t been trained yet and it’s essentially guessing using random weights.\n\n\n7.4 Use torchinfo to get an idea of the shapes going through our model\nPrinting out our model with print(model) gives us an idea of what’s going on with our model.\nAnd we can print out the shapes of our data throughout the forward() method.\nHowever, a helpful way to get information from our model is to use torchinfo.\ntorchinfo comes with a summary() method that takes a PyTorch model as well as an input_shape and returns what happens as a tensor moves through your model.\n\n참고: If you’re using Google Colab, you’ll need to install torchinfo.\n\n\n# Install torchinfo if it's not available, import it if it is\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size \n\nCollecting torchinfo\n  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.6.5\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  --                        --\n├─Sequential: 1-1                        [1, 10, 32, 32]           --\n│    └─Conv2d: 2-1                       [1, 10, 64, 64]           280\n│    └─ReLU: 2-2                         [1, 10, 64, 64]           --\n│    └─Conv2d: 2-3                       [1, 10, 64, 64]           910\n│    └─ReLU: 2-4                         [1, 10, 64, 64]           --\n│    └─MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n├─Sequential: 1-2                        [1, 10, 16, 16]           --\n│    └─Conv2d: 2-6                       [1, 10, 32, 32]           910\n│    └─ReLU: 2-7                         [1, 10, 32, 32]           --\n│    └─Conv2d: 2-8                       [1, 10, 32, 32]           910\n│    └─ReLU: 2-9                         [1, 10, 32, 32]           --\n│    └─MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n├─Sequential: 1-3                        [1, 3]                    --\n│    └─Flatten: 2-11                     [1, 2560]                 --\n│    └─Linear: 2-12                      [1, 3]                    7,683\n==========================================================================================\nTotal params: 10,693\nTrainable params: 10,693\nNon-trainable params: 0\nTotal mult-adds (M): 6.75\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.91\n==========================================================================================\n\n\nNice!\nThe output of torchinfo.summary() gives us a whole bunch of information about our model.\nSuch as Total params, the total number of parameters in our model, the Estimated Total Size (MB) which is the size of our model.\nYou can also see the change in input and output shapes as data of a certain input_size moves through our model.\nRight now, our parameter numbers and total model size is low.\nThis because we’re starting with a small model.\nAnd if we need to increase its size later, we can.\n\n\n7.5 Create train & test loop functions\nWe’ve got data and we’ve got a model.\nNow let’s make some training and test loop functions to train our model on the training data and evaluate our model on the testing data.\nAnd to make sure we can use these the training and testing loops again, we’ll functionize them.\nSpecifically, we’re going to make three functions: 1. train_step() - takes in a model, a DataLoader, a loss function and an optimizer and trains the model on the DataLoader. 2. test_step() - takes in a model, a DataLoader and a loss function and evaluates the model on the DataLoader. 3. train() - performs 1. and 2. together for a given number of epochs and returns a results dictionary.\n\n참고: We covered the steps in a PyTorch opimization loop in notebook 01, as well as theUnofficial PyTorch Optimization Loop Song and we’ve built similar functions in notebook 03.\n\nLet’s start by building train_step().\nBecause we’re dealing with batches in the DataLoader’s, we’ll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them.\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\nWoohoo! train_step() function done.\nNow let’s do the same for the test_step() function.\nThe main difference here will be the test_step() won’t take in an optimizer and therefore won’t perform gradient descent.\nBut since we’ll be doing inference, we’ll make sure to turn on the torch.inference_mode() context manager for making predictions.\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n\nExcellent!\n\n\n7.6 Creating a train() function to combine train_step() and test_step()\nNow we need a way to put our train_step() and test_step() functions together.\nTo do so, we’ll package them up in a train() function.\nThis function will train the model as well as evaluate it.\nSpecificially, it’ll: 1. Take in a model, a DataLoader for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for. 2. Create an empty results dictionary for train_loss, train_acc, test_loss and test_acc values (we can fill this up as training goes on). 3. Loop through the training and test step functions for a number of epochs. 4. Print out what’s happening at the end of each epoch. 5. Update the empty results dictionary with the updated metrics each epoch. 6. Return the filled\nTo keep track of the number of epochs we’ve been through, let’s import tqdm from tqdm.auto (tqdm is one of the most popular progress bar libraries for Python and tqdm.auto automatically decides what kind of progress bar is best for your computing environment, e.g. Jupyter Notebook vs. Python script).\n\nfrom tqdm.auto import tqdm\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n\n\n\n7.7 Train and Evaluate Model 0\nAlright, alright, alright we’ve got all of the ingredients we need to train and evaluate our model.\nTime to put our TinyVGG model, DataLoader’s and train() function together to see if we can build a model capable of discerning between pizza, steak and sushi!\nLet’s recreate model_0 (we don’t need to but we will for completeness) then call our train() function passing in the necessary parameters.\nTo keep our experiments quick, we’ll train our model for 5 epochs (though you could increase this if you want).\nAs for an optimizer and loss function, we’ll use torch.nn.CrossEntropyLoss() (since we’re working with multi-class classification data) and torch.optim.Adam() with a learning rate of 1e-3 respecitvely.\nTo see how long things take, we’ll import Python’s timeit.default_timer() method to calculate the training time.\n\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n\n\n\nEpoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1360 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0847 | train_acc: 0.4258 | test_loss: 1.1620 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.1157 | train_acc: 0.2930 | test_loss: 1.1695 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.0955 | train_acc: 0.4141 | test_loss: 1.1380 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.0985 | train_acc: 0.2930 | test_loss: 1.1420 | test_acc: 0.1979\nTotal training time: 65.122 seconds\n\n\nHmm…\nIt looks like our model performed pretty poorly.\nBut that’s okay for now, we’ll keep persevering.\nWhat are some ways you could potentially improve it?\n\n참고: Check out the Improving a model (from a model perspective) section in notebook 02 for ideas on improving our TinyVGG model.\n\n\n\n7.8 Plot the loss curves of Model 0\nFrom the print outs of our model_0 training, it didn’t look like it did too well.\nBut we can further evaluate it by plotting the model’s loss curves.\nLoss curves show the model’s results over time.\nAnd they’re a great way to see how your model performs on different datasets (e.g. training and test).\nLet’s create a function to plot the values in our model_0_results dictionary.\n\n# Check the model_0_results keys\nmodel_0_results.keys()\n\ndict_keys(['train_loss', 'train_acc', 'test_loss', 'test_acc'])\n\n\nWe’ll need to extract each of these keys and turn them into a plot.\n\ndef plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n\nOkay, let’s test our plot_loss_curves() function out.\n\nplot_loss_curves(model_0_results)\n\n\n\n\n\n\n\n\nWoah.\nLooks like things are all over the place…\nBut we kind of knew that because our model’s print out results during training didn’t show much promise.\nYou could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#what-should-an-ideal-loss-curve-look-like",
    "href": "04_pytorch_custom_datasets.html#what-should-an-ideal-loss-curve-look-like",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "8. What should an ideal loss curve look like?",
    "text": "8. What should an ideal loss curve look like?\nLooking at training and test loss curves is a great way to see if your model is overfitting.\nAn overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.\nIf your training loss is far lower than your test loss, your model is overfitting.\nAs in, it’s learning the patterns in the training too well and those patterns aren’t generalizing to the test data.\nThe other side is when your training and test loss are not as low as you’d like, this is considered underfitting.\nThe ideal position for a training and test loss curve is for them to line up closely with each other.\n\nLeft: If your training and test loss curves aren’t as low as you’d like, this is considered underfitting. Middle:* When your test/validation loss is higher than your training loss this is considered overfitting. Right: The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Google’s Interpreting Loss Curves guide.*\n\n8.1 How to deal with overfitting\nSince the main problem with overfitting is that you’re model is fitting the training data too well, you’ll want to use techniques to “reign it in”.\nA common technique of preventing overfitting is known as regularization.\nI like to think of this as “making our models more regular”, as in, capable of fitting more kinds of data.\nLet’s discuss a few methods to prevent overfitting.\n\n\n\nMethod to prevent overfitting\nWhat is it?\n\n\n\n\nGet more data\nHaving more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples.\n\n\nSimplify your model\nIf the current model is already overfitting the training data, it may be too complicated of a model. This means it’s learning the patterns of the data too well and isn’t able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer.\n\n\nUse data augmentation\nData augmentation manipulates the training data in a way so that’s harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data.\n\n\nUse transfer learning\nTransfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.\n\n\nUse dropout layers\nDropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See torch.nn.Dropout() for more.\n\n\nUse learning rate decay\nThe idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to convergence, the smaller you’ll want your weight updates to be.\n\n\nUse early stopping\nEarly stopping stops model training before it begins to overfit. As in, say the model’s loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior).\n\n\n\nThere are more methods for dealing with overfitting but these are some of the main ones.\nAs you start to build more and more deep models, you’ll find because deep learnings are so good at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning.\n\n\n8.2 과소적합 해결 방법\n모델이 과소적합(underfitting) 상태라면 훈련 및 테스트 세트에서 예측 능력이 떨어지는 것으로 간주됩니다.\n본질적으로 과소적합 모델은 손실 값을 원하는 수준으로 줄이는 데 실패합니다.\n현재 손실 곡선을 보았을 때, 우리의 TinyVGG 모델인 model_0은 데이터에 과소적합된 것으로 보입니다.\n과소적합을 처리하는 주요 아이디어는 모델의 예측 능력을 높이는 것입니다.\n이를 위한 몇 가지 방법이 있습니다.\n\n\n\n\n\n\n\n과소적합 방지 방법\n설명\n\n\n\n\n모델에 더 많은 레이어/유닛 추가\n모델이 과소적합 상태라면, 예측력을 갖추기 위해 필요한 데이터의 패턴/가중치/표현을 학습할 능력이 부족할 수 있습니다. 모델에 더 많은 예측 능력을 추가하는 한 가지 방법은 은닉 레이어의 수나 해당 레이어 내의 유닛 수를 늘리는 것입니다.\n\n\n학습률 조정\n아마 모델의 학습률이 처음부터 너무 높을 수 있습니다. 그래서 에포크마다 가중치를 너무 많이 업데이트하려고 시도하여 결국 아무것도 배우지 못하게 됩니다. 이 경우 학습률을 낮추고 어떤 일이 일어나는지 지켜볼 수 있습니다.\n\n\n전이 학습 사용\n전이 학습은 과적합과 과소적합 모두를 방지할 수 있습니다. 이미 작동하는 모델의 패턴을 가져와 자신의 문제에 맞게 조정하는 것입니다.\n\n\n더 오래 훈련\n때로는 모델이 데이터의 표현을 학습하는 데 더 많은 시간이 필요할 수 있습니다. 소규모 실험에서 모델이 아무것도 학습하지 못한다면, 더 많은 에포크 동안 훈련시키는 것이 더 나은 성능으로 이어질 수 있습니다.\n\n\n정규화 줄이기\n과적합을 너무 많이 방지하려고 노력하다 보니 모델이 과소적합되었을 수 있습니다. 정규화 기술을 조금 줄이면 모델이 데이터에 더 잘 맞도록 도울 수 있습니다.\n\n\n\n\n\n8.3 과적합과 과소적합 사이의 균형\n위에서 논의한 방법 중 어떤 것도 만능 해결책은 아니며, 항상 작동하는 것은 아닙니다.\n과적합과 과소적합을 방지하는 것은 머신러닝 연구에서 아마도 가장 활발한 분야일 것입니다.\n모든 사람이 자신의 모델이 더 잘 맞기를 원하지만(과소적합 감소), 실세계에서 일반화되지 못할 정도로 잘 맞기를 원하지는 않기 때문입니다(과적합 감소).\n과적합과 과소적합 사이에는 미세한 경계가 있습니다.\n각각이 너무 지나치면 다른 쪽을 유발할 수 있기 때문입니다.\n전이 학습은 자신의 문제에 대한 과적합과 과소적합 문제를 모두 다룰 때 가장 강력한 기술 중 하나일 것입니다.\n서로 다른 과적합 및 과소적합 기술을 직접 하나하나 만들 필요 없이, 전이 학습을 사용하면 자신의 문제 공간과 유사한 문제 공간에서 이미 작동하는 모델(예: paperswithcode.com/sota 또는 Hugging Face 모델에서 제공하는 모델)을 가져와 자신의 데이터셋에 적용할 수 있습니다.\n나중 노트북에서 전이 학습의 위력을 보게 될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#모델-1-데이터-증강을-포함한-tinyvgg",
    "href": "04_pytorch_custom_datasets.html#모델-1-데이터-증강을-포함한-tinyvgg",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "9. 모델 1: 데이터 증강을 포함한 TinyVGG",
    "text": "9. 모델 1: 데이터 증강을 포함한 TinyVGG\n이제 다른 모델을 시도해 볼 시간입니다!\n이번에는 데이터를 로드하고 데이터 증강을 사용하여 결과가 개선되는지 확인해 보겠습니다.\n먼저 transforms.TrivialAugmentWide() 뿐만 아니라 이미지 크기 조정 및 텐서 변환을 포함하도록 훈련용 변환을 구성하겠습니다.\n테스트용 변환도 데이터 증강만 제외하고 동일하게 수행하겠습니다.\n\n9.1 데이터 증강을 포함한 변환 생성\n\n# TrivialAugment를 포함한 훈련 변환 생성\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# 테스트 변환 생성 (데이터 증강 미포함)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n\n멋지네요!\n이제 torchvision.datasets.ImageFolder()를 사용하여 이미지를 Dataset으로 변환한 다음, torch.utils.data.DataLoader()를 사용하여 DataLoader로 변환하겠습니다.\n\n\n9.2 훈련 및 테스트용 Dataset 및 DataLoader 생성\n훈련용 Dataset은 train_transform_trivial_augment를 사용하고 테스트용 Dataset은 test_transform을 사용하도록 하겠습니다.\n\n# Turn image folders into Datasets\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n\n(Dataset ImageFolder\n     Number of datapoints: 225\n     Root location: data\\pizza_steak_sushi\\train\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n                ToTensor()\n            ),\n Dataset ImageFolder\n     Number of datapoints: 75\n     Root location: data\\pizza_steak_sushi\\test\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                ToTensor()\n            ))\n\n\nAnd we’ll make DataLoader’s with a batch_size=32 and with num_workers set to the number of CPUs available on our machine (we can get this using Python’s os.cpu_count()).\n\n# Turn Datasets into DataLoader's\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2d0531c0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94dc0&gt;)\n\n\n\n\n9.3 Construct and train Model 1\nData loaded!\nNow to build our next model, model_1, we can reuse our TinyVGG class from before.\nWe’ll make sure to send it to the target device.\n\n# Create model_1 and send it to the target device\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)\n\n\nModel ready!\nTime to train!\nSince we’ve already got functions for the training loop (train_step()) and testing loop (test_step()) and a function to put them together in train(), let’s reuse those.\nWe’ll use the same setup as model_0 with only the train_dataloader parameter varying: * Train for 5 epochs. * Use train_dataloader=train_dataloader_augmented as the training data in train(). * Use torch.nn.CrossEntropyLoss() as the loss function (since we’re working with multi-class classification). * Use torch.optim.Adam() with lr=0.001 as the learning rate as the optimizer.\n\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n\n\n\nEpoch: 1 | train_loss: 1.1074 | train_acc: 0.2461 | test_loss: 1.1058 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0791 | train_acc: 0.4258 | test_loss: 1.1382 | test_acc: 0.2604\nEpoch: 3 | train_loss: 1.0804 | train_acc: 0.4258 | test_loss: 1.1683 | test_acc: 0.2604\nEpoch: 4 | train_loss: 1.1287 | train_acc: 0.3047 | test_loss: 1.1626 | test_acc: 0.2604\nEpoch: 5 | train_loss: 1.0884 | train_acc: 0.4258 | test_loss: 1.1481 | test_acc: 0.2604\nTotal training time: 67.543 seconds\n\n\nHmm…\nIt doesn’t look like our model performed very well again.\nLet’s check out its loss curves.\n\n\n9.4 Plot the loss curves of Model 1\nSince we’ve got the results of model_1 saved in a results dictionary, model_1_results, we can plot them using plot_loss_curves().\n\nplot_loss_curves(model_1_results)\n\n\n\n\n\n\n\n\nWow…\nThese don’t look very good either…\nIs our model underfitting or overfitting?\nOr both?\nIdeally we’d like it have higher accuracy and lower loss right?\nWhat are some methods you could try to use to achieve these?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#compare-model-results",
    "href": "04_pytorch_custom_datasets.html#compare-model-results",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "10. Compare model results",
    "text": "10. Compare model results\nEven though our models our performing quite poorly, we can still write code to compare them.\nLet’s first turn our model results in pandas DataFrames.\n\nimport pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\n\n\n\n\n0\n1.107832\n0.257812\n1.136025\n0.260417\n\n\n1\n1.084726\n0.425781\n1.161953\n0.197917\n\n\n2\n1.115656\n0.292969\n1.169479\n0.197917\n\n\n3\n1.095543\n0.414062\n1.137993\n0.197917\n\n\n4\n1.098464\n0.292969\n1.142002\n0.197917\n\n\n\n\n\n\n\n이제 matplotlib을 사용하여 model_0과 model_1의 결과를 함께 시각화하는 플로팅 코드를 작성해 보겠습니다.\n\n# Setup a plot \nplt.figure(figsize=(15, 10))\n\n# Get number of epochs\nepochs = range(len(model_0_df))\n\n# Plot train loss\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test loss\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot train accuracy\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test accuracy\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\nIt looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).\nIf you built model_2, what would you do differently to try and improve performance?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#make-a-prediction-on-a-custom-image",
    "href": "04_pytorch_custom_datasets.html#make-a-prediction-on-a-custom-image",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "11. Make a prediction on a custom image",
    "text": "11. Make a prediction on a custom image\nIf you’ve trained a model on a certain dataset, chances are you’d like to make a prediction on on your own custom data.\nIn our case, since we’ve trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images?\nTo do so, we can load an image and then preprocess it in a way that matches the type of data our model was trained on.\nIn other words, we’ll have to convert our own custom image to a tensor and make sure it’s in the right datatype before passing it to our model.\nLet’s start by downloading a custom image.\nSince our model predicts whether an image contains pizza, steak or sushi, let’s download a photo of my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub.\nWe download the image using Python’s requests module.\n\n참고: If you’re using Google Colab, you can also upload an image to the current session by going to the left hand side menu -&gt; Files -&gt; Upload to session storage. Beware though, this image will delete when your Google Colab session ends.\n\n\n# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\ndata\\04-pizza-dad.jpeg already exists, skipping download.\n\n\n\n11.1 Loading in a custom image with PyTorch\nExcellent!\nLooks like we’ve got a custom image downloaded and ready to go at data/04-pizza-dad.jpeg.\nTime to load it in.\nPyTorch’s torchvision has several input and output (“IO” or “io” for short) methods for reading and writing images and video in torchvision.io.\nSince we want to load in an image, we’ll use torchvision.io.read_image().\nThis method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale torch.Tensor with values of datatype uint8 in range [0, 255].\nLet’s try it out.\n\nimport torchvision\n\n# Read in custom image\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n\nCustom image tensor:\ntensor([[[154, 175, 181,  ...,  21,  18,  14],\n         [146, 167, 180,  ...,  21,  18,  15],\n         [124, 146, 171,  ...,  18,  17,  15],\n         ...,\n         [ 72,  59,  45,  ..., 152, 150, 148],\n         [ 64,  55,  41,  ..., 150, 147, 144],\n         [ 64,  60,  46,  ..., 149, 146, 143]],\n\n        [[171, 189, 193,  ...,  22,  19,  15],\n         [163, 181, 194,  ...,  22,  19,  16],\n         [141, 163, 185,  ...,  19,  18,  16],\n         ...,\n         [ 55,  42,  28,  ..., 106, 104, 102],\n         [ 47,  38,  24,  ..., 108, 105, 102],\n         [ 47,  43,  29,  ..., 107, 104, 101]],\n\n        [[117, 138, 145,  ...,  17,  14,  10],\n         [109, 130, 145,  ...,  17,  14,  11],\n         [ 87, 111, 136,  ...,  14,  13,  11],\n         ...,\n         [ 35,  22,   8,  ...,  54,  52,  50],\n         [ 27,  18,   4,  ...,  50,  47,  44],\n         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.uint8\n\n\nNice! Looks like our image is in tensor format, however, is this image format compatible with our model?\nOur custom_image tensor is of datatype torch.uint8 and its values are between [0, 255].\nBut our model takes image tensors of datatype torch.float32 and with values between [0, 1].\nSo before we use our custom image with our model, we’ll need to convert it to the same format as the data our model is trained on.\nIf we don’t do this, our model will error.\n\n# Try to make a prediction on image in uint8 format (this will error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [65], in &lt;cell line: 3&gt;()\n      2 model_1.eval()\n      3 with torch.inference_mode():\n----&gt; 4     model_1(custom_image_uint8.to(device))\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447, in Conv2d.forward(self, input)\n    446 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 447     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n    439 if self.padding_mode != 'zeros':\n    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    441                     weight, bias, self.stride,\n    442                     _pair(0), self.dilation, self.groups)\n--&gt; 443 return F.conv2d(input, weight, bias, self.stride,\n    444                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same\n\n\n\nIf we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same\n\nLet’s fix this by converting our custom image to the same datatype as what our model was trained on (torch.float32).\n\n# Load in custom image and convert the tensor values to float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divide the image pixel values by 255 to get them between [0, 1]\ncustom_image = custom_image / 255. \n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n\nCustom image tensor:\ntensor([[[0.6039, 0.6863, 0.7098,  ..., 0.0824, 0.0706, 0.0549],\n         [0.5725, 0.6549, 0.7059,  ..., 0.0824, 0.0706, 0.0588],\n         [0.4863, 0.5725, 0.6706,  ..., 0.0706, 0.0667, 0.0588],\n         ...,\n         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],\n         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],\n         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],\n\n        [[0.6706, 0.7412, 0.7569,  ..., 0.0863, 0.0745, 0.0588],\n         [0.6392, 0.7098, 0.7608,  ..., 0.0863, 0.0745, 0.0627],\n         [0.5529, 0.6392, 0.7255,  ..., 0.0745, 0.0706, 0.0627],\n         ...,\n         [0.2157, 0.1647, 0.1098,  ..., 0.4157, 0.4078, 0.4000],\n         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4118, 0.4000],\n         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],\n\n        [[0.4588, 0.5412, 0.5686,  ..., 0.0667, 0.0549, 0.0392],\n         [0.4275, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],\n         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],\n         ...,\n         [0.1373, 0.0863, 0.0314,  ..., 0.2118, 0.2039, 0.1961],\n         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1843, 0.1725],\n         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.float32\n\n\n\n\n11.2 Predicting on custom images with a trained PyTorch model\nBeautiful, it looks like our image data is now in the same format our model was trained on.\nExcept for one thing…\nIt’s shape.\nOur model was trained on images with shape [3, 64, 64], whereas our custom image is currently [3, 4032, 3024].\nHow could we make sure our custom image is the same shape as the images our model was trained on?\nAre there any torchvision.transforms that could help?\nBefore we answer that question, let’s plot the image with matplotlib to make sure it looks okay, remember we’ll have to permute the dimensions from CHW to HWC to suit matplotlib’s requirements.\n\n# Plot custom image\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n\n\n\n\n\n\n\n\nTwo thumbs up!\nNow how could we get our image to be the same size as the images our model was trained on?\nOne way to do so is with torchvision.transforms.Resize().\nLet’s compose a transform pipeline to do so.\n\n# Create transform pipleine to resize image\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transform target image\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Print out original shape and new shape\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n\nOriginal shape: torch.Size([3, 4032, 3024])\nNew shape: torch.Size([3, 64, 64])\n\n\nWoohoo!\nLet’s finally make a prediction on our own custom image.\n\nmodel_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [69], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447, in Conv2d.forward(self, input)\n    446 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 447     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n    439 if self.padding_mode != 'zeros':\n    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    441                     weight, bias, self.stride,\n    442                     _pair(0), self.dilation, self.groups)\n--&gt; 443 return F.conv2d(input, weight, bias, self.stride,\n    444                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)\n\n\n\nOh my goodness…\nDespite our preparations our custom image and model are on different devices.\nAnd we get the error:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)\n\nLet’s fix that by putting our custom_image_transformed on the target device.\n\nmodel_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [70], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed.to(device))\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     42 x = self.conv_block_2(x)\n     43 # print(x.shape)\n---&gt; 44 x = self.classifier(x)\n     45 # print(x.shape)\n     46 return x\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103, in Linear.forward(self, input)\n    102 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 103     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)\n\n\n\nWhat now?\nIt looks like we’re getting a shape error.\nWhy might this be?\nWe converted our custom image to be the same size as the images our model was trained on…\nOh wait…\nThere’s one dimension we forgot about.\nThe batch size.\nOur model expects image tensors with a batch size dimension at the start (NCHW where N is the batch size).\nExcept our custom image is currently only CHW.\nWe can add a batch size dimension using torch.unsqueeze(dim=0) to add an extra dimension our image and finally make a prediction.\nEssentially we’ll be telling our model to predict on a single image (an image with a batch_size of 1).\n\nmodel_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n\nCustom image transformed shape: torch.Size([3, 64, 64])\nUnsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n\n\nYes!!!\nIt looks like it worked!\n\n참고: What we’ve just gone through are three of the classical and most common deep learning and PyTorch issues: 1. Wrong datatypes - our model expects torch.float32 where our original custom image was uint8. 2. Wrong device - our model was on the target device (in our case, the GPU) whereas our target data hadn’t been moved to the target device yet. 3. Wrong shapes - our model expected an input image of shape [N, C, H, W] or [batch_size, color_channels, height, width] whereas our custom image tensor was of shape [color_channels, height, width].\nKeep in mind, these errors aren’t just for predicting on custom images.\nThey will be present with almost every kind of data type (text, audio, structured data) and problem you work with.\n\nNow let’s take a look at our model’s predictions.\n\ncustom_image_pred\n\ntensor([[ 0.1161,  0.0213, -0.1422]], device='cuda:0')\n\n\nAlright, these are still in logit form (the raw outputs of a model are called logits).\nLet’s convert them from logits -&gt; prediction probabilities -&gt; prediction labels.\n\n# Print out prediction logits\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convert prediction probabilities -&gt; prediction labels\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n\nPrediction logits: tensor([[ 0.1161,  0.0213, -0.1422]], device='cuda:0')\nPrediction probabilities: tensor([[0.3729, 0.3392, 0.2880]], device='cuda:0')\nPrediction label: tensor([0], device='cuda:0')\n\n\nAlright!\nLooking good.\nBut of course our prediction label is still in index/tensor form.\nWe can convert it to a string class name prediction by indexing on the class_names list.\n\n# Find the predicted label\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n\n'pizza'\n\n\nWow.\nIt looks like the model gets the prediction right, even though it was performing poorly based on our evaluation metrics.\n\n참고: The model in its current form will predict “pizza”, “steak” or “sushi” no matter what image it’s given. If you wanted your model to predict on a different class, you’d have to train it to do so.\n\nBut if we check the custom_image_pred_probs, we’ll notice that the model gives almost equal weight (the values are similar) to every class.\n\n# The values of the prediction probabilities are quite similar\ncustom_image_pred_probs\n\ntensor([[0.3729, 0.3392, 0.2880]], device='cuda:0')\n\n\nHaving prediction probabilities this similar could mean a couple of things: 1. The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi). 2. The model doesn’t really know what it wants to predict and is in turn just assigning similar values to each of the classes.\nOur case is number 2, since our model is poorly trained, it is basically guessing the prediction.\n\n\n11.3 Putting custom image prediction together: building a function\nDoing all of the above steps every time you’d like to make a prediction on a custom image would quickly become tedious.\nSo let’s put them all together in a function we can easily use over and over again.\nSpecifically, let’s make a function that: 1. Takes in a target image path and converts to the right datatype for our model (torch.float32). 2. Makes sure the target image pixel values are in the range [0, 1]. 3. Transforms the target image if necessary. 4. Makes sure the model is on the target device. 5. Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model). 6. Converts the model’s output logits to prediction probabilities. 7. Converts the prediction probabilities to prediction labels. 8. Plots the target image alongside the model prediction and prediction probability.\nA fair few steps but we’ve got this!\n\ndef pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n\nWhat a nice looking function, let’s test it out.\n\n# Pred on our custom image\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n\n\n\n\n\n\n\n\nTwo thumbs up again!\nLooks like our model got the prediction right just by guessing.\nThis won’t always be the case with other images though…\nThe image is pixelated too because we resized it to [64, 64] using custom_image_transform.\n\nExercise: Try making a prediction with one of your own images of pizza, steak or sushi and see what happens.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#main-takeaways",
    "href": "04_pytorch_custom_datasets.html#main-takeaways",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "Main takeaways",
    "text": "Main takeaways\nWe’ve covered a fair bit in this module.\nLet’s summarise it with a few dot points.\n\nPyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.\nIf PyTorch’s built-in data loading functions don’t suit your requirements, you can write code to create your own custom datasets by subclassing torch.utils.data.Dataset.\ntorch.utils.data.DataLoader’s in PyTorch help turn your Dataset’s into iterables that can be used when training and testing a model.\nA lot of machine learning is dealing with the balance between overfitting and underfitting (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).\nPredicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:\n\nWrong datatypes - Your model expected torch.float32 when your data is torch.uint8.\nWrong data shapes - Your model expected [batch_size, color_channels, height, width] when your data is [color_channels, height, width].\nWrong devices - Your model is on the GPU but your data is on the CPU.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#연습-문제",
    "href": "04_pytorch_custom_datasets.html#연습-문제",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "연습 문제",
    "text": "연습 문제\nAll of the exercises are focused on practicing the code in the sections above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 04 * Example solutions notebook for 04 (try the exercises before looking at this)\n\nOur models are underperforming (not fitting the data well). What are 3 methods for preventing underfitting? Write them down and explain each with a sentence.\nRecreate the data loading functions we built in sections 1, 2, 3 and 4. You should have train and test DataLoader’s ready to use.\nRecreate model_0 we built in section 7.\nCreate training and testing functions for model_0.\nTry training the model you made in exercise 3 for 5, 20 and 50 epochs, what happens to the results?\n\nUse torch.optim.Adam() with a learning rate of 0.001 as the optimizer.\n\nDouble the number of hidden units in your model and train it for 20 epochs, what happens to the results?\nDouble the data you’re using with your model and train it for 20 epochs, what happens to the results?\n\n참고: You can use the custom data creation notebook to scale up your Food101 dataset.\nYou can also find the already formatted double data (20% instead of 10% subset) dataset on GitHub, you will need to write download code like in exercise 2 to get it into this notebook.\n\nMake a prediction on your own custom image of pizza/steak/sushi (you could even download one from the internet) and share your prediction.\n\nDoes the model you trained in exercise 7 get it right?\nIf not, what do you think you could do to improve it?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#추가-학습-자료",
    "href": "04_pytorch_custom_datasets.html#추가-학습-자료",
    "title": "04 - PyTorch 사용자 정의 데이터셋",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\nTo practice your knowledge of PyTorch Dataset’s and DataLoader’s through PyTorch datasets and dataloaders tutorial notebook.\nSpend 10-minutes reading the PyTorch torchvision.transforms documentation.\n\nYou can see demos of transforms in action in the illustrations of transforms tutorial.\n\nSpend 10-minutes reading the PyTorch torchvision.datasets documentation.\n\nWhat are some datasets that stand out to you?\nHow could you try building a model on these?\n\nTorchData is currently in beta (as of April 2022), it’ll be a future way of loading data in PyTorch, but you can start to check it out now.\nTo speed up deep learning models, you can do a few tricks to improve compute, memory and overhead computations, for more read the post Making Deep Learning Go Brrrr From First Principles by Horace He.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>04 - PyTorch 사용자 정의 데이터셋</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html",
    "href": "05_pytorch_going_modular.html",
    "title": "05 - PyTorch 모듈화",
    "section": "",
    "text": "What is cell mode?\nThis notebook is part 1/2 of section 05. Going Modular.\nFor reference, the two parts are: 1. 05. Going Modular: Part 1 (cell mode) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of notebook 04. 2. 05. Going Modular: Part 2 (script mode) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, data_setup.py and train.py.\nWhy two parts?\nBecause sometimes the best way to learn something is to see how it differs from something else.\nIf you run each notebook side-by-side you’ll see how they differ and that’s where the key learnings are.\nA cell mode notebook is a regular notebook run exactly how we’ve been running them through the course.\nSome cells contain text and others contain code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#whats-the-difference-between-this-notebook-part-1-and-the-script-mode-notebook-part-2",
    "href": "05_pytorch_going_modular.html#whats-the-difference-between-this-notebook-part-1-and-the-script-mode-notebook-part-2",
    "title": "05 - PyTorch 모듈화",
    "section": "What’s the difference between this notebook (Part 1) and the script mode notebook (Part 2)?",
    "text": "What’s the difference between this notebook (Part 1) and the script mode notebook (Part 2)?\nThis notebook, 05. PyTorch Going Modular: Part 1 (cell mode), runs a cleaned up version of the most useful code from section 04. PyTorch Custom Datasets.\nRunning this notebook end-to-end will result in recreating the image classification model we built in notebook 04 (TinyVGG) trained on images of pizza, steak and sushi.\nThe main difference between this notebook (Part 1) and Part 2 is that each section in Part 2 (script mode) has an extra subsection (e.g. 2.1, 3.1, 4.1) for turning cell code into script code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#where-can-you-get-help",
    "href": "05_pytorch_going_modular.html#where-can-you-get-help",
    "title": "05 - PyTorch 모듈화",
    "section": "Where can you get help?",
    "text": "Where can you get help?\nYou can find the book version of this section 05. PyTorch Going Modular on learnpytorch.io.\nThe rest of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, there’s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#running-a-notebook-in-cell-mode",
    "href": "05_pytorch_going_modular.html#running-a-notebook-in-cell-mode",
    "title": "05 - PyTorch 모듈화",
    "section": "0. Running a notebook in cell mode",
    "text": "0. Running a notebook in cell mode\nAs discussed, we’re going to be running this notebook normally.\nOne cell at a time.\nThe code is from notebook 04, however, it has been condensed down to its core functionality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#get-data",
    "href": "05_pytorch_going_modular.html#get-data",
    "title": "05 - PyTorch 모듈화",
    "section": "1. Get data",
    "text": "1. Get data\nWe’re going to start by downloading the same data we used in notebook 04, the pizza_steak_sushi dataset with images of pizza, steak and sushi.\n\nimport os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n    \n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")\n\ndata/pizza_steak_sushi directory exists.\nDownloading pizza, steak, sushi data...\nUnzipping pizza, steak, sushi data...\n\n\n\n# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n\n(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#create-datasets-and-dataloaders",
    "href": "05_pytorch_going_modular.html#create-datasets-and-dataloaders",
    "title": "05 - PyTorch 모듈화",
    "section": "2. Create Datasets and DataLoaders",
    "text": "2. Create Datasets and DataLoaders\nNow we’ll turn the image dataset into PyTorch Dataset’s and DataLoader’s.\n\nfrom torchvision import datasets, transforms\n\n# Create simple transform\ndata_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n\n# Use ImageFolder to create dataset(s)\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n\nTrain data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data/pizza_steak_sushi/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data/pizza_steak_sushi/test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               ToTensor()\n           )\n\n\n\n# Get class names as a list\nclass_names = train_data.classes\nclass_names\n\n['pizza', 'steak', 'sushi']\n\n\n\n# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n\n{'pizza': 0, 'steak': 1, 'sushi': 2}\n\n\n\n# Check the lengths\nlen(train_data), len(test_data)\n\n(225, 75)\n\n\n\n# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7f853747bbe0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f853747b550&gt;)\n\n\n\n# Check out single image size/shape\nimg, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n\nImage shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#making-a-model-tinyvgg",
    "href": "05_pytorch_going_modular.html#making-a-model-tinyvgg",
    "title": "05 - PyTorch 모듈화",
    "section": "3. Making a model (TinyVGG)",
    "text": "3. Making a model (TinyVGG)\nWe’re going to use the same model we used in notebook 04: TinyVGG from the CNN Explainer website.\nThe only change here from notebook 04 is that a docstring has been added using Google’s Style Guide for Python.\n\nimport torch\n\nfrom torch import nn\n\nclass TinyVGG(nn.Module):\n  \"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n  \n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, # how big is the square that's going over the image?\n                    stride=1, # default\n                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2) # default stride value is same as kernel_size\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n    \n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.block_2(self.block_1(x))) # &lt;- leverage the benefits of operator fusion\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=1690, out_features=3, bias=True)\n  )\n)\n\n\nTo test our model let’s do a single forward pass (pass a sample batch from the training set through our model).\n\n# 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n\nSingle image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[ 0.0208, -0.0019,  0.0095]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3371, 0.3295, 0.3333]], device='cuda:0')\n\nOutput prediction label:\ntensor([0], device='cuda:0')\n\nActual label:\n0",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#creating-train_step-and-test_step-functions-and-train-to-combine-them",
    "href": "05_pytorch_going_modular.html#creating-train_step-and-test_step-functions-and-train-to-combine-them",
    "title": "05 - PyTorch 모듈화",
    "section": "4. Creating train_step() and test_step() functions and train() to combine them",
    "text": "4. Creating train_step() and test_step() functions and train() to combine them\nRather than writing them again, we can reuse the train_step() and test_step() functions from notebook 04.\nThe same goes for the train() function we created.\nThe only difference here is that these functions have had docstrings added to them in Google’s Python Functions and Methods Style Guide.\nLet’s start by making train_step().\n\nfrom typing import Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n    \n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n  \n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n  \n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\nNow we’ll do test_step().\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n    \n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n  \n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n  \n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n  \n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n          \n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n          \n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\nAnd we’ll combine train_step() and test_step() into train().\n\nfrom typing import Dict, List\n\nfrom tqdm.auto import tqdm\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List[float]]:\n  \"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n  \n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n      \n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#creating-a-function-to-save-the-model",
    "href": "05_pytorch_going_modular.html#creating-a-function-to-save-the-model",
    "title": "05 - PyTorch 모듈화",
    "section": "5. Creating a function to save the model",
    "text": "5. Creating a function to save the model\nLet’s setup a function to save our model to a directory.\n\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n  \"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n  \n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n  \n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#train-evaluate-and-save-the-model",
    "href": "05_pytorch_going_modular.html#train-evaluate-and-save-the-model",
    "title": "05 - PyTorch 모듈화",
    "section": "6. Train, evaluate and save the model",
    "text": "6. Train, evaluate and save the model\nLet’s leverage the functions we’ve got above to train, test and save a model to file.\n\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader,\n                        test_dataloader=test_dataloader,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS,\n                        device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n\n# Save the model\nsave_model(model=model_0,\n           target_dir=\"models\",\n           model_name=\"05_going_modular_cell_mode_tinyvgg_model.pth\")\n\n\n\n\nEpoch: 1 | train_loss: 1.0955 | train_acc: 0.3867 | test_loss: 1.0627 | test_acc: 0.4133\nEpoch: 2 | train_loss: 1.0102 | train_acc: 0.5200 | test_loss: 1.0222 | test_acc: 0.4400\nEpoch: 3 | train_loss: 0.9552 | train_acc: 0.5822 | test_loss: 1.0067 | test_acc: 0.4667\nEpoch: 4 | train_loss: 0.8913 | train_acc: 0.5867 | test_loss: 1.0090 | test_acc: 0.4400\nEpoch: 5 | train_loss: 0.8608 | train_acc: 0.6311 | test_loss: 1.0214 | test_acc: 0.4533\n[INFO] Total training time: 5.859 seconds\n[INFO] Saving model to: models/05_going_modular_cell_mode_tinyvgg_model.pth\n\n\nWe finish with a saved image classification model at models/05_going_modular_cell_mode_tinyvgg_model.pth.\nThe code continues in 05. Going Modular: Part 2 (script mode).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>05 - PyTorch 모듈화</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html",
    "href": "06_pytorch_transfer_learning.html",
    "title": "06 - PyTorch 전이 학습",
    "section": "",
    "text": "What is transfer learning?\nWe’ve built a few models by hand so far.\nBut their performance has been poor.\nYou might be thinking, is there a well-performing model that already exists for our problem?\nAnd in the world of deep learning, the answer is often yes.\nWe’ll see how by using a powerful technique called transfer learning.\nTransfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\nFor example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.\nOr we could take the patterns from a language model (a model that’s been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\nThe premise remains: find a well-performing existing model and apply it to your own problem.\nExample of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#why-use-transfer-learning",
    "href": "06_pytorch_transfer_learning.html#why-use-transfer-learning",
    "title": "06 - PyTorch 전이 학습",
    "section": "Why use transfer learning?",
    "text": "Why use transfer learning?\nThere are two main benefits to using transfer learning:\n\nCan leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\nCan leverage a working model which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.\n\n\nWe’ll be putting these to the test for our FoodVision Mini problem, we’ll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.\nBoth research and practice support the use of transfer learning too.\nA finding from a recent machine learning research paper recommended practioner’s use transfer learning wherever possible.\n\nA study into the effects of whether training from scratch or using transfer learning was better from a practioner’s point of view, found transfer learning to be far more beneficial in terms of cost and time. Source: How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers paper section 6 (conclusion).\nAnd Jeremy Howard (founder of fastai) is a big proponent of transfer learning.\n\nThe things that really make a difference (transfer learning), if we can do better at transfer learning, it’s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. — Jeremy Howard on the Lex Fridman Podcast",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#where-to-find-pretrained-models",
    "href": "06_pytorch_transfer_learning.html#where-to-find-pretrained-models",
    "title": "06 - PyTorch 전이 학습",
    "section": "Where to find pretrained models",
    "text": "Where to find pretrained models\nThe world of deep learning is an amazing place.\nSo amazing that many people around the world share their work.\nOften, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\nAnd there are several places you can find pretrained models to use for your own problems.\n\n\n\nLocation\nWhat’s there?\nLink(s)\n\n\n\n\nPyTorch domain libraries\nEach of the PyTorch domain libraries (torchvision, torchtext) come with pretrained models of some form. The models there work right within PyTorch.\ntorchvision.models, torchtext.models, torchaudio.models, torchrec.models\n\n\nHuggingFace Hub\nA series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There’s plenty of different datasets too.\nhttps://huggingface.co/models, https://huggingface.co/datasets\n\n\ntimm (PyTorch Image Models) library\nAlmost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features.\nhttps://github.com/rwightman/pytorch-image-models\n\n\nPaperswithcode\nA collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks.\nhttps://paperswithcode.com/\n\n\n\n\nWith access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, “Does a pretrained model exist for my problem?”\n\nExercise: Spend 5-minutes going through torchvision.models as well as the HuggingFace Hub Models page, what do you find? (there’s no right answers here, it’s just to practice exploring)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#이번-장에서-다룰-내용",
    "href": "06_pytorch_transfer_learning.html#이번-장에서-다룰-내용",
    "title": "06 - PyTorch 전이 학습",
    "section": "이번 장에서 다룰 내용",
    "text": "이번 장에서 다룰 내용\nWe’re going to take a pretrained model from torchvision.models and customise it to work on (and hopefully improve) our FoodVision Mini problem.\n\n\n\n\n\n\n\nTopic\nContents\n\n\n\n\n0. Getting setup\nWe’ve written a fair bit of useful code over the past few sections, let’s download it and make sure we can use it again.\n\n\n1. Get data\nLet’s get the pizza, steak and sushi image classification dataset we’ve been using to try and improve our model’s results.\n\n\n2. Create Datasets and DataLoaders\nWe’ll use the data_setup.py script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders.\n\n\n3. Get and customise a pretrained model\nHere we’ll download a pretrained model from torchvision.models and customise it to our own problem.\n\n\n4. Train model\nLet’s see how the new pretrained model goes on our pizza, steak, sushi dataset. We’ll use the training functions we created in the previous chapter.\n\n\n5. Evaluate the model by plotting loss curves\nHow did our first transfer learning model go? Did it overfit or underfit?\n\n\n6. Make predictions on images from the test set\nIt’s one thing to check out a model’s evaluation metrics but it’s another thing to view its predictions on test samples, let’s visualize, visualize, visualize!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#where-can-you-get-help",
    "href": "06_pytorch_transfer_learning.html#where-can-you-get-help",
    "title": "06 - PyTorch 전이 학습",
    "section": "Where can you get help?",
    "text": "Where can you get help?\nAll of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, there’s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#getting-setup",
    "href": "06_pytorch_transfer_learning.html#getting-setup",
    "title": "06 - PyTorch 전이 학습",
    "section": "0. Getting setup",
    "text": "0. Getting setup\nLet’s get started by importing/downloading the required modules for this section.\nTo save us writing extra code, we’re going to be leveraging some of the Python scripts (such as data_setup.py and engine.py) we created in the previous section, 05. PyTorch Going Modular.\nSpecifically, we’re going to download the going_modular directory from the pytorch-deep-learning repository (if we don’t already have it).\nWe’ll also get the torchinfo package if it’s not available.\ntorchinfo will help later on to give us a visual representation of our model.\n\n참고: As of June 2022, this notebook uses the nightly versions of torch and torchvision as torchvision v0.13+ is required for using the updated multi-weights API. You can install these using the command below.\n\n\n# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\ntorch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n\n\n\n# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n\nNow let’s setup device agnostic code.\n\n참고: If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.\n\n\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#get-data",
    "href": "06_pytorch_transfer_learning.html#get-data",
    "title": "06 - PyTorch 전이 학습",
    "section": "1. Get data",
    "text": "1. Get data\nBefore we can start to use transfer learning, we’ll need a dataset.\nTo see how transfer learning compares to our previous attempts at model building, we’ll download the same dataset we’ve been using for FoodVision Mini.\nLet’s write some code to download the pizza_steak_sushi.zip dataset from the course GitHub and then unzip it.\nWe can also make sure if we’ve already got the data, it doesn’t redownload.\n\nimport os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n\ndata/pizza_steak_sushi directory exists.\n\n\nExcellent!\nNow we’ve got the same dataset we’ve been using previously, a series of images of pizza, steak and sushi in standard image classification format.\nLet’s now create paths to our training and test directories.\n\n# Setup Dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#create-datasets-and-dataloaders",
    "href": "06_pytorch_transfer_learning.html#create-datasets-and-dataloaders",
    "title": "06 - PyTorch 전이 학습",
    "section": "2. Create Datasets and DataLoaders",
    "text": "2. Create Datasets and DataLoaders\nSince we’ve downloaded the going_modular directory, we can use the data_setup.py script we created in section 05. PyTorch Going Modular to prepare and setup our DataLoaders.\nBut since we’ll be using a pretrained model from torchvision.models, there’s a specific transform we need to prepare our images first.\n\n2.1 Creating a transform for torchvision.models (manual creation)\n\n참고: As of torchvision v0.13+, there’s an update to how data transforms can be created using torchvision.models. I’ve called the previous method “manual creation” and the new method “auto creation”. This notebook showcases both.\n\nWhen using a pretrained model, it’s important that your custom data going into the model is prepared in the same way as the original training data that went into the model.\nPrior to torchvision v0.13+, to create a transform for a pretrained model in torchvision.models, the documentation stated:\n\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\nThe images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\nYou can use the following transform to normalize:\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\nThe good news is, we can achieve the above transformations with a combination of:\n\n\n\n\n\n\n\n\nTransform number\nTransform required\nCode to perform transform\n\n\n\n\n1\nMini-batches of size [batch_size, 3, height, width] where height and width are at least 224x224^.\ntorchvision.transforms.Resize() to resize images into [3, 224, 224]^ and torch.utils.data.DataLoader() to create batches of images.\n\n\n2\nValues between 0 & 1.\ntorchvision.transforms.ToTensor()\n\n\n3\nA mean of [0.485, 0.456, 0.406] (values across each colour channel).\ntorchvision.transforms.Normalize(mean=...) to adjust the mean of our images.\n\n\n4\nA standard deviation of [0.229, 0.224, 0.225] (values across each colour channel).\ntorchvision.transforms.Normalize(std=...) to adjust the standard deviation of our images.\n\n\n\n\n참고: ^some pretrained models from torchvision.models in different sizes to [3, 224, 224], for example, some might take them in [3, 240, 240]. For specific input image sizes, see the documentation.\n\n\nQuestion: Where did the mean and standard deviation values come from? Why do we need to do this?\nThese were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.\nWe also don’t need to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (they’ll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.\n\nLet’s compose a series of torchvision.transforms to perform the above steps.\n\n# Create a transforms pipeline manually (required for torchvision &lt; 0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n\nWonderful!\nNow we’ve got a manually created series of transforms ready to prepare our images, let’s create training and testing DataLoaders.\nWe can create these using the create_dataloaders function from the data_setup.py script we created in 05. PyTorch Going Modular Part 2.\nWe’ll set batch_size=32 so our model see’s mini-batches of 32 samples at a time.\nAnd we can transform our images using the transform pipeline we created above by setting transform=simple_transform.\n\n참고: I’ve included this manual creation of transforms in this notebook because you may come across resources that use this style. It’s also important to note that because these transforms are manually created, they’re also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could.\n\n\n# Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a3a60&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a37c0&gt;,\n ['pizza', 'steak', 'sushi'])\n\n\n\n\n2.2 Creating a transform for torchvision.models (auto creation)\nAs previously stated, when using a pretrained model, it’s important that your custom data going into the model is prepared in the same way as the original training data that went into the model.\nAbove we saw how to manually create a transform for a pretrained model.\nBut as of torchvision v0.13+, an automatic transform creation feature has been added.\nWhen you setup a model from torchvision.models and select the pretrained model weights you’d like to use, for example, say we’d like to use:\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\nWhere, * EfficientNet_B0_Weights is the model architecture weights we’d like to use (there are many different model architecture options in torchvision.models). * DEFAULT means the best available weights (the best performance in ImageNet). * 참고: Depending on the model architecture you choose, you may also see other options such as IMAGENET_V1 and IMAGENET_V2 where generally the higher version number the better. Though if you want the best available, DEFAULT is the easiest option. See the torchvision.models documentation for more.\nLet’s try it out.\n\n# Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n\nEfficientNet_B0_Weights.IMAGENET1K_V1\n\n\nAnd now to access the transforms assosciated with our weights, we can use the transforms() method.\nThis is essentially saying “get the data transforms that were used to train the EfficientNet_B0_Weights on ImageNet”.\n\n# Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms\n\nImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n\n\nNotice how auto_transforms is very similar to manual_transforms, the only difference is that auto_transforms came with the model architecture we chose, where as we had to create manual_transforms by hand.\nThe benefit of automatically creating a transform through weights.transforms() is that you ensure you’re using the same data transformation as the pretrained model used when it was trained.\nHowever, the tradeoff of using automatically created transforms is a lack of customization.\nWe can use auto_transforms to create DataLoaders with create_dataloaders() just as before.\n\n# Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951460&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951550&gt;,\n ['pizza', 'steak', 'sushi'])",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#getting-a-pretrained-model",
    "href": "06_pytorch_transfer_learning.html#getting-a-pretrained-model",
    "title": "06 - PyTorch 전이 학습",
    "section": "3. Getting a pretrained model",
    "text": "3. Getting a pretrained model\nAlright, here comes the fun part!\nOver the past few notebooks we’ve been building PyTorch neural networks from scratch.\nAnd while that’s a good skill to have, our models haven’t been performing as well as we’d like.\nThat’s where transfer learning comes in.\nThe whole idea of transfer learning is to take an already well-performing model on a problem-space similar to yours and then customising it to your use case.\nSince we’re working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in torchvision.models.\nExploring the documentation, you’ll find plenty of common computer vision architecture backbones such as:\n\n\n\nArchitecuture backbone\nCode\n\n\n\n\nResNet’s\ntorchvision.models.resnet18(), torchvision.models.resnet50()…\n\n\nVGG (similar to what we used for TinyVGG)\ntorchvision.models.vgg16()\n\n\nEfficientNet’s\ntorchvision.models.efficientnet_b0(), torchvision.models.efficientnet_b1()…\n\n\nVisionTransformer (ViT’s)\ntorchvision.models.vit_b_16(), torchvision.models.vit_b_32()…\n\n\nConvNeXt\ntorchvision.models.convnext_tiny(), torchvision.models.convnext_small()…\n\n\nMore available in torchvision.models\ntorchvision.models...\n\n\n\n\n3.1 Which pretrained model should you use?\nIt depends on your problem/the device you’re working with.\nGenerally, the higher number in the model name (e.g. efficientnet_b0() -&gt; efficientnet_b1() -&gt; efficientnet_b7()) means better performance but a larger model.\nYou might think better performance is always better, right?\nThat’s true but some better performing models are too big for some devices.\nFor example, say you’d like to run your model on a mobile-device, you’ll have to take into account the limited compute resources on the device, thus you’d be looking for a smaller model.\nBut if you’ve got unlimited compute power, as The Bitter Lesson states, you’d likely take the biggest, most compute hungry model you can.\nUnderstanding this performance vs. speed vs. size tradeoff will come with time and practice.\nFor me, I’ve found a nice balance in the efficientnet_bX models.\nAs of May 2022, Nutrify (the machine learning powered app I’m working on) is powered by an efficientnet_b0.\nComma.ai (a company that makes open source self-driving car software) uses an efficientnet_b2 to learn a representation of the road.\n\n참고: Even though we’re using efficientnet_bX, it’s important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem.\n\n\n\n3.2 Setting up a pretrained model\nThe pretrained model we’re going to be using is torchvision.models.efficientnet_b0().\nThe architecture is from the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n\nExample of what we’re going to create, a pretrained EfficientNet_B0 model from torchvision.models with the output layer adjusted for our use case of classifying pizza, steak and sushi images.\nWe can setup the EfficientNet_B0 pretrained ImageNet weights using the same code as we used to create the transforms.\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights for ImageNet\nThis means the model has already been trained on millions of images and has a good base representation of image data.\nThe PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNet’s 1000 classes.\nWe’ll also send it to the target device.\n\n# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n\n# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n#model # uncomment to output (it's very long)\n\n\n참고: In previous versions of torchvision, you’d create a prertained model with code like:\nmodel = torchvision.models.efficientnet_b0(pretrained=True).to(device)\nHowever, running this using torchvision v0.13+ will result in errors such as the following:\nUserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\nAnd…\nUserWarning: Arguments other than a weight enum or None for weights are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing weights=EfficientNet_B0_Weights.IMAGENET1K_V1. You can also use weights=EfficientNet_B0_Weights.DEFAULT to get the most up-to-date weights.\n\nIf we print the model, we get something similar to the following:\n\nLots and lots and lots of layers.\nThis is one of the benefits of transfer learning, taking an existing model, that’s been crafted by some of the best engineers in the world and applying to your own problem.\nOur efficientnet_b0 comes in three main parts: 1. features - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as features or feature extractor, “the base layers of the model learn the different features of images”). 2. avgpool - Takes the average of the output of the features layer(s) and turns it into a feature vector. 3. classifier - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since efficientnet_b0 is pretrained on ImageNet and because ImageNet has 1000 classes, out_features=1000 is the default).\n\n\n3.3 Getting a summary of our model with torchinfo.summary()\nTo learn more about our model, let’s use torchinfo’s summary() method.\nTo do so, we’ll pass in: * model - the model we’d like to get a summary of. * input_size - the shape of the data we’d like to pass to our model, for the case of efficientnet_b0, the input size is (batch_size, 3, 224, 224), though other variants of efficientnet_bX have different input sizes. * 참고: Many modern models can handle input images of varying sizes thanks to torch.nn.AdaptiveAvgPool2d(), this layer adaptively adjusts the output_size of a given input as required. You can try this out by passing different size input images to summary() or your models. * col_names - the various information columns we’d like to see about our model. * col_width - how wide the columns should be for the summary. * row_settings - what features to show in a row.\n\n# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n) \n\n============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n├─Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n│    └─Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n============================================================================================================================================\nTotal params: 5,288,548\nTrainable params: 5,288,548\nNon-trainable params: 0\nTotal mult-adds (G): 12.35\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.35\nParams size (MB): 21.15\nEstimated Total Size (MB): 3492.77\n============================================================================================================================================\n\n\n\nWoah!\nNow that’s a big model!\nFrom the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.\nAnd there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data.\nFor reference, our model from previous sections, TinyVGG had 8,083 parameters vs. 5,288,548 parameters for efficientnet_b0, an increase of ~654x!\nWhat do you think, will this mean better performance?\n\n\n3.4 Freezing the base model and changing the output layer to suit our needs\nThe process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the features section) and then adjust the output layers (also called head/classifier layers) to suit your needs.\n\nYou can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original torchvision.models.efficientnet_b0() comes with out_features=1000 because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need out_features=3.\nLet’s freeze all of the layers/parameters in the features section of our efficientnet_b0 model.\n\n참고: To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, “don’t change any of the patterns in these layers during training, keep them how they are.” In essence, we’d like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.\n\nWe can freeze all of the layers/parameters in the features section by setting the attribute requires_grad=False.\nFor parameters with requires_grad=False, PyTorch doesn’t track gradient updates and in turn, these parameters won’t be changed by our optimizer during training.\nIn essence, a parameter with requires_grad=False is “untrainable” or “frozen” in place.\n\n# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n\nFeature extractor layers frozen!\nLet’s now adjust the output layer or the classifier portion of our pretrained model to our needs.\nRight now our pretrained model has out_features=1000 because there are 1000 classes in ImageNet.\nHowever, we don’t have 1000 classes, we only have three, pizza, steak and sushi.\nWe can change the classifier portion of our model by creating a new series of layers.\nThe current classifier consists of:\n(classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\nWe’ll keep the Dropout layer the same using torch.nn.Dropout(p=0.2, inplace=True).\n\n참고: Dropout layers randomly remove connections between two neural network layers with a probability of p. For example, if p=0.2, 20% of connections between neural network layers will be removed at random each pass. This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections (hopefully these remaining features are more general).\n\nAnd we’ll keep in_features=1280 for our Linear output layer but we’ll change the out_features value to the length of our class_names (len(['pizza', 'steak', 'sushi']) = 3).\nOur new classifier layer should be on the same device as our model.\n\n# Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n\nNice!\nOutput layer updated, let’s get another summary of our model and see what’s changed.\n\n# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n\n============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n============================================================================================================================================\nTotal params: 4,011,391\nTrainable params: 3,843\nNon-trainable params: 4,007,548\nTotal mult-adds (G): 12.31\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.09\nParams size (MB): 16.05\nEstimated Total Size (MB): 3487.41\n============================================================================================================================================\n\n\n\nHo, ho! There’s a fair few changes here!\nLet’s go through them: * Trainable column - You’ll see that many of the base layers (the ones in the features portion) have their Trainable value as False. This is because we set their attribute requires_grad=False. Unless we change this, these layers won’t be updated during furture training. * Output shape of classifier - The classifier portion of the model now has an Output Shape value of [32, 3] instead of [32, 1000]. It’s Trainable value is also True. This means its parameters will be updated during training. In essence, we’re using the features portion to feed our classifier portion a base representation of an image and then our classifier layer is going to learn how to base representation aligns with our problem. * Less trainable parameters - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the classifier as trainable, there’s now only 3,843 trainable parameters (even less than our TinyVGG model). Though there’s also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our classifier layer.\n\n참고: The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#train-model",
    "href": "06_pytorch_transfer_learning.html#train-model",
    "title": "06 - PyTorch 전이 학습",
    "section": "4. Train model",
    "text": "4. Train model\nNow we’ve got a pretraiend model that’s semi-frozen and has a customised classifier, how about we see transfer learning in action?\nTo begin training, let’s create a loss function and an optimizer.\nBecause we’re still working with multi-class classification, we’ll use nn.CrossEntropyLoss() for the loss function.\nAnd we’ll stick with torch.optim.Adam() as our optimizer with lr=0.001.\n\n# Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nWonderful!\nTo train our model, we can use train() function we defined in the 05. PyTorch Going Modular section 04.\nThe train() function is in the engine.py script inside the going_modular directory.\nLet’s see how long it takes to train our model for 5 epochs.\n\n참고: We’re only going to be training the parameters classifier here as all of the other parameters in our model have been frozen.\n\n\n# Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n\n\n\n\nEpoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8717 | train_acc: 0.7773 | test_loss: 0.7912 | test_acc: 0.8153\nEpoch: 3 | train_loss: 0.7648 | train_acc: 0.7930 | test_loss: 0.7463 | test_acc: 0.8561\nEpoch: 4 | train_loss: 0.7108 | train_acc: 0.7539 | test_loss: 0.6372 | test_acc: 0.8655\nEpoch: 5 | train_loss: 0.6254 | train_acc: 0.7852 | test_loss: 0.6260 | test_acc: 0.8561\n[INFO] Total training time: 8.977 seconds\n\n\nWow!\nOur model trained quite fast (~5 seconds on my local machine with a NVIDIA TITAN RTX GPU/about 15 seconds on Google Colab with a NVIDIA P100 GPU).\nAnd it looks like it smashed our previous model results out of the park!\nWith an efficientnet_b0 backbone, our model achieves almost 85%+ accuracy on the test dataset, almost double what we were able to achieve with TinyVGG.\nNot bad for a model we downloaded with a few lines of code.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#evaluate-model-by-plotting-loss-curves",
    "href": "06_pytorch_transfer_learning.html#evaluate-model-by-plotting-loss-curves",
    "title": "06 - PyTorch 전이 학습",
    "section": "5. Evaluate model by plotting loss curves",
    "text": "5. Evaluate model by plotting loss curves\nOur model looks like it’s performing pretty well.\nLet’s plot it’s loss curves to see what the training looks like over time.\nWe can plot the loss curves using the function plot_loss_curves() we created in 04. PyTorch Custom Datasets section 7.8.\nThe function is stored in the helper_functions.py script so we’ll try to import it and download the script if we don’t have it.\n\n# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Plot the loss curves of our model\nplot_loss_curves(results)\n\n\n\n\n\n\n\n\nThose are some excellent looking loss curves!\nIt looks like the loss for both datasets (train and test) is heading in the right direction.\nThe same with the accuracy values, trending upwards.\nThat goes to show the power of transfer learning. Using a pretrained model often leads to pretty good results with a small amount of data in less time.\nI wonder what would happen if you tried to train the model for longer? Or if we added more data?\n\nQuestion: Looking at the loss curves, does our model look like it’s overfitting or underfitting? Or perhaps neither? Hint: Check out notebook 04. PyTorch Custom Datasets part 8. What should an ideal loss curve look like? for ideas.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#make-predictions-on-images-from-the-test-set",
    "href": "06_pytorch_transfer_learning.html#make-predictions-on-images-from-the-test-set",
    "title": "06 - PyTorch 전이 학습",
    "section": "6. Make predictions on images from the test set",
    "text": "6. Make predictions on images from the test set\nIt looks like our model performs well quantitatively but how about qualitatively?\nLet’s find out by making some predictions with our model on images from the test set (these aren’t seen during training) and plotting them.\nVisualize, visualize, visualize!\nOne thing we’ll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.\nThis means we’ll need to make sure our images have: * Same shape - If our images are different shapes to what our model was trained on, we’ll get shape errors. * Same datatype - If our images are a different datatype (e.g. torch.int8 vs. torch.float32) we’ll get datatype errors. * Same device - If our images are on a different device to our model, we’ll get device errors. * Same transformations - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.\n\n참고: These requirements go for all kinds of data if you’re trying to make predictions with a trained model. Data you’d like to predict on should be in the same format as your model was trained on.\n\nTo do all of this, we’ll create a function pred_and_plot_image() to:\n\nTake in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.\nOpen an image with PIL.Image.open().\nCreate a transform for the image (this will default to the manual_transforms we created above or it could use a transform generated from weights.transforms()).\nMake sure the model is on the target device.\nTurn on model eval mode with model.eval() (this turns off layers like nn.Dropout(), so they aren’t used for inference) and the inference mode context manager.\nTransform the target image with the transform made in step 3 and add an extra batch dimension with torch.unsqueeze(dim=0) so our input image has shape [batch_size, color_channels, height, width].\nMake a prediction on the image by passing it to the model ensuring it’s on the target device.\nConvert the model’s output logits to prediction probabilities with torch.softmax().\nConvert model’s prediction probabilities to prediction labels with torch.argmax().\nPlot the image with matplotlib and set the title to the prediction label from step 9 and prediction probability from step 8.\n\n\n참고: This is a similar function to 04. PyTorch Custom Datasets section 11.3’s pred_and_plot_image() with a few tweaked steps.\n\n\nfrom typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Take in a trained model, class names, image path, image size, a transform and target device\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n\nWhat a good looking function!\nLet’s test it out by making predictions on a few random images from the test set.\nWe can get a list of all the test image paths using list(Path(test_dir).glob(\"*/*.jpg\")), the stars in the glob() method say “any file matching this pattern”, in other words, any file ending in .jpg (all of our images).\nAnd then we can randomly sample a number of these using Python’s random.sample(populuation, k) where population is the sequence to sample and k is the number of samples to retrieve.\n\n# Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWoohoo!\nThose predictions look far better than the ones our TinyVGG model was previously making.\n\n6.1 Making predictions on a custom image\nIt looks like our model does well qualitatively on data from the test set.\nBut how about on our own custom image?\nThat’s where the real fun of machine learning is!\nPredicting on your own custom data, outisde of any training or test set.\nTo test our model on a custom image, let’s import the old faithful pizza-dad.jpeg image (an image of my dad eating pizza).\nWe’ll then pass it to the pred_and_plot_image() function we created above and see what happens.\n\n# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n\ndata/04-pizza-dad.jpeg already exists, skipping download.\n\n\n\n\n\n\n\n\n\nTwo thumbs up!\nLooks like our model go it right again!\nBut this time the prediction probability is higher than the one from TinyVGG (0.373) in 04. PyTorch Custom Datasets section 11.3.\nThis indicates our efficientnet_b0 model is more confident in its prediction where as our TinyVGG model was par with just guessing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#main-takeaways",
    "href": "06_pytorch_transfer_learning.html#main-takeaways",
    "title": "06 - PyTorch 전이 학습",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nTransfer learning often allows to you get good results with a relatively small amount of custom data.\nKnowing the power of transfer learning, it’s a good idea to ask at the start of every problem, “does an existing well-performing model exist for my problem?”\nWhen using a pretrained model, it’s important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.\nThe same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.\nThere are several different places to find pretrained models from the PyTorch domain libraries, HuggingFace Hub and libraries such as timm (PyTorch Image Models).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#연습-문제",
    "href": "06_pytorch_transfer_learning.html#연습-문제",
    "title": "06 - PyTorch 전이 학습",
    "section": "연습 문제",
    "text": "연습 문제\nAll of the exercises are focused on practicing the code above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 06 * Example solutions notebook for 06 (try the exercises before looking at this) * See a live video walkthrough of the solutions on YouTube (errors and all)\n\nMake predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels. Check out 03. PyTorch Computer Vision section 10 for ideas.\nGet the “most wrong” of the predictions on the test dataset and plot the 5 “most wrong” images. You can do this by:\n\nPredicting across all of the test dataset, storing the labels and predicted probabilities.\nSort the predictions by wrong prediction and then descending predicted probabilities, this will give you the wrong predictions with the highest prediction probabilities, in other words, the “most wrong”.\nPlot the top 5 “most wrong” images, why do you think the model got these wrong?\n\nPredict on your own image of pizza/steak/sushi - how does the model go? What happens if you predict on an image that isn’t pizza/steak/sushi?\nTrain the model from section 4 above for longer (10 epochs should do), what happens to the performance?\nTrain the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.\n\nYou can find the 20% Pizza, Steak, Sushi dataset on the course GitHub. It was created with the notebook extras/04_custom_data_creation.ipynb.\n\nTry a different model from torchvision.models on the Pizza, Steak, Sushi data, how does this model perform?\n\nYou’ll have to change the size of the classifier layer to suit our problem.\nYou may want to try an EfficientNet with a higher number than our B0, perhaps torchvision.models.efficientnet_b2()?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#추가-학습-자료",
    "href": "06_pytorch_transfer_learning.html#추가-학습-자료",
    "title": "06 - PyTorch 전이 학습",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\nLook up what “model fine-tuning” is and spend 30-minutes researching different methods to perform it with PyTorch. How would we change our code to fine-tine? Tip: fine-tuning usually works best if you have lots of custom data, where as, feature extraction is typically better if you have less custom data.\nCheck out the new/upcoming PyTorch multi-weights API (still in beta at time of writing, May 2022), it’s a new way to perform transfer learning in PyTorch. What changes to our code would need to made to use the new API?\nTry to create your own classifier on two classes of images, for example, you could collect 10 photos of your dog and your friends dog and train a model to classify the two dogs. This would be a good way to practice creating a dataset as well as building a model on that dataset.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>06 - PyTorch 전이 학습</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html",
    "href": "07_pytorch_experiment_tracking.html",
    "title": "07 - PyTorch 실험 추적",
    "section": "",
    "text": "What is experiment tracking?\nWe’ve trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).\nAnd so far we’ve keep track of them via Python dictionaries.\nOr just comparing them by the metric print outs during training.\nWhat if you wanted to run a dozen (or more) different models at once?\nSurely there’s a better way…\nThere is.\nExperiment tracking.\nAnd since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.\nSo welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.\nWe’re going to answer the question: how do I track my machine learning experiments?\nMachine learning and deep learning are very experimental.\nYou have to put on your artist’s beret/chef’s hat to cook up lots of different models.\nAnd you have to put on your scientist’s coat to track the results of various combinations of data, model architectures and training regimes.\nThat’s where experiment tracking comes in.\nIf you’re running lots of different experiments, experiment tracking helps you figure out what works and what doesn’t.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#why-track-experiments",
    "href": "07_pytorch_experiment_tracking.html#why-track-experiments",
    "title": "07 - PyTorch 실험 추적",
    "section": "Why track experiments?",
    "text": "Why track experiments?\nIf you’re only running a handful of models (like we’ve done so far), it might be okay just to track their results in print outs and a few dictionaries.\nHowever, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.\nSo if you’re following the machine learning practitioner’s motto of experiment, experiment, experiment!, you’ll want a way to track them.\n\nAfter building a few models and tracking their results, you’ll start to notice how quickly it can get out of hand.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#different-ways-to-track-machine-learning-experiments",
    "href": "07_pytorch_experiment_tracking.html#different-ways-to-track-machine-learning-experiments",
    "title": "07 - PyTorch 실험 추적",
    "section": "Different ways to track machine learning experiments",
    "text": "Different ways to track machine learning experiments\nThere are as many different ways to track machine learning experiments as there is experiments to run.\nThis table covers a few.\n\n\n\nMethod\nSetup\nPros\nCons\nCost\n\n\n\n\nPython dictionaries, CSV files, print outs\nNone\nEasy to setup, runs in pure Python\nHard to keep track of large numbers of experiments\nFree\n\n\nTensorBoard\nMinimal, install tensorboard\nExtensions built into PyTorch, widely recognized and used, easily scales.\nUser-experience not as nice as other options.\nFree\n\n\nWeights & Biases Experiment Tracking\nMinimal, install wandb, make an account\nIncredible user experience, make experiments public, tracks almost anything.\nRequires external resource outside of PyTorch.\nFree for personal use\n\n\nMLFlow\nMinimal, install mlflow and starting tracking\nFully open-source MLOps lifecycle management, many integrations.\nLittle bit harder to setup a remote tracking server than other services.\nFree\n\n\n\n\nVarious places and techniques you can use to track your machine learning experiments. 참고: There are various other options similar to Weights & Biases and open-source options similar to MLflow but I’ve left them out for brevity. You can find more by searching “machine learning experiment tracking”.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#이번-장에서-다룰-내용",
    "href": "07_pytorch_experiment_tracking.html#이번-장에서-다룰-내용",
    "title": "07 - PyTorch 실험 추적",
    "section": "이번 장에서 다룰 내용",
    "text": "이번 장에서 다룰 내용\nWe’re going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.\nAnd due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.\nHowever, the principles we’re going to cover are similar across all of the other tools for experiment tracking.\n\n\n\n\n\n\n\nTopic\nContents\n\n\n\n\n0. Getting setup\nWe’ve written a fair bit of useful code over the past few sections, let’s download it and make sure we can use it again.\n\n\n1. Get data\nLet’s get the pizza, steak and sushi image classification dataset we’ve been using to try and improve our FoodVision Mini model’s results.\n\n\n2. Create Datasets and DataLoaders\nWe’ll use the data_setup.py script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders.\n\n\n3. Get and customise a pretrained model\nJust like the last section, 06. PyTorch Transfer Learning we’ll download a pretrained model from torchvision.models and customise it to our own problem.\n\n\n4. Train model amd track results\nLet’s see what it’s like to train and track the training results of a single model using TensorBoard.\n\n\n5. View our model’s results in TensorBoard\nPreviously we visualized our model’s loss curves with a helper function, now let’s see what they look like in TensorBoard.\n\n\n6. Creating a helper function to track experiments\nIf we’re going to be adhering to the machine learner practitioner’s motto of experiment, experiment, experiment!, we best create a function that will help us save our modelling experiment results.\n\n\n7. Setting up a series of modelling experiments\nInstead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times.\n\n\n8. View modelling experiments in TensorBoard\nBy this stage we’ll have run eight modelling experiments in one go, a fair bit to keep track of, let’s what their results look like in TensorBoard.\n\n\n9. Load in the best model and make predictions with it\nThe point of experiment tracking is to figure out which model performs the best, let’s load in the best performing model and make some predictions with it to visualize, visualize, visualize!.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#where-can-you-get-help",
    "href": "07_pytorch_experiment_tracking.html#where-can-you-get-help",
    "title": "07 - PyTorch 실험 추적",
    "section": "Where can you get help?",
    "text": "Where can you get help?\nAll of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, there’s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#getting-setup",
    "href": "07_pytorch_experiment_tracking.html#getting-setup",
    "title": "07 - PyTorch 실험 추적",
    "section": "0. Getting setup",
    "text": "0. Getting setup\nLet’s start by downloading all of the modules we’ll need for this section.\nTo save us writing extra code, we’re going to be leveraging some of the Python scripts (such as data_setup.py and engine.py) we created in section, 05. PyTorch Going Modular.\nSpecifically, we’re going to download the going_modular directory from the pytorch-deep-learning repository (if we don’t already have it).\nWe’ll also get the torchinfo package if it’s not available.\ntorchinfo will help later on to give us visual summaries of our model(s).\nAnd since we’re using a newer version of the torchvision package (v0.13 as of June 2022), we’ll make sure we’ve got the latest versions.\n\n# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\ntorch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n\n\n\n참고: If you’re using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you’ve got the right versions of torch (0.12+) and torchvision (0.13+).\n\n\n# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n\nNow let’s setup device agnostic code.\n\n참고: If you’re using Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nCreate a helper function to set seeds\nSince we’ve been setting random seeds a whole bunch throughout previous sections, how about we functionize it?\nLet’s create a function to “set the seeds” called set_seeds().\n\n참고: Recall a random seed is a way of flavouring the randomness generated by a computer. They aren’t necessary to always set when running machine learning code, however, they help ensure there’s an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally aren’t required.\n\n\n# Set seeds\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#get-data",
    "href": "07_pytorch_experiment_tracking.html#get-data",
    "title": "07 - PyTorch 실험 추적",
    "section": "1. Get data",
    "text": "1. Get data\nAs always, before we can run machine learning experiments, we’ll need a dataset.\nWe’re going to continue trying to improve upon the results we’ve been getting on FoodVision Mini.\nIn the previous section, 06. PyTorch Transfer Learning, we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.\nSo how about we run some experiments and try to further improve our results?\nTo do so, we’ll use similar code to the previous section to download the pizza_steak_sushi.zip (if the data doesn’t already exist) except this time its been functionised.\nThis will allow us to use it again later.\n\nimport os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    \n    Returns:\n        pathlib.Path to downloaded data.\n    \n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n\n[INFO] data/pizza_steak_sushi directory exists, skipping download.\n\n\nPosixPath('data/pizza_steak_sushi')\n\n\nExcellent! Looks like we’ve got our pizza, steak and sushi images in standard image classification format ready to go.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#create-datasets-and-dataloaders",
    "href": "07_pytorch_experiment_tracking.html#create-datasets-and-dataloaders",
    "title": "07 - PyTorch 실험 추적",
    "section": "2. Create Datasets and DataLoaders",
    "text": "2. Create Datasets and DataLoaders\nNow we’ve got some data, let’s turn it into PyTorch DataLoaders.\nWe can do so using the create_dataloaders() function we created in 05. PyTorch Going Modular part 2.\nAnd since we’ll be using transfer learning and specifically pretrained models from torchvision.models, we’ll create a transform to prepare our images correctly.\nTo transform our images in tensors, we can use: 1. Manually created transforms using torchvision.transforms. 2. Automatically created transforms using torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms(). * Where MODEL_NAME is a specific torchvision.models architecture, MODEL_WEIGHTS is a specific set of pretrained weights and DEFAULT means the “best available weights”.\nWe saw an example of each of these in 06. PyTorch Transfer Learning section 2.\nLet’s see first an example of manually creating a torchvision.transforms pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms don’t match the pretrained model).\nThe main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained torchvision.models are all pretrained on ImageNet).\nWe can do this with:\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n2.1 Create DataLoaders using manually created transforms\n\n# Setup directories\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n\nManually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,\n ['pizza', 'steak', 'sushi'])\n\n\n\n\n2.2 Create DataLoaders using automatically created transforms\nData transformed and DataLoaders created!\nLet’s now see what the same transformation pipeline looks like but this time by using automatic transforms.\nWe can do this by first instantiating a set of pretrained weights (for example weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT) we’d like to use and calling the transforms() method on it.\n\n# Setup dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup pretrained weights (plenty of these available in torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Get transforms from weights (these are the transforms that were used to obtain the weights)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n\nAutomatically created transforms: ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,\n ['pizza', 'steak', 'sushi'])",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head",
    "href": "07_pytorch_experiment_tracking.html#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head",
    "title": "07 - PyTorch 실험 추적",
    "section": "3. Getting a pretrained model, freezing the base layers and changing the classifier head",
    "text": "3. Getting a pretrained model, freezing the base layers and changing the classifier head\nBefore we run and track multiple modelling experiments, let’s see what it’s like to run and track a single one.\nAnd since our data is ready, the next thing we’ll need is a model.\nLet’s download the pretrained weights for a torchvision.models.efficientnet_b0() model and prepare it for use with our own data.\n\n# 참고: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n\n# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Setup the model with the pretrained weights and send it to the target device\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# View the output of the model\n# model\n\nWonderful!\nNow we’ve got a pretrained model let’s turn into a feature extractor model.\nIn essence, we’ll freeze the base layers of the model (we’ll use these to extract features from our input images) and we’ll change the classifier head (output layer) to suit the number of classes we’re working with (we’ve got 3 classes: pizza, steak, sushi).\n\n참고: The idea of creating a feature extractor model (what we’re doing here) was covered in more depth in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model.\n\n\n# Freeze all base layers by setting requires_grad attribute to False\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Since we're creating a new layer with random weights (torch.nn.Linear), \n# let's set the seeds\nset_seeds() \n\n# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n\nBase layers frozen, classifier head changed, let’s get a summary of our model with torchinfo.summary().\n\nfrom torchinfo import summary\n\n# # Get a summary of the model (uncomment for full output)\n# summary(model, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n#         verbose=0,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n\n\nOutput of torchinfo.summary() with our feature extractor EffNetB0 model, notice how the base layers are frozen (not trainable) and the output layers are customized to our own problem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#train-model-and-track-results",
    "href": "07_pytorch_experiment_tracking.html#train-model-and-track-results",
    "title": "07 - PyTorch 실험 추적",
    "section": "4. Train model and track results",
    "text": "4. Train model and track results\nModel ready to go!\nLet’s get ready to train it by creating a loss function and an optimizer.\nSince we’re working with multiple classes, we’ll use torch.nn.CrossEntropyLoss() as the loss function.\nAnd we’ll stick with torch.optim.Adam() with learning rate of 0.001 for the optimizer.\n\n# Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\nAdjust train() function to track results with SummaryWriter()\nBeautiful!\nAll of the pieces of our training code are starting to come together.\nLet’s now add the final piece to track our experiments.\nPreviously, we’ve tracked our modelling experiments using multiple Python dictionaries (one for each model).\nBut you can imagine this could get out of hand if we were running anything more than a few experiments.\nNot to worry, there’s a better option!\nWe can use PyTorch’s torch.utils.tensorboard.SummaryWriter() class to save various parts of our model’s training progress to file.\nBy default, the SummaryWriter() class saves various information about our model to a file set by the log_dir parameter.\nThe default location for log_dir is under runs/CURRENT_DATETIME_HOSTNAME, where the HOSTNAME is the name of your computer.\nBut of course, you can change where your experiments are tracked (the filename is as customisable as you’d like).\nThe outputs of the SummaryWriter() are saved in TensorBoard format.\nTensorBoard is a part of the TensorFlow deep learning library and is an excellent way to visualize different parts of your model.\nTo start tracking our modelling experiments, let’s create a default SummaryWriter() instance.\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept:\n    print(\"[INFO] Couldn't find tensorboard... installing it.\")\n    !pip install -q tensorboard\n    from torch.utils.tensorboard import SummaryWriter\n\n\n# Create a writer with all default settings\nwriter = SummaryWriter()\n\nNow to use the writer, we could write a new training loop or we could adjust the existing train() function we created in 05. PyTorch Going Modular section 4.\nLet’s take the latter option.\nWe’ll get the train() function from engine.py and adjust it to use writer.\nSpecifically, we’ll add the ability for our train() function to log our model’s training and test loss and accuracy values.\nWe can do this with writer.add_scalars(main_tag, tag_scalar_dict), where: * main_tag (string) - the name for the scalars being tracked (e.g. “Accuracy”) * tag_scalar_dict (dict) - a dictionary of the values being tracked (e.g. {\"train_loss\": 0.3454}) * &gt; 참고: The method is called add_scalars() because our loss and accuracy values are generally scalars (single values).\nOnce we’ve finished tracking values, we’ll call writer.close() to tell the writer to stop looking for values to track.\nTo start modifying train() we’ll also import train_step() and test_step() from engine.py.\n\n참고: You can track information about your model almost anywhere in your code. But quite often experiments will be tracked while a model is training (inside a training/testing loop).\nThe torch.utils.tensorboard.SummaryWriter() class also has many different methods to track different things about your model/data, such as add_graph() which tracks the computation graph of your model. For more options, check the SummaryWriter() documentation.\n\n\nfrom typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Import train() function from: \n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      \n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n\nWoohoo!\nOur train() function is now updated to use a SummaryWriter() instance to track our model’s results.\nHow about we try it out for 5 epochs?\n\n# Train model\n# 참고: Not using engine.train() since the original script isn't updated to use writer\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n\n\n\n\nEpoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561\nEpoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864\nEpoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049\nEpoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737\n\n\n\n참고: You might notice the results here are slightly different to what our model got in 06. PyTorch Transfer Learning. The difference comes from using the engine.train() and our modified train() function. Can you guess why? The PyTorch documentation on randomness may help more.\n\nRunning the cell above we get similar outputs we got in 06. PyTorch Transfer Learning section 4: Train model but the difference is behind the scenes our writer instance has created a runs/ directory storing our model’s results.\nFor example, the save location might look like:\nruns/Jun21_00-46-03_daniels_macbook_pro\nWhere the default format is runs/CURRENT_DATETIME_HOSTNAME.\nWe’ll check these out in a second but just as a reminder, we were previously tracking our model’s results in a dictionary.\n\n# Check out the model results\nresults\n\n{'train_loss': [1.0923754647374153,\n  0.8974628075957298,\n  0.803724929690361,\n  0.6769256368279457,\n  0.7064960040152073],\n 'train_acc': [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],\n 'test_loss': [0.9132757981618246,\n  0.7837507526079813,\n  0.6722926497459412,\n  0.6698453426361084,\n  0.6746167540550232],\n 'test_acc': [0.5397727272727273,\n  0.8560606060606061,\n  0.8863636363636364,\n  0.8049242424242425,\n  0.7736742424242425]}\n\n\nHmmm, we could format this to be a nice plot but could you image keeping track of a bunch of these dictionaries?\nThere has to be a better way…",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#view-our-models-results-in-tensorboard",
    "href": "07_pytorch_experiment_tracking.html#view-our-models-results-in-tensorboard",
    "title": "07 - PyTorch 실험 추적",
    "section": "5. View our model’s results in TensorBoard",
    "text": "5. View our model’s results in TensorBoard\nThe SummaryWriter() class stores our model’s results in a directory called runs/ in TensorBoard format by default.\nTensorBoard is a visualization program created by the TensorFlow team to view and inspect information about models and data.\nYou know what that means?\nIt’s time to follow the data visualizer’s motto and visualize, visualize, visualize!\nYou can view TensorBoard in a number of ways:\n\n\n\nCode environment\nHow to view TensorBoard\nResource\n\n\n\n\nVS Code (notebooks or Python scripts)\nPress SHIFT + CMD + P to open the Command Palette and search for the command “Python: Launch TensorBoard”.\nVS Code Guide on TensorBoard and PyTorch\n\n\nJupyter and Colab Notebooks\nMake sure TensorBoard is installed, load it with %load_ext tensorboard and then view your results with %tensorboard --logdir DIR_WITH_LOGS.\ntorch.utils.tensorboard and Get started with TensorBoard\n\n\n\nYou can also upload your experiments to tensorboard.dev to share them publicly with others.\nRunning the following code in a Google Colab or Jupyter Notebook will start an interactive TensorBoard session to view TensorBoard files in the runs/ directory.\n%load_ext tensorboard # line magic to load TensorBoard\n%tensorboard --logdir runs # run TensorBoard session with the \"runs/\" directory\n\n# Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n\nIf all went correctly, you should see something like the following:\n\nViewing a single modelling experiment’s results for accuracy and loss in TensorBoard.\n\n참고: For more information on running TensorBoard in notebooks or in other locations, see the following: * Using TensorBoard in Notebooks guide by TensorFlow * Get started with TensorBoard.dev (helpful for uploading your TensorBoard logs to a shareable link)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#create-a-helper-function-to-build-summarywriter-instances",
    "href": "07_pytorch_experiment_tracking.html#create-a-helper-function-to-build-summarywriter-instances",
    "title": "07 - PyTorch 실험 추적",
    "section": "6. Create a helper function to build SummaryWriter() instances",
    "text": "6. Create a helper function to build SummaryWriter() instances\nThe SummaryWriter() class logs various information to a directory specified by the log_dir parameter.\nHow about we make a helper function to create a custom directory per experiment?\nIn essence, each experiment gets its own logs directory.\nFor example, say we’d like to track things like: * Experiment date/timestamp - when did the experiment take place? * Experiment name - is there something we’d like to call the experiment? * Model name - what model was used? * Extra - should anything else be tracked?\nYou could track almost anything here and be as creative as you want but these should be enough to start.\nLet’s create a helper function called create_writer() that produces a SummaryWriter() instance tracking to a custom log_dir.\nIdeally, we’d like the log_dir to be something like:\nruns/YYYY-MM-DD/experiment_name/model_name/extra\nWhere YYYY-MM-DD is the date the experiment was run (you could add the time if you wanted to as well).\n\ndef create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n\nBeautiful!\nNow we’ve got a create_writer() function, let’s try it out.\n\n# Create an example writer\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n\n\nLooking good, now we’ve got a way to log and trace back our various experiments.\n\n6.1 Update the train() function to include a writer parameter\nOur create_writer() function works fantastic.\nHow about we give our train() function the ability to take in a writer parameter so we actively update the SummaryWriter() instance we’re using each time we call train().\nFor example, say we’re running a series of experiments, calling train() multiple times for multiple different models, it would be good if each experiment used a different writer.\nOne writer per experiment = one logs directory per experiment.\nTo adjust the train() function we’ll add a writer parameter to the function and then we’ll add some code to see if there’s a writer and if so, we’ll track our information there.\n\nfrom typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Add writer parameter to train()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#setting-up-a-series-of-modelling-experiments",
    "href": "07_pytorch_experiment_tracking.html#setting-up-a-series-of-modelling-experiments",
    "title": "07 - PyTorch 실험 추적",
    "section": "7. Setting up a series of modelling experiments",
    "text": "7. Setting up a series of modelling experiments\nIt’s to step things up a notch.\nPreviously we’ve been running various experiments and inspecting the results one by one.\nBut what if we could run multiple experiments and then inspect the results all together?\nYou in?\nC’mon, let’s go.\n\n7.1 What kind of experiments should you run?\nThat’s the million dollar question in machine learning.\nBecause there’s really no limit to the experiments you can run.\nSuch a freedom is why machine learning is so exciting and terrifying at the same time.\nThis is where you’ll have to put on your scientist coat and remember the machine learning practitioner’s motto: experiment, experiment, experiment!\nEvery hyperparameter stands as a starting point for a different experiment: * Change the number of epochs. * Change the number of layers/hidden units. * Change the amount of data. * Change the learning rate. * Try different kinds of data augmentation. * Choose a different model architecture.\nWith practice and running many different experiments, you’ll start to build an intuition of what might help your model.\nI say might on purpose because there’s no guarantees.\nBut generally, in light of The Bitter Lesson (I’ve mentioned this twice now because it’s an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.\nHowever, when you’re first approaching a machine learning problem: start small and if something works, scale it up.\nYour first batch of experiments should take no longer than a few seconds to a few minutes to run.\nThe quicker you can experiment, the faster you can work out what doesn’t work, in turn, the faster you can work out what does work.\n\n\n7.2 What experiments are we going to run?\nOur goal is to improve the model powering FoodVision Mini without it getting too big.\nIn essence, our ideal model achieves a high level of test set accuracy (90%+) but doesn’t take too long to train/perform inference (make predictions).\nWe’ve got plenty of options but how about we keep things simple?\nLet’s try a combination of: 1. A different amount of data (10% of Pizza, Steak, Sushi vs. 20%) 2. A different model (torchvision.models.efficientnet_b0 vs. torchvision.models.efficientnet_b2) 3. A different training time (5 epochs vs. 10 epochs)\nBreaking these down we get:\n\n\n\n\n\n\n\n\n\nExperiment number\nTraining Dataset\nModel (pretrained on ImageNet)\nNumber of epochs\n\n\n\n\n1\nPizza, Steak, Sushi 10% percent\nEfficientNetB0\n5\n\n\n2\nPizza, Steak, Sushi 10% percent\nEfficientNetB2\n5\n\n\n3\nPizza, Steak, Sushi 10% percent\nEfficientNetB0\n10\n\n\n4\nPizza, Steak, Sushi 10% percent\nEfficientNetB2\n10\n\n\n5\nPizza, Steak, Sushi 20% percent\nEfficientNetB0\n5\n\n\n6\nPizza, Steak, Sushi 20% percent\nEfficientNetB2\n5\n\n\n7\nPizza, Steak, Sushi 20% percent\nEfficientNetB0\n10\n\n\n8\nPizza, Steak, Sushi 20% percent\nEfficientNetB2\n10\n\n\n\nNotice how we’re slowly scaling things up.\nWith each experiment we slowly increase the amount of data, the model size and the length of training.\nBy the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.\n\n참고: I want to be clear that there truly is no limit to amount of experiments you can run. What we’ve designed here is only a very small subset of options. However, you can’t test everything so best to try a few things to begin with and then follow the ones which work the best.\nAnd as a reminder, the datasets we’re using are a subset of the Food101 dataset (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the 04_custom_data_creation.ipynb notebook.\n\n\n\n7.3 Download different datasets\nBefore we start running our series of experiments, we need to make sure our datasets are ready.\nWe’ll need two forms of a training set: 1. A training set with 10% of the data of Food101 pizza, steak, sushi images (we’ve already created this above but we’ll do it again for completeness). 2. A training set with 20% of the data of Food101 pizza, steak, sushi images.\nFor consistency, all experiments will use the same testing dataset (the one from the 10% data split).\nWe’ll start by downloading the various datasets we need using the download_data() function we created earlier.\nBoth datasets are available from the course GitHub: 1. Pizza, steak, sushi 10% training data. 2. Pizza, steak, sushi 20% training data.\n\n# Download 10 percent and 20 percent training data (if necessary)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\n[INFO] data/pizza_steak_sushi directory exists, skipping download.\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n\n\nData downloaded!\nNow let’s setup the filepaths to data we’ll be using for the different experiments.\nWe’ll create different training directory paths but we’ll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%).\n\n# Setup training directory paths\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Setup testing directory paths (note: use the same test dataset for both to compare the results)\ntest_dir = data_10_percent_path / \"test\"\n\n# Check the directories\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n\nTraining directory 10%: data/pizza_steak_sushi/train\nTraining directory 20%: data/pizza_steak_sushi_20_percent/train\nTesting directory: data/pizza_steak_sushi/test\n\n\n\n\n7.4 Transform Datasets and create DataLoaders\nNext we’ll create a series of transforms to prepare our images for our model(s).\nTo keep things consistent, we’ll manually create a transform (just like we did above) and use the same transform across all of the datasets.\nThe transform will: 1. Resize all the images (we’ll start with 224, 224 but this could be changed). 2. Turn them into tensors with values between 0 & 1. 3. Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from torchvision.models have been pretrained on ImageNet).\n\nfrom torchvision import transforms\n\n# Create a transform to normalize data distribution to be inline with ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose transforms into a pipeline\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n\nTransform ready!\nNow let’s create our DataLoaders using the create_dataloaders() function from data_setup.py we created in 05. PyTorch Going Modular section 2.\nWe’ll create the DataLoaders with a batch size of 32.\nFor all of our experiments we’ll be using the same test_dataloader (to keep comparisons consistent).\n\nBATCH_SIZE = 32\n\n# Create 10% training and test DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create 20% training and test data DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n\nNumber of batches of size 32 in 10 percent training data: 8\nNumber of batches of size 32 in 20 percent training data: 15\nNumber of batches of size 32 in testing data: 8 (all experiments will use the same test set)\nNumber of classes: 3, class names: ['pizza', 'steak', 'sushi']\n\n\n\n\n7.5 Create feature extractor models\nTime to start building our models.\nWe’re going to create two feature extractor models:\n\ntorchvision.models.efficientnet_b0() pretrained backbone + custom classifier head (EffNetB0 for short).\ntorchvision.models.efficientnet_b2() pretrained backbone + custom classifier head (EffNetB2 for short).\n\nTo do this, we’ll freeze the base layers (the feature layers) and update the model’s classifier heads (output layers) to suit our problem just like we did in 06. PyTorch Transfer Learning section 3.4.\nWe saw in the previous chapter the in_features parameter to the classifier head of EffNetB0 is 1280 (the backbone turns the input image into a feature vector of size 1280).\nSince EffNetB2 has a different number of layers and parameters, we’ll need to adapt it accordingly.\n\n참고: Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you’ll know how you’ll have to prepare your input data/update the model to have the correct output shape.\n\nWe can find the input and output shapes of EffNetB2 using torchinfo.summary() and passing in the input_size=(32, 3, 224, 224) parameter ((32, 3, 224, 224) is equivalent to (batch_size, color_channels, height, width), i.e we pass in an example of what a single batch of data would be to our model).\n\n참고: Many modern models can handle input images of varying sizes thanks to torch.nn.AdaptiveAvgPool2d() layer, this layer adaptively adjusts the output_size of a given input as required. You can try this out by passing different size input images to torchinfo.summary() or to your own models using the layer.\n\nTo find the required input shape to the final layer of EffNetB2, let’s: 1. Create an instance of torchvision.models.efficientnet_b2(pretrained=True). 2. See the various input and output shapes by running torchinfo.summary(). 3. Print out the number of in_features by inspecting state_dict() of the classifier portion of EffNetB2 and printing the length of the weight matrix. * 참고: You could also just inspect the output of effnetb2.classifier.\n\nimport torchvision\nfrom torchinfo import summary\n\n# 1. Create an instance of EffNetB2 with pretrained weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n# 3. Get the number of in_features of the EfficientNetB2 classifier layer\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n\nNumber of in_features to final layer of EfficientNetB2: 1408\n\n\n\nModel summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.\nNow we know the required number of in_features for the EffNetB2 model, let’s create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.\nWe want these functions to: 1. Get the base model from torchvision.models 2. Freeze the base layers in the model (set requires_grad=False) 3. Set the random seeds (we don’t need to do this but since we’re running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment) 4. Change the classifier head (to suit our problem) 5. Give the model a name (e.g. “effnetb0” for EffNetB0)\n\nimport torchvision\nfrom torch import nn\n\n# Get num out features (one for each class pizza, steak, sushi)\nOUT_FEATURES = len(class_names)\n\n# Create an EffNetB0 feature extractor\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Create an EffNetB2 feature extractor\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\nThose are some nice looking functions!\nLet’s test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their summary().\n\neffnetb0 = create_effnetb0() \n\n# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n# summary(model=effnetb0, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n[INFO] Created new effnetb0 model.\n\n\n\nModel summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).\n\neffnetb2 = create_effnetb2()\n\n# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n[INFO] Created new effnetb2 model.\n\n\n\nModel summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).\nLooking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.\n\n\n\n\n\n\n\n\n\nModel\nTotal parameters (before freezing/changing head)\nTotal parameters (after freezing/changing head)\nTotal trainable parameters (after freezing/changing head)\n\n\n\n\nEfficientNetB0\n5,288,548\n4,011,391\n3,843\n\n\nEfficientNetB2\n9,109,994\n7,705,221\n4,227\n\n\n\nThis gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.\nHowever, the trainable parameters for each model (the classifier heads) aren’t very different.\nWill these extra parameters lead to better results?\nWe’ll have to wait and see…\n\n참고: In the spirit of experimenting, you really could try almost any model from torchvision.models in a similar fashion to what we’re doing here. I’ve only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like torchvision.models.convnext_tiny() or torchvision.models.convnext_small() into the mix.\n\n\n\n7.6 Create experiments and set up training code\nWe’ve prepared our data and prepared our models, the time has come to setup some experiments!\nWe’ll start by creating two lists and a dictionary: 1. A list of the number of epochs we’d like to test ([5, 10]) 2. A list of the models we’d like to test ([\"effnetb0\", \"effnetb2\"]) 3. A dictionary of the different training DataLoaders\n\n# 1. Create epochs list\nnum_epochs = [5, 10]\n\n# 2. Create models list (need to create a new model for each experiment)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Create dataloaders dictionary for various dataloaders\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n\nLists and dictionary created!\nNow we can write code to iterate through each of the different options and try out each of the different combinations.\nWe’ll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.\nSpecifically, let’s go through the following steps: 1. Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results). 2. Keep track of different experiment numbers (this is mostly for pretty print outs). 3. Loop through the train_dataloaders dictionary items for each of the different training DataLoaders. 4. Loop through the list of epoch numbers. 5. Loop through the list of different model names. 6. Create information print outs for the current running experiment (so we know what’s happening). 7. Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint). 8. Create a new loss function (torch.nn.CrossEntropyLoss()) and optimizer (torch.optim.Adam(params=model.parameters(), lr=0.001)) for each new experiment. 9. Train the model with the modified train() function passing the appropriate details to the writer parameter. 10. Save the trained model with an appropriate file name to file with save_model() from utils.py.\nWe can also use the %%time magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.\nLet’s do it!\n\n%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Set the random seeds\nset_seeds(seed=42)\n\n# 2. Keep track of experiment numbers\nexperiment_number = 0\n\n# 3. Loop through each DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n\n[INFO] Experiment number: 1\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 2\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 3\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\nEpoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864\nEpoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864\nEpoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759\nEpoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873\nEpoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 4\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\nEpoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769\nEpoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977\nEpoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466\nEpoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 5\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 6\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 7\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\nEpoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072\nEpoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072\nEpoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873\nEpoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280\nEpoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 8\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\nEpoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977\nEpoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384\nEpoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280\nEpoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\nCPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s\nWall time: 2min 33s",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#view-experiments-in-tensorboard",
    "href": "07_pytorch_experiment_tracking.html#view-experiments-in-tensorboard",
    "title": "07 - PyTorch 실험 추적",
    "section": "8. View experiments in TensorBoard",
    "text": "8. View experiments in TensorBoard\nHo, ho!\nLook at us go!\nTraining eight models in one go?\nNow that’s living up to the motto!\nExperiment, experiment, experiment!\nHow about we check out the results in TensorBoard?\n\n# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n\nRunning the cell above we should get an output similar to the following.\n\n참고: Depending on the random seeds you used/hardware you used there’s a chance your numbers aren’t exactly the same as what’s here. This is okay. It’s due to the inherent randomness of deep learning. What matters most is the trend. Where your numbers are heading. If they’re off by a large amount, perhaps there’s something wrong and best to go back and check the code. But if they’re off by a small amount (say a couple of decimal places or so), that’s okay.\n\n\nVisualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB2 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.\nYou can also upload your TensorBoard experiment results to tensorboard.dev to host them publically for free.\nFor example, running code similiar to the following:\n\n# # Upload the results to TensorBoard.dev (uncomment to try it out)\n# !tensorboard dev upload --logdir runs \\\n#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n#     --description \"Comparing results of different model size, training data amount and training time.\"\n\nRunning the cell above results in the experiments from this notebook being publically viewable at: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/\n\n참고: Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they don’t contain sensitive information.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#load-in-the-best-model-and-make-predictions-with-it",
    "href": "07_pytorch_experiment_tracking.html#load-in-the-best-model-and-make-predictions-with-it",
    "title": "07 - PyTorch 실험 추적",
    "section": "9. Load in the best model and make predictions with it",
    "text": "9. Load in the best model and make predictions with it\nLooking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).\nThis is the experiment that used: * EffNetB2 (double the parameters of EffNetB0) * 20% pizza, steak, sushi training data (double the original training data) * 10 epochs (double the original training time)\nIn essence, our biggest model achieved the best results.\nThough it wasn’t as if these results were far better than the other models.\nThe same model on the same data achieved similar results in half the training time (experiment number 6).\nThis suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.\nInspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).\nMore experiments could be done to further test this but for now, let’s import our best performing model from experiment eight (saved to: models/07_effnetb2_data_20_percent_10_epochs.pth, you can download this model from the course GitHub) and perform some qualitative evaluations.\nIn other words, let’s visualize, visualize, visualize!\nWe can import the best saved model by creating a new instance of EffNetB2 using the create_effnetb2() function and then load in the saved state_dict() with torch.load().\n\n# Setup the best model filepath\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\nbest_model = create_effnetb2()\n\n# Load the saved best model state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n\n[INFO] Created new effnetb2 model.\n\n\n&lt;All keys matched successfully&gt;\n\n\nBest model loaded!\nWhile we’re here, let’s check its filesize.\nThis is an important consideration later on when deploying the model (incorporating it in an app).\nIf the model is too large, it can be hard to deploy.\n\n# Check the model file size\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n\nEfficientNetB2 feature extractor model size: 29 MB\n\n\nLooks like our best model so far is 29 MB in size. We’ll keep this in mind if we wanted to deploy it later on.\nTime to make and visualize some predictions.\nWe created a pred_and_plot_image() function use a trained model to make predictions on an image in 06. PyTorch Transfer Learning section 6.\nAnd we can reuse this function by importing it from going_modular.going_modular.predictions.py (I put the pred_and_plot_image() function in a script so we could reuse it).\nSo to make predictions on various images the model hasn’t seen before, we’ll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then we’ll randomly select a subset of these filepaths to pass to our pred_and_plot_image() function.\n\n# Import function to make predictions on images and plot them \n# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Get a random list of 3 images from 20% test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterate through random test image paths, make predictions on them and plot them\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNice!\nRunning the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models we’ve built.\nThis suggests the model is more confident in the decisions it’s making.\n\n9.1 Predict on a custom image with the best model\nMaking predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.\nSo let’s import the trusty pizza dad image (a photo of my dad in front of a pizza) we’ve been using for the past couple of sections and see how our model performs on it.\n\n# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n\ndata/04-pizza-dad.jpeg already exists, skipping download.\n\n\n\n\n\n\n\n\n\nWoah!\nTwo thumbs again!\nOur best model predicts “pizza” correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in 06. PyTorch Transfer Learning section 6.1.\nThis again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.\nI wonder what could improve our model’s performance even further?\nI’ll leave that as a challenge for you to investigate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#main-takeaways",
    "href": "07_pytorch_experiment_tracking.html#main-takeaways",
    "title": "07 - PyTorch 실험 추적",
    "section": "Main takeaways",
    "text": "Main takeaways\nWe’ve now gone full circle on the PyTorch workflow introduced in 01. PyTorch Workflow Fundamentals, we’ve gotten data ready, we’ve built and picked a pretrained model, we’ve used our various helper functions to train and evaluate the model and in this notebook we’ve improved our FoodVision Mini model by running and tracking a series of experiments.\n\nYou should be proud of yourself, this is no small feat!\nThe main ideas you should take away from this Milestone Project 1 are:\n\nThe machine learning practioner’s motto: experiment, experiment, experiment! (though we’ve been doing plenty of this already).\nIn the beginning, keep your experiments small so you can work fast, your first few experiments shouldn’t take more than a few seconds to a few minutes to run.\nThe more experiments you do, the quicker you can figure out what doesn’t work.\nScale up when you find something that works. For example, since we’ve found a pretty good performing model with EffNetB2 as a feature extractor, perhaps you’d now like to see what happens when you scale it up to the whole Food101 dataset from torchvision.datasets.\nProgrammatically tracking your experiments takes a few steps to set up but it’s worth it in the long run so you can figure out what works and what doesn’t.\n\nThere are many different machine learning experiment trackers out there so explore a few and try them out.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#연습-문제",
    "href": "07_pytorch_experiment_tracking.html#연습-문제",
    "title": "07 - PyTorch 실험 추적",
    "section": "연습 문제",
    "text": "연습 문제\n\n참고: These exercises expect the use of torchvision v0.13+ (currently in beta as of June 2022). You can install the nightly version via the PyTorch getting started page.\n\nAll of the exercises are focused on practicing the code above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 07 * Example solutions notebook for 07 (try the exercises before looking at this) * See a live video walkthrough of the solutions on YouTube (errors and all)\n\nPick a larger model from torchvision.models to add to the list of experiments (for example, EffNetB3 or higher).\n\nHow does it perform compared to our existing models?\n\nIntroduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n\nFor example, you could have one training DataLoader that uses data augmentation (e.g. train_dataloader_20_percent_aug and train_dataloader_20_percent_no_aug) and then compare the results of two of the same model types training on these two DataLoaders.\n참고: You may need to alter the create_dataloaders() function to be able to take a transform for the training data and the testing data (because you don’t need to perform data augmentation on the test data). See 04. PyTorch Custom Datasets section 6 for examples of using data augmentation or the script below for an example:\n\n\n# 참고: Data augmentation transform like this should only be performed on training data\ntrain_transform_data_aug = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Helper function to view images in a DataLoader (works with data augmentation transforms or not) \ndef view_dataloader_images(dataloader, n=10):\n    if n &gt; 10:\n        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n        n = 10\n    imgs, labels = next(iter(dataloader))\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        # Min max scale the image for display purposes\n        targ_image = imgs[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # Plot images with appropriate axes information\n        plt.subplot(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n        plt.title(class_names[labels[i]])\n        plt.axis(False)\n\n# Have to update `create_dataloaders()` to handle different augmentations\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\nNUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n\n# 참고: this is an update version of data_setup.create_dataloaders to handle\n# differnt train and test transforms.\ndef create_dataloaders(\n    train_dir, \n    test_dir, \n    train_transform, # add parameter for train transform (transforms on train dataset)\n    test_transform,  # add parameter for test transform (transforms on test dataset)\n    batch_size=32, num_workers=NUM_WORKERS\n):\n    # Use ImageFolder to create dataset(s)\n    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    # Get class names\n    class_names = train_data.classes\n\n    # Turn images into data loaders\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names\n\nScale up the dataset to turn FoodVision Mini into FoodVision Big using the entire Food101 dataset from torchvision.models\n\nYou could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\nIf you try more than one model, it would be good to have the model’s results tracked.\nIf you load the Food101 dataset from torchvision.models, you’ll have to create PyTorch DataLoaders to use it in training.\n참고: Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#추가-학습-자료",
    "href": "07_pytorch_experiment_tracking.html#추가-학습-자료",
    "title": "07 - PyTorch 실험 추적",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\nRead The Bitter Lesson blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.\nGo through the PyTorch YouTube/code tutorial for TensorBoard for 20-minutes and see how it compares to the code we’ve written in this notebook.\nPerhaps you may want to view and rearrange your model’s TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), there’s a guide for this in the TensorBoard documentation.\nIf you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the PyTorch Development in VSCode guide.\nTo go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the PyTorch documentation for the PyTorch profiler.\nMade With ML is an outstanding resource for all things machine learning by Goku Mohandas and their guide on experiment tracking contains a fantastic introduction to tracking machine learning experiments with MLflow.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>07 - PyTorch 실험 추적</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html",
    "href": "08_pytorch_paper_replicating.html",
    "title": "08 - PyTorch 논문 복제",
    "section": "",
    "text": "TK - 논문 복제(Paper Replicating)란 무엇인가요?\nTK 서론\nViT 논문 재현하기: “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” - https://arxiv.org/abs/2010.11929 - 본 노트북 전체에서 이를 “ViT 논문”으로 지칭합니다.\n머신러닝이 빠르게 발전하고 있다는 사실은 비밀이 아닙니다.\n이러한 발전의 상당수는 머신러닝 연구 논문에 발표됩니다.\n그리고 논문 복제의 목표는 이러한 발전을 코드로 재현하여 자신의 문제에 기술을 사용할 수 있도록 하는 것입니다.\n예를 들어, 다양한 벤치마크에서 이전의 어떤 아키텍처보다 성능이 뛰어난 새로운 모델 아키텍처가 출시되었다고 가정해 봅시다. 해당 아키텍처를 여러분의 문제에 직접 시도해 보는 것이 좋지 않을까요?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---논문-복제paper-replicating란-무엇인가요",
    "href": "08_pytorch_paper_replicating.html#tk---논문-복제paper-replicating란-무엇인가요",
    "title": "08 - PyTorch 논문 복제",
    "section": "",
    "text": "TK 이미지: 논문 복제 = 연구 논문 (언어 + 도표 + 수학) -&gt; 코드 (언어, 도표, 수학을 사용 가능한 코드로 전환) / (연구 논문을 사용 가능한 코드로 번역)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---머신러닝-연구-논문이란-무엇인가요",
    "href": "08_pytorch_paper_replicating.html#tk---머신러닝-연구-논문이란-무엇인가요",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 머신러닝 연구 논문이란 무엇인가요?",
    "text": "TK - 머신러닝 연구 논문이란 무엇인가요?\n머신러닝 연구 논문은 특정 분야에 대한 연구 그룹의 연구 결과를 상세히 기술한 과학 논문입니다.\n머신러닝 연구 논문의 내용은 논문마다 다를 수 있지만 일반적으로 다음과 같은 구조를 따릅니다.\n\n\n\n\n\n\n\n섹션\n내용\n\n\n\n\n초록 (Abstract)\n논문의 주요 결과/기여에 대한 개요/요약입니다.\n\n\n서론 (Introduction)\n논문의 주요 문제와 이를 해결하기 위해 사용된 이전 방법은 무엇인가요?\n\n\n방법 (Method)\n연구자들은 연구를 어떻게 수행했나요? 예를 들어, 어떤 모델이 사용되었는지, 데이터 소스, 훈련 설정 등입니다.\n\n\n결과 (Results)\n논문의 성과는 무엇인가요? 새로운 유형의 모델이나 훈련 설정이 사용된 경우, 연구 결과가 이전 작업과 어떻게 비교되었나요(여기에서 실험 추적이 유용합니다)?\n\n\n결론 (Conclusion)\n제안된 방법의 한계는 무엇인가요? 연구 커뮤니티를 위한 다음 단계는 무엇인가요?\n\n\n참고 문헌 (References)\n연구자들이 자신의 작업을 구축하기 위해 살펴본 리소스/다른 논문은 무엇인가요?\n\n\n부록 (Appendix)\n위의 섹션에 포함되지 않은 추가 리소스/결과가 있나요?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---머신러닝-연구-논문을-복제하는-이유는-무엇인가요",
    "href": "08_pytorch_paper_replicating.html#tk---머신러닝-연구-논문을-복제하는-이유는-무엇인가요",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 머신러닝 연구 논문을 복제하는 이유는 무엇인가요?",
    "text": "TK - 머신러닝 연구 논문을 복제하는 이유는 무엇인가요?\n머신러닝 연구 논문은 종종 세계 최고의 머신러닝 팀이 수행한 수개월 간의 작업과 실험을 몇 페이지의 텍스트로 압축하여 발표한 결과물입니다.\n그리고 이러한 실험이 작업 중인 문제와 관련된 영역에서 더 나은 결과로 이어진다면, 이를 시도해 보는 것이 좋습니다.\n또한 다른 사람의 작업을 재현하는 것은 기술을 연습할 수 있는 환상적인 방법입니다.\n\nGeorge Hotz는 자율 주행 자동차 회사인 comma.ai의 설립자이며 Twitch에서 머신러닝 코딩을 라이브 스트리밍하고 해당 동영상은 YouTube에 전체로 게시됩니다. 그의 라이브 스트림 중 하나에서 이 인용문을 가져왔습니다. “٭”는 머신러닝 엔지니어링에는 종종 데이터를 전처리하고 다른 사람이 사용할 수 있도록 모델을 제공하는 추가 단계(배포)가 포함된다는 점을 참고하기 위한 것입니다.\n연구 논문 복제를 처음 시작할 때는 압도당할 가능성이 높습니다.\n그것은 정상입니다.\n연구 팀은 이러한 성과물을 만들기 위해 수주, 수개월, 때로는 수년을 소비하므로 연구 성과물을 재현하는 것은 고사하고 읽는 데도 어느 정도 시간이 걸리는 것은 당연합니다.\n연구 복제는 매우 어려운 문제이기 때문에 HuggingFace, PyTorch Image Models (timm 라이브러리) 및 fast.ai와 같은 놀라운 머신러닝 라이브러리와 도구가 머신러닝 연구를 더 쉽게 접근할 수 있도록 탄생했습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---머신러닝-연구-논문에-대한-코드-예제는-어디에서-찾을-수-있나요",
    "href": "08_pytorch_paper_replicating.html#tk---머신러닝-연구-논문에-대한-코드-예제는-어디에서-찾을-수-있나요",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 머신러닝 연구 논문에 대한 코드 예제는 어디에서 찾을 수 있나요?",
    "text": "TK - 머신러닝 연구 논문에 대한 코드 예제는 어디에서 찾을 수 있나요?\n머신러닝 연구와 관련하여 가장 먼저 눈에 띄는 것 중 하나는 연구 자료가 매우 많다는 것입니다.\n따라서 최신 상태를 유지하려고 노력하는 것은 쳇바퀴를 돌리는 것과 같다는 점을 주의하세요.\n관심을 따라가며 눈에 띄는 몇 가지를 선택하세요.\n이와 관련하여 머신러닝 연구 논문을 찾고 읽을 수 있는 장소는 다음과 같습니다. * arXiv - “archive”라고 발음하는 arXiv는 물리학에서 컴퓨터 과학(머신러닝 포함)에 이르기까지 모든 분야의 기술 기사를 읽을 수 있는 무료 개방형 리소스입니다. * Papers with Code - 트렌드, 활발하고 위대한 머신러닝 논문을 엄선한 컬렉션으로, 대부분 코드 리소스가 포함되어 있습니다. 또한 일반적인 머신러닝 데이터셋, 벤치마크 및 현재 최첨단(state-of-the-art) 모델의 컬렉션도 포함되어 있습니다. * AK Twitter - AK Twitter 계정은 거의 매일 라이브 데모와 함께 머신러닝 연구 하이라이트를 게시합니다. 게시물의 9/10은 이해하지 못하지만 가끔씩 살펴보는 것이 재미있습니다. * lucidrains의 vit-pytorch GitHub 저장소 - 연구 논문을 찾는 곳이라기보다는 대규모 코드 복제 논문의 예시입니다. vit-pytorch 저장소는 PyTorch 코드로 재현된 다양한 연구 논문의 Vision Transformer 모델 아키텍처 컬렉션입니다(본 노트북의 많은 영감을 이 저장소에서 얻었습니다).\nTK 이미지: 위 내용 전시",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---이-모듈에서-다룰-내용",
    "href": "08_pytorch_paper_replicating.html#tk---이-모듈에서-다룰-내용",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 이 모듈에서 다룰 내용",
    "text": "TK - 이 모듈에서 다룰 내용\n할 일\n\nViT -&gt; FoodVision Mini\n레이어(Layers) = 데이터를 조작하기 위한 함수의 컬렉션 -&gt; 아키텍처(Architectures) = 레이어의 컬렉션 (블록) -&gt; 모든 레이어(및 블록)에는 입력과 출력이 있음\n\n연구 논문 복제는 레이어 -&gt; 블록 -&gt; 모델의 입력과 출력을 파악하는 것부터 시작됩니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---도움을-받을-수-있는-곳",
    "href": "08_pytorch_paper_replicating.html#tk---도움을-받을-수-있는-곳",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 도움을 받을 수 있는 곳",
    "text": "TK - 도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 개발자 포럼과 PyTorch 문서도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-0.-설정하기",
    "href": "08_pytorch_paper_replicating.html#tk-0.-설정하기",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 0. 설정하기",
    "text": "TK 0. 설정하기\n이전에 했던 것처럼 이 섹션에 필요한 모든 모듈이 있는지 확인해 보겠습니다.\n05. PyTorch 모듈화에서 만든 Python 스크립트(data_setup.py 및 engine.py 등)를 활용하겠습니다.\n이를 위해 pytorch-deep-learning 저장소에서 going_modular 디렉토리를 다운로드합니다(이미 가지고 있지 않은 경우).\n또한 torchinfo 패키지가 없는 경우 가져옵니다.\ntorchinfo는 나중에 모델의 시각적 요약을 제공하는 데 도움이 됩니다.\n그리고 나중에 torchvision 패키지의 최신 버전(2022년 6월 현재 v0.13)을 사용할 것이므로 최신 버전이 있는지 확인하겠습니다.\n\n# 업데이트된 API로 이 노트북을 실행하려면 torch 1.12+ 및 torchvision 0.13+가 필요합니다.\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision 버전이 요구 사항을 충족하지 않음, nightly 버전을 설치합니다.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\n\n참고: Google Colab을 사용하는 경우 위 셀을 실행한 후 런타임을 다시 시작해야 할 수도 있습니다. 다시 시작한 후 셀을 다시 실행하고 올바른 버전의 torch 및 torchvision이 있는지 확인할 수 있습니다.\n\n이제 일반적인 임포트, 장치 독립적(device agnostic) 코드 설정을 계속하고 이번에는 GitHub에서 helper_functions.py 스크립트도 가져오겠습니다.\nhelper_functions.py 스크립트에는 이전 섹션에서 만든 몇 가지 함수가 포함되어 있습니다. * set_seeds(): 무작위 시드 설정(07. PyTorch 실험 추적 섹션 0에서 생성). * download_data(): 링크가 주어지면 데이터 소스 다운로드(07. PyTorch 실험 추적 섹션 1에서 생성). * plot_loss_curves(): 모델의 훈련 결과 검사(04. PyTorch 사용자 정의 데이터셋 섹션 7.8에서 생성).\n\n참고: helper_functions.py 스크립트의 많은 함수를 going_modular/going_modular/utils.py로 병합하는 것이 더 좋은 아이디어일 수 있습니다. 그것이 여러분이 시도해 볼 수 있는 확장 작업일 것입니다.\n\n\n# 일반적인 임포트 계속\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# torchinfo 가져오기 시도, 실패 시 설치\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] torchinfo를 찾을 수 없음... 설치합니다.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# going_modular 디렉토리 임포트 시도, 실패 시 GitHub에서 다운로드\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # going_modular 스크립트 가져오기\n    print(\"[INFO] going_modular 또는 helper_functions 스크립트를 찾을 수 없음... GitHub에서 다운로드합니다.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # helper_functions.py 스크립트 가져오기\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n\n\n참고: Google Colab을 사용 중이고 아직 GPU를 켜지 않았다면 지금 런타임 -&gt; 런타임 유형 변경 -&gt; 하드웨어 가속기 -&gt; GPU를 통해 켤 시간입니다.\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-1.-데이터-가져오기",
    "href": "08_pytorch_paper_replicating.html#tk-1.-데이터-가져오기",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 1. 데이터 가져오기",
    "text": "TK 1. 데이터 가져오기\nFoodVision Mini를 계속 진행하고 있으므로 사용해 온 피자, 스테이크, 초밥 이미지 데이터셋을 다운로드해 보겠습니다.\n이를 위해 07. PyTorch 실험 추적 섹션 1에서 만든 helper_functions.py의 download_data() 함수를 사용할 수 있습니다.\npizza_steak_sushi.zip 데이터의 raw GitHub 링크를 source로, destination을 pizza_steak_sushi로 설정합니다.\n\n# GitHub에서 피자, 스테이크, 초밥 이미지 다운로드\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n\n좋습니다! 데이터가 다운로드되었습니다. 훈련 및 테스트 디렉토리를 설정하겠습니다.\n\n# 훈련 및 테스트 이미지 디렉토리 경로 설정\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-2.-dataset-및-dataloader-만들기",
    "href": "08_pytorch_paper_replicating.html#tk-2.-dataset-및-dataloader-만들기",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 2. Dataset 및 DataLoader 만들기",
    "text": "TK 2. Dataset 및 DataLoader 만들기\n데이터가 있으므로 이제 DataLoader로 변환해 보겠습니다.\n이를 위해 data_setup.py의 create_dataloaders() 함수를 사용할 수 있습니다.\n먼저 이미지를 준비하기 위한 트랜스폼(transform)을 만들겠습니다.\n여기서 ViT 논문에 대한 첫 번째 참조 중 하나가 나타납니다.\n표 3에서 훈련 해상도는 224(높이=224, 너비=224)로 언급되어 있습니다.\n\n테이블에서 나열된 다양한 하이퍼파라미터 설정을 종종 찾을 수 있습니다. 이 경우 우리는 여전히 데이터를 준비하고 있으므로 주로 이미지 크기 및 배치 크기와 같은 사항에 관심이 있습니다. 출처: ViT 논문의 표 3.\n따라서 트랜스폼이 이미지 크기를 적절하게 조정하도록 합니다.\n그리고 처음부터 모델을 훈련할 것이므로(처음에는 전이 학습 없음), 06. PyTorch 전이 학습 섹션 2.1에서 했던 것처럼 normalize 트랜스폼을 제공하지 않겠습니다.\n\n2.1 이미지 트랜스폼 준비\n\n# 이미지 크기 생성 (ViT 논문의 표 3에서 가져옴) \nIMG_SIZE = 224\n\n# 트랜스폼 파이프라인 수동 생성\nmanual_transforms = transforms.Compose([\n    \"transforms.Resize((IMG_SIZE, IMG_SIZE)),\\n\",\n    \"transforms.ToTensor(),\\n\",\n])\"\n\n\n\n2.2 이미지를 DataLoader로 변환\n트랜스폼이 생성되었습니다!\n이제 DataLoader를 만들어 보겠습니다.\nViT 논문에서는 배치 크기 4096을 사용한다고 명시되어 있는데, 이는 우리가 사용해 온 배치 크기(32)의 128배입니다.\n우리는 배치 크기 32를 유지할 것입니다.\n왜 그럴까요?\n일부 하드웨어(Google Colab의 무료 티어 포함)는 배치 크기 4096을 처리하지 못할 수도 있기 때문입니다.\n배치 크기가 4096이라는 것은 한 번에 4096개의 이미지가 GPU 메모리에 들어갈 수 있어야 함을 의미합니다.\n이는 Google의 연구 팀처럼 하드웨어를 갖춘 경우에 작동하지만, 단일 GPU(예: Google Colab 사용)에서 실행하는 경우 먼저 더 작은 배치 크기로 작동하는지 확인하는 것이 좋습니다.\n더 높은 배치 크기 값을 시도해 보고 어떤 일이 일어나는지 확인하는 것이 이 프로젝트의 확장이 될 수 있습니다.\n\n참고: 계산 속도를 높이기 위해 create_dataloaders() 함수에서 pin_memory=True 매개변수를 사용하고 있습니다. pin_memory=True는 이전에 본 예제를 “고정(pinning)”하여 CPU와 GPU 메모리 간의 불필요한 메모리 복사를 방지합니다. 이에 대한 더 자세한 내용은 PyTorch torch.utils.data.DataLoader 문서 또는 Horace He의 Making Deep Learning Go Brrrr from First Principles를 참조하세요. 비록 이 기능의 이점은 더 큰 데이터셋 크기에서 나타날 가능성이 높지만(우리의 FoodVision Mini 데이터셋은 매우 작음) 말이죠.\n\n\n# 배치 크기 설정\nBATCH_SIZE = 32 # ViT 논문보다 낮지만 작게 시작하기 때문입니다.\n\n# 데이터 로더 생성\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # 수동으로 생성한 트랜스폼 사용\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n\n\n\nTK 2.3 단일 이미지 시각화\n이제 데이터를 로드했으므로 시각화, 시각화, 시각화! 해보겠습니다.\nViT 논문에서 중요한 단계는 이미지를 패치(patch)로 준비하는 것입니다.\n이게 무엇을 의미하는지는 곧 살펴보겠지만, 지금은 단일 이미지와 그 레이블을 보겠습니다.\n이를 위해 데이터 배치에서 단일 이미지와 레이블을 가져와 모양을 확인해 보겠습니다.\n\n# 이미지 배치 가져오기\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# 배치에서 단일 이미지 가져오기\nimage, label = image_batch[0], label_batch[0]\n\n# 배치 모양 확인\nimage.shape, label\n\n좋습니다!\n이제 matplotlib을 사용하여 이미지와 레이블을 그려보겠습니다.\n\n# matplotlib을 사용하여 이미지 그리기\nplt.imshow(image.permute(1, 2, 0)) # matplotlib에 맞게 이미지 차원 재배열 [색상_채널, 높이, 너비] -&gt; [높이, 너비, 색상_채널]\nplt.title(class_names[label])\nplt.axis(False);\n\n좋네요!\n이미지가 올바르게 임포트된 것 같습니다. 논문 재현을 계속해 보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-3.-vit-논문-재현-개요",
    "href": "08_pytorch_paper_replicating.html#tk-3.-vit-논문-재현-개요",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 3. ViT 논문 재현: 개요",
    "text": "TK 3. ViT 논문 재현: 개요\n더 많은 코드를 작성하기 전에 우리가 무엇을 하고 있는지 논의해 보겠습니다.\n자체 문제인 FoodVision Mini를 위해 ViT 논문을 재현하고자 합니다.\n따라서 입력은 피자, 스테이크, 초밥 이미지입니다.\n그리고 이상적인 모델 출력은 피자, 스테이크, 초밥의 예측 레이블입니다.\n이전 섹션에서 해왔던 것과 다르지 않습니다.\n문제는 입력에서 원하는 출력으로 어떻게 이동하느냐입니다.\n\n3.1 입력 및 출력, 레이어 및 블록\nViT는 딥러닝 신경망 아키텍처입니다.\n모든 신경망 아키텍처는 일반적으로 레이어(layers)로 구성됩니다.\n그리고 레이어의 컬렉션을 종종 블록(block)이라고 합니다.\n그리고 많은 블록을 쌓는 것이 전체 아키텍처를 제공하는 것입니다.\n레이어는 입력(예: 이미지 텐서)을 가져와서 어떤 종류의 기능을 수행한(예: 레이어의 forward() 메서드에 있는 내용) 다음 출력을 반환합니다.\n따라서 단일 레이어가 입력을 받고 출력을 제공한다면, 레이어의 컬렉션 또는 블록도 입력을 받고 출력을 제공합니다.\n이를 구체화해 보겠습니다. * 레이어 - 입력을 받고 함수를 수행하며 출력을 반환합니다. * 블록 - 레이어의 컬렉션으로, 입력을 받고 일련의 함수를 수행하며 출력을 반환합니다. * 아키텍처 (또는 모델) - 블록의 컬렉션으로, 입력을 받고 일련의 함수를 수행하며 출력을 반환합니다.\n이 이념은 우리가 ViT 논문을 재현하기 위해 사용할 내용입니다.\n레이어별로, 블록별로, 함수별로 퍼즐 조각을 레고처럼 맞춰서 원하는 전체 아키텍처를 얻을 것입니다.\n이렇게 하는 이유는 전체 연구 논문을 한꺼번에 보는 것이 위협적일 수 있기 때문입니다.\n따라서 더 나은 이해를 위해 단일 레이어의 입력과 출력부터 시작하여 전체 모델의 입력과 출력까지 단계적으로 구축해 나갈 것입니다.\nTK 이미지: 레고처럼 네트워크 쌓기 (함수 + 레이어 + 블록 = 모델).\n\n\n3.2 구체적으로 살펴보기: ViT는 무엇으로 구성되어 있나요?\n논문 곳곳에는 ViT 모델에 대한 많은 세부 정보가 흩어져 있습니다.\n그것들을 모두 찾는 것은 마치 하나의 큰 보물 찾기와 같습니다!\n연구 논문은 종종 수개월 간의 작업을 몇 페이지로 압축한 것이므로 재현하는 데 연습이 필요한 것은 당연하다는 점을 기억하세요.\n하지만 아키텍처 설계를 위해 살펴볼 주요 세 가지 리소스는 다음과 같습니다. 1. 그림 1 - 그래픽적인 의미에서 모델의 개요를 보여주며, 이 그림만으로도 아키텍처를 거의 재현할 수 있습니다. 2. 섹션 3.1의 네 가지 방정식 - 이 방정식은 그림 1의 색칠된 블록에 좀 더 수학적인 토대를 제공합니다. 3. 표 1 - 이 표는 다양한 ViT 모델 변형에 대한 다양한 하이퍼파라미터 설정(예: 레이어 수 및 은닉 유닛 수)을 보여줍니다. 우리는 가장 작은 버전인 ViT-Base에 집중할 것입니다.\n\nTK 3.2.1 그림 1 살펴보기\nViT 논문의 그림 1을 살펴보는 것부터 시작하겠습니다.\n주로 주의를 기울여야 할 사항은 다음과 같습니다. 1. 레이어 (Layers) - 입력을 받고 연산이나 함수를 수행하며 출력을 생성합니다. 2. 블록 (Blocks) - 레이어의 컬렉션이며, 역시 입력을 받고 출력을 생성합니다.\n\n아키텍처를 구성하는 다양한 입력, 출력, 레이어 및 블록을 보여주는 ViT 논문의 그림 1. 우리의 목표는 PyTorch 코드를 사용하여 이들 각각을 재현하는 것입니다.\nViT 아키텍처는 여러 단계로 구성됩니다. * 패치 + 위치 임베딩 (Patch + Position Embedding, 입력) - 입력 이미지를 이미지 패치 시퀀스로 변환하고 패치가 나오는 순서인 위치 번호를 추가합니다. * 평탄화된 패치의 선형 투영 (Linear projection of flattened patches, 임베딩된 패치) - 이미지 패치는 임베딩으로 변환되는데, 이미지 값만 사용하는 대신 임베딩을 사용하는 이점은 임베딩이 훈련을 통해 개선될 수 있는 이미지의 학습 가능한 표현(일반적으로 벡터 형태)이라는 점입니다. * 노름 (Norm) - “레이어 정규화(Layer Normalization)” 또는 “LayerNorm”의 약자로, 신경망을 규제(오버피팅 감소)하기 위한 기술입니다. PyTorch 레이어 torch.nn.LayerNorm()을 통해 LayerNorm을 사용할 수 있습니다. * 멀티헤드 어텐션 (Multi-Head Attention) - 이것은 멀티헤드 셀프 어텐션 레이어(Multi-Headed Self-Attention layer) 또는 줄여서 “MSA”입니다. PyTorch 레이어 torch.nn.MultiheadAttention()을 통해 MSA 레이어를 만들 수 있습니다. * MLP (또는 [다층 퍼셉트론(Multilayer perceptron)]) - MLP는 종종 피드포워드 레이어의 모든 컬렉션(또는 PyTorch의 경우 forward() 메서드가 있는 레이어 컬렉션)을 의미할 수 있습니다. ViT 논문에서 저자는 MLP를 “MLP 블록”이라고 지칭하며, 여기에는 두 개의 torch.nn.Linear() 레이어와 그 사이에 torch.nn.GELU() 비선형 활성화 함수가 포함되어 있으며(섹션 3.1), 각 레이어 뒤에 torch.nn.Dropout() 레이어가 있습니다(부록 B.1). * 트랜스포머 인코더 (Transformer Encoder) - 트랜스포머 인코더는 위에 나열된 레이어들의 컬렉션입니다. 트랜스포머 인코더 내부에는 두 개의 스킵 연결(“+” 기호)이 있는데, 이는 레이어의 입력이 즉각적인 레이어뿐만 아니라 후속 레이어에도 직접 공급됨을 의미합니다. 전체 ViT 아키텍처는 서로 쌓인 다수의 트랜스포머 인코더로 구성됩니다. * MLP 헤드 (MLP Head) - 아키텍처의 출력 레이어로, 입력의 학습된 특성을 클래스 출력으로 변환합니다. 이미지 분류 작업을 수행하고 있으므로 이를 “분류 헤드(classifier head)”라고 부를 수도 있습니다. MLP 헤드의 구조는 MLP 블록과 유사합니다.\nViT 아키텍처의 많은 부분이 기존 PyTorch 레이어로 생성될 수 있음을 알 수 있습니다.\n이는 PyTorch가 연구자와 머신러닝 실무자 모두를 위해 재사용 가능한 신경망 레이어를 만들도록 설계되었기 때문입니다.\n\n질문: 왜 모든 것을 처음부터 코딩하지 않나요?\n논문의 모든 수학 방정식을 사용자 정의 PyTorch 레이어로 재현하여 확실히 그렇게 할 수도 있고, 그것은 분명 교육적인 연습이 될 것이지만, 기존 PyTorch 레이어를 사용하는 것이 일반적으로 선호됩니다. 기존 레이어는 종종 올바르고 빠르게 실행되는지 확인하기 위해 광범위하게 테스트되고 성능이 점검되었기 때문입니다.\n\n\n참고: 우리는 이러한 레이어를 만들기 위한 PyTorch 코드를 작성하는 데 집중할 것이며, 각 레이어가 무엇을 하는지에 대한 배경 지식은 ViT 논문을 전체적으로 읽거나 각 레이어에 대해 링크된 리소스를 읽는 것을 권장합니다.\n\n그림 1을 가져와서 이미지를 피자, 스테이크 또는 초밥으로 분류하는 FoodVision Mini 문제에 맞게 조정해 보겠습니다.\n\nFoodVision Mini를 위해 조정된 ViT 논문의 그림 1. 음식 이미지(피자)가 들어가고 이미지는 패치로 변환된 다음 임베딩으로 투영됩니다. 임베딩은 다양한 레이어와 블록을 통과하고 (바라건대) 클래스 “피자”가 반환됩니다.\n\n\nTK - 3.2.2 네 가지 방정식 살펴보기\n다음으로 살펴볼 ViT 논문의 주요 부분은 섹션 3.1의 네 가지 방정식입니다.\n\n이 네 가지 방정식은 ViT 아키텍처의 네 가지 주요 부분 뒤에 있는 수학을 나타냅니다.\n섹션 3.1에서 각각에 대해 설명합니다(간결함을 위해 일부 텍스트는 생략되었으며 굵은 글씨는 강조를 위한 것입니다).\n\n\n\n\n\n\n\n방정식 번호\nViT 논문 섹션 3.1의 설명\n\n\n\n\n1\n…트랜스포머는 모든 레이어에서 일정한 잠재 벡터 크기 \\(D\\)를 사용하므로, 패치를 평탄화하고 훈련 가능한 선형 투영을 사용하여 \\(D\\) 차원으로 매핑합니다(식 1). 이 투영의 출력을 패치 임베딩이라고 합니다.\n\n\n2\n트랜스포머 인코더(Vaswani et al., 2017)는 교대로 나타나는 멀티헤드 셀프 어텐션(MSA, 부록 A 참조) 및 MLP 블록 레이어로 구성됩니다(식 2, 3). 모든 블록 이전에 레이어 노름(LN)이 적용되고, 모든 블록 이후에 잔차 연결(residual connections)이 적용됩니다(Wang et al., 2019; Baevski & Auli, 2019).\n\n\n3\n위와 동일합니다.\n\n\n4\nBERT의 [ class ] 토큰과 유사하게, 임베딩된 패치 시퀀스 앞에 학습 가능한 임베딩을 추가하며 \\(\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)\\), 트랜스포머 인코더 출력에서의 상태 \\(\\left(\\mathbf{z}_{L}^{0}\\right)\\)는 이미지 표현 \\(\\mathbf{y}\\) 역할을 합니다(식 4)…\n\n\n\n이러한 설명을 그림 1의 ViT 아키텍처에 매핑해 보겠습니다.\n\n그림 1의 ViT 아키텍처를 레이어/블록 뒤의 수학을 설명하는 섹션 3.1의 네 가지 방정식에 연결합니다. “모든 블록 뒤의 잔차 연결”과 같은 일부 세부 사항은 그림 1과 텍스트에는 언급되어 있지만 방정식에는 언급되어 있지 않습니다.\n위 이미지에는 많은 일이 일어나고 있지만 색칠된 선과 화살표를 따라가면 ViT 아키텍처의 주요 개념이 드러납니다.\n각 방정식을 더 자세히 분석해 보는 건 어떨까요(이것들을 코드로 재현하는 것이 우리의 목표가 될 것입니다)?\n모든 방정식(방정식 4 제외)에서 “\\(\\mathbf{z}\\)”는 특정 레이어의 원시 출력입니다.\n\n\\(\\mathbf{z}_{0}\\)은 “z zero”입니다 (이는 초기 패치 임베딩 레이어의 출력입니다).\n\\(\\mathbf{z}_{\\ell}^{\\prime}\\)는 “특정 레이어 prime의 z”입니다 (또는 z의 중간 값).\n\\(\\mathbf{z}_{\\ell}\\)은 “특정 레이어의 z”입니다.\n\n그리고 \\(\\mathbf{y}\\)는 아키텍처의 전체 출력입니다.\n방정식 1\n\\[\n\\begin{aligned}\n\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n\\end{aligned}\n\\]\n이 방정식은 입력 이미지의 클래스 토큰, 패치 임베딩 및 위치 임베딩(\\(\\mathbf{E}\\)는 임베딩을 의미함)을 다룹니다.\n벡터 형식에서 임베딩은 다음과 같이 보일 수 있습니다.\nTK - 벡터 형식을 업데이트하여 실제 사례를 반영하세요.\nx_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n여기서 벡터의 각 요소는 학습 가능합니다(requires_grad=True).\n방정식 2\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n\\end{aligned}\n\\]\n이는 레이어 1부터 \\(L\\)(총 레이어 수)까지 모든 레이어에 대해 LayerNorm(LN) 레이어를 래핑하는 MSA(Multi-Head Attention) 레이어가 있음을 나타냅니다.\n끝부분의 덧셈은 입력을 출력에 더하고 스킵/잔차 연결(skip/residual connection)을 형성하는 것과 같습니다.\n이 레이어를 “MSA 블록”이라고 부르겠습니다.\n의사코드(pseudocode)로는 다음과 같을 수 있습니다.\nx_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n끝부분의 스킵 연결(레이어의 출력을 레이어의 입력에 더함)에 주의하세요.\n방정식 3\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\\\\n\\end{aligned}\n\\]\n이는 레이어 1부터 \\(L\\)(총 레이어 수)까지 모든 레이어에 대해 LayerNorm(LN) 레이어를 래핑하는 다층 퍼셉트론(MLP) 레이어도 있음을 나타냅니다.\n끝부분의 덧셈은 잔차 연결의 존재를 보여줍니다.\n이 레이어를 “MLP 블록”이라고 부르겠습니다.\n의사코드로는 다음과 같을 수 있습니다.\nx_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n끝부분의 스킵 연결(레이어의 출력을 레이어의 입력에 더함)에 주의하세요.\n방정식 4\n\\[\n\\begin{aligned}\n\\mathbf{y} &=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & &\n\\end{aligned}\n\\]\n이는 마지막 레이어 \\(L\\)에 대해 출력 \\(y\\)가 LayerNorm(LN) 레이어에 래핑된 \\(z\\)의 0번 인덱스 토큰임을 의미합니다.\n우리의 경우 x_output_MLP_block의 0번 인덱스입니다.\ny = LN_layer(Linear_layer(x_output_MLP_block[0]))\n물론 위에는 약간의 단순화가 있지만 각 섹션에 대한 PyTorch 코드를 작성하기 시작할 때 이를 처리할 것입니다.\n\n참고: 위 섹션은 많은 정보를 다룹니다. 하지만 이해가 되지 않는 부분이 있다면 항상 더 자세히 조사할 수 있다는 점을 잊지 마세요. “잔차 연결이란 무엇인가?”와 같은 질문을 던지면서 말이죠.\n\n\n\nTK - 3.2.3 표 1 살펴보기\nViT 아키텍처 퍼즐의 마지막 조각(현재로서는)은 표 1입니다.\n\n\n\n\n\n\n\n\n\n\n\n모델\n레이어 (Layers)\n은닉 크기 (Hidden size) \\(D\\)\nMLP 크기\n헤드 (Heads)\n파라미터 (Params)\n\n\n\n\nViT-Base\n12\n768\n3072\n12\n\\(86M\\)\n\n\nViT-Large\n24\n1024\n4096\n16\n\\(307M\\)\n\n\nViT-Huge\n32\n1280\n5120\n16\n\\(632M\\)\n\n\n\n\n&lt;i&gt;표 1: Vision Transformer 모델 변형의 세부 정보. 출처: &lt;a href=\"https://arxiv.org/abs/2010.11929\"&gt;ViT 논문&lt;/a&gt;.&lt;/i&gt;\n\n\n이 표는 각 ViT 아키텍처의 다양한 하이퍼파라미터를 보여줍니다.\n숫자가 ViT-Base에서 ViT-Huge로 갈수록 점진적으로 증가하는 것을 볼 수 있습니다.\n우리는 ViT-Base를 재현하는 데 집중할 것이지만(작게 시작하여 필요할 때 확장), 더 큰 변형으로 쉽게 확장될 수 있는 코드를 작성할 것입니다.\n하이퍼파라미터를 분석해 보면 다음과 같습니다. * 레이어 (Layers) - 트랜스포머 인코더 블록이 몇 개 있나요? (이들 각각에는 MSA 블록과 MLP 블록이 포함됩니다) * 은닉 크기 (Hidden size) \\(D\\) - 이는 아키텍처 전체의 임베딩 차원으로, 이미지가 패치되고 임베딩될 때 이미지가 변환되는 벡터의 크기가 됩니다. 일반적으로 임베딩 차원이 클수록 더 많은 정보를 캡처할 수 있고 결과가 더 좋아집니다. 그러나 큰 임베딩은 더 많은 계산 비용을 수반합니다. * MLP 크기 - MLP 레이어의 은닉 유닛 수는 얼마인가요? * 헤드 (Heads) - Multi-Head Attention 레이어에는 헤드가 몇 개 있나요? * 파라미터 (Params) - 모델의 총 파라미터 수는 얼마인가요? 일반적으로 파라미터가 많을수록 성능이 좋아지지만 계산 비용이 더 많이 듭니다. ViT-Base조차 이전에 사용했던 다른 어떤 모델보다 파라미터가 훨씬 많다는 것을 알 수 있습니다.\n이러한 값을 ViT 아키텍처의 하이퍼파라미터 설정으로 사용하겠습니다.\n\n\n\nTK - 3.3 논문 재현을 위한 나의 워크플로우\n논문 재현 작업을 시작할 때 나는 다음과 같은 단계를 거칩니다.\n\n전체 논문을 처음부터 끝까지 한 번 읽습니다 (주요 개념을 파악하기 위해).\n각 섹션을 다시 살펴보며 서로 어떻게 연결되는지 확인하고 이를 코드로 변환하는 방법을 생각하기 시작합니다 (위에서 했던 것처럼).\n꽤 좋은 개요를 얻을 때까지 2단계를 반복합니다.\nmathpix.com (매우 유용한 도구)을 사용하여 논문의 모든 섹션을 노트북에 넣을 마크다운/LaTeX로 변환합니다.\n가능한 한 가장 단순한 버전의 모델을 재현합니다.\n막히면 다른 예시를 찾아봅니다.\n\nTK - mathpix의 gif\n우리는 이미 위의 처음 몇 단계를 거쳤으며(아직 전체 논문을 읽지 않았다면 한 번 읽어보시길 권장합니다), 다음으로 집중할 것은 5단계인 가능한 한 가장 단순한 버전의 모델을 재현하는 것입니다.\n이것이 우리가 ViT-Base부터 시작하는 이유입니다.\n가장 작은 버전의 아키텍처를 재현하고 작동시킨 다음 원한다면 확장할 수 있습니다.\n\n참고: 이전에 연구 논문을 읽어본 적이 없다면 위의 많은 단계가 위협적일 수 있습니다. 하지만 다른 것과 마찬가지로 논문을 읽고 재현하는 기술도 연습을 통해 향상될 것입니다. 연구 논문은 종종 여러 사람의 수개월 간의 작업을 몇 페이지로 압축한 결과물이라는 점을 잊지 마세요. 따라서 혼자서 이를 재현하려고 노력하는 것은 결코 작은 성과가 아닙니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-4.-방정식-1-데이터를-패치로-분할하고-클래스-위치-및-패치-임베딩-생성",
    "href": "08_pytorch_paper_replicating.html#tk-4.-방정식-1-데이터를-패치로-분할하고-클래스-위치-및-패치-임베딩-생성",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 4. 방정식 1: 데이터를 패치로 분할하고 클래스, 위치 및 패치 임베딩 생성",
    "text": "TK 4. 방정식 1: 데이터를 패치로 분할하고 클래스, 위치 및 패치 임베딩 생성\n내 머신러닝 엔지니어 친구 중 한 명이 “모든 것은 임베딩에 달려 있다”고 말하곤 했던 것이 기억납니다.\n임베딩은 학습 가능한 표현이므로 데이터를 좋고 학습 가능한 방식으로 표현할 수 있다면 학습 알고리즘이 해당 데이터에서 좋은 성능을 낼 가능성이 높다는 뜻입니다.\n따라서 이를 바탕으로 ViT 아키텍처를 위한 클래스, 위치 및 패치 임베딩을 만들어 보겠습니다.\n패치 임베딩부터 시작하겠습니다.\n이는 입력 이미지를 패치 시퀀스로 변환한 다음 해당 패치를 임베딩하는 것을 의미합니다.\n임베딩은 어떤 형태의 학습 가능한 표현이며 종종 벡터라는 점을 기억하세요. 학습 가능하다는 용어는 입력 이미지의 표현이 시간이 지남에 따라 개선되고 학습될 수 있음을 의미하기 때문에 중요합니다.\nViT 논문의 섹션 3.1 첫 번째 단락을 따르는 것으로 시작해 보겠습니다(굵은 글씨는 강조를 위한 것입니다).\n\n표준 트랜스포머는 토큰 임베딩의 1D 시퀀스를 입력으로 받습니다. 2D 이미지를 처리하기 위해 이미지 \\(\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}\\)를 평탄화된 2D 패치 시퀀스 \\(\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}\\)로 재구성합니다. 여기서 \\((H, W)\\)는 원본 이미지의 해상도이고, \\(C\\)는 채널 수이며, \\((P, P)\\)는 각 이미지 패치의 해상도이고, \\(N=H W / P^{2}\\)는 결과 패치 수이며 트랜스포머의 유효 입력 시퀀스 길이 역할도 합니다. 트랜스포머는 모든 레이어에서 일정한 잠재 벡터 크기 \\(D\\)를 사용하므로, 패치를 평탄화하고 학습 가능한 선형 투영(식 1)을 사용하여 \\(D\\) 차원으로 매핑합니다. 이 투영의 출력을 패치 임베딩이라고 합니다.\n\n그리고 이미지 모양을 다루고 있으므로 ViT 논문 표 3의 한 줄을 명심합시다.\n\n훈련 해상도는 224입니다.\n\n위의 텍스트를 분석해 보겠습니다.\n\n\\(D\\)는 패치 임베딩의 크기이며, \\(D\\)에 대한 다양한 값은 표 1에서 찾을 수 있습니다.\n이미지는 크기가 \\({H \\times W \\times C}\\)인 2D로 시작합니다.\n이미지는 크기가 \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)인 평탄화된 2D 패치 시퀀스로 변환됩니다.\n\n\\((H, W)\\)는 원본 이미지의 해상도입니다.\n\\(C\\)는 채널 수입니다.\n\\((P, P)\\)는 각 이미지 패치의 해상도(패치 크기)입니다.\n\\(N=H W / P^{2}\\)는 결과 패치 수이며, 트랜스포머의 유효 입력 시퀀스 길이 역할도 합니다.\n\n\n\n그림 1의 ViT 아키텍처 패치 및 위치 임베딩 부분을 방정식 1에 매핑합니다. 섹션 3.1의 도입 단락에서는 패치 임베딩 레이어의 다양한 입력 및 출력 모양을 설명합니다.\n\nTK - 4.1 패치 임베딩 입력 및 출력 모양 수동 계산\n이러한 입력 및 출력 모양 값을 수동으로 계산하는 것부터 시작해 보는 건 어떨까요?\n그렇게 하기 위해 위의 각 용어(\\(H\\), \\(W\\) 등)를 모방하는 몇 가지 변수를 만들어 보겠습니다.\n패치 크기(\\(P\\))는 ViT-Base가 사용하는 가장 성능이 좋은 버전인 16을 사용하겠습니다(자세한 내용은 ViT 논문의 표 5에서 “ViT-B/16” 열 참조).\n\n# 예시 값 생성\nheight = 224 # H (\"훈련 해상도는 224입니다.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# N (패치 수) 계산\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"이미지 높이(H={height}), 너비(W={width}) 및 패치 크기(P={patch_size})인 경우 패치 수(N): {number_of_patches}\")\n\n패치 수를 구했습니다. 이미지 출력 크기도 만들어 보는 건 어떨까요?\n더 나아가 패치 임베딩 레이어의 입력 및 출력 모양을 재현해 보겠습니다.\n다시 기억해 봅시다.\n\n입력: 이미지는 크기가 \\({H \\times W \\times C}\\)인 2D로 시작합니다.\n출력: 이미지는 크기가 \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)인 평탄화된 2D 패치 시퀀스로 변환됩니다.\n\n\n# 입력 모양\ninput_shape = (height, width, color_channels)\n\n# 출력 모양\noutput_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"입력 모양 (2D 이미지): {input_shape}\")\nprint(f\"출력 모양 (평탄화된 2D 패치): {output_shape}\")\n\n입력 및 출력 모양 획득!\n\n\nTK - 4.2 단일 이미지를 패치로 변환\n이제 패치 임베딩 레이어에 대한 이상적인 입력 및 출력 모양을 알았습니다.\n여기서 우리가 하고 있는 작업은 전체 아키텍처를 더 작은 조각으로 나누어 개별 레이어의 입력과 출력에 집중하는 것입니다.\n그렇다면 패치 임베딩 레이어는 어떻게 만들까요?\n곧 다루겠지만, 먼저 이미지를 패치로 변환하는 것이 어떤 모습인지 시각화, 시각화, 시각화! 해보겠습니다.\n단일 이미지부터 시작하겠습니다.\n\n# 단일 이미지 보기\nplt.imshow(image.permute(1, 2, 0)) # matplotlib에 맞게 조정\nplt.title(class_names[label])\nplt.axis(False);\n\nViT 논문의 그림 1과 일치하도록 이 이미지를 자체 패치로 변환하고 싶습니다.\n먼저 패치된 픽셀의 상단 행만 시각화해 보는 건 어떨까요?\n다른 이미지 차원을 인덱싱하여 이를 수행할 수 있습니다.\n\n# matplotlib과 호환되도록 이미지 모양 변경 [색상_채널, 높이, 너비] -&gt; [높이, 너비, 색상_채널] \nimage_permuted = image.permute(1, 2, 0)\n\n# 패치된 픽셀의 상단 행을 그리기 위한 인덱스\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n\n이제 상단 행을 얻었으므로 패치로 바꾸어 보겠습니다.\n상단 행에 있을 패치 수를 반복하여 이를 수행할 수 있습니다.\n\n# 하이퍼파라미터 설정 및 IMG_SIZE와 patch_size가 호환되는지 확인\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"이미지 크기는 패치 크기로 나누어 떨어져야 합니다.\"\nprint(f\"행당 패치 수: {num_patches}\")\n\n# 일련의 서브플롯 생성\nfig, axs = plt.subplots(nrows=1, \n                        ncols=img_size // patch_size, # 각 패치 당 하나의 열\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# 상단 행의 패치 수를 반복\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # 높이 인덱스는 일정하게 유지, 너비 인덱스 변경\n    axs[i].set_xlabel(i+1) # 레이블 설정\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n\n패치들이 아주 좋아 보이네요!\n이미지 전체에 대해 수행해 보면 어떨까요?\n이번에는 높이와 너비에 대한 인덱스를 반복하고 각 패치를 자체 서브플롯으로 그릴 것입니다.\n\n# 하이퍼파라미터 설정 및 IMG_SIZE와 patch_size가 호환되는지 확인\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"이미지 크기는 패치 크기로 나누어 떨어져야 합니다.\"\nprint(f\"행당 패치 수: {num_patches}\\n열당 패치 수: {num_patches}\\n총 패치 수: {num_patches*num_patches}\")\n\n# 일련의 서브플롯 생성\nfig, axs = plt.subplots(nrows=img_size // patch_size, # float가 아닌 int가 필요함\n                        ncols=img_size // patch_size, \n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# 이미지의 높이와 너비를 반복\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # 높이 반복\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # 너비 반복\n        \n        # 치환된 이미지 패치 그리기 (image_permuted -&gt; (높이, 너비, 색상 채널))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # 높이 반복 \n                                        patch_width:patch_width+patch_size, # 너비 반복\n                                        :]) # 모든 색상 채널 가져오기\n        \n        # 레이블 정보 설정, 명확성을 위해 틱(ticks) 제거 및 레이블을 바깥쪽으로 설정\n        axs[i, j].set_ylabel(i+1, \n                             rotation=\"horizontal\", \n                             horizontalalignment=\"right\", \n                             verticalalignment=\"center\") \n        axs[i, j].set_xlabel(j+1) \n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# 슈퍼 타이틀 설정\nfig.suptitle(f\"{class_names[label]} -&gt; 패치화됨\", fontsize=16)\nplt.show()\n\n이미지가 패치화되었습니다!\n와, 정말 멋지네요.\n이제 이러한 각 패치를 어떻게 임베딩으로 변환하고 시퀀스로 바꿀까요?\n힌트: PyTorch 레이어를 사용할 수 있습니다. 어떤 것인지 맞힐 수 있나요?\n\n\nTK - 4.3 torch.nn.Conv2d()로 이미지 패치 생성하기\n이제 PyTorch로 패치 임베딩 레이어를 재현하는 쪽으로 넘어가 볼 시간입니다.\n단일 이미지를 시각화하기 위해 단일 이미지의 다른 높이 및 너비 차원을 반복하고 개별 패치를 그리는 코드를 작성했습니다.\n이 연산은 03. PyTorch 컴퓨터 비전 섹션 7.1: nn.Conv2d() 단계별 살펴보기에서 보았던 컨볼루션 연산과 매우 유사합니다.\n사실, ViT 논문의 저자들은 섹션 3.1에서 패치 임베딩이 컨볼루션 신경망(CNN)으로 달성 가능하다는 점을 언급합니다.\n\n하이브리드 아키텍처. 원시 이미지 패치의 대안으로, CNN의 특성 맵(feature map)으로부터 입력 시퀀스를 형성할 수 있습니다(LeCun et al., 1989). 이 하이브리드 모델에서, 패치 임베딩 투영 \\(\\mathbf{E}\\)(식 1)는 CNN 특성 맵에서 추출된 패치에 적용됩니다. 특수한 경우로, 패치는 \\(1 \\times 1\\)의 공간 크기를 가질 수 있는데, 이는 특성 맵의 공간 차원을 단순하게 평탄화하고 트랜스포머 차원으로 투영함으로써 입력 시퀀스를 얻음을 의미합니다. 분류 입력 임베딩 및 위치 임베딩은 위에서 설명한 대로 추가됩니다.\n\n여기서 언급하는 “특성 맵(feature map)”은 주어진 이미지를 통과하는 컨볼루션 레이어에 의해 생성된 가중치/활성화를 의미합니다.\n\ntorch.nn.Conv2d() 레이어의 kernel_size 및 stride 매개변수를 patch_size와 동일하게 설정함으로써, 이미지를 패치로 분할하고 각 패치의 학습 가능한 임베딩(ViT 논문에서는 “선형 투영”이라고 지칭함)을 생성하는 레이어를 효과적으로 얻을 수 있습니다.\n패치 임베딩 레이어에 대한 우리의 이상적인 입력 및 출력 모양을 기억하시나요?\n\n입력: 이미지는 크기가 \\({H \\times W \\times C}\\)인 2D로 시작합니다.\n출력: 이미지는 크기가 \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)인 평탄화된 2D 패치 시퀀스로 변환됩니다.\n\n또는 이미지 크기가 224이고 패치 크기가 16인 경우:\n\n입력 (2D 이미지): (224, 224, 3)\n출력 (평탄화된 2D 패치): (196, 768)\n\n우리는 다음과 같은 방법으로 이것들을 재현할 수 있습니다. * torch.nn.Conv2d(): 이미지를 CNN 특성 맵 패치로 변환합니다. * torch.nn.Flatten(): 특성 맵의 공간 차원을 평탄화합니다.\ntorch.nn.Conv2d() 레이어부터 시작하겠습니다.\nkernel_size 및 stride를 patch_size와 동일하게 설정하여 패치 생성을 재현할 수 있습니다.\n이는 각 컨볼루션 커널이 (patch_size x patch_size) 크기이거나 patch_size=16인 경우 (16 x 16)(한 개의 전체 패치와 동일함)임을 의미합니다.\n그리고 컨볼루션 커널의 각 단계 또는 stride는 16픽셀 길이가 됩니다(다음 패치로 넘어가는 것과 동일함).\n이미지의 색상 채널 수에 대해 in_channels=3을 설정하고, ViT-Base에 대한 표 1의 \\(D\\) 값과 동일한 out_channels=768을 설정합니다(이는 임베딩 차원으로, 각 이미지는 크기가 768인 벡터로 임베딩됩니다).\n\nfrom torch import nn\n\n# 패치 크기 설정\npatch_size=16\n\n# ViT 논문의 하이퍼파라미터를 사용하여 Conv2d 레이어 생성\nconv2d = nn.Conv2d(in_channels=3, # 색상 채널 수\n                   out_channels=768, # 표 1에서 가져옴: 은닉 크기 D, 이것이 임베딩 크기가 됩니다.\n                   kernel_size=patch_size, # (patch_size, patch_size)를 사용할 수도 있습니다.\n                   stride=patch_size,\n                   padding=0)\n\n이제 컨볼루션 레이어가 생겼으므로 단일 이미지를 통과시킬 때 어떤 일이 일어나는지 확인해 보겠습니다.\n\n# 단일 이미지 보기\nplt.imshow(image.permute(1, 2, 0)) # matplotlib에 맞게 조정\nplt.title(class_names[label])\nplt.axis(False);\n\n\n# 이미지를 컨볼루션 레이어에 통과시킴 \nimage_out_of_conv = conv2d(image.unsqueeze(0)) # 단일 배치 차원 추가 (높이, 너비, 색상_채널) -&gt; (배치, 높이, 너비, 색상_채널)\nprint(image_out_of_conv.shape)\n\n이미지를 컨볼루션 레이어에 통과시키면 일련의 768개(이는 임베딩 크기 또는 \\(D\\)입니다) 특성/활성화 맵으로 변환됩니다.\n따라서 출력 모양은 다음과 같이 읽을 수 있습니다.\ntorch.Size([1, 768, 14, 14]) -&gt; [배치_크기, 임베딩_차원, 특성_맵_높이, 특성_맵_너비]\n임의의 특성 맵 5개를 시각화하여 어떻게 생겼는지 확인해 보겠습니다.\n\n# 임의의 5개 컨볼루션 특성 맵 플롯\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # 0과 임베딩 크기 사이에서 5개의 숫자 선택\nprint(f\"인덱스에서 무작위 컨볼루션 특성 맵 표시: {random_indexes}\")\n\n# 플롯 생성\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# 무작위 이미지 특성 맵 플롯\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # 컨볼루션 레이어의 출력 텐서에서 인덱싱\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n\n특성 맵이 모두 원본 이미지를 어느 정도 나타내고 있음을 알 수 있습니다. 몇 가지를 시각화하면 서로 다른 주요 윤곽선과 몇 가지 주요 특성을 볼 수 있습니다.\n주목해야 할 중요한 점은 이러한 특성이 신경망이 학습함에 따라 시간이 지나면 변할 수 있다는 것입니다.\n이러한 점 때문에 이러한 특성 맵은 우리 이미지의 학습 가능한 임베딩(learnable embedding)으로 간주될 수 있습니다.\n수치 형식으로 하나 확인해 보겠습니다.\n\n# 텐서 형식으로 단일 특성 맵 가져오기\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n\nsingle_feature_map의 grad_fn 출력과 required_grad=True 속성은 PyTorch가 이 특성 맵의 그래디언트를 추적하고 있으며 훈련 중에 경사 하강법에 의해 업데이트될 것임을 의미합니다.\n\n\nTK - 4.4 torch.nn.Flatten()을 사용하여 패치 임베딩 평탄화하기\n이미지를 패치 임베딩으로 변환했지만 여전히 2D 형식입니다.\n어떻게 하면 ViT 모델의 패치 임베딩 레이어의 원하는 출력 모양으로 만들 수 있을까요?\n\n원하는 출력 (평탄화된 2D 패치): (196, 768) -&gt; \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)\n\n현재 모양을 확인해 보겠습니다.\n\n# 현재 텐서 모양\nprint(f\"현재 텐서 모양: {image_out_of_conv.shape} -&gt; [배치, 임베딩_차원, 특성_맵_높이, 특성_맵_너비]\")\n\n768 부분( \\((P^{2} \\cdot C)\\) )은 얻었지만 여전히 패치 수(\\(N\\))가 필요합니다.\nViT 논문의 섹션 3.1을 다시 읽어보면 다음과 같이 적혀 있습니다(굵은 글씨는 강조를 위한 것입니다).\n\n특수한 경우로, 패치는 \\(1 \\times 1\\)의 공간 크기를 가질 수 있는데, 이는 특성 맵의 공간 차원을 단순하게 평탄화하고 트랜스포머 차원으로 투영함으로써 입력 시퀀스를 얻음을 의미합니다.\n\n특성 맵의 공간 차원을 평탄화한다고요?\nPyTorch에 평탄화할 수 있는 레이어가 있나요?\ntorch.nn.Flatten()은 어떨까요?\n하지만 전체 텐서를 평탄화하고 싶지는 않고 “특성 맵의 공간 차원”만 평탄화하고 싶습니다.\n우리의 경우에는 image_out_of_conv의 feature_map_height 및 feature_map_width 차원입니다.\n따라서 해당 차원만 평탄화하도록 torch.nn.Flatten() 레이어를 만들고, start_dim 및 end_dim 매개변수를 사용하여 이를 설정해 보는 건 어떨까요?\n\n# Flatten 레이어 생성\nflatten = nn.Flatten(start_dim=2, # feature_map_height(차원 2) 평탄화\n                     end_dim=3) # feature_map_width(차원 3) 평탄화\n\n좋습니다! 이제 모든 것을 하나로 합쳐 봅시다!\n우리는 다음 단계를 수행할 것입니다. 1. 단일 이미지를 가져옵니다. 2. 이미지를 컨볼루션 레이어(conv2d)에 통과시켜 2D 특성 맵(패치 임베딩)으로 변환합니다. 3. 2D 특성 맵을 단일 시퀀스로 평탄화합니다.\n\n# 1. 단일 이미지 보기\nplt.imshow(image.permute(1, 2, 0)) # matplotlib에 맞게 조정\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"원본 이미지 모양: {image.shape}\")\n\n# 2. 이미지를 특성 맵으로 변환\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # 모양 오류를 방지하기 위해 배치 차원 추가\nprint(f\"이미지 특성 맵 모양: {image_out_of_conv.shape}\")\n\n# 3. 특성 맵 평탄화\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"평탄화된 이미지 특성 맵 모양: {image_out_of_conv_flattened.shape}\")\n\n좋아요! image_out_of_conv_flattened 모양이 우리가 원하는 출력 모양과 매우 유사해 보입니다.\n\n원하는 출력 (평탄화된 2D 패치): (196, 768) -&gt; \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)\n현재 모양: (1, 768, 196)\n\n유일한 차이점은 현재 모양에 배치 크기가 있고 차원 순서가 원하는 출력과 다르다는 것입니다.\n이 문제를 어떻게 해결할 수 있을까요?\n차원을 재배열해 보는 건 어떨까요?\nmatplotlib으로 이미지를 그리기 위해 이미지 텐서를 재배열할 때와 마찬가지로 torch.Tensor.permute()를 사용하여 그렇게 할 수 있습니다.\n시도해 봅시다.\n\n# 올바른 모양의 평탄화된 이미지 패치 임베딩 가져오기 \nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [배치_크기, P^2•C, N] -&gt; [배치_크기, N, P^2•C]\nprint(f\"패치 임베딩 시퀀스 모양: {image_out_of_conv_flattened_reshaped.shape} -&gt; [배치_크기, 패치_수, 임베딩_크기]\")\n\n좋아요!!!\n이제 몇 개의 PyTorch 레이어를 사용하여 ViT 아키텍처의 패치 임베딩 레이어에 대해 원하는 입력 및 출력 모양을 일치시켰습니다.\n평탄화된 특성 맵 중 하나를 시각화해 보는 건 어떨까요?\n\n# 단일 평탄화된 특성 맵 가져오기\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0]\n\n# 평탄화된 특성 맵 시각적으로 플롯\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"평탄화된 특성 맵 모양: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n\n흠, 평탄화된 특성 맵은 시각적으로는 별 의미가 없어 보이지만, 우리가 걱정할 부분은 아닙니다. 이것이 패치 임베딩 레이어의 출력이자 나머지 ViT 아키텍처의 입력이 될 것입니다.\nTK 이미지 - 단일 이미지 -&gt; conv2d -&gt; 평탄화 -&gt; 위 출력 얻기 (워크플로우와 변환 과정을 보여주며, 이는 우리가 사용해 온 gif일 수 있지만 평탄화 섹션과 작동하도록 확장됨)\n\n참고: 원본 트랜스포머 아키텍처는 텍스트와 함께 작동하도록 설계되었습니다. Vision Transformer 아키텍처(ViT)는 원본 트랜스포머를 이미지에 사용하는 것을 목표로 했습니다. 이것이 ViT 아키텍처의 입력이 이러한 방식으로 처리되는 이유입니다. 우리는 본질적으로 2D 이미지를 가져와서 1D 텍스트 시퀀스처럼 보이도록 형식을 지정하고 있습니다.\n\n평탄화된 특성 맵을 텐서 형식으로 보는 건 어떨까요?\n\n# 평탄화된 특성 맵을 텐서로 보기\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n\n좋습니다!\n우리는 단일 2D 이미지를 단일 1D 학습 가능한 임베딩 벡터(또는 ViT 논문의 그림 1에 있는 “Linear Projection of Flattned Patches”)로 변환했습니다.\n\n\nTK - 4.5 ViT 패치 임베딩 레이어를 PyTorch 모듈로 변환하기\n패치 임베딩을 만들기 위해 수행한 모든 작업을 단일 PyTorch 레이어에 넣을 시간입니다.\nnn.Module을 서브클래싱하고 위의 모든 단계를 수행하는 작은 PyTorch “모델”을 만들어 이를 수행할 수 있습니다.\n구체적으로 다음을 수행합니다. 1. PatchEmbedding이라는 클래스를 만듭니다. 이 클래스는 nn.Module을 상속합니다(따라서 PyTorch 레이어로 사용될 수 있습니다). 2. 매개변수 in_channels=3, patch_size=16(ViT-Base용) 및 embedding_dim=768(표 1의 ViT-Base용 \\(D\\))을 사용하여 클래스를 초기화합니다. 3. nn.Conv2d()를 사용하여 이미지를 패치로 변환하는 레이어를 만듭니다(위의 4.3에서와 동일). 4. 패치 특성 맵을 단일 차원으로 평탄화하는 레이어를 만듭니다(위의 4.4에서와 동일). 5. 입력을 받아 3단계와 4단계에서 생성된 레이어를 통과시키는 forward() 메서드를 정의합니다. 6. 출력 모양이 ViT 아키텍처의 필수 출력 모양(\\({N \\times\\left(P^{2} \\cdot C\\right)}\\))을 반영하는지 확인합니다.\n해봅시다!\n\n# 1. nn.Module을 상속받는 클래스 생성\nclass PatchEmbedding(nn.Module):\n    \"\"\"2D 입력 이미지를 1D 시퀀스 학습 가능 임베딩 벡터로 변환합니다.\n    \n    인자:\n        in_channels (int): 입력 이미지의 색상 채널 수. 기본값은 3.\n        patch_size (int): 입력 이미지를 나눌 패치 크기. 기본값은 16.\n        embedding_dim (int): 이미지를 변환할 임베딩 크기. 기본값은 768.\n    \"\"\" \n    # 2. 적절한 변수를 사용하여 클래스 초기화\n    def __init__(self, \n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n        \n        # 3. 이미지를 패치로 변환하는 레이어 생성\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. 패치 특성 맵을 단일 차원으로 평탄화하는 레이어 생성\n        self.flatten = nn.Flatten(start_dim=2, # 특성 맵 차원만 단일 벡터로 평탄화\n                                  end_dim=3)\n\n    # 5. forward 메서드 정의 \n    def forward(self, x):\n        # 입력이 올바른 모양인지 확인하기 위해 assertion 생성\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"입력 이미지 크기는 패치 크기로 나누어 떨어져야 함, 이미지 모양: {image_resolution}, 패치 크기: {patch_size}\"\n        \n        # 순전파 수행\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched) \n        # 6. 출력 모양이 올바른 순서가 되도록 보장 \n        return x_flattened.permute(0, 2, 1) # 임베딩이 마지막 차원이 되도록 조정 [배치_크기, P^2•C, N] -&gt; [배치_크기, N, P^2•C]\n\nPatchEmbedding 레이어가 생성되었습니다!\n단일 이미지에서 시도해 봅시다.\n\nset_seeds()\n\n# 패치 임베딩 레이어 인스턴스 생성\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# 단일 이미지 통과\nprint(f\"입력 이미지 모양: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # 0번 인덱스에 추가 배치 차원 추가, 그렇지 않으면 오류 발생\nprint(f\"출력 패치 임베딩 모양: {patch_embedded_image.shape}\")\n\n좋습니다!\n출력 모양은 패치 임베딩 레이어에서 보고 싶은 이상적인 입력 및 출력 모양과 일치합니다.\n\n입력: 이미지는 크기가 \\({H \\times W \\times C}\\)인 2D로 시작합니다.\n출력: 이미지는 크기가 \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)인 평탄화된 2D 패치 시퀀스로 변환됩니다.\n\n여기서: * \\((H, W)\\)는 원본 이미지의 해상도입니다. * \\(C\\)는 채널 수입니다. * \\((P, P)\\)는 각 이미지 패치의 해상도(패치 크기)입니다. * \\(N=H W / P^{2}\\)는 결과 패치 수이며, 트랜스포머의 유효 입력 시퀀스 길이 역할도 합니다.\n이제 방정식 1에 대한 패치 임베딩을 재현했지만 클래스 토큰/위치 임베딩은 아직 재현하지 않았습니다.\n이것들은 나중에 다룰 것입니다.\n\n우리의 PatchEmbedding 클래스(오른쪽)는 ViT 논문(왼쪽)의 그림 1 및 식 1로부터 ViT 아키텍처의 패치 임베딩을 재현합니다. 그러나 학습 가능한 클래스 임베딩 및 위치 임베딩은 아직 생성되지 않았습니다. 이것들은 곧 나올 것입니다.\n이제 PatchEmbedding 레이어의 요약을 확인해 보겠습니다.\n\n# 무작위 입력 크기 생성\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # 이미지 크기가 patch_size와 호환되지 않으므로 오류가 발생함\n\n# PatchEmbedding의 입력 및 출력 요약 가져오기\nsummary(PatchEmbedding(), \n        input_size=random_input_image, # 이를 \"random_input_image_error\"로 바꾸어 보세요 \n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n\n\n\nTK 4.6 클래스 토큰 임베딩 생성\n이미지 패치 임베딩을 만들었으니, 이제 클래스 토큰 임베딩 작업을 시작할 시간입니다.\n또는 방정식 1의 \\(\\mathbf{x}_\\text {class }\\)입니다.\n\n왼쪽: 재현할 “분류 토큰” 또는 [class] 임베딩 토큰이 강조된 ViT 논문의 그림 1. 오른쪽: 학습 가능한 클래스 임베딩 토큰과 관련된 ViT 논문의 식 1 및 섹션 3.1.\nViT 논문의 섹션 3.1 두 번째 단락을 읽어보면 다음과 같은 설명을 볼 수 있습니다.\n\nBERT의 [ class ] 토큰과 유사하게, 임베딩된 패치 시퀀스 \\(\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)\\) 앞에 학습 가능한 임베딩을 추가하며, 트랜스포머 인코더의 출력에서의 상태 \\(\\left(\\mathbf{z}_{L}^{0}\\right)\\)는 이미지 표현 \\(\\mathbf{y}\\) 역할을 합니다(식 4).\n\n\n참고: BERT(Bidirectional Encoder Representations from Transformers)는 트랜스포머 아키텍처를 사용하여 자연어 처리(NLP) 작업에서 뛰어난 결과를 달성한 최초의 머신러닝 연구 논문 중 하나이며, 시퀀스 시작 부분에 [ class ] 토큰을 두는 아이디어가 시작된 곳으로, 클래스는 시퀀스가 속한 “분류” 클래스에 대한 설명입니다.\n\n따라서 “임베딩된 패치 시퀀스 앞에 학습 가능한 임베딩을 추가”해야 합니다.\n임베딩된 패치 텐서 시퀀스(4.5에서 생성됨)와 그 모양을 보는 것으로 시작해 보겠습니다.\n\n# 패치 임베딩 및 패치 임베딩 모양 보기\nprint(patch_embedded_image) \nprint(f\"패치 임베딩 모양: {patch_embedded_image.shape} -&gt; [배치_크기, 패치_수, 임베딩_차원]\")\n\n“임베딩된 패치 시퀀스 앞에 학습 가능한 임베딩을 추가”하려면 embedding_dimension(\\(D\\)) 모양의 학습 가능한 임베딩을 만든 다음 이를 number_of_patches 차원에 추가해야 합니다.\n또는 의사코드로는 다음과 같습니다.\npatch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = learnable_embedding\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n연결(torch.cat())이 dim=1(number_of_patches 차원)에서 발생하는 것을 확인하세요.\n클래스 토큰에 대한 학습 가능한 임베딩을 만들어 보겠습니다.\n이를 위해 배치 크기와 임베딩 차원 모양을 얻은 다음 [batch_size, 1, embedding_dimension] 모양의 torch.ones() 텐서를 만듭니다.\n그리고 requires_grad=True와 함께 nn.Parameter()에 전달하여 텐서를 학습 가능하게 만듭니다.\n\n# 배치 크기 및 임베딩 차원 가져오기\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# 임베딩 차원(D)과 동일한 크기를 공유하는 학습 가능한 매개변수로 클래스 토큰 임베딩 생성\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [배치_크기, 패치_수, 임베딩_차원]\n                           requires_grad=True) # 임베딩을 학습 가능하도록 설정\n\n# class_token의 처음 10개 예제 표시\nprint(class_token[:, :, :10])\n\n# class_token 모양 출력\nprint(f\"클래스 토큰 모양: {class_token.shape} -&gt; [배치_크기, 토큰_수, 임베딩_차원]\")\n\n\n참고: 여기서는 시연 목적으로 클래스 토큰 임베딩을 torch.ones()로만 생성하고 있습니다. 실제로는 torch.randn()을 사용하여 클래스 토큰 임베딩을 생성할 가능성이 높습니다(무작위 숫자로 시작).\n\n패치 임베딩 시퀀스 시작 부분에 하나의 클래스 토큰 값만 추가하고 싶으므로 class_token의 number_of_patches 차원이 1인 것을 확인할 수 있습니다.\n이제 클래스 토큰 임베딩을 얻었으므로 이를 이미지 패치 시퀀스인 patch_embedded_image 앞에 추가해 보겠습니다.\ntorch.cat()을 사용하고 dim=1로 설정하여 그렇게 할 수 있습니다(따라서 class_token의 number_of_patches 차원이 patch_embedded_image의 number_of_patches 차원 앞에 추가됨).\n\n# 패치 임베딩 앞에 클래스 토큰 임베딩 추가\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), \n                                                      dim=1) # 첫 번째 차원에서 연결\n\n# 클래스 토큰 임베딩이 앞에 추가된 패치 임베딩 시퀀스 출력\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"클래스 토큰이 추가된 패치 임베딩 시퀀스 모양: {patch_embedded_image_with_class_embedding.shape} -&gt; [배치_크기, 패치_수, 임베딩_차원]\")\n\n좋아요! 학습 가능한 클래스 토큰이 앞에 추가되었습니다!\n\n학습 가능한 클래스 토큰을 만들기 위해 수행한 작업을 검토해 보면, 단일 이미지에서 PatchEmbedding()으로 생성된 이미지 패치 임베딩 시퀀스로 시작하여 각 임베딩 차원에 대해 하나의 값을 갖는 학습 가능한 클래스 토큰을 만든 다음 원본 패치 임베딩 시퀀스 앞에 추가했습니다. 참고: 학습 가능한 클래스 토큰을 만들기 위해 torch.ones()를 사용하는 것은 주로 시연용이며, 실제로는 torch.randn()을 사용하여 생성할 가능성이 높습니다.\n\n\nTK 4.7 위치 임베딩 생성\n클래스 토큰 임베딩과 패치 임베딩이 생겼는데, 이제 위치 임베딩은 어떻게 만들 수 있을까요?\n또는 방정식 1의 \\(\\mathbf{E}_{\\text {pos }}\\)입니다. 여기서 \\(E\\)는 “임베딩(embedding)”을 의미합니다.\n\n왼쪽: 재현할 위치 임베딩이 강조된 ViT 논문의 그림 1. 오른쪽: 위치 임베딩과 관련된 ViT 논문의 식 1 및 섹션 3.1.\nViT 논문의 섹션 3.1을 읽어 더 자세히 알아보겠습니다(굵은 글씨는 강조를 위한 것입니다).\n\n위치 정보를 유지하기 위해 패치 임베딩에 위치 임베딩이 추가됩니다. 더 발전된 2D 인식 위치 임베딩(부록 D.4)을 사용하더라도 상당한 성능 향상을 관찰하지 못했기 때문에 표준 학습 가능 1D 위치 임베딩을 사용합니다. 결과적인 임베딩 벡터 시퀀스는 인코더의 입력으로 사용됩니다.\n\n위치 임베딩 생성을 시작하기 위해 현재 임베딩을 살펴보겠습니다.\n\n# 클래스 임베딩이 앞에 추가된 패치 임베딩 시퀀스 보기\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n\n방정식 1은 위치 임베딩이 \\((N + 1) \\times D\\) 모양이어야 한다고 명시하고 있습니다. 여기서: * \\(N=H W / P^{2}\\)는 결과 패치 수이며 트랜스포머의 유효 입력 시퀀스 길이 역할도 합니다. * \\(D\\)는 패치 임베딩의 크기이며, \\(D\\)에 대한 다양한 값은 표 1에서 찾을 수 있습니다.\n다행히 우리는 이미 이 두 값을 모두 가지고 있습니다.\n따라서 torch.ones()로 학습 가능한 1D 임베딩을 만들어 \\(\\mathbf{E}_{\\text {pos }}\\)를 만들어 보겠습니다.\n\n# N (패치 수) 계산\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# 임베딩 차원 가져오기\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# 학습 가능한 1D 위치 임베딩 생성\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1, \n                                             embedding_dimension),\n                                  requires_grad=True) # 학습 가능하도록 설정\n\n# 처음 10개 시퀀스와 10개 위치 임베딩 값을 표시하고 위치 임베딩의 모양 확인\nprint(position_embedding[:, :10, :10])\nprint(f\"위치 임베딩 모양: {position_embedding.shape} -&gt; [배치_크기, 패치_수, 임베딩_차원]\")\n\n\n참고: 시연 목적으로 위치 임베딩을 torch.ones()로만 생성하고 있으며, 실제로는 torch.randn()을 사용하여 위치 임베딩을 생성할 가능성이 높습니다(무작위 숫자로 시작하여 경사 하강법을 통해 개선).\n\n위치 임베딩이 생성되었습니다!\n이를 클래스 토큰이 추가된 패치 임베딩 시퀀스에 추가해 보겠습니다.\n\n# 패치 및 클래스 토큰 임베딩에 위치 임베딩 추가\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"패치 임베딩, 클래스 토큰 추가 및 위치 임베딩이 더해진 모양: {patch_and_position_embedding.shape} -&gt; [배치_크기, 패치_수, 임베딩_차원]\")\n\n임베딩 텐서의 각 요소 값이 1씩 증가하는 것을 볼 수 있습니다(이는 위치 임베딩이 torch.ones()로 생성되었기 때문입니다).\n\n참고: 원한다면 클래스 토큰 임베딩과 위치 임베딩을 모두 자체 레이어에 넣을 수도 있습니다. 하지만 나중에 전체 ViT 아키텍처의 forward() 메서드에 어떻게 통합될 수 있는지 살펴보겠습니다.\n\n\n패치 임베딩 시퀀스와 클래스 토큰에 위치 임베딩을 추가하는 데 사용한 워크플로우입니다. 참고: torch.ones()는 그림 설명을 위해 임베딩을 생성하는 데만 사용되었으며, 실제로는 torch.randn()을 사용하여 무작위 숫자로 시작할 가능성이 높습니다.\n\n\nTK 4.8 전체 과정 합치기: 이미지에서 임베딩까지\n입력 이미지를 임베딩으로 변환하고 ViT 논문의 섹션 3.1에서 방정식 1을 재현하는 데 많은 진척이 있었습니다.\n\\[\n\\begin{aligned}\n\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n\\end{aligned}\n\\]\n이제 모든 것을 단일 코드 셀에 넣고 입력 이미지(\\(x\\))에서 출력 임베딩 \\({z}_0\\)까지 진행해 보겠습니다.\n다음과 같이 할 수 있습니다. 1. 패치 크기를 설정합니다(논문 전체와 ViT-Base에서 널리 사용되므로 16을 사용하겠습니다). 2. 단일 이미지를 가져와 모양을 출력하고 높이와 너비를 저장합니다. 3. 단일 이미지에 배치 차원을 추가하여 PatchEmbedding 레이어와 호환되도록 합니다. 4. patch_size=16 및 embedding_dim=768(ViT-Base용 표 1에서 가져옴)을 사용하여 PatchEmbedding 레이어를 만듭니다. 5. 4단계의 PatchEmbedding 레이어에 단일 이미지를 통과시켜 패치 임베딩 시퀀스를 생성합니다. 6. 섹션 4.6과 같이 클래스 토큰 임베딩을 만듭니다. 7. 클래스 토큰 임베딩을 5단계에서 생성된 패치 임베딩 앞에 추가합니다. 8. 섹션 4.7과 같이 위치 임베딩을 만듭니다. 9. 7단계에서 생성된 클래스 토큰 및 패치 임베딩에 위치 임베딩을 추가합니다.\n또한 set_seeds()를 사용하여 무작위 시드를 설정하고 진행 과정에서 각 텐서의 모양을 출력하겠습니다.\n\nset_seeds()\n\n# 1. 패치 크기 설정\npatch_size = 16\n\n# 2. 원본 이미지 텐서 모양 출력 및 이미지 크기 가져오기\nprint(f\"이미지 텐서 모양: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. 이미지 텐서 가져오기 및 배치 차원 추가\nx = image.unsqueeze(0)\nprint(f\"배치 차원이 추가된 입력 이미지 모양: {x.shape}\")\n\n# 4. 패치 임베딩 레이어 생성\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. 패치 임베딩 레이어에 이미지 통과\npatch_embedding = patch_embedding_layer(x)\nprint(f\"패치 임베딩 모양: {patch_embedding.shape}\")\n\n# 6. 클래스 토큰 임베딩 생성\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # 학습 가능하도록 설정\nprint(f\"클래스 토큰 임베딩 모양: {class_token.shape}\")\n\n# 7. 패치 임베딩 앞에 클래스 토큰 임베딩 추가\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"클래스 토큰이 포함된 패치 임베딩 모양: {patch_embedding_class_token.shape}\")\n\n# 8. 위치 임베딩 생성\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # 학습 가능하도록 설정\n\n# 9. 클래스 토큰이 포함된 패치 임베딩에 위치 임베딩 추가\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"패치 및 위치 임베딩 모양: {patch_and_position_embedding.shape}\")\n\n좋아요!\n단일 이미지에서 단일 코드 셀의 패치 및 위치 임베딩까지 완료했습니다.\n\nViT 논문의 식 1을 PyTorch 코드에 매핑합니다. 이것이 논문 재현의 본질이며, 연구 논문을 실행 가능한 코드로 바꾸는 것입니다.\n이제 이미지를 인코딩하여 ViT 논문의 그림 1에 있는 트랜스포머 인코더로 전달할 방법이 생겼습니다.\n\n전체 ViT 워크플로우 애니메이션: 패치 임베딩에서 트랜스포머 인코더, MLP 헤드까지.\n코드 관점에서 패치 임베딩을 만드는 것은 아마도 ViT 논문을 재현하는 데 있어 가장 큰 섹션일 것입니다.\nMulti-Head Attention 및 Norm 레이어와 같은 ViT 논문의 다른 많은 부분은 기존 PyTorch 레이어를 사용하여 만들 수 있습니다.\n계속 가보죠!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk.-5.-방정식-2-멀티헤드-어텐션-msa",
    "href": "08_pytorch_paper_replicating.html#tk.-5.-방정식-2-멀티헤드-어텐션-msa",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK. 5. 방정식 2: 멀티헤드 어텐션 (MSA)",
    "text": "TK. 5. 방정식 2: 멀티헤드 어텐션 (MSA)\n입력 데이터를 패치화하고 임베딩했으니, 이제 ViT 아키텍처의 다음 부분으로 넘어가 보겠습니다.\n시작하기 위해 트랜스포머 인코더 섹션을 두 부분으로 나누겠습니다(작게 시작하여 필요할 때 확장).\n첫 번째는 방정식 2이고 두 번째는 방정식 3입니다.\n방정식 2는 다음과 같습니다.\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n\\end{aligned}\n\\]\n이는 잔차 연결(레이어의 입력이 출력에 더해짐)과 함께 LayerNorm(LN) 레이어에 래핑된 MSA(Multi-Head Attention) 레이어를 나타냅니다.\n\n왼쪽: 트랜스포머 인코더 블록 내에서 Multi-Head Attention 및 Norm 레이어와 잔차 연결(+)이 강조된 ViT 논문의 그림 1. 오른쪽: Multi-Head Self Attention(MSA) 레이어, Norm 레이어 및 잔차 연결을 ViT 논문의 식 2의 각 부분에 매핑.\n연구 논문에서 볼 수 있는 많은 레이어는 이미 PyTorch와 같은 현대적인 딥러닝 프레임워크에 구현되어 있습니다.\n이를 감안할 때, 이러한 레이어와 잔차 연결을 PyTorch 코드로 재현하기 위해 다음을 사용할 수 있습니다. * 멀티헤드 셀프 어텐션 (MSA) - torch.nn.MultiheadAttention(). * 노름 (LN 또는 LayerNorm) - torch.nn.LayerNorm(). * 잔차 연결 (Residual connection) - 입력과 출력을 더합니다 (나중에 전체 트랜스포머 인코더 블록을 만들 때 살펴보겠습니다).\n\n5.1 LayerNorm (LN) 레이어\n레이어 정규화(Layer Normalization)(torch.nn.LayerNorm() 또는 Norm 또는 LayerNorm 또는 LN)는 마지막 차원에 대해 입력을 정규화합니다.\nnormalized_shape를 정규화하려는 차원 크기와 동일하게 설정할 수 있습니다(우리의 경우에는 ViT-Base용 \\(D\\) 또는 768이 됩니다).\ntorch.nn.LayerNorm()의 공식 정의는 PyTorch 문서에서 찾을 수 있습니다.\n이게 무슨 역할을 할까요?\n레이어 정규화는 훈련 시간과 모델 일반화(보이지 않는 데이터에 적응하는 능력)를 개선하는 데 도움이 됩니다.\n나는 어떤 종류의 정규화이든 “데이터를 유사한 형식으로 만드는 것” 또는 “데이터 샘플을 유사한 분포로 만드는 것”으로 생각하기를 좋아합니다.\n높이와 길이가 모두 다른 계단을 (위아래로) 걸어간다고 상상해 보세요.\n각 단계마다 조정이 필요하겠죠?\n그리고 각 단계에서 배우는 내용이 계단이 모두 다르기 때문에 다음 단계에 반드시 도움이 되는 것은 아닙니다.\n정규화(레이어 정규화 포함)는 계단이 데이터 샘플이라는 점을 제외하면 모든 계단을 동일한 높이와 길이로 만드는 것과 같습니다.\n따라서 높이와 길이가 비슷한 계단을 높이와 너비가 불규칙한 계단보다 훨씬 쉽게 오르내릴 수 있는 것처럼, 신경망은 분포가 다양한 데이터 샘플보다 분포가 비슷한 데이터 샘플(평균과 표준편차가 비슷함)에서 더 쉽게 최적화할 수 있습니다.\n\n\n5.2 멀티헤드 셀프 어텐션 (MSA) 레이어\n셀프 어텐션(self-attention)과 멀티헤드 어텐션(셀프 어텐션이 여러 번 적용됨)의 강력함은 Attention is all you need 연구 논문에서 소개된 원본 트랜스포머 아키텍처의 형태로 드러났습니다.\n트랜스포머 아키텍처 및 어텐션 매커니즘에 대해 자세히 알아볼 수 있는 온라인 리소스는 Jay Alammar의 훌륭한 Illustrated Transformer 게시물 및 Illustrated Attention 게시물 등이 있습니다.\n하지만 우리는 자체적으로 만들기보다는 기존 PyTorch MSA 구현을 코딩하는 데 더 집중할 것입니다.\n그러나 ViT 논문의 MSA 구현에 대한 공식 정의는 부록 A에 정의되어 있습니다.\n\n왼쪽: ViT 논문의 그림 1에서 가져온 Vision Transformer 아키텍처 개요. 오른쪽: 그림 1의 각 부분을 반영하도록 강조된 ViT 논문의 식 2, 섹션 3.1 및 부록 A의 정의.\n위 이미지는 MSA 레이어에 대한 삼중 입력을 강조합니다.\n이는 셀프 어텐션 메커니즘의 기본인 쿼리(query), 키(key), 값(value) 입력 또는 줄여서 qkv로 알려져 있습니다.\n우리의 경우 삼중 입력은 Norm 레이어 출력의 세 가지 버전이 됩니다.\n또는 섹션 4.8에서 생성된 레이어 정규화된 이미지 패치 및 위치 임베딩의 세 가지 버전입니다.\n다음 매개변수와 함께 torch.nn.MultiheadAttention()을 사용하여 PyTorch에서 MSA 레이어를 구현할 수 있습니다. * embed_dim: 표 1의 임베딩 차원(은닉 크기 \\(D\\)). * num_heads: 사용할 어텐션 헤드 수(“멀티헤드”라는 용어가 여기서 유래됨), 이 값도 표 1(헤드)에 있습니다. * dropout: 어텐션 레이어에 드롭아웃을 적용할지 여부(부록 B.1에 따르면 qkv-투영 후에는 드롭아웃이 사용되지 않음).\n\n\n5.3 식 2를 PyTorch 레이어로 재현하기\n방정식 2의 LayerNorm(LN) 및 Multi-Head Attention(MSA) 레이어에 대해 논의한 모든 내용을 실제로 적용해 보겠습니다.\n이를 위해 다음을 수행합니다.\n\ntorch.nn.Module을 상속받는 MultiheadSelfAttentionBlock()이라는 클래스를 만듭니다.\nViT-Base 모델에 대한 ViT 논문의 표 1 하이퍼파라미터로 클래스를 초기화합니다.\n임베딩 차원(표 1의 \\(D\\))과 동일한 normalized_shape 매개변수를 사용하여 torch.nn.LayerNorm()으로 레이어 정규화(LN) 레이어를 만듭니다.\n적절한 embed_dim, num_heads, dropout 및 batch_first 매개변수를 사용하여 멀티헤드 어텐션(MSA) 레이어를 만듭니다.\nLN 레이어와 MSA 레이어를 통해 입력을 전달하는 클래스의 forward() 메서드를 만듭니다.\n\n\n# 1. nn.Module을 상속받는 클래스 생성\nclass MultiheadSelfAttentionBlock(nn.Module):\n    \"\"\"멀티헤드 셀프 어텐션 블록(줄여서 \"MSA 블록\")을 만듭니다.\n    \"\"\"\n    # 2. 표 1의 하이퍼파라미터를 사용하여 클래스 초기화\n    def __init__(self,\n                 embedding_dim:int=768, # ViT-Base용 표 1에서 가져옴\n                 num_heads:int=12, # ViT-Base용 표 1에서 가져옴\n                 attn_dropout:int=0): # 논문에서는 MSABlocks에서 드롭아웃을 사용하지 않는 것 같음\n        super().__init__()\n        \n        # 3. 노름 레이어(LN) 생성\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        # 4. 멀티헤드 어텐션(MSA) 레이어 생성\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # 배치 차원이 먼저 오나요?\n        \n    # 5. 레이어를 통해 데이터를 전달하는 forward() 메서드 생성\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # 쿼리 임베딩 \n                                             key=x, # 키 임베딩\n                                             value=x, # 값 임베딩\n                                             need_weights=False) # 가중치가 필요한가요, 아니면 레이어 출력만 필요한가요?\n        return attn_output\n\n\n참고: 그림 1과 달리, 우리의 MultiheadSelfAttentionBlock()은 스킵 또는 잔차 연결(식 2의 “\\(+\\mathbf{z}_{\\ell-1}\\)”)을 포함하지 않는데, 나중에 전체 트랜스포머 인코더를 만들 때 이를 포함시킬 것입니다.\n\nMSABlock이 생성되었습니다!\nMultiheadSelfAttentionBlock의 인스턴스를 만들고 섹션 4.8에서 생성한 patch_and_position_embedding 변수를 통과시켜 시도해 보겠습니다.\n\n# MSABlock 인스턴스 생성\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # 표 1에서 가져옴 \n                                                             num_heads=12) # 표 1에서 가져옴\n\n# 패치 및 위치 이미지 임베딩을 MSABlock에 통과시킴\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"MSA 블록의 입력 모양: {patch_and_position_embedding.shape}\")\nprint(f\"MSA 블록의 출력 모양: {patched_image_through_msa_block.shape}\")\n\n데이터가 MSA 블록을 통과할 때 입력 및 출력 모양이 동일하게 유지되는 것을 확인하세요.\n이는 데이터가 통과하면서 변경되지 않는다는 것을 의미하지는 않습니다.\n입력 및 출력 텐서를 출력하여 어떻게 변하는지 확인할 수 있습니다(단, 이 변화는 1 * 197 * 768 값에 걸쳐 발생합니다).\n\n왼쪽: 그림 1에서 Multi-Head Attention 및 LayerNorm 레이어가 강조된 Vision Transformer 아키텍처. 이러한 레이어는 논문 섹션 3.1의 식 2를 구성합니다. 오른쪽: PyTorch 레이어를 사용하여 식 2(끝 부분의 스킵 연결 제외)를 재현합니다.\n이제 공식적으로 방정식 2를 재현했습니다(끝부분의 잔차 연결은 제외되었지만 섹션 7에서 다룰 것입니다)!\n다음으로 가봅시다!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-6.-식-3-다층-퍼셉트론-mlp",
    "href": "08_pytorch_paper_replicating.html#tk-6.-식-3-다층-퍼셉트론-mlp",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 6. 식 3: 다층 퍼셉트론 (MLP)",
    "text": "TK 6. 식 3: 다층 퍼셉트론 (MLP)\n여기까지: * 식 2를 재현한 것처럼 식 3을 재현합니다.\n\nTK는 “피드포워드(feedforward)”라고도 불립니다.\n\n\n드롭아웃이 사용되는 경우, qkv-투영을 제외한 모든 밀집 레이어 뒤와 위치-패치 임베딩을 추가한 직후에 적용됩니다.\n\n\nMLP에는 GELU 비선형성이 있는 두 개의 레이어가 포함됩니다.\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L\n\\end{aligned}\n\\]\n\nTK - PyTorch의 GELU – https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\n\n\n# \"피드포워드(FeedForward)\"라고도 부를 수 있습니다.\nclass MLPBlock(nn.Module):\n    \"\"\"Vision Transformer 아키텍처의 MLPBlock을 만듭니다.\"\"\"\n    def __init__(self,\n                 embedding_dim, # 임베딩 차원 (표 1의 은닉 크기 D)\n                 mlp_size, # 표 1의 MLP 크기\n                 dropout=0): # \"드롭아웃...은 모든 밀집 레이어에 적용됩니다... (부록 B.1)\"\n        super().__init__()\n        \n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"MLP에는 GELU 비선형성을 갖는 두 개의 레이어가 포함됩니다(섹션 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # 위 레이어의 out_features와 동일한 in_features를 가져야 함\n                      out_features=embedding_dim), # 다시 embedding_dim으로 가져옴\n            nn.Dropout(p=dropout)\n        )\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n\n\nmlp_block = MLPBlock(embedding_dim=768, # 표 1 \n                     mlp_size=3072) # 표 1\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\npatched_image_through_mlp_block.shape",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-7.-트랜스포머-인코더-생성",
    "href": "08_pytorch_paper_replicating.html#tk-7.-트랜스포머-인코더-생성",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 7. 트랜스포머 인코더 생성",
    "text": "TK 7. 트랜스포머 인코더 생성\n\nTk - “인코더(encoder)”란 무엇인가요?\nTk - “트랜스포머 블록” 또는 “트랜스포머 인코더”? - 논문과 일치시키세요\n\n사전 구축된 트랜스포머 블록/레이어는 여기를 참조하세요: https://pytorch.org/docs/stable/nn.html#transformer-layers\n\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"트랜스포머 인코더 블록을 만듭니다.\"\"\"\n    def __init__(self,\n                 embedding_dim=768, # 표 1에서 가져옴\n                 num_heads=12, # 표 1에서 가져옴\n                 mlp_size=3072, # 표 1에서 가져옴\n                 mlp_dropout=0.1,\n                 attn_dropout=0):\n        super().__init__()\n\n        # MSA 블록 생성 (식 2용)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n        # MLP 블록 생성 (식 3용)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n        \n    def forward(self, x):\n        x = self.msa_block(x) + x # 스킵 연결 생성\n        x = self.mlp_block(x) + x # 스킵 연결 생성\n        return x",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-8.-전체-과정-합치기-vit-생성",
    "href": "08_pytorch_paper_replicating.html#tk-8.-전체-과정-합치기-vit-생성",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 8. 전체 과정 합치기: ViT 생성",
    "text": "TK 8. 전체 과정 합치기: ViT 생성\nTK - 이를 TransformerEncoderLayer로 재현하세요 - https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/\n트랜스포머 블록과 패치된 임베딩을 결합하여 ViT 아키텍처를 만듭니다.\n\nclass ViT(nn.Module):\n    \"\"\"Vision Transformer 아키텍처를 만듭니다.\"\"\"\n    def __init__(self,\n                 img_size=224, # ViT 논문의 표 3에서 가져옴\n                 in_channels=3,\n                 patch_size=16,\n                 num_transformer_layers=12, # ViT 논문의 표 1에서 가져옴\n                 embedding_dim=768,\n                 mlp_size=3072,\n                 num_heads=12,\n                 attn_dropout=0,\n                 mlp_dropout=0.1,\n                 embedding_dropout=0.1,\n                 num_classes=1000): # ImageNet용 기본값\n        super().__init__() # super().__init__()을 잊지 마세요!\n    \n        # 이미지 크기 가져오기\n        self.img_height, self.img_width = img_size, img_size\n        \n        # 패치 수 계산 (높이 * 너비 / 패치^2)\n        self.num_patches = (self.img_height * self.img_width) // patch_size**2\n        \n                 \n        # 클래스 임베딩 생성 (시퀀스 임베딩 앞에 와야 함)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n        \n         # 위치 임베딩 생성\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n                \n        # 임베딩 드롭아웃 생성\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n        \n        # 패치 임베딩 레이어 생성\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n        \n        # 트랜스포머 인코더 블록 생성\n        self.transformer_enedoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n       \n        # 분류 헤드 생성 (식 4)\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim, \n                      out_features=num_classes)\n        )\n    \n    def forward(self, x):\n        # 배치 크기 가져오기\n        batch_size = x.shape[0]\n        # 클래스 토큰 임베딩 생성\n        class_token = self.class_embedding.expand(batch_size, -1, -1)\n\n        # 패치 임베딩 생성\n        x = self.patch_embedding(x)\n\n        # 클래스 임베딩과 패치 임베딩 연결 (식 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # 모든 배치에 대해 패치 임베딩에 위치 임베딩 추가 (식 1)\n        x = self.position_embedding + x\n\n        # 임베딩 드롭아웃 실행\n        x = self.embedding_dropout(x)\n\n        # 트랜스포머 인코더 레이어를 통해 패치, 위치 및 클래스 임베딩 전달 (식 2 및 3)\n        x = self.transformer_enedoder(x)\n\n        # 분류기를 통해 0번 인덱스 로짓 입력 (식 4)\n        x = self.classifier(x[:, 0]) # 배치의 각 샘플에 대해 0번 인덱스에서 실행\n\n        return x\n        \n\n\nbatch_size = 32\nclass_tokens = nn.Parameter(data=torch.randn(1, 1, 768))\nclass_tokens.expand(batch_size, -1, -1).shape\n\n\nset_seeds()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nrand_image = torch.randn(1, 3, 224, 224)\n# vit = ViT(num_classes=len(class_names)) \nvit = ViT(num_classes=3)\nvit(rand_image)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-9.-모델-검사",
    "href": "08_pytorch_paper_replicating.html#tk-9.-모델-검사",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 9. 모델 검사",
    "text": "TK 9. 모델 검사\n\n참고: 너무 크게 설정하면 하드웨어가 감당하지 못할 수도 있습니다… (예: 배치 크기가 너무 큼…)\n\nTK - 파라미터 수는 다음 사이트와 동일해야 함: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 (num_params=86,567,656)\n\nfrom torchinfo import summary\n\n# TK - 요약을 정리하여 출력 시 보기 좋게 만듭니다. \n# torchinfo를 사용하여 요약 출력 (실제 출력을 보려면 주석 해제)\nsummary(model=vit, \n        input_size=(128, 3, 224, 224), # \"input_shape\"이 아닌 \"input_size\"인지 확인\n        # col_names=[\"input_size\"], # 더 작은 출력을 원할 시 주석 해제\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n\n\nTK - 다음 사이트와 동일한 파라미터 수: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 -&gt; 86567656\n\n\nbatch_size = 32\ncls_embedding = nn.Parameter(torch.randn(1, 1, 768))\n# 여기 참조: https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html\ncls_embedding.shape, cls_embedding.expand(batch_size, -1, -1).shape",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-10.-모델-훈련",
    "href": "08_pytorch_paper_replicating.html#tk-10.-모델-훈련",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 10. 모델 훈련",
    "text": "TK 10. 모델 훈련\n\nfrom going_modular.going_modular import engine\n\noptimizer = torch.optim.Adam(params=vit.parameters(), \n                             lr=1e-3,\n                             betas=(0.9, 0.999), # 기본값\n                             weight_decay=0.1) # ViT 논문 섹션 4.1에서 가져옴\nloss_fn = torch.nn.CrossEntropyLoss()\n\nset_seeds()\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-11.-모델-평가",
    "href": "08_pytorch_paper_replicating.html#tk-11.-모델-평가",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 11. 모델 평가",
    "text": "TK 11. 모델 평가\nTK - 손실 곡선 플롯\n\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(results)\n\nTK - 손실 곡선이 왜 그런 식으로 나타날까요? (모델이 너무 크고 데이터가 충분하지 않음)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-12.-동일한-데이터셋에서-torchvision.models의-사전-훈련된-vit-가져오기",
    "href": "08_pytorch_paper_replicating.html#tk-12.-동일한-데이터셋에서-torchvision.models의-사전-훈련된-vit-가져오기",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK 12. 동일한 데이터셋에서 torchvision.models의 사전 훈련된 ViT 가져오기",
    "text": "TK 12. 동일한 데이터셋에서 torchvision.models의 사전 훈련된 ViT 가져오기\n\n여기에서 비슷한 모델 가져오기 - https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n\n\n# 다음은 torch v0.12+ 및 torchvision v0.13+가 필요합니다.\nimport torch\nimport torchvision\nprint(torch.__version__) \nprint(torchvision.__version__)\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n\n# 시드 설정\ndef set_seeds(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\n# torchvision &gt;= 0.13 필요\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# 기본 파라미터 고정\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n    \n# 분류 헤드 변경\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n\n\n# torchinfo를 사용하여 요약 출력 (실제 출력을 보려면 주석 해제)\nsummary(model=pretrained_vit, \n        input_size=(128, 3, 224, 224), # \"input_shape\"이 아닌 \"input_size\"인지 확인\n        # col_names=[\"input_size\"], # 더 작은 출력을 원할 시 주석 해제\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n\n\n# TK - 위 출력은 우리가 직접 만든 모델과 동일한 수의 파라미터를 가지고 있습니다.\n\n\n# GitHub에서 피자, 스테이크, 초밥 이미지 다운로드\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n\n\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\" \ntrain_dir, test_dir\n\n\n# 사전 훈련된 ViT를 위한 데이터셋 생성\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=1024) # 여기에서 가져옴: https://arxiv.org/abs/2205.01580 (다른 개선 사항도 있음...)\n\n\n# 피자, 스테이크, 초밥에 대해 사전 훈련된 특성 추출기 ViT를 10 에포크 동안 훈련\n# TK - 전체 모델을 훈련하는 것이 아니라 특성 추출을 사용하므로 여기에서 batch_size를 늘릴 수 있을 것입니다.\nfrom going_modular.going_modular import engine\n\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n\n\n# 손실 곡선 플롯\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results) \n\n\n# 모델 저장\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n\n\nfrom pathlib import Path\n\n# 모델 크기를 바이트 단위로 가져온 다음 메가바이트로 변환\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024)\nprint(f\"사전 훈련된 ViT 특성 추출기 모델 크기: {pretrained_vit_model_size} MB\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---이-재현에서-빠진-부분들",
    "href": "08_pytorch_paper_replicating.html#tk---이-재현에서-빠진-부분들",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 이 재현에서 빠진 부분들",
    "text": "TK - 이 재현에서 빠진 부분들\nTK 논문과 이 재현의 차이점 기록 * 이 중 상당수는 표 3에 있습니다. * 훈련 데이터 (바닥부터 훈련한 ImageNet vs FoodVision Mini 데이터) * LR 웜업(warmup) * LR 감쇠(decay) * 가중치 감쇠(Weight decay) * 에포크 수",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---연습-문제",
    "href": "08_pytorch_paper_replicating.html#tk---연습-문제",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 연습 문제",
    "text": "TK - 연습 문제",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---추가-학습-자료",
    "href": "08_pytorch_paper_replicating.html#tk---추가-학습-자료",
    "title": "08 - PyTorch 논문 복제",
    "section": "TK - 추가 학습 자료",
    "text": "TK - 추가 학습 자료\n\nlayernorm\n트랜스포머 모델의 개요를 보려면 illustrated transformer를 참조하세요: https://jalammar.github.io/illustrated-transformer/ + https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\nAttention is all you need 논문 - Yannic 비디오\nVision transformer - yannic 비디오",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>08 - PyTorch 논문 복제</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html",
    "href": "09_pytorch_model_deployment.html",
    "title": "09 - PyTorch 모델 배포",
    "section": "",
    "text": "머신러닝 모델 배포란 무엇인가요?\n소스 코드 보기 | 슬라이드 보기\n마일스톤 프로젝트 3: PyTorch 모델 배포에 오신 것을 환영합니다!\nFoodVision Mini 프로젝트를 통해 여기까지 먼 길을 달려왔습니다.\n하지만 지금까지 우리의 PyTorch 모델은 우리 자신만 접근할 수 있었습니다.\n이제 FoodVision Mini를 실생활에 적용하고 공개적으로 접근 가능하게 만드는 건 어떨까요?\n다시 말해, 우리의 FoodVision Mini 모델을 인터넷에 사용 가능한 앱으로 배포할 것입니다!\nFoodVision Mini 배포 버전(우리가 만들 것)을 점심 식사 중에 사용해 보았습니다. 모델이 정답을 맞혔네요 🍣!\n머신러닝 모델 배포(Machine learning model deployment)는 머신러닝 모델을 다른 사람이나 다른 대상이 접근할 수 있도록 만드는 과정입니다.\n여기서 ’다른 사람’이란 어떤 방식으로든 모델과 상호작용할 수 있는 사람을 의미합니다.\n예를 들어, 스마트폰으로 음식 사진을 찍은 다음 FoodVision Mini 모델이 해당 음식을 피자, 스테이크, 초밥 중 하나로 분류하게 하는 사람입니다.\n’다른 대상’은 여러분의 머신러닝 모델과 상호작용하는 또 다른 프로그램, 앱 또는 다른 모델일 수 있습니다.\n예를 들어, 은행 데이터베이스는 자금을 송금하기 전에 거래가 사기인지 여부를 예측하는 머신러닝 모델에 의존할 수 있습니다.\n또는 운영 체제는 특정 시간대에 누군가가 일반적으로 얼마나 많은 전력을 사용하는지 예측하는 머신러닝 모델을 기반으로 리소스 소비를 줄일 수 있습니다.\n이러한 유스케이스는 서로 혼합될 수도 있습니다.\n예를 들어, 테슬라 자동차의 컴퓨터 비전 시스템은 자동차의 경로 계획 프로그램(다른 대상)과 상호작용하고, 경로 계획 프로그램은 운전자(다른 사람)로부터 입력과 피드백을 받습니다.\n머신러닝 모델 배포에는 모델을 다른 사람이나 다른 대상이 사용할 수 있도록 만드는 작업이 포함됩니다. 예를 들어, 누군가 음식 인식 앱(FoodVision Mini 또는 Nutrify와 같은)의 일부로 모델을 사용할 수 있습니다. 또한 은행 시스템에서 거래 사기 여부를 탐지하기 위해 머신러닝 모델을 사용하는 것과 같이 다른 모델이나 프로그램이 모델을 사용할 수도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#머신러닝-모델을-왜-배포해야-하나요",
    "href": "09_pytorch_model_deployment.html#머신러닝-모델을-왜-배포해야-하나요",
    "title": "09 - PyTorch 모델 배포",
    "section": "머신러닝 모델을 왜 배포해야 하나요?",
    "text": "머신러닝 모델을 왜 배포해야 하나요?\n머신러닝에서 가장 중요한 철학적 질문 중 하나는 다음과 같습니다.\n\n\n\n모델을 배포하는 것은 모델을 훈련하는 것만큼이나 중요합니다.\n잘 만들어진 테스트 세트에서 모델을 평가하거나 결과를 시각화하여 모델이 어떻게 작동할지 꽤 잘 알 수 있지만, 실제로 세상에 공개하기 전까지는 모델이 어떻게 성능을 낼지 결코 알 수 없기 때문입니다.\n모델을 한 번도 사용해 본 적이 없는 사람들과 상호작용하게 하면 훈련 중에는 생각지도 못했던 예외 상황(edge cases)이 종종 드러납니다.\n예를 들어, 누군가 음식이 아닌 사진을 FoodVision Mini 모델에 업로드하면 어떻게 될까요?\n한 가지 해결책은 이미지를 먼저 “음식” 또는 “음식 아님”으로 분류하는 또 다른 모델을 만들고, 대상 이미지를 해당 모델에 먼저 통과시키는 것입니다(Nutrify가 하는 방식).\n이미지가 “음식”인 경우 FoodVision Mini 모델로 전달되어 피자, 스테이크 또는 초밥으로 분류됩니다.\n“음식 아님”인 경우 메시지가 표시됩니다.\n그런데 만약 이러한 예측이 틀렸다면 어떻게 될까요?\n그때는 어떤 일이 벌어질까요?\n이러한 질문들이 꼬리에 꼬리를 물고 이어질 수 있다는 것을 알 수 있습니다.\n따라서 이것은 모델 배포의 중요성을 강조합니다. 배포는 훈련/테스트 중에는 명확하지 않았던 모델의 오류를 파악하는 데 도움이 됩니다.\n\nPyTorch 워크플로우는 01. PyTorch 워크플로우에서 다루었습니다. 하지만 좋은 모델이 생겼다면 배포가 다음 단계로 좋습니다. 모니터링은 모델이 가장 중요한 데이터 분할인 실제 세상의 데이터에서 어떻게 작동하는지 확인하는 작업을 포함합니다. 배포 및 모니터링에 대한 더 많은 자료는 PyTorch 추가 리소스를 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#머신러닝-모델-배포의-다양한-유형",
    "href": "09_pytorch_model_deployment.html#머신러닝-모델-배포의-다양한-유형",
    "title": "09 - PyTorch 모델 배포",
    "section": "머신러닝 모델 배포의 다양한 유형",
    "text": "머신러닝 모델 배포의 다양한 유형\n머신러닝 모델 배포의 다양한 유형에 대해서는 책 한 권을 쓸 수도 있을 정도입니다(많은 훌륭한 자료들이 PyTorch 추가 리소스에 나열되어 있습니다).\n그리고 이 분야는 여전히 모범 사례(best practices) 측면에서 발전하고 있습니다.\n하지만 저는 다음과 같은 질문으로 시작하는 것을 좋아합니다.\n\n“내 머신러닝 모델이 사용되기에 가장 이상적인 시나리오는 무엇인가?”\n\n그런 다음 거기서부터 거꾸로 작업해 나갑니다.\n물론 미리 알지 못할 수도 있습니다. 하지만 여러분은 그런 것들을 상상할 수 있을 만큼 충분히 똑똑합니다.\nFoodVision Mini의 경우, 가장 이상적인 시나리오는 다음과 같을 수 있습니다.\n\n누군가 모바일 장치(앱이나 웹 브라우저를 통해)에서 사진을 찍습니다.\n예측 결과가 빠르게 돌아옵니다.\n\n간단하죠.\n따라서 두 가지 주요 기준이 있습니다.\n\n모델은 모바일 장치에서 작동해야 합니다(이는 일부 컴퓨팅 제약이 있음을 의미함).\n모델은 예측을 빠르게 해야 합니다(느린 앱은 지루한 앱이기 때문입니다).\n\n물론 유스케이스에 따라 요구 사항이 달라질 수 있습니다.\n위의 두 가지 사항은 다음 두 가지 질문으로 나뉜다는 것을 알 수 있습니다.\n\n어디로 가는가? - 즉, 어디에 저장될 것인가?\n어떻게 작동하는가? - 즉, 예측 결과를 즉시 반환하는가? 아니면 나중에 반환하는가?\n\n\n머신러닝 모델 배포를 시작할 때, 가장 이상적인 유스케이스가 무엇인지 물어본 다음 거기서부터 거꾸로 작업하여 모델이 어디로 가고 어떻게 작동할지 묻는 것이 도움이 됩니다.\n\n어디로 가는가?\n머신러닝 모델을 배포할 때, 모델은 어디에 존재할까요?\n여기서 주요 논쟁은 보통 온디바이스(on-device, 에지/브라우저라고도 함) 또는 클라우드(누군가/무엇인가가 모델을 호출하는 실제 장치가 아닌 컴퓨터/서버)입니다.\n둘 다 장단점이 있습니다.\n\n\n\n\n\n\n\n\n배포 위치\n장점\n단점\n\n\n\n\n온디바이스 (에지/브라우저)\n데이터가 장치를 떠나지 않으므로 매우 빠를 수 있음\n제한된 컴퓨팅 파워 (큰 모델은 실행하는 데 더 오래 걸림)\n\n\n\n개인정보 보호 (데이터가 장치를 떠날 필요가 없음)\n제한된 저장 공간 (더 작은 모델 크기 필요)\n\n\n\n인터넷 연결이 필요 없음 (때때로)\n장치별 기술이 종종 요구됨\n\n\n\n\n\n\n\n클라우드\n거의 무제한의 컴퓨팅 파워 (필요할 때 확장 가능)\n비용이 걷잡을 수 없이 커질 수 있음 (적절한 확장 한도가 강제되지 않는 경우)\n\n\n\n모델 하나를 배포하고 어디서든 사용 가능 (API를 통해)\n데이터가 장치를 떠나고 예측 결과가 돌아와야 하므로 예측이 더 느려질 수 있음 (네트워크 지연)\n\n\n\n기존 클라우드 에코시스템과 연결됨\n데이터가 장치를 떠나야 함 (이로 인해 개인정보 보호 문제가 발생할 수 있음)\n\n\n\n이에 대한 더 자세한 내용은 많지만, 더 배우고 싶다면 추가 학습 자료에 리소스를 남겨두었습니다.\n예를 들어 보겠습니다.\nFoodVision Mini를 앱으로 배포한다면, 성능이 좋고 빨라야 합니다.\n그렇다면 어떤 모델을 선호할까요?\n\n예측당 1초의 추론 시간(지연 시간)과 95%의 정확도로 작동하는 온디바이스 모델.\n예측당 10초의 추론 시간과 98%의 정확도로 작동하는 클라우드 모델(더 크고 좋은 모델이지만 계산하는 데 더 오래 걸림).\n\n이 수치들은 제가 임의로 만든 것이지만 온디바이스와 클라우드의 잠재적인 차이를 보여줍니다.\n옵션 1은 모바일 장치에 적합하여 빠르게 실행되지만 성능은 약간 떨어지는 작은 모델일 수 있습니다.\n옵션 2는 더 많은 컴퓨팅과 저장 공간이 필요하지만 실행하는 데 약간 더 오래 걸리는 더 크고 성능이 좋은 모델일 수 있습니다. 데이터가 장치를 떠나고 다시 돌아와야 하기 때문에 실제 예측은 빠르더라도 네트워크 시간과 데이터 전송 시간을 고려해야 하기 때문입니다.\nFoodVision Mini의 경우, 약간의 성능 하락보다 훨씬 빠른 추론 속도가 더 중요하므로 옵션 1을 선호할 가능성이 높습니다.\n\n테슬라 자동차의 컴퓨터 비전 시스템의 경우, 어떤 것이 더 좋을까요? 온디바이스(모델이 자동차에 있음)에서 잘 작동하는 작은 모델일까요, 아니면 클라우드에 있는 더 성능이 좋은 큰 모델일까요? 이 경우에는 자동차에 모델이 있는 것을 훨씬 선호할 것입니다. 데이터가 자동차에서 클라우드로 갔다가 다시 자동차로 돌아오는 데 걸리는 추가 네트워크 시간은 그만한 가치가 없거나 신호가 약한 지역에서는 아예 불가능할 수도 있기 때문입니다.\n\n참고: 에지 장치에 PyTorch 모델을 배포하는 것이 어떤 것인지 전체 예제를 보려면 라즈베리 파이에서 컴퓨터 비전 모델을 사용하여 실시간 추론(30fps+)을 달성하는 방법에 대한 PyTorch 튜토리얼을 참조하세요.\n\n\n\n어떻게 작동하는가?\n이상적인 유스케이스로 돌아가서, 머신러닝 모델을 배포할 때 모델은 어떻게 작동해야 할까요?\n즉, 예측 결과가 즉시 반환되기를 원하시나요?\n아니면 나중에 발생해도 괜찮나요?\n이 두 가지 시나리오는 일반적으로 다음과 같이 불립니다.\n\n온라인 (실시간) - 예측/추론이 즉시 발생합니다. 예를 들어, 누군가 이미지를 업로드하면 이미지가 변환되고 예측 결과가 반환되거나, 누군가 구매를 하면 모델에 의해 거래가 사기가 아님이 확인되어 구매가 진행될 수 있습니다.\n오프라인 (배치) - 예측/추론이 주기적으로 발생합니다. 예를 들어, 모바일 장치가 충전기에 연결되어 있는 동안 사진 애플리케이션이 이미지를 여러 카테고리(해변, 식사 시간, 가족, 친구 등)로 정렬하는 작업입니다.\n\n\n참고: “배치(Batch)”는 한 번에 여러 샘플에 대해 추론이 수행되는 것을 의미합니다. 하지만 약간의 혼동을 주자면, 배치 처리는 즉시/온라인(여러 이미지를 한 번에 분류) 및/또는 오프라인(여러 이미지를 한 번에 예측/훈련)으로 발생할 수 있습니다.\n\n둘 사이의 주요 차이점은 예측이 즉시 이루어지는지 아니면 주기적으로 이루어지는지입니다.\n주기적이라는 것은 몇 초마다부터 몇 시간 또는 며칠마다까지 다양한 시간 척도를 가질 수 있습니다.\n그리고 두 가지를 혼합하여 사용할 수도 있습니다.\nFoodVision Mini의 경우, 누군가 피자, 스테이크 또는 초밥 이미지를 업로드했을 때 예측 결과가 즉시 반환되어야 하므로 추론 파이프라인이 온라인(실시간)으로 이루어지기를 원할 것입니다(실시간보다 느리면 지루한 경험이 될 것입니다).\n하지만 훈련 파이프라인의 경우, 이전 챕터들에서 해왔던 방식인 배치(오프라인) 방식으로 이루어져도 괜찮습니다.\n\n\n머신러닝 모델을 배포하는 방법\n머신러닝 모델을 배포하기 위한 몇 가지 옵션(온디바이스 및 클라우드)에 대해 논의했습니다.\n그리고 이들 각각은 고유한 요구 사항이 있을 것입니다.\n\n\n\n도구/리소스\n배포 유형\n\n\n\n\nGoogle의 ML Kit\n온디바이스 (Android 및 iOS)\n\n\nApple의 Core ML 및 coremltools Python 패키지\n온디바이스 (모든 Apple 장치)\n\n\nAmazon Web Service(AWS)의 Sagemaker\n클라우드\n\n\nGoogle Cloud의 Vertex AI\n클라우드\n\n\nMicrosoft의 Azure Machine Learning\n클라우드\n\n\nHugging Face Spaces\n클라우드\n\n\nFastAPI를 이용한 API\n클라우드/자체 호스팅 서버\n\n\nTorchServe를 이용한 API\n클라우드/자체 호스팅 서버\n\n\nONNX (Open Neural Network Exchange)\n다목적/일반\n\n\n기타 다수…\n\n\n\n\n\n참고: API(Application Programming Interface)는 두 개 이상의 컴퓨터 프로그램이 서로 상호작용하는 방법입니다. 예를 들어, 모델이 API로 배포되었다면 모델에 데이터를 보내고 예측 결과를 다시 받을 수 있는 프로그램을 작성할 수 있습니다.\n\n어떤 옵션을 선택할지는 무엇을 만드는지, 누구와 협력하는지에 따라 크게 달라집니다.\n하지만 옵션이 너무 많아서 매우 위협적일 수 있습니다.\n따라서 작게 시작하고 단순하게 유지하는 것이 좋습니다.\n그리고 그렇게 하는 가장 좋은 방법 중 하나는 머신러닝 모델을 Gradio를 사용하여 데모 앱으로 만든 다음 Hugging Face Spaces에 배포하는 것입니다.\n나중에 FoodVision Mini로 바로 그 작업을 수행할 것입니다.\n\n머신러닝 모델을 호스팅하고 배포하기 위한 몇 가지 장소와 도구입니다. 제가 놓친 것들도 많으니 더 추가하고 싶으시다면 GitHub Discussion에 남겨주세요.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#이번-장에서-다룰-내용",
    "href": "09_pytorch_model_deployment.html#이번-장에서-다룰-내용",
    "title": "09 - PyTorch 모델 배포",
    "section": "이번 장에서 다룰 내용",
    "text": "이번 장에서 다룰 내용\n머신러닝 모델 배포에 대한 이야기는 이 정도로 충분합니다.\n머신러닝 엔지니어가 되어 실제로 모델을 배포해 봅시다.\n우리의 목표는 다음과 같은 지표를 가진 데모 Gradio 앱을 통해 FoodVision 모델을 배포하는 것입니다. 1. 성능: 95% 이상의 정확도. 2. 속도: 30FPS 이상의 실시간 추론(각 예측의 지연 시간이 약 0.03초 미만).\n먼저 지금까지 가장 성능이 좋았던 두 모델인 EffNetB2와 ViT 특성 추출기를 비교하는 실험을 진행하겠습니다.\n그런 다음 목표 지표에 가장 근접한 모델을 배포할 것입니다.\n마지막으로 (커다란) 깜짝 보너스로 마무리하겠습니다.\n\n\n\n주제\n내용\n\n\n\n\n0. 설정하기\n지난 몇 섹션 동안 작성한 유용한 코드들을 다운로드하고 다시 사용할 수 있도록 설정합니다.\n\n\n1. 데이터 가져오기\n이전에 가장 성능이 좋았던 모델들을 동일한 데이터셋에서 훈련하기 위해 pizza_steak_sushi_20_percent.zip 데이터셋을 다운로드합니다.\n\n\n2. FoodVision Mini 모델 배포 실험 개요\n세 번째 마일스톤 프로젝트에서도 어떤 모델(EffNetB2 또는 ViT)이 목표 지표에 가장 근접한지 확인하기 위해 여러 실험을 진행할 것입니다.\n\n\n3. EffNetB2 특성 추출기 만들기\n07. PyTorch 실험 추적에서 피자, 스테이크, 초밥 데이터셋에 대해 가장 좋은 성능을 보였던 EfficientNetB2 특성 추출기를 배포 후보로 다시 만듭니다.\n\n\n4. ViT 특성 추출기 만들기\n08. PyTorch 논문 복제에서 피자, 스테이크, 초밥 데이터셋에 대해 지금까지 가장 성능이 좋았던 ViT 특성 추출기를 EffNetB2와 함께 배포 후보로 다시 만듭니다.\n\n\n5. 훈련된 모델로 예측하고 시간 측정하기\n지금까지 가장 성능이 좋았던 두 모델을 구축하고, 이를 사용하여 예측을 수행하며 결과를 추적합니다.\n\n\n6. 모델 결과, 예측 시간 및 크기 비교\n우리의 목표에 가장 부합하는 모델이 무엇인지 비교합니다.\n\n\n7. Gradio 데모를 만들어 FoodVision Mini 활성화하기\n목표 지표 측면에서 더 나은 성능을 보이는 모델 중 하나를 선택하여 실제 작동하는 앱 데모로 만듭니다!\n\n\n8. FoodVision Mini Gradio 데모를 배포 가능한 앱으로 변환하기\n로컬에서 작동하는 Gradio 앱 데모를 배포할 수 있도록 준비합니다!\n\n\n9. Gradio 데모를 HuggingFace Spaces에 배포하기\nFoodVision Mini를 웹으로 가져와 모든 사람이 공개적으로 접근할 수 있게 만듭니다!\n\n\n10. FoodVision Big 만들기\nFoodVision Mini를 만들었으니, 이제 한 단계 더 나아갈 시간입니다.\n\n\n11. FoodVision Big 배포하기\n앱 하나를 배포하는 것도 즐거웠지만, 두 개를 배포해 보는 건 어떨까요?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#도움을-받을-수-있는-곳",
    "href": "09_pytorch_model_deployment.html#도움을-받을-수-있는-곳",
    "title": "09 - PyTorch 모델 배포",
    "section": "도움을 받을 수 있는 곳",
    "text": "도움을 받을 수 있는 곳\n이 과정의 모든 자료는 GitHub에 있습니다.\n문제가 발생하면 해당 페이지의 Discussions 페이지에서 질문할 수 있습니다.\n또한 PyTorch와 관련된 모든 것에 대해 매우 도움이 되는 장소인 PyTorch 개발자 포럼과 PyTorch 문서도 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#설정하기",
    "href": "09_pytorch_model_deployment.html#설정하기",
    "title": "09 - PyTorch 모델 배포",
    "section": "0. 설정하기",
    "text": "0. 설정하기\n이전에 했던 것처럼 이 섹션에 필요한 모든 모듈이 있는지 확인해 보겠습니다.\n05. PyTorch 모듈화에서 만든 Python 스크립트(data_setup.py 및 engine.py 등)를 가져오겠습니다.\n이를 위해 pytorch-deep-learning 저장소에서 going_modular 디렉토리를 다운로드합니다(이미 가지고 있지 않은 경우).\n또한 torchinfo 패키지가 없는 경우 가져옵니다.\ntorchinfo는 나중에 모델의 시각적 표현을 제공하는 데 도움이 됩니다.\n그리고 나중에 torchvision v0.13 패키지(2022년 7월 현재 사용 가능)를 사용할 것이므로 최신 버전이 있는지 확인하겠습니다.\n\n참고: Google Colab을 사용 중이고 아직 GPU를 켜지 않았다면 지금 런타임 -&gt; 런타임 유형 변경 -&gt; 하드웨어 가속기 -&gt; GPU를 통해 켤 시간입니다.\n\n\n# 업데이트된 API로 이 노트북을 실행하려면 torch 1.12+ 및 torchvision 0.13+가 필요합니다.\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision 버전이 요구 사항을 충족하지 않음, nightly 버전을 설치합니다.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\n\n참고: Google Colab을 사용 중이고 위 셀이 다양한 소프트웨어 패키지를 설치하기 시작한다면, 위 셀을 실행한 후 런타임을 다시 시작해야 할 수도 있습니다. 다시 시작한 후 셀을 다시 실행하고 올바른 버전의 torch 및 torchvision이 있는지 확인할 수 있습니다.\n\n이제 일반적인 임포트, 장치 독립적(device agnostic) 코드 설정을 계속하고 이번에는 GitHub에서 helper_functions.py 스크립트도 가져오겠습니다.\nhelper_functions.py 스크립트에는 이전 섹션에서 만든 몇 가지 함수가 포함되어 있습니다. * set_seeds(): 무작위 시드 설정(07. PyTorch 실험 추적 섹션 0에서 생성). * download_data(): 링크가 주어지면 데이터 소스 다운로드(07. PyTorch 실험 추적 섹션 1에서 생성). * plot_loss_curves(): 모델의 훈련 결과 검사(04. PyTorch 사용자 정의 데이터셋 섹션 7.8)\n\n참고: helper_functions.py 스크립트의 많은 함수를 going_modular/going_modular/utils.py로 병합하는 것이 더 좋은 아이디어일 수 있습니다. 그것이 여러분이 시도해 볼 수 있는 확장 작업일 것입니다.\n\n\n# 일반적인 임포트 계속\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# torchinfo 가져오기 시도, 실패 시 설치\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] torchinfo를 찾을 수 없음... 설치합니다.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# going_modular 디렉토리 임포트 시도, 실패 시 GitHub에서 다운로드\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # going_modular 스크립트 가져오기\n    print(\"[INFO] going_modular 또는 helper_functions 스크립트를 찾을 수 없음... GitHub에서 다운로드합니다.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # helper_functions.py 스크립트 가져오기\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n\n마지막으로 모델이 GPU에서 실행되도록 장치 독립적 코드를 설정하겠습니다.\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#데이터-가져오기",
    "href": "09_pytorch_model_deployment.html#데이터-가져오기",
    "title": "09 - PyTorch 모델 배포",
    "section": "1. 데이터 가져오기",
    "text": "1. 데이터 가져오기\n우리는 08. PyTorch 논문 복제의 마지막 부분에서 자체 Vision Transformer(ViT) 특성 추출기 모델을 07. PyTorch 실험 추적에서 만든 EfficientNetB2(EffNetB2) 특성 추출기 모델과 비교했습니다.\n그리고 비교 과정에서 약간의 차이가 있음을 발견했습니다.\nEffNetB2 모델은 Food101의 피자, 스테이크, 초밥 데이터 중 20%를 사용하여 훈련된 반면, ViT 모델은 10%를 사용하여 훈련되었습니다.\nFoodVision Mini 문제에 대해 최상의 모델을 배포하는 것이 목표이므로, 우선 20% 피자, 스테이크, 초밥 데이터셋을 다운로드하고 EffNetB2 특성 추출기와 ViT 특성 추출기를 이 데이터셋으로 훈련시킨 후 두 모델을 비교해 보겠습니다.\n이렇게 하면 동일한 데이터셋으로 훈련된 두 모델을 서로 동등하게 비교할 수 있습니다.\n\n참고: 다운로드하는 데이터셋은 전체 Food101 데이터셋(각 1,000개의 이미지가 있는 101개 음식 클래스)의 샘플입니다. 구체적으로 20%는 피자, 스테이크, 초밥 클래스에서 무작위로 선택된 이미지의 20%를 의미합니다. 이 데이터셋이 어떻게 생성되었는지는 extras/04_custom_data_creation.ipynb에서, 더 자세한 내용은 04. PyTorch 사용자 정의 데이터셋 섹션 1에서 확인할 수 있습니다.\n\nhelper_functions.py에서 07. PyTorch 실험 추적 섹션 1에서 만든 download_data() 함수를 사용하여 데이터를 다운로드할 수 있습니다.\n\n# GitHub에서 피자, 스테이크, 초밥 이미지 다운로드\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\ndata_20_percent_path\n\n좋습니다!\n이제 데이터셋이 생겼으므로 훈련 및 테스트 경로를 생성하겠습니다.\n\n# 훈련 및 테스트 이미지 디렉토리 경로 설정\ntrain_dir = data_20_percent_path / \"train\"\ntest_dir = data_20_percent_path / \"test\"",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#foodvision-mini-모델-배포-실험-개요",
    "href": "09_pytorch_model_deployment.html#foodvision-mini-모델-배포-실험-개요",
    "title": "09 - PyTorch 모델 배포",
    "section": "2. FoodVision Mini 모델 배포 실험 개요",
    "text": "2. FoodVision Mini 모델 배포 실험 개요\n이상적인 배포 모델 FoodVision Mini는 성능이 좋고 빨라야 합니다.\n우리는 모델이 가능한 한 실시간에 가깝게 작동하기를 원합니다.\n여기서 실시간이란 ~30FPS(frames per second)를 의미하는데, 이는 사람의 눈이 볼 수 있는 속도 정도이기 때문입니다(이에 대해서는 논란이 있지만, 일단 ~30FPS를 벤치마크로 삼겠습니다).\n그리고 세 가지 클래스(피자, 스테이크, 초밥)를 분류하기 위해 95% 이상의 정확도로 작동하는 모델을 원합니다.\n물론 정확도가 높을수록 좋겠지만, 이는 속도를 희생할 수도 있습니다.\n따라서 우리의 목표는 다음과 같습니다.\n\n성능 (Performance) - 95% 이상의 정확도로 작동하는 모델.\n속도 (Speed) - 약 30FPS(이미지당 추론 시간 0.03초, 지연 시간(latency)이라고도 함)로 이미지를 분류할 수 있는 모델.\n\n\nFoodVision Mini 배포 목표. 우리는 빠르게 예측하고 성능이 좋은 모델을 원합니다(느린 앱은 지루하기 때문입니다).\n우리는 속도에 중점을 둘 것이며, 이는 10FPS에서 95% 이상의 정확도를 보이는 모델보다는 ~30FPS에서 90% 이상의 정확도를 보이는 모델을 더 선호한다는 의미입니다.\n이러한 결과를 달성하기 위해 이전 섹션에서 가장 성능이 좋았던 모델들을 가져오겠습니다.\n\nEffNetB2 특성 추출기 (줄여서 EffNetB2) - 원래 07. PyTorch 실험 추적 섹션 7.5에서 classifier 레이어를 조정한 torchvision.models.efficientnet_b2()를 사용하여 만들었습니다.\nViT-B/16 특성 추출기 (줄여서 ViT) - 원래 08. PyTorch 논문 복제 섹션 10에서 head 레이어를 조정한 torchvision.models.vit_b_16()을 사용하여 만들었습니다.\n\n참고: ViT-B/16은 “Vision Transformer Base, 패치 크기 16”을 의미합니다.\n\n\n\n\n참고: “특성 추출기 모델(feature extractor model)”은 종종 여러분의 문제와 유사한 데이터셋으로 사전 훈련된 모델에서 시작합니다. 사전 훈련된 모델의 기본 레이어는 종종 고정된(사전 훈련된 패턴/가중치가 동일하게 유지됨) 상태로 두는 반면, 상단(또는 분류기/분류 헤드) 레이어 중 일부는 자신의 데이터로 훈련하여 자신의 문제에 맞게 사용자 정의합니다. 특성 추출기 모델의 개념은 06. PyTorch 전이 학습 섹션 3.4에서 다루었습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#effnetb2-특성-추출기-만들기",
    "href": "09_pytorch_model_deployment.html#effnetb2-특성-추출기-만들기",
    "title": "09 - PyTorch 모델 배포",
    "section": "3. EffNetB2 특성 추출기 만들기",
    "text": "3. EffNetB2 특성 추출기 만들기\n우리는 07. PyTorch 실험 추적 섹션 7.5에서 EffNetB2 특성 추출기 모델을 처음 만들었습니다.\n그리고 해당 섹션의 마지막 부분에서 아주 좋은 성능을 보인 것을 확인했습니다.\n이제 여기에서 해당 모델을 다시 만들어 동일한 데이터로 훈련된 ViT 특성 추출기와 결과를 비교해 보겠습니다.\n다음과 같이 할 수 있습니다. 1. weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT를 사용하여 사전 훈련된 가중치를 설정합니다. 여기서 “DEFAULT”는 “현재 사용 가능한 최상의 가중치”를 의미합니다 (또는 weights=\"DEFAULT\"를 사용할 수 있습니다). 2. 가중치에서 transforms() 메서드를 사용하여 사전 훈련된 모델 이미지 트랜스폼을 가져옵니다(사전 훈련된 EffNetB2가 훈련된 것과 동일한 형식으로 이미지를 변환하기 위해 필요합니다). 3. 가중치를 torchvision.models.efficientnet_b2의 인스턴스에 전달하여 사전 훈련된 모델 인스턴스를 생성합니다. 4. 모델의 기본 레이어를 고정합니다. 5. 자신의 데이터에 맞게 분류 헤드를 업데이트합니다.\n\n# 1. 사전 훈련된 EffNetB2 가중치 설정\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n\n# 2. EffNetB2 트랜스폼 가져오기\neffnetb2_transforms = effnetb2_weights.transforms()\n\n# 3. 사전 훈련된 모델 설정\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # weights=\"DEFAULT\"를 사용할 수도 있습니다.\n\n# 4. 모델의 기본 레이어 고정 (처음에는 모든 레이어를 고정함)\nfor param in effnetb2.parameters():\n    param.requires_grad = False\n\n이제 분류 헤드를 변경하기 위해 모델의 classifier 속성을 사용하여 먼저 검사해 보겠습니다.\n\n# EffNetB2 분류 헤드 확인\neffnetb2.classifier\n\n좋습니다! 자신의 문제에 맞게 분류 헤드를 변경하려면 out_features 변수를 우리가 가진 클래스 수와 동일하게 바꿉니다(우리의 경우 out_features=3, 피자, 스테이크, 초밥용).\n\n참고: 출력 레이어/분류 헤드를 변경하는 이 과정은 작업 중인 문제에 따라 달라집니다. 예를 들어, 다른 출력 수나 다른 출력 종류를 원한다면 그에 따라 출력 레이어를 변경해야 합니다.\n\n\n# 5. 분류 헤드 업데이트\neffnetb2.classifier = nn.Sequential(\n    nn.Dropout(p=0.3, inplace=True), # 드롭아웃 레이어 유지\n    nn.Linear(in_features=1408, # in_features 유지 \n              out_features=3)) # 우리 클래스 수에 맞게 out_features 변경\n\n훌륭합니다!\n\n3.1 EffNetB2 특성 추출기를 만드는 함수 만들기\nEffNetB2 특성 추출기가 준비된 것 같습니다. 하지만 여기에 몇 가지 단계가 포함되어 있으므로 나중에 다시 사용할 수 있도록 위의 코드를 함수로 만드는 건 어떨까요?\ncreate_effnetb2_model()이라고 명명하고, 사용자 정의 가능한 클래스 수와 재현성을 위한 무작위 시드 매개변수를 받도록 하겠습니다.\n이상적으로는 EffNetB2 특성 추출기와 관련 트랜스폼을 반환할 것입니다.\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"EfficientNetB2 특성 추출기 모델과 트랜스폼을 생성합니다.\n\n    인자:\n        num_classes (int, optional): 분류 헤드의 클래스 수. \n            기본값은 3.\n        seed (int, optional): 무작위 시드 값. 기본값은 42.\n\n    반환값:\n        model (torch.nn.Module): EffNetB2 특성 추출기 모델. \n        transforms (torchvision.transforms): EffNetB2 이미지 트랜스폼.\n    \"\"\"\n    # 1, 2, 3. 사전 훈련된 EffNetB2 가중치, 트랜스폼 및 모델 생성\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 4. 기본 모델의 모든 레이어 고정\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 5. 재현성을 위해 무작위 시드와 함께 분류 헤드 변경\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n\n와우! 아주 멋진 함수네요. 한번 시도해 보겠습니다.\n\neffnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n                                                      seed=42)\n\n오류가 없네요. 좋습니다. 이제 실제로 확인해 보기 위해 torchinfo.summary()로 요약을 확인해 보겠습니다.\n\nfrom torchinfo import summary\n\n# # EffNetB2 모델 요약 출력 (전체 출력을 보려면 주석 해제) \n# summary(effnetb2, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n\n\n기본 레이어는 고정되었고, 상위 레이어는 학습 가능하며 사용자 정의되었습니다!\n\n\n3.2 EffNetB2를 위한 DataLoader 생성\nEffNetB2 특성 추출기가 준비되었으니 이제 DataLoader를 생성할 시간입니다.\n05. PyTorch 모듈화 섹션 2에서 만든 data_setup.create_dataloaders() 함수를 사용하여 이를 수행할 수 있습니다.\n배치 크기를 32로 사용하고 effnetb2_transforms를 사용하여 이미지를 변환함으로써 이미지가 effnetb2 모델이 훈련된 것과 동일한 형식이 되도록 하겠습니다.\n\n# DataLoader 설정\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                 test_dir=test_dir,\n                                                                                                 transform=effnetb2_transforms,\n                                                                                                 batch_size=32)\n\n\n\n3.3 EffNetB2 특성 추출기 훈련\n모델도 준비되었고 DataLoader도 준비되었으니 이제 훈련을 시작해 봅시다!\n07. PyTorch 실험 추적 섹션 7.6에서와 마찬가지로 좋은 결과를 얻기 위해 10 에포크면 충분할 것입니다.\n옵티마이저(학습률 1e-3인 torch.optim.Adam() 사용)와 손실 함수(다중 클래스 분류를 위한 torch.nn.CrossEntropyLoss() 사용)를 생성한 다음, 이들과 DataLoader를 05. PyTorch 모듈화 섹션 4에서 만든 engine.train() 함수에 전달하여 수행할 수 있습니다.\n\nfrom going_modular.going_modular import engine\n\n# 옵티마이저 설정\noptimizer = torch.optim.Adam(params=effnetb2.parameters(),\n                             lr=1e-3)\n# 손실 함수 설정\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# 재현성을 위해 시드를 설정하고 모델을 훈련시킴\nset_seeds()\neffnetb2_results = engine.train(model=effnetb2,\n                                train_dataloader=train_dataloader_effnetb2,\n                                test_dataloader=test_dataloader_effnetb2,\n                                epochs=10,\n                                optimizer=optimizer,\n                                loss_fn=loss_fn,\n                                device=device)\n\n\n\n3.4 EffNetB2 손실 곡선 검사\n좋네요!\n\nPyTorch 실험 추적에서 보았듯이 EffNetB2 특성 추출기 모델은 우리 데이터에서 꽤 잘 작동합니다.\n\n결과를 손실 곡선으로 변환하여 더 자세히 살펴보겠습니다.\n\n참고: 손실 곡선은 모델의 성능을 시각화하는 가장 좋은 방법 중 하나입니다. 손실 곡선에 대한 자세한 내용은 04. PyTorch 사용자 정의 데이터셋 섹션 8: 이상적인 손실 곡선은 어떤 모습이어야 할까요?를 참조하세요.\n\n\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)\n\n와!\n손실 곡선이 아주 예쁘게 나왔네요.\n우리 모델이 꽤 잘 수행되고 있으며, 아마도 훈련 시간을 조금 더 늘리고 잠재적으로 데이터 증강(data augmentation)을 추가한다면 더 좋은 결과를 얻을 수도 있을 것입니다(더 긴 훈련으로 인해 발생할 수 있는 오버피팅을 방지하기 위해).\n\n\n3.5 EffNetB2 특성 추출기 저장\n성능이 좋은 훈련된 모델을 얻었으므로, 나중에 임포트하여 사용할 수 있도록 파일로 저장해 보겠습니다.\n모델을 저장하기 위해 05. PyTorch 모듈화 섹션 5에서 만든 utils.save_model() 함수를 사용할 수 있습니다.\ntarget_dir을 \"models\"로, model_name을 \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"로 설정하겠습니다(좀 길긴 하지만 적어도 무슨 내용인지는 알 수 있습니다).\n\nfrom going_modular.going_modular import utils\n\n# 모델 저장\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n\n\n\n3.6 EffNetB2 특성 추출기 크기 확인\nFoodVision Mini를 구동할 모델을 배포하기 위한 기준 중 하나가 속도(~30FPS 이상)이므로 모델의 크기를 확인해 보겠습니다.\n크기를 왜 확인할까요?\n항상 그런 것은 아니지만 모델의 크기는 추론 속도에 영향을 줄 수 있기 때문입니다.\n즉, 모델에 파라미터가 더 많으면 일반적으로 더 많은 연산을 수행하고 각 연산에는 어느 정도의 컴퓨팅 파워가 필요합니다.\n그리고 우리는 모델이 컴퓨팅 파워가 제한된 장치(예: 모바일 장치 또는 웹 브라우저)에서 작동하기를 원하므로 일반적으로 정확도 측면에서 여전히 잘 수행된다면 크기가 작을수록 좋습니다.\n모델의 크기를 바이트 단위로 확인하기 위해 Python의 pathlib.Path.stat(\"path_to_model\").st_size를 사용할 수 있으며, 이를 (1024*1024)로 나누어 (대략) 메가바이트로 변환할 수 있습니다.\n\nfrom pathlib import Path\n\n# 모델 크기를 바이트 단위로 가져온 다음 메가바이트로 변환\npretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # 나눗셈을 통해 바이트를 메가바이트로 변환(대략) \nprint(f\"사전 훈련된 EffNetB2 특성 추출기 모델 크기: {pretrained_effnetb2_model_size} MB\")\n\n\n\n3.7 EffNetB2 특성 추출기 통계 수집\n테스트 손실, 테스트 정확도, 모델 크기와 같은 EffNetB2 특성 추출기 모델에 대한 몇 가지 통계를 얻었습니다. 나중에 나올 ViT 특성 추출기와 비교할 수 있도록 이들을 모두 딕셔너리에 수집해 보겠습니다.\n그리고 재미 삼아 총 파라미터 수도 계산해 보겠습니다.\neffnetb2.parameters()에 있는 요소(또는 패턴/가중치)의 수를 세어 그렇게 할 수 있습니다. torch.numel()(“number of elements”의 약자) 메서드를 사용하여 각 파라미터의 요소 수에 접근할 것입니다.\n\n# EffNetB2의 파라미터 수 계산\neffnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\neffnetb2_total_params\n\n훌륭합니다!\n이제 나중에 비교할 수 있도록 모든 것을 딕셔너리에 넣겠습니다.\n\n# EffNetB2 통계가 포함된 딕셔너리 생성\neffnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n                  \"number_of_parameters\": effnetb2_total_params,\n                  \"model_size (MB)\": pretrained_effnetb2_model_size}\neffnetb2_stats\n\n환상적이네요!\n우리의 EffNetB2 모델이 95% 이상의 정확도로 수행되고 있는 것 같습니다!\n기준 1번: 95% 이상의 정확도로 수행, 완료!",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#vit-특성-추출기-만들기",
    "href": "09_pytorch_model_deployment.html#vit-특성-추출기-만들기",
    "title": "09 - PyTorch 모델 배포",
    "section": "4. ViT 특성 추출기 만들기",
    "text": "4. ViT 특성 추출기 만들기\n이제 FoodVision Mini 모델링 실험을 계속해 볼 시간입니다.\n이번에는 ViT 특성 추출기를 만들어 보겠습니다.\nEffNetB2 특성 추출기와 거의 동일한 방식으로 진행하되, 이번에는 torchvision.models.efficientnet_b2() 대신 torchvision.models.vit_b_16()을 사용하겠습니다.\ncreate_vit_model()이라는 함수를 만드는 것으로 시작하겠습니다. 이 함수는 create_effnetb2_model()과 매우 유사하지만 당연히 EffNetB2 대신 ViT 특성 추출기 모델과 트랜스폼을 반환할 것입니다.\n또 다른 약간의 차이점은 torchvision.models.vit_b_16()의 출력 레이어가 classifier가 아닌 heads라는 이름이라는 점입니다.\n\n# ViT heads 레이어 확인\nvit = torchvision.models.vit_b_16()\nvit.heads\n\n이 사실을 바탕으로 우리는 필요한 퍼즐 조각을 모두 갖추었습니다.\n\ndef create_vit_model(num_classes:int=3, \n                     seed:int=42):\n    \"\"\"ViT-B/16 특성 추출기 모델과 트랜스폼을 생성합니다.\n\n    인자:\n        num_classes (int, optional): 타겟 클래스 수. 기본값은 3.\n        seed (int, optional): 출력 레이어의 무작위 시드 값. 기본값은 42.\n\n    반환값:\n        model (torch.nn.Module): ViT-B/16 특성 추출기 모델. \n        transforms (torchvision.transforms): ViT-B/16 이미지 트랜스폼.\n    \"\"\"\n    # 사전 훈련된 ViT_B_16 가중치, 트랜스폼 및 모델 생성\n    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.vit_b_16(weights=weights)\n\n    # 모델의 모든 레이어 고정\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 요구 사항에 맞게 분류 헤드 변경 (이 부분은 훈련 가능함)\n    torch.manual_seed(seed)\n    model.heads = nn.Sequential(nn.Linear(in_features=768, # 원본 모델과 동일하게 유지\n                                          out_features=num_classes)) # 타겟 클래스 수를 반영하도록 업데이트\n    \n    return model, transforms\n\nViT 특성 추출 모델 생성 함수 준비 완료!\n한번 테스트해 봅시다.\n\n# ViT 모델 및 트랜스폼 생성\nvit, vit_transforms = create_vit_model(num_classes=3,\n                                       seed=42)\n\n오류가 없네요. 보기 좋습니다!\n이제 torchinfo.summary()를 사용하여 ViT 모델의 멋진 요약을 확인해 보겠습니다.\n\nfrom torchinfo import summary\n\n# # ViT 특성 추출기 모델 요약 출력 (전체 출력을 보려면 주석 해제)\n# summary(vit, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n\n\nEffNetB2 특성 추출기 모델과 마찬가지로 ViT 모델의 기본 레이어는 고정되었고 출력 레이어는 우리의 요구 사항에 맞게 사용자 정의되었습니다!\n하지만 큰 차이점이 보이시나요?\nViT 모델은 EffNetB2 모델보다 파라미터가 훨씬 더 많습니다. 아마도 나중에 속도와 성능 측면에서 모델을 비교할 때 이 점이 작용할 것입니다.\n\n4.1 ViT를 위한 DataLoader 생성\nViT 모델이 준비되었으니 이제 DataLoader를 생성해 보겠습니다.\n이미지를 ViT 모델이 훈련된 것과 동일한 형식으로 변환하기 위해 vit_transforms를 사용하는 것을 제외하면 EffNetB2에서 했던 방식과 동일하게 수행하겠습니다.\n\n# ViT DataLoader 설정\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                       test_dir=test_dir,\n                                                                                       transform=vit_transforms,\n                                                                                       batch_size=32)\n\n\n\n4.2 ViT 특성 추출기 훈련\n지금이 무슨 시간인지 아시죠…\n…바로 훈련 시간입니다.\n옵티마이저로 torch.optim.Adam()과 학습률 1e-3을 사용하고 손실 함수로 torch.nn.CrossEntropyLoss()를 사용하는 engine.train() 함수를 사용하여 10 에포크 동안 ViT 특성 추출기 모델을 훈련해 보겠습니다.\n훈련 전에는 set_seeds() 함수를 사용하여 결과를 최대한 재현할 수 있도록 하겠습니다.\n\nfrom going_modular.going_modular import engine\n\n# 옵티마이저 설정\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=1e-3)\n# 손실 함수 설정\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# 재현성을 위해 시드를 설정하고 ViT 모델 훈련\nset_seeds()\nvit_results = engine.train(model=vit,\n                           train_dataloader=train_dataloader_vit,\n                           test_dataloader=test_dataloader_vit,\n                           epochs=10,\n                           optimizer=optimizer,\n                           loss_fn=loss_fn,\n                           device=device)\n\n\n\n4.3 ViT 손실 곡선 검사\n좋습니다. ViT 모델 훈련이 끝났으니 이제 시각화하여 손실 곡선을 확인해 보겠습니다.\n\n참고: 이상적인 손실 곡선이 어떤 모습이어야 하는지는 04. PyTorch 사용자 정의 데이터셋 섹션 8에서 확인할 수 있습니다.\n\n\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(vit_results)\n\n와우!\n손실 곡선이 아주 예쁘네요. EffNetB2 특성 추출기 모델과 마찬가지로, ViT 모델도 훈련 시간을 조금 더 늘리고 잠재적으로 데이터 증강을 추가한다면 더 좋은 결과를 얻을 수 있을 것입니다(오버피팅 방지).\n\n\n4.4 ViT 특성 추출기 저장\n우리의 ViT 모델이 뛰어난 성능을 보이고 있습니다!\n나중에 필요할 때 임포트하여 사용할 수 있도록 파일로 저장해 보겠습니다.\n05. PyTorch 모듈화 섹션 5에서 만든 utils.save_model() 함수를 사용하여 그렇게 할 수 있습니다.\n\n# 모델 저장\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=vit,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n\n\n\n4.5 ViT 특성 추출기 크기 확인\n여러 특성에 걸쳐 EffNetB2 모델과 ViT 모델을 비교하고 싶으므로 모델의 크기를 알아보겠습니다.\n모델의 크기를 바이트 단위로 확인하기 위해 Python의 pathlib.Path.stat(\"path_to_model\").st_size를 사용할 수 있으며, 이를 (1024*1024)로 나누어 (대략) 메가바이트로 변환할 수 있습니다.\n\nfrom pathlib import Path\n\n# 모델 크기를 바이트 단위로 가져온 다음 메가바이트로 변환\npretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # 나눗셈을 통해 바이트를 메가바이트로 변환(대략) \nprint(f\"사전 훈련된 ViT 특성 추출기 모델 크기: {pretrained_vit_model_size} MB\")\n\n흠, ViT 특성 추출기 모델 크기는 EffNetB2 모델 크기와 비교하면 어떨까요?\n잠시 후에 모든 모델 특성을 비교할 때 이를 확인해 보겠습니다.\n\n\n4.6 ViT 특성 추출기 통계 수집\n모든 ViT 특성 추출기 모델 통계를 정리해 보겠습니다.\n위의 요약 출력에서 보았지만, 총 파라미터 수도 계산해 보겠습니다.\n\n# ViT의 파라미터 수 계산\nvit_total_params = sum(torch.numel(param) for param in vit.parameters())\nvit_total_params\n\n와, EffNetB2보다 꽤 많은 것 같네요!\n\n참고: 파라미터(또는 가중치/패턴) 수가 많다는 것은 일반적으로 모델이 학습할 수 있는 용량(capacity)이 더 크다는 것을 의미하지만, 실제로 이 추가 용량을 사용하는지는 별개의 이야기입니다. 이를 고려할 때 EffNetB2 모델은 7,705,221개의 파라미터를 가지고 있는 반면 ViT 모델은 85,800,963개(11.1배 더 많음)를 가지고 있으므로, 더 많은 데이터(학습 기회)가 주어진다면 ViT 모델이 학습 용량이 더 크다고 가정할 수 있습니다. 하지만 이러한 큰 학습 용량은 종종 모델 파일 크기 증가와 더 긴 추론 시간으로 이어집니다.\n\n이제 ViT 모델의 몇 가지 중요한 특성을 포함하는 딕셔너리를 만들어 보겠습니다.\n\n# ViT 통계 딕셔너리 생성\nvit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n             \"test_acc\": vit_results[\"test_acc\"][-1],\n             \"number_of_parameters\": vit_total_params,\n             \"model_size (MB)\": pretrained_vit_model_size}\n\nvit_stats\n\n좋습니다! ViT 모델도 95% 이상의 정확도를 달성하는 것 같습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#훈련된-모델로-예측하고-시간-측정하기",
    "href": "09_pytorch_model_deployment.html#훈련된-모델로-예측하고-시간-측정하기",
    "title": "09 - PyTorch 모델 배포",
    "section": "5. 훈련된 모델로 예측하고 시간 측정하기",
    "text": "5. 훈련된 모델로 예측하고 시간 측정하기\n몇 가지 훈련된 모델을 얻었고 둘 다 꽤 잘 수행되고 있습니다.\n이제 우리가 원하는 작업을 수행하여 모델들을 테스트해 보는 건 어떨까요?\n즉, 모델들이 예측(추론 수행)을 어떻게 하는지 살펴보겠습니다.\n두 모델 모두 테스트 데이터셋에서 95% 이상의 정확도를 보이고 있다는 것을 알고 있지만, 속도는 어느 정도일까요?\n이상적으로 사람들이 음식 사진을 찍고 식별할 수 있도록 모바일 장치에 FoodVision Mini 모델을 배포한다면, 실시간(~초당 30프레임)으로 예측이 이루어지기를 원할 것입니다.\n이것이 우리의 두 번째 기준인 ’빠른 모델’인 이유입니다.\n각 모델이 추론을 수행하는 데 걸리는 시간을 알아보기 위해, 테스트 데이터셋 이미지를 하나씩 반복하며 예측을 수행하는 pred_and_store()라는 함수를 만들어 보겠습니다.\n각 예측 시간을 측정하고 그 결과를 일반적인 예측 형식인 딕셔너리 리스트(리스트의 각 요소는 단일 예측이고 각 단일 예측은 딕셔너리임)로 저장하겠습니다.\n\n참고: 모델이 배포될 때 한 번에 하나의 이미지에 대해서만 예측을 수행할 가능성이 높으므로 배치가 아닌 하나씩 예측 시간을 측정합니다. 즉, 누군가 사진을 찍으면 우리 모델은 그 단일 이미지에 대해 예측합니다.\n\n테스트 세트의 모든 이미지에 대해 예측을 수행하고 싶으므로, 먼저 모든 테스트 이미지 경로 리스트를 가져와 반복할 수 있도록 하겠습니다.\n이를 위해 Python의 pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))를 사용하여 확장자가 .jpg인 타겟 디렉토리의 모든 파일 경로를 찾겠습니다(모든 테스트 이미지).\n\nfrom pathlib import Path\n\n# 모든 테스트 데이터 경로 가져오기\nprint(f\"[INFO] 디렉토리에서 '.jpg'로 끝나는 모든 파일 경로 찾는 중: {test_dir}\")\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\ntest_data_paths[:5]\n\n\n5.1 테스트 데이터셋에 대해 예측을 수행하는 함수 만들기\n이제 테스트 이미지 경로 리스트를 얻었으므로 pred_and_store() 함수 작업을 시작하겠습니다.\n\n경로 리스트, 훈련된 PyTorch 모델, 일련의 트랜스폼(이미지 준비용), 타겟 클래스 이름 리스트 및 타겟 장치를 인자로 받는 함수를 만듭니다.\n예측 딕셔너리를 저장할 빈 리스트를 생성합니다(함수가 각 예측에 대해 하나씩 딕셔너리 리스트를 반환하기를 원합니다).\n타겟 입력 경로를 반복합니다(4~14단계는 루프 내부에서 발생함).\n샘플당 예측 값을 저장하기 위해 루프의 각 반복마다 빈 딕셔너리를 생성합니다.\n샘플 경로와 정답 클래스 이름을 가져옵니다(경로에서 클래스를 추론하여 수행할 수 있음).\nPython의 timeit.default_timer()를 사용하여 예측 타이머를 시작합니다.\nPIL.Image.open(path)를 사용하여 이미지를 엽니다.\n이미지를 타겟 모델과 함께 사용할 수 있도록 변환하고 배치 차원을 추가하며 이미지를 타겟 장치로 보냅니다.\n모델을 타겟 장치로 보내고 eval() 모드를 켜서 추론을 위한 모델을 준비합니다.\ntorch.inference_mode()를 켜고 타겟 변환된 이미지를 모델에 전달하고 torch.softmax()를 사용하여 예측 확률을, torch.argmax()를 사용하여 타겟 레이블을 계산합니다.\n4단계에서 생성된 예측 딕셔너리에 예측 확률과 예측 클래스를 추가합니다. 또한 나중에 검사할 때 NumPy 및 pandas와 같은 비 GPU 라이브러리와 함께 사용할 수 있도록 예측 확률이 CPU에 있는지 확인합니다.\n6단계에서 시작된 예측 타이머를 종료하고 4단계에서 생성된 예측 딕셔너리에 시간을 추가합니다.\n예측된 클래스가 5단계의 정답 클래스와 일치하는지 확인하고 그 결과를 4단계에서 생성된 예측 딕셔너리에 추가합니다.\n업데이트된 예측 딕셔너리를 2단계에서 생성된 빈 예측 리스트에 추가합니다.\n예측 딕셔너리 리스트를 반환합니다.\n\n몇 가지 단계가 있지만, 우리가 처리할 수 없는 것은 아닙니다!\n해봅시다.\n\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom timeit import default_timer as timer \nfrom tqdm.auto import tqdm\nfrom typing import List, Dict\n\n# 1. 샘플, 정답 레이블, 예측, 예측 확률 및 예측 시간이 포함된 딕셔너리 리스트를 반환하는 함수 생성\ndef pred_and_store(paths: List[pathlib.Path], \n                   model: torch.nn.Module,\n                   transform: torchvision.transforms, \n                   class_names: List[str], \n                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:\n    \n    # 2. 예측 딕셔너리를 저장할 빈 리스트 생성\n    pred_list = []\n    \n    # 3. 타겟 경로를 반복\n    for path in tqdm(paths):\n        \n        # 4. 각 샘플에 대한 예측 정보를 저장할 빈 딕셔너리 생성\n        pred_dict = {}\n\n        # 5. 샘플 경로와 정답 클래스 이름 가져오기\n        pred_dict[\"image_path\"] = path\n        class_name = path.parent.stem\n        pred_dict[\"class_name\"] = class_name\n        \n        # 6. 예측 타이머 시작\n        start_time = timer()\n        \n        # 7. 이미지 경로 열기\n        img = Image.open(path)\n        \n        # 8. 이미지 변환, 배치 차원 추가 및 이미지를 타겟 장치에 배치\n        transformed_image = transform(img).unsqueeze(0).to(device) \n        \n        # 9. 모델을 타겟 장치로 보내고 eval() 모드를 켜서 추론 준비\n        model.to(device)\n        model.eval()\n        \n        # 10. 예측 확률, 예측 레이블 및 예측 클래스 가져오기\n        with torch.inference_mode():\n            pred_logit = model(transformed_image) # 타겟 샘플에 대해 추론 수행 \n            pred_prob = torch.softmax(pred_logit, dim=1) # 로짓을 예측 확률로 변환\n            pred_label = torch.argmax(pred_prob, dim=1) # 예측 확률을 예측 레이블로 변환\n            pred_class = class_names[pred_label.cpu()] # 예측 클래스가 CPU에 있도록 하드코딩\n\n            # 11. 딕셔너리의 항목들이 CPU에 있는지 확인 (나중에 예측을 검사하는 데 필요함) \n            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n            pred_dict[\"pred_class\"] = pred_class\n            \n            # 12. 타이머를 종료하고 예측당 시간 계산\n            end_time = timer()\n            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n\n        # 13. 예측이 정답 레이블과 일치하나요?\n        pred_dict[\"correct\"] = class_name == pred_class\n\n        # 14. 딕셔너리를 예측 리스트에 추가\n        pred_list.append(pred_dict)\n    \n    # 15. 예측 딕셔너리 리스트 반환\n    return pred_list\n\n호호!\n정말 멋진 함수네요!\npred_and_store()는 예측을 수행하고 저장하는 데 아주 좋은 유틸리티 함수이므로 나중에 사용할 수 있도록 going_modular.going_modular.predictions.py에 저장할 수도 있습니다. 시도해 보고 싶은 확장 작업일 수도 있으니 아이디어를 위해 05. PyTorch 모듈화를 확인해 보세요.\n\n\n5.2 EffNetB2로 예측 수행 및 시간 측정\n이제 pred_and_store() 함수를 테스트해 볼 시간입니다!\n먼저 EffNetB2 모델을 사용하여 테스트 데이터셋 전체에 대해 예측을 수행해 보겠으며, 두 가지 세부 사항에 주의를 기울이겠습니다.\n\n장치 (Device) - 모델을 배포할 때 항상 \"cuda\"(GPU) 장치에 접근할 수 있는 것은 아니므로 device 매개변수를 \"cpu\"를 사용하도록 하드코딩하겠습니다.\n\nCPU에서 예측을 수행하는 것은 추론 속도를 나타내는 좋은 지표가 될 것입니다. 일반적으로 CPU 장치에서의 예측이 GPU 장치보다 느리기 때문입니다.\n\n트랜스폼 (Transforms) - transform 매개변수를 effnetb2_transforms로 설정하여 이미지가 effnetb2 모델이 훈련된 것과 동일한 방식으로 열리고 변환되도록 하겠습니다.\n\n\n# EffNetB2로 테스트 데이터셋 전체에 대해 예측 수행\neffnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                          model=effnetb2,\n                                          transform=effnetb2_transforms,\n                                          class_names=class_names,\n                                          device=\"cpu\") # CPU에서 예측 수행 \n\n좋습니다! 예측이 아주 빠르게 진행되네요!\n처음 몇 개를 조사하여 어떻게 생겼는지 확인해 보겠습니다.\n\n# 처음 2개 예측 딕셔너리 검사\neffnetb2_test_pred_dicts[:2]\n\n좋아요!\npred_and_store() 함수가 잘 작동한 것 같습니다.\n딕셔너리 리스트 데이터 구조 덕분에 더 자세히 검사할 수 있는 유용한 정보가 많이 생겼습니다.\n이를 위해 딕셔너리 리스트를 pandas DataFrame으로 변환해 보겠습니다.\n\n# test_pred_dicts를 DataFrame으로 변환\nimport pandas as pd\neffnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\neffnetb2_test_pred_df.head()\n\n아주 좋네요!\n해당 예측 딕셔너리가 분석을 수행할 수 있는 정형화된 형식으로 얼마나 쉽게 변환되는지 보세요.\n예를 들어 EffNetB2 모델이 얼마나 많은 예측을 틀렸는지 찾는 것과 같은 분석 말이죠…\n\n# 정답 예측 수 확인\neffnetb2_test_pred_df.correct.value_counts()\n\n총 150개 중 5개 오답이라니, 나쁘지 않네요!\n평균 예측 시간은 어떨까요?\n\n# 예측당 평균 시간 찾기 \neffnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"EffNetB2 예측당 평균 시간: {effnetb2_average_time_per_pred} 초\")\n\n흠, 이 평균 예측 시간이 모델의 실시간 성능 기준(~30FPS 또는 예측당 0.03초)에 얼마나 부합하나요?\n\n참고: 예측 시간은 하드웨어 유형(예: 로컬 Intel i9 vs Google Colab CPU)에 따라 달라집니다. 하드웨어가 더 좋고 빠를수록 일반적으로 예측도 빨라집니다. 예를 들어 Intel i9 칩이 탑재된 제 로컬 딥러닝 PC에서 EffNetB2를 사용한 평균 예측 시간은 약 0.031초(실시간보다 약간 느림)입니다. 그러나 Google Colab(Colab이 어떤 CPU 하드웨어를 사용하는지는 확실하지 않지만 Intel(R) Xeon(R)인 것 같습니다)에서 EffNetB2를 사용한 평균 예측 시간은 약 0.1396초(3~4배 느림)였습니다.\n\nEffNetB2 예측당 평균 시간을 effnetb2_stats 딕셔너리에 추가하겠습니다.\n\n# EffNetB2 평균 예측 시간을 통계 딕셔너리에 추가 \neffnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\neffnetb2_stats\n\n\n\n5.3 ViT로 예측 수행 및 시간 측정\nEffNetB2 모델로 예측을 수행했으니 이제 ViT 모델에 대해서도 동일하게 수행해 보겠습니다.\n이를 위해 위에서 만든 pred_and_store() 함수를 사용할 수 있는데, 이번에는 vit 모델과 vit_transforms를 전달하겠습니다.\n그리고 device=\"cpu\"를 통해 예측을 CPU에 유지하겠습니다(여기서 자연스러운 확장은 CPU와 GPU에서 예측 시간을 테스트해 보는 것입니다).\n\n# 테스트 이미지에 대해 ViT 특성 추출기 모델을 사용하여 예측 딕셔너리 리스트 생성\nvit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                     model=vit,\n                                     transform=vit_transforms,\n                                     class_names=class_names,\n                                     device=\"cpu\")\n\n예측 완료!\n이제 처음 몇 개를 확인해 보겠습니다.\n\n# 테스트 데이터셋에 대한 처음 몇 개의 ViT 예측 확인\nvit_test_pred_dicts[:2]\n\n멋지네요!\n이전과 마찬가지로 ViT 모델의 예측이 딕셔너리 리스트 형식이므로, 더 자세히 검사하기 위해 pandas DataFrame으로 쉽게 변환할 수 있습니다.\n\n# vit_test_pred_dicts를 DataFrame으로 변환\nimport pandas as pd\nvit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\nvit_test_pred_df.head()\n\n우리 ViT 모델이 얼마나 많은 예측을 맞혔나요?\n\n# 정답 예측 수 계산\nvit_test_pred_df.correct.value_counts()\n\n와!\n우리 ViT 모델이 정답 예측 측면에서 EffNetB2 모델보다 약간 더 잘 수행되었으며, 전체 테스트 데이터셋에서 오답 샘플이 2개뿐이었습니다.\n확장 과제로 ViT 모델의 오답 예측을 시각화하고 왜 틀렸을지 이유가 있는지 확인해 볼 수 있습니다.\nViT 모델이 예측당 걸린 시간을 계산해 보는 건 어떨까요?\n\n# ViT 모델의 예측당 평균 시간 계산\nvit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"ViT 예측당 평균 시간: {vit_average_time_per_pred} 초\")\n\n음, EffNetB2 모델의 예측당 평균 시간보다 약간 느려 보이지만 두 번째 기준인 속도 측면에서는 어떤가요?\n일단 이 값을 vit_stats 딕셔너리에 추가하여 EffNetB2 모델의 통계와 비교해 보겠습니다.\n\n참고: 예측당 평균 시간 값은 이를 수행하는 하드웨어에 따라 크게 달라집니다. 예를 들어 ViT 모델의 경우, Intel i9 CPU가 장착된 제 로컬 딥러닝 PC에서의 예측당 평균 시간(CPU 사용 시)은 0.0693~0.0777초였습니다. 반면 Google Colab에서 ViT 모델을 사용한 예측당 평균 시간은 0.6766~0.7113초였습니다.\n\n\n# CPU에서 ViT 모델의 평균 예측 시간 추가\nvit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\nvit_stats",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#모델-결과-예측-시간-및-크기-비교",
    "href": "09_pytorch_model_deployment.html#모델-결과-예측-시간-및-크기-비교",
    "title": "09 - PyTorch 모델 배포",
    "section": "6. 모델 결과, 예측 시간 및 크기 비교",
    "text": "6. 모델 결과, 예측 시간 및 크기 비교\n가장 강력한 두 후보 모델이 훈련되고 평가되었습니다.\n이제 그들을 정면으로 맞붙여서 서로 다른 통계치를 비교해 보겠습니다.\n이를 위해 effnetb2_stats 및 vit_stats 딕셔너리를 pandas DataFrame으로 변환하겠습니다.\n모델 이름을 볼 수 있는 열을 추가하고 테스트 정확도를 소수점이 아닌 백분율로 변환하겠습니다.\n\n# 통계 딕셔너리를 DataFrame으로 변환\ndf = pd.DataFrame([effnetb2_stats, vit_stats])\n\n# 모델 이름 열 추가\ndf[\"model\"] = [\"EffNetB2\", \"ViT\"]\n\n# 정확도를 백분율로 변환\ndf[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n\ndf\n\n훌륭하네요!\n전체적인 테스트 정확도 측면에서 우리 모델들이 꽤 근접한 것 같지만, 다른 필드들에서는 어떤가요?\n이를 알아보는 한 가지 방법은 ViT 모델 통계를 EffNetB2 모델 통계로 나누어 두 모델 사이의 비율을 알아보는 것입니다.\n그렇게 하기 위해 또 다른 DataFrame을 만들어 보겠습니다.\n\n# 다양한 특성에 걸쳐 ViT와 EffNetB2 비교\npd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # ViT 통계를 EffNetB2 통계로 나눔\n             columns=[\"ViT to EffNetB2 ratios\"]).T\n\nViT 모델이 성능 지표(낮을수록 좋은 테스트 손실과 높을수록 좋은 테스트 정확도)에서 EffNetB2 모델보다 우수하지만, 다음과 같은 희생이 따르는 것으로 보입니다. * 11배 이상의 파라미터 수 * 11배 이상의 모델 크기 * 이미지당 2.5배 이상의 예측 시간\n이러한 트레이드오프(tradeoffs)를 감수할 가치가 있을까요?\n아마 무제한의 컴퓨팅 파워가 있다면 그렇겠지만, FoodVision Mini 모델을 더 작은 장치(예: 모바일 폰)에 배포하려는 우리의 유스케이스에서는 성능은 약간 떨어지더라도 예측 속도가 더 빠르고 크기가 훨씬 작은 EffNetB2 모델로 시작할 가능성이 높습니다.\n\n6.1 속도 대 성능 트레이드오프 시각화\nViT 모델이 테스트 손실 및 테스트 정확도와 같은 성능 지표 측면에서 EffNetB2 모델보다 성능이 우수함을 확인했습니다.\n그러나 EffNetB2 모델은 예측을 더 빠르게 수행하며 모델 크기가 훨씬 작습니다.\n\n참고: 성능 또는 추론 시간은 종종 “지연 시간(latency)”이라고도 합니다.\n\n이 사실을 시각화해 보는 건 어떨까요?\nmatplotlib을 사용하여 다음과 같이 플롯을 생성할 수 있습니다. 1. 비교 DataFrame에서 산점도(scatter plot)를 생성하여 EffNetB2와 ViT의 time_per_pred_cpu 및 test_acc 값을 비교합니다. 2. 데이터에 맞게 제목과 레이블을 추가하고 미적 요소를 위해 폰트 크기를 조정합니다. 3. 1단계의 산점도 샘플에 적절한 레이블(모델 이름)로 주석을 추가합니다. 4. 모델 크기(model_size (MB))를 기반으로 범례(legend)를 만듭니다.\n\n# 1. 모델 비교 DataFrame으로부터 플롯 생성\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(data=df, \n                     x=\"time_per_pred_cpu\", \n                     y=\"test_acc\", \n                     c=[\"blue\", \"orange\"], # 어떤 색상을 사용할까요?\n                     s=\"model_size (MB)\") # 모델 크기에 따라 점의 크기 조절\n\n# 2. 제목, 레이블을 추가하고 미적 요소를 위해 폰트 크기 조정\nax.set_title(\"FoodVision Mini 추론 속도 대 성능\", fontsize=18)\nax.set_xlabel(\"이미지당 예측 시간 (초)\", fontsize=14)\nax.set_ylabel(\"테스트 정확도 (%)\", fontsize=14)\nax.tick_params(axis='both', labelsize=12)\nax.grid(True)\n\n# 3. 모델 이름으로 주석 추가\nfor index, row in df.iterrows():\n    ax.annotate(text=row[\"model\"], # 참고: Matplotlib 버전에 따라 \"s=...\" 또는 \"text=...\"를 사용해야 할 수도 있습니다. https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 참조 \n                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n                size=12)\n\n# 4. 모델 크기를 기반으로 범례 생성\nhandles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\nmodel_size_legend = ax.legend(handles, \n                              labels, \n                              loc=\"lower right\", \n                              title=\"모델 크기 (MB)\",\n                              fontsize=12)\n\n# 그림 저장\n!mkdir images/\nplt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n\n# 그림 표시\nplt.show()\n\n와!\n플롯이 속도 대 성능 트레이드오프를 정말 잘 시각화해주네요. 즉, 더 크고 성능이 좋은 딥 모델(우리 ViT 모델처럼)을 사용할 때 일반적으로 추론을 수행하는 데 더 오래 걸립니다(지연 시간이 더 김).\n이 규칙에는 예외가 있으며 대형 모델이 더 빠르게 작동하도록 돕는 새로운 연구 결과가 항상 발표되고 있습니다.\n그리고 단순히 가장 좋은 성능을 내는 모델을 배포하고 싶은 유혹이 생길 수 있지만, 모델이 어디에서 실행될지 고려하는 것도 좋습니다.\n우리의 경우, 모델의 성능 수준(테스트 손실 및 테스트 정확도) 차이가 너무 크지는 않습니다.\n하지만 처음부터 속도에 중점을 두고 싶기 때문에, 더 빠르고 적은 리소스를 차지하는 EffNetB2 배포를 고수할 것입니다.\n\n참고: 예측 시간은 하드웨어 유형(예: Intel i9 vs Google Colab CPU vs GPU)에 따라 달라지므로 모델이 최종적으로 어디에 위치할지 생각하고 테스트하는 것이 중요합니다. “모델이 어디에서 실행될 것인가?” 또는 “모델을 실행하기 위한 가장 이상적인 시나리오는 무엇인가?”와 같은 질문을 던지고 배포 과정에서 답을 얻기 위해 실험을 실행하는 것이 매우 도움이 됩니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#gradio-데모를-만들어-foodvision-mini-활성화하기",
    "href": "09_pytorch_model_deployment.html#gradio-데모를-만들어-foodvision-mini-활성화하기",
    "title": "09 - PyTorch 모델 배포",
    "section": "7. Gradio 데모를 만들어 FoodVision Mini 활성화하기",
    "text": "7. Gradio 데모를 만들어 FoodVision Mini 활성화하기\n우리는 (처음에는) EffNetB2 모델을 배포하기로 결정했습니다(이는 나중에 언제든지 변경될 수 있습니다).\n그럼 어떻게 배포할 수 있을까요?\n머신러닝 모델을 배포하는 방법은 여러 가지가 있으며 각 방법에는 고유한 유스케이스가 있습니다(위에서 논의한 바와 같이).\n우리는 인터넷에 모델을 배포하는 가장 빠르고 확실히 가장 재미있는 방법 중 하나에 집중할 것입니다.\n바로 Gradio를 사용하는 것입니다.\nGradio란 무엇일까요?\n홈페이지에 아주 잘 설명되어 있습니다.\n\nGradio는 친숙한 웹 인터페이스를 통해 머신러닝 모델의 데모를 누구나 어디서나 사용할 수 있도록 만드는 가장 빠른 방법입니다!\n\n모델의 데모를 왜 만드나요?\n테스트 세트의 지표는 보기 좋지만, 실제로 세상에서 모델을 사용해 보기 전까지는 모델의 성능을 제대로 알 수 없기 때문입니다.\n그럼 배포를 시작해 봅시다!\nGradio를 일반적인 별칭인 gr로 임포트하는 것부터 시작하고, 설치되어 있지 않다면 설치하겠습니다.\n\n# Gradio 임포트/설치 \ntry:\n    import gradio as gr\nexcept: \n    !pip -q install gradio\n    import gradio as gr\n    \nprint(f\"Gradio 버전: {gr.__version__}\")\n\nGradio 준비 완료!\nFoodVision Mini를 데모 애플리케이션으로 만들어 봅시다.\n\n7.1 Gradio 개요\nGradio의 전반적인 전제는 우리가 과정 전반에 걸쳐 반복해 온 내용과 매우 유사합니다.\n우리의 입력(inputs)과 출력(outputs)은 무엇인가요?\n그리고 어떻게 그곳에 도달해야 할까요?\n바로 우리 머신러닝 모델이 하는 일입니다.\n입력 -&gt; 머신러닝 모델 -&gt; 출력\n우리의 FoodVision Mini의 경우, 입력은 음식 이미지이고, 머신러닝 모델은 EffNetB2이며, 출력은 음식 클래스(피자, 스테이크 또는 초밥)입니다.\n음식 이미지 -&gt; EffNetB2 -&gt; 출력\n입력과 출력의 개념은 거의 모든 종류의 머신러닝 문제에 적용될 수 있습니다.\n입력과 출력은 다음과 같은 조합일 수 있습니다. * 이미지 * 텍스트 * 비디오 * 정형 데이터 (Tabular data) * 오디오 * 숫자 * 등등\n그리고 구축하는 머신러닝 모델은 입력과 출력에 따라 달라질 것입니다.\nGradio는 입력에서 출력까지의 인터페이스(gradio.Interface())를 생성하여 이 패러다임을 모방합니다.\ngradio.Interface(fn, inputs, outputs)\n여기서 fn은 입력을 출력으로 매핑하는 Python 함수입니다.\n\nGradio는 입력 -&gt; 모델/함수 -&gt; 출력 워크플로우를 쉽게 만들 수 있는 매우 유용한 Interface 클래스를 제공하며, 여기서 입력과 출력은 원하는 거의 모든 것이 될 수 있습니다. 예를 들어, 트윗(텍스트)을 입력하여 머신러닝에 관한 것인지 확인하거나 이미지를 생성하기 위해 텍스트 프롬프트를 입력할 수 있습니다.\n\n참고: Gradio에는 이미지에서 텍스트, 숫자, 오디오, 비디오 등에 이르기까지 “구성 요소(Components)”라고 알려진 방대한 수의 가능한 입력 및 출력 옵션이 있습니다. Gradio 구성 요소 문서에서 이들을 모두 확인할 수 있습니다.\n\n\n\n7.2 입력과 출력을 매핑하는 함수 만들기\nGradio로 FoodVision Mini 데모를 만들려면 입력을 출력으로 매핑하는 함수가 필요합니다.\n이전에 지정된 모델을 사용하여 타겟 파일 리스트에 대해 예측을 수행하고 이를 딕셔너리 리스트에 저장하는 pred_and_store() 함수를 만들었습니다.\n유사한 함수를 만들되 이번에는 EffNetB2 모델을 사용하여 단일 이미지에 대해 예측을 수행하는 데 중점을 두는 건 어떨까요?\n더 구체적으로, 이미지를 입력으로 받아 전처리(변환)하고, EffNetB2를 사용하여 예측을 수행한 다음 예측(간단히 pred 또는 pred label)과 예측 확률(pred prob)을 반환하는 함수를 원합니다.\n그리고 하는 김에 이에 걸린 시간도 반환해 보겠습니다.\n입력: 이미지 -&gt; 변환 -&gt; EffNetB2로 예측 -&gt; 출력: 예측 결과, 예측 확률, 소요 시간\n이것이 Gradio 인터페이스의 fn 매개변수가 될 것입니다.\n먼저, EffNetB2 모델이 CPU에 있는지 확인하겠습니다(CPU 전용 예측을 고수하고 있으므로, 단 GPU에 접근할 수 있다면 이를 변경할 수도 있습니다).\n\n# EffNetB2를 CPU에 배치\neffnetb2.to(\"cpu\") \n\n# 장치 확인\nnext(iter(effnetb2.parameters())).device\n\n이제 위의 워크플로우를 재현하기 위해 predict()라는 함수를 만들어 보겠습니다.\n\nfrom typing import Tuple, Dict\n\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"img에 대해 변환 및 예측을 수행하고 예측 결과와 소요 시간을 반환합니다.\n    \"\"\"\n    # 타이머 시작\n    start_time = timer()\n    \n    # 타겟 이미지 변환 및 배치 차원 추가\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # 모델을 평가 모드로 설정하고 추론 모드 활성화\n    effnetb2.eval()\n    with torch.inference_mode():\n        # 변환된 이미지를 모델에 통과시키고 예측 로짓을 예측 확률로 변환\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # 각 예측 클래스에 대한 예측 레이블 및 예측 확률 딕셔너리 생성 (이는 Gradio의 출력 매개변수에 필요한 형식입니다)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # 예측 시간 계산\n    pred_time = round(timer() - start_time, 5)\n    \n    # 예측 딕셔너리와 예측 시간 반환 \n    return pred_labels_and_probs, pred_time\n\n좋습니다!\n이제 테스트 데이터셋의 임의 이미지에 대해 예측을 수행하여 함수가 작동하는지 확인해 보겠습니다.\n먼저 테스트 디렉토리의 모든 이미지 경로 리스트를 가져와서 그중 하나를 무작위로 선택하겠습니다.\n그런 다음 무작위로 선택된 이미지를 PIL.Image.open()으로 열겠습니다.\n마지막으로 이미지를 predict() 함수에 전달하겠습니다.\n\nimport random\nfrom PIL import Image\n\n# 모든 테스트 이미지 파일 경로 리스트 가져오기\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n\n# 테스트 이미지 경로 무작위 선택\nrandom_image_path = random.sample(test_data_paths, k=1)[0]\n\n# 타겟 이미지 열기\nimage = Image.open(random_image_path)\nprint(f\"[INFO] 경로에 있는 이미지로 예측 중: {random_image_path}\\n\")\n\n# 타겟 이미지에 대해 예측을 수행하고 출력 결과 프린트\npred_dict, pred_time = predict(img=image)\nprint(f\"예측 레이블 및 확률 딕셔너리: \\n{pred_dict}\")\nprint(f\"예측 시간: {pred_time} 초\")\n\n좋네요!\n위의 셀을 몇 번 실행하면 EffNetB2 모델로부터 각 레이블에 대한 서로 다른 예측 확률과 예측당 소요 시간을 확인할 수 있습니다.\n\n\n7.3 예시 이미지 리스트 만들기\n우리의 predict() 함수를 사용하면 입력 -&gt; 변환 -&gt; 머신러닝 모델 -&gt; 출력 과정을 거칠 수 있습니다.\n이것이 바로 Gradio 데모에 필요한 것입니다.\n하지만 데모를 만들기 전에 한 가지 더 만들 것이 있습니다. 바로 예시 리스트입니다.\nGradio의 Interface 클래스는 선택적 매개변수로 examples 리스트를 받습니다(gradio.Interface(examples=List[Any])).\n그리고 examples 매개변수의 형식은 리스트의 리스트입니다.\n따라서 테스트 이미지에 대한 무작위 파일 경로를 포함하는 리스트의 리스트를 만들어 보겠습니다.\n세 개의 예시면 충분할 것입니다.\n\n# Gradio 데모를 위한 예시 입력 리스트 생성\nexample_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\nexample_list\n\n완벽합니다!\n우리의 Gradio 데모는 이들을 데모의 예시 입력으로 보여줄 것이며, 사람들은 자신의 데이터를 업로드하지 않고도 데모가 무엇을 하는지 시도해 보고 확인할 수 있습니다.\n\n\n7.4 Gradio 인터페이스 구축\n이제 모든 것을 하나로 합쳐 FoodVision Mini 데모를 세상에 선보일 시간입니다!\n워크플로우를 재현하기 위해 Gradio 인터페이스를 만들어 보겠습니다.\n입력: 이미지 -&gt; 변환 -&gt; EffNetB2로 예측 -&gt; 출력: 예측 결과, 예측 확률, 소요 시간\n다음 매개변수와 함께 gradio.Interface() 클래스를 사용하여 이를 수행할 수 있습니다. * fn: 입력을 출력으로 매핑하는 Python 함수입니다. 우리의 경우에는 predict() 함수를 사용합니다. * inputs: 인터페이스에 대한 입력입니다. gradio.Image() 또는 \"image\"를 사용하는 이미지와 같습니다. * outputs: 입력이 fn을 통과한 후의 인터페이스 출력입니다. 모델의 예측 레이블을 위한 gradio.Label() 또는 모델의 예측 시간을 위한 숫자 gradio.Number()와 같습니다. * 참고: Gradio에는 “구성 요소(Components)”로 알려진 많은 내장 입력 및 출력 옵션이 제공됩니다. * examples: 데모를 위해 보여줄 예시 리스트입니다. * title: 데모의 문자열 제목입니다. * description: 데모의 문자열 설명입니다. * article: 데모 하단의 참조 노트입니다.\ngr.Interface()의 데모 인스턴스를 생성한 후에는 gradio.Interface().launch() 또는 demo.launch() 명령을 사용하여 실행할 수 있습니다.\n간단하죠!\n\nimport gradio as gr\n\n# 제목, 설명 및 기사 문자열 생성\ntitle = \"FoodVision Mini 🍕🥩🍣\"\ndescription = \"음식 이미지를 피자, 스테이크, 초밥으로 분류하는 EfficientNetB2 특성 추출기 컴퓨터 비전 모델입니다.\"\narticle = \"[09. PyTorch 모델 배포](https://www.learnpytorch.io/09_pytorch_model_deployment/)에서 생성되었습니다.\"\n\n# Gradio 데모 생성\ndemo = gr.Interface(fn=predict, # 입력을 출력으로 매핑하는 함수\n                    inputs=gr.Image(type=\"pil\"), # 입력은 무엇인가요?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # 출력은 무엇인가요?\n                             gr.Number(label=\"Prediction time (s)\")], # fn에는 두 개의 출력이 있으므로 두 개의 출력이 있습니다.\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# 데모 실행!\ndemo.launch(debug=False, # 로컬에서 오류를 출력할까요?\n            share=True) # 공개적으로 공유 가능한 URL을 생성할까요?\n\n\nGoogle Colab 및 브라우저에서 실행 중인 FoodVision Mini Gradio 데모(Google Colab에서 실행할 때의 링크는 72시간 동안만 유지됩니다). Hugging Face Spaces에서 영구 라이브 데모를 볼 수 있습니다.\n와아아!!! 정말 대단한 데모네요!!!\nFoodVision Mini가 누구나 사용하고 시도해 볼 수 있는 인터페이스를 통해 공식적으로 세상에 나왔습니다.\nlaunch() 메서드에서 share=True 매개변수를 설정하면, Gradio는 72시간 동안 유효한 https://123XYZ.gradio.app(이 링크는 예시일 뿐이며 만료되었을 가능성이 높습니다)와 같은 공유 가능한 링크를 제공합니다.\n이 링크는 실행한 Gradio 인터페이스에 대한 프록시를 제공합니다.\n더 영구적인 호스팅을 위해서는 Gradio 앱을 Hugging Face Spaces나 Python 코드가 실행되는 곳이면 어디든 업로드할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#foodvision-mini-gradio-데모를-배포-가능한-앱으로-변환하기",
    "href": "09_pytorch_model_deployment.html#foodvision-mini-gradio-데모를-배포-가능한-앱으로-변환하기",
    "title": "09 - PyTorch 모델 배포",
    "section": "8. FoodVision Mini Gradio 데모를 배포 가능한 앱으로 변환하기",
    "text": "8. FoodVision Mini Gradio 데모를 배포 가능한 앱으로 변환하기\nGradio 데모를 통해 FoodVision Mini 모델이 살아 움직이는 것을 보았습니다.\n하지만 이 데모를 친구들과 공유하고 싶다면 어떻게 해야 할까요?\n제공된 Gradio 링크를 사용할 수 있지만, 공유 링크는 72시간 동안만 유지됩니다.\nFoodVision Mini 데모를 더 영구적으로 만들기 위해 앱으로 패키징하여 Hugging Face Spaces에 업로드할 수 있습니다.\n\n8.1 Hugging Face Spaces란 무엇인가요?\nHugging Face Spaces는 머신러닝 앱을 호스팅하고 공유할 수 있는 리소스입니다.\n데모를 만드는 것은 여러분이 한 일을 보여주고 테스트하는 가장 좋은 방법 중 하나입니다.\n그리고 Spaces를 사용하면 바로 그 일을 할 수 있습니다.\nHugging Face를 머신러닝계의 GitHub라고 생각하면 됩니다.\n좋은 GitHub 포트폴리오가 여러분의 코딩 능력을 보여준다면, 좋은 Hugging Face 포트폴리오는 여러분의 머신러닝 능력을 보여줄 수 있습니다.\n\n참고: Gradio 앱을 Google Cloud, AWS(Amazon Web Services) 또는 다른 클라우드 공급업체와 같은 다른 곳에 업로드하고 호스팅할 수도 있지만, 사용 편의성과 머신러닝 커뮤니티의 광범위한 채택으로 인해 Hugging Face Spaces를 사용할 것입니다.\n\n\n\n8.2 배포된 Gradio 앱 구조\n데모 Gradio 앱을 업로드하려면 데모와 관련된 모든 내용을 단일 디렉토리에 넣어야 합니다.\n예를 들어, 데모는 다음과 같은 파일 구조를 가진 demos/foodvision_mini/ 경로에 위치할 수 있습니다.\ndemos/\n└── foodvision_mini/\n    ├── 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n    ├── app.py\n    ├── examples/\n    │   ├── example_1.jpg\n    │   ├── example_2.jpg\n    │   └── example_3.jpg\n    ├── model.py\n    └── requirements.txt\n여기서: * 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth는 훈련된 PyTorch 모델 파일입니다. * app.py에는 Gradio 앱이 포함되어 있습니다(앱을 실행한 코드와 유사함). * 참고: app.py는 Hugging Face Spaces에서 사용하는 기본 파일 이름이며, 앱을 배포하면 Spaces는 기본적으로 실행할 app.py라는 파일을 찾습니다. 이는 설정에서 변경할 수 있습니다. * examples/에는 Gradio 앱과 함께 사용할 예시 이미지가 포함되어 있습니다. * model.py에는 모델 정의와 모델과 관련된 트랜스폼이 포함되어 있습니다. * requirements.txt에는 torch, torchvision, gradio와 같이 앱을 실행하기 위한 의존성(dependencies)이 포함되어 있습니다.\n왜 이런 방식으로 할까요?\n우리가 시작할 수 있는 가장 단순한 레이아웃 중 하나이기 때문입니다.\n우리의 중점은 실험, 실험, 실험! 입니다.\n작은 실험을 더 빨리 실행할수록 큰 실험도 더 잘 수행될 것입니다.\n우리는 위의 구조를 재현하는 작업을 수행할 것이지만, Hugging Face Spaces에서 실행 중인 라이브 데모 앱과 파일 구조를 확인할 수 있습니다. * FoodVision Mini 🍕🥩🍣 라이브 Gradio 데모. * Hugging Face Spaces의 FoodVision Mini 파일 구조.\n\n\n8.3 FoodVision Mini 앱 파일을 저장할 demos 폴더 생성\n시작하기 위해 먼저 모든 FoodVision Mini 앱 파일을 저장할 demos/ 디렉토리를 생성하겠습니다.\nPython의 pathlib.Path(\"path_to_dir\")를 사용하여 디렉토리 경로를 설정하고 pathlib.Path(\"path_to_dir\").mkdir()을 사용하여 생성할 수 있습니다.\n\nimport shutil\nfrom pathlib import Path\n\n# FoodVision mini 데모 경로 생성\nfoodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n\n# 이미 존재하는 파일이 있으면 제거하고 새 디렉토리 생성\nif foodvision_mini_demo_path.exists():\n    shutil.rmtree(foodvision_mini_demo_path)\n# 파일이 존재하지 않으면 그래도 생성\nfoodvision_mini_demo_path.mkdir(parents=True, \n                                exist_ok=True)\n    \n# 폴더에 무엇이 있는지 확인\n!ls demos/foodvision_mini/\n\n\n\n8.4 FoodVision Mini 데모와 함께 사용할 예시 이미지 폴더 생성\n이제 FoodVision Mini 데모 파일을 저장할 디렉토리가 생겼으니 여기에 예시를 추가해 보겠습니다.\n테스트 데이터셋에서 가져온 세 개의 예시 이미지면 충분할 것입니다.\n이를 위해 다음을 수행합니다. 1. demos/foodvision_mini 디렉토리 내에 examples/ 디렉토리를 생성합니다. 2. 테스트 데이터셋에서 무작위로 세 개의 이미지를 선택하고 그 파일 경로를 리스트에 수집합니다. 3. 테스트 데이터셋의 무작위 이미지 세 개를 demos/foodvision_mini/examples/ 디렉토리로 복사합니다.\n\nimport shutil\nfrom pathlib import Path\n\n# 1. examples 디렉토리 생성\nfoodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\nfoodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n\n# 2. 테스트 데이터셋 이미지 경로 세 개 무작위 수집\nfoodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n\n# 3. 무작위 이미지 세 개를 examples 디렉토리로 복사\nfor example in foodvision_mini_examples:\n    destination = foodvision_mini_examples_path / example.name\n    print(f\"[INFO] {example}을 {destination}으로 복사 중\")\n    shutil.copy2(src=example, dst=destination)\n\n이제 예시가 있는지 확인하기 위해 os.listdir()을 사용하여 demos/foodvision_mini/examples/ 디렉토리의 내용을 나열한 다음, 파일 경로를 리스트의 리스트 형식으로 지정하겠습니다(Gradio의 gradio.Interface() example 매개변수와 호환되도록).\n\nimport os\n\n# 예시 파일 경로를 리스트의 리스트로 가져오기\nexample_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\nexample_list\n\n\n\n8.5 훈련된 EffNetB2 모델을 FoodVision Mini 데모 디렉토리로 이동\n이전에 FoodVision Mini EffNetB2 특성 추출기 모델을 models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth에 저장했습니다.\n저장된 모델 파일을 중복해서 만들지 않고, 모델을 demos/foodvision_mini 디렉토리로 이동하겠습니다.\nPython의 shutil.move() 메서드를 사용하여 src(대상 파일의 소스 경로) 및 dst(이동할 대상 파일의 목적지 경로) 매개변수를 전달하여 그렇게 할 수 있습니다.\n\nimport shutil\n\n# 타겟 모델의 소스 경로 생성\neffnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n\n# 타겟 모델의 목적지 경로 생성 \neffnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n\n# 파일 이동 시도\ntry:\n    print(f\"[INFO] {effnetb2_foodvision_mini_model_path}을 {effnetb2_foodvision_mini_model_destination}으로 이동 시도 중\")\n    \n    # 모델 이동\n    shutil.move(src=effnetb2_foodvision_mini_model_path, \n                dst=effnetb2_foodvision_mini_model_destination)\n    \n    print(f\"[INFO] 모델 이동 완료.\")\n\n# 모델이 이미 이동된 경우 이미 존재하는지 확인\nexcept:\n    print(f\"[INFO] {effnetb2_foodvision_mini_model_path}에서 모델을 찾을 수 없음, 이미 이동되었을까요?\")\n    print(f\"[INFO] {effnetb2_foodvision_mini_model_destination}에 모델 존재 여부: {effnetb2_foodvision_mini_model_destination.exists()}\")\n\n\n\n8.6 EffNetB2 모델을 Python 스크립트로 변환 (model.py)\n현재 모델의 state_dict는 demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth에 저장되어 있습니다.\n이를 불러오기 위해 torch.load()와 함께 model.load_state_dict()를 사용할 수 있습니다.\n\n참고: PyTorch에서 모델(또는 모델의 state_dict)을 저장하고 불러오는 방법을 다시 보려면 01. PyTorch 워크플로우 기초 섹션 5: PyTorch 모델 저장 및 불러오기를 참조하거나 PyTorch에서 state_dict란 무엇인가요?에 대한 PyTorch 레시피를 참조하세요.\n\n하지만 이렇게 하기 전에 먼저 model을 인스턴스화할 방법이 필요합니다.\n모듈식 방식으로 이를 수행하기 위해 섹션 3.1: EffNetB2 특성 추출기를 만드는 함수 만들기에서 만든 create_effnetb2_model() 함수가 포함된 model.py라는 스크립트를 만들겠습니다.\n이렇게 하면 또 다른 스크립트(아래의 app.py 참조)에서 함수를 임포트한 다음 이를 사용하여 EffNetB2 model 인스턴스를 생성하고 적절한 트랜스폼을 가져올 수 있습니다.\n05. PyTorch 모듈화에서와 마찬가지로, %%writefile path/to/file 매직 명령어를 사용하여 코드 셀을 파일로 변환하겠습니다.\n\n%%writefile demos/foodvision_mini/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"EfficientNetB2 특성 추출기 모델과 트랜스폼을 생성합니다.\n\n    인자:\n        num_classes (int, optional): 분류 헤드의 클래스 수. \n            기본값은 3.\n        seed (int, optional): 무작위 시드 값. 기본값은 42.\n\n    반환값:\n        model (torch.nn.Module): EffNetB2 특성 추출기 모델. \n        transforms (torchvision.transforms): EffNetB2 이미지 트랜스폼.\n    \"\"\"\n    # 사전 훈련된 EffNetB2 가중치, 트랜스폼 및 모델 생성\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 기본 모델의 모든 레이어 고정\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 재현성을 위해 무작위 시드와 함께 분류 헤드 변경\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n\n\n\n8.7 FoodVision Mini Gradio 앱을 Python 스크립트로 변환 (app.py)\n이제 model.py 스크립트와 저장된 모델 state_dict 경로를 얻었으니 로드할 수 있습니다.\napp.py를 구성할 시간입니다.\nHuggingFace Space를 생성할 때 기본적으로 실행 및 호스팅할 app.py라는 파일을 찾기 때문에 이름을 app.py라고 부릅니다(설정에서 변경 가능함).\n우리의 app.py 스크립트는 Gradio 데모를 만들기 위해 모든 퍼즐 조각을 하나로 모을 것이며, 네 가지 주요 부분으로 구성됩니다.\n\n임포트 및 클래스 이름 설정 - 여기서는 model.py의 create_effnetb2_model() 함수를 포함하여 데모를 위한 다양한 의존성을 임포트하고 FoodVision Mini 앱을 위한 서로 다른 클래스 이름을 설정합니다.\n모델 및 트랜스폼 준비 - 여기서는 EffNetB2 모델 인스턴스와 이에 수반되는 트랜스폼을 생성한 다음 저장된 모델 가중치/state_dict를 불러옵니다. 모델을 불러올 때 torch.load()에서 map_location=torch.device(\"cpu\")를 설정하여 모델이 훈련된 장치에 상관없이 CPU에 로드되도록 합니다(배포할 때 GPU가 반드시 있는 것은 아니며, 모델이 GPU에서 훈련되었는데 명시적인 언급 없이 CPU에 배포하려고 하면 오류가 발생하기 때문입니다).\n예측 함수 - Gradio의 gradio.Interface()는 입력을 출력으로 매핑하는 fn 매개변수를 받습니다. 우리의 predict() 함수는 위 섹션 7.2: 입력과 출력을 매핑하는 함수 만들기에서 정의한 것과 동일하며, 이미지를 받아 로드된 트랜스폼을 사용하여 전처리를 수행한 후 로드된 모델을 사용하여 예측을 수행합니다.\n\n참고: examples 매개변수를 통해 예시 리스트를 즉석에서 생성해야 합니다. examples/ 디렉토리 내의 파일 리스트를 [[\"examples/\" + example] for example in os.listdir(\"examples\")]와 같이 생성할 수 있습니다.\n\nGradio 앱 - 데모의 주요 로직이 위치하는 곳으로, 입력, predict() 함수 및 출력을 하나로 모으기 위해 demo라는 gradio.Interface() 인스턴스를 생성합니다. 그리고 스크립트 마지막에 demo.launch()를 호출하여 FoodVision Mini 데모를 실행하며 마무리합니다!”\n\n\n%%writefile demos/foodvision_mini/app.py\n### 1. 임포트 및 클래스 이름 설정 ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# 클래스 이름 설정\nclass_names = [\"pizza\", \"steak\", \"sushi\"]\n\n### 2. 모델 및 트랜스폼 준비 ###\n\n# EffNetB2 모델 생성\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=3, # len(class_names)도 작동함\n)\n\n# 저장된 가중치 불러오기\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # CPU로 불러오기\n    )\n)\n\n### 3. 예측 함수 ###\n\n# 예측 함수 생성\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"img에 대해 변환 및 예측을 수행하고 예측 결과와 소요 시간을 반환합니다.\n    \"\"\"\n    # 타이머 시작\n    start_time = timer()\n    \n    # 타겟 이미지 변환 및 배치 차원 추가\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # 모델을 평가 모드로 설정하고 추론 모드 활성화\n    effnetb2.eval()\n    with torch.inference_mode():\n        # 변환된 이미지를 모델에 통과시키고 예측 로짓을 예측 확률로 변환\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # 각 예측 클래스에 대한 예측 레이블 및 예측 확률 딕셔너리 생성 (이는 Gradio의 출력 매개변수에 필요한 형식입니다)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # 예측 시간 계산\n    pred_time = round(timer() - start_time, 5)\n    \n    # 예측 딕셔너리와 예측 시간 반환 \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio 앱 ###\n\n# 제목, 설명 및 기사 문자열 생성\ntitle = \"FoodVision Mini 🍕🥩🍣\"\ndescription = \"음식 이미지를 피자, 스테이크, 초밥으로 분류하는 EfficientNetB2 특성 추출기 컴퓨터 비전 모델입니다.\"\narticle = \"[09. PyTorch 모델 배포](https://www.learnpytorch.io/09_pytorch_model_deployment/)에서 생성되었습니다.\"\n\n# \"examples/\" 디렉토리에서 예시 리스트 생성\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Gradio 데모 생성\ndemo = gr.Interface(fn=predict, # 입력을 출력으로 매핑하는 함수\n                    inputs=gr.Image(type=\"pil\"), # 입력은 무엇인가요?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # 출력은 무엇인가요?\n                             gr.Number(label=\"Prediction time (s)\")], # fn에는 두 개의 출력이 있으므로 두 개의 출력이 있습니다.\n                    # \"examples/\" 디렉토리에서 예시 리스트 생성\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# 데모 실행!\ndemo.launch()\n\n\n\n8.8 FoodVision Mini를 위한 요구 사항 파일 생성 (requirements.txt)\nFoodVision Mini 앱을 위해 마지막으로 생성해야 할 파일은 requirements.txt 파일입니다.\n이것은 데모를 위해 필요한 모든 의존성이 포함된 텍스트 파일입니다.\nHugging Face Spaces에 데모 앱을 배포하면, 이 파일을 검색하여 정의한 의존성을 설치하여 앱이 실행될 수 있도록 할 것입니다.\n다행히도 세 개뿐입니다!\n\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n\n“==1.12.0”은 설치할 버전 번호를 명시합니다.\n버전 번호를 정의하는 것이 100% 필수적인 것은 아니지만, 나중에 버전 업데이트 시 앱이 계속 실행되도록 하기 위해 지금 정의하겠습니다(만약 오류를 발견하면 과정 GitHub Issues에 자유롭게 게시해 주세요).\n\n%%writefile demos/foodvision_mini/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n\n좋네요!\nFoodVision Mini 데모를 배포하는 데 필요한 모든 파일을 공식적으로 얻었습니다!",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#foodvision-mini-앱을-huggingface-spaces에-배포하기",
    "href": "09_pytorch_model_deployment.html#foodvision-mini-앱을-huggingface-spaces에-배포하기",
    "title": "09 - PyTorch 모델 배포",
    "section": "9. FoodVision Mini 앱을 HuggingFace Spaces에 배포하기",
    "text": "9. FoodVision Mini 앱을 HuggingFace Spaces에 배포하기\nFoodVision Mini 데모가 포함된 파일이 생겼는데, 이제 Hugging Face Spaces에서 어떻게 실행할까요?\nHugging Face Space(git 저장소와 유사한 Hugging Face Repository라고도 함)에 업로드하는 두 가지 주요 옵션이 있습니다. 1. Hugging Face 웹 인터페이스를 통해 업로드 (가장 쉬움). 2. 명령줄 또는 터미널을 통해 업로드. * 보너스: huggingface_hub 라이브러리를 사용하여 Hugging Face와 상호작용할 수도 있으며, 이는 위의 두 옵션에 대한 좋은 확장이 될 것입니다.\n두 옵션에 대한 문서를 자유롭게 읽어보시되, 우리는 옵션 2로 진행하겠습니다.\n\n참고: Hugging Face에서 무엇이든 호스팅하려면 무료 Hugging Face 계정에 가입해야 합니다.\n\n\n9.1 FoodVision Mini 앱 파일 다운로드\ndemos/foodvision_mini 내부에 있는 데모 파일들을 확인해 봅시다.\n타겟 파일 경로와 함께 !ls 명령어를 사용하여 확인할 수 있습니다.\nls는 “list”의 약자이며, !는 쉘 레벨에서 명령을 실행하고 싶다는 의미입니다.\n\n!ls demos/foodvision_mini\n\n이들은 모두 우리가 생성한 파일들입니다!\nHugging Face에 파일을 업로드하기 위해, 이제 Google Colab(또는 이 노트북을 실행 중인 곳)에서 파일을 다운로드하겠습니다.\n먼저 다음 명령어를 통해 파일들을 단일 zip 폴더로 압축하겠습니다.\nzip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n여기서: * zip은 “압축”을 의미하며 “다음 디렉토리의 파일들을 함께 압축해 주세요”라는 뜻입니다. * -r은 “recursive”의 약자로, “타겟 디렉토리의 모든 파일을 거쳐 가세요”라는 뜻입니다. * ../foodvision_mini.zip은 파일이 압축될 타겟 디렉토리입니다. * *은 “현재 디렉토리의 모든 파일”을 의미합니다. * -x는 “이 파일들은 제외해 주세요”라는 뜻입니다.\nGoogle Colab에서 google.colab.files.download(\"demos/foodvision_mini.zip\")을 사용하여 zip 파일을 다운로드할 수 있습니다(Google Colab 내부에서 코드를 실행하지 않을 경우를 대비해 try 및 except 블록 안에 넣고, 만약 실행되지 않는다면 수동으로 파일을 다운로드하라는 메시지를 출력하도록 하겠습니다).\n한번 시도해 보죠!\n\n# foodvision_mini 폴더로 이동한 후 특정 파일을 제외하고 압축\n!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# 압축된 FoodVision Mini 앱 다운로드 (Google Colab에서 실행 중인 경우)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_mini.zip\")\nexcept:\n    print(\"Google Colab에서 실행 중이 아님, google.colab.files.download()를 사용할 수 없으므로 수동으로 다운로드해 주세요.\")\n\n와아아!!!\nzip 명령어가 성공한 것 같네요.\nGoogle Colab에서 이 노트북을 실행 중이라면 브라우저에서 파일 다운로드가 시작되는 것을 볼 수 있을 것입니다.\n그렇지 않다면, 과정 GitHub의 demos/ 디렉토리에서 foodvision_mini.zip 폴더(및 기타 파일)를 확인할 수 있습니다.\n\n\n9.2 로컬에서 FoodVision Mini 데모 실행\nfoodvision_mini.zip 파일을 다운로드했다면 다음 단계를 통해 로컬에서 테스트할 수 있습니다. 1. 파일의 압축을 풉니다. 2. 터미널 또는 명령줄 프롬프트를 엽니다. 3. foodvision_mini 디렉토리로 이동합니다 (cd foodvision_mini). 4. 환경을 생성합니다 (python3 -m venv env). 5. 환경을 활성화합니다 (source env/bin/activate). 5. 요구 사항을 설치합니다 (pip install -r requirements.txt, “-r”은 recursive의 약자). * 참고: 이 단계는 인터넷 연결 속도에 따라 5~10분 정도 걸릴 수 있습니다. 오류가 발생하면 먼저 pip를 업그레이드해야 할 수도 있습니다: pip install --upgrade pip. 6. 앱을 실행합니다 (python3 app.py).\n이렇게 하면 위에서 구축한 것과 똑같은 Gradio 데모가 여러분의 기기에서 http://127.0.0.1:7860/과 같은 URL로 로컬하게 실행됩니다.\n\n참고: 앱을 로컬에서 실행하고 flagged/ 디렉토리가 나타난다면, 해당 디렉토리에는 “플래그(flagged)”된 샘플들이 들어 있습니다.\n예를 들어, 누군가 데모를 시도했는데 모델이 잘못된 결과를 생성했다면, 해당 샘플을 “플래그”하여 나중에 검토할 수 있습니다.\nGradio의 플래깅(flagging)에 대한 자세한 내용은 플래깅 문서를 참조하세요.\n\n\n\n9.3 Hugging Face에 업로드\nFoodVision Mini 앱이 로컬에서 작동하는 것을 확인했습니다. 하지만 머신러닝 데모를 만드는 즐거움은 다른 사람들에게 보여주고 사용할 수 있게 하는 데 있습니다.\n이를 위해 FoodVision Mini 데모를 Hugging Face에 업로드하겠습니다.\n\n참고: 다음 단계 시리즈는 Git(파일 추적 시스템) 워크플로우를 사용합니다. Git의 작동 방식에 대해 더 자세히 알고 싶다면, freeCodeCamp의 초보자를 위한 Git 및 GitHub 튜토리얼을 시청하시길 권장합니다.\n\n\nHugging Face 계정에 가입합니다.\n프로필로 이동한 후 “New Space”를 클릭하여 새 Hugging Face Space를 시작합니다.\n\n참고: Hugging Face의 Space는 “코드 저장소(code repository)” 또는 줄여서 “저장소(repo)”라고도 불립니다.\n\nSpace에 이름을 지정합니다. 예를 들어, 제 것은 mrdbourke/foodvision_mini이며 여기에서 볼 수 있습니다: https://huggingface.co/spaces/mrdbourke/foodvision_mini\n라이선스를 선택합니다(저는 MIT를 사용했습니다).\nSpace SDK(software development kit)로 Gradio를 선택합니다.\n\n참고: Streamlit과 같은 다른 옵션도 사용할 수 있지만 우리 앱은 Gradio로 구축되었으므로 Gradio를 고수하겠습니다.\n\nSpace를 공개(public)로 할지 비공개(private)로 할지 선택합니다(저는 제 Space를 다른 사람들이 사용할 수 있도록 공개를 선택했습니다).\n“Create Space”를 클릭합니다.\n터미널 또는 명령 프롬프트에서 git clone https://huggingface.co/spaces/[사용자_이름]/[SPACE_이름]을 실행하여 저장소를 로컬로 복제합니다.\n\n참고: “Files and versions” 탭에서 파일을 업로드하여 추가할 수도 있습니다.\n\n다운로드한 foodvision_mini 폴더의 내용을 복제된 저장소 폴더로 복사하거나 이동합니다.\n대용량 파일(예: 10MB 이상의 파일 또는 우리의 경우 PyTorch 모델 파일)을 업로드하고 추적하려면 Git LFS(Git Large File Storage)를 설치해야 합니다.\nGit LFS를 설치한 후 git lfs install을 실행하여 활성화할 수 있습니다.\nfoodvision_mini 디렉토리에서 git lfs track \"*.file_extension\"으로 10MB 이상의 파일을 추적합니다.\n\ngit lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\" 명령으로 EffNetB2 PyTorch 모델 파일을 추적합니다.\n\n.gitattributes(HuggingFace에서 복제할 때 자동으로 생성됨, 이 파일은 큰 파일들이 Git LFS로 추적되도록 보장함)를 추적합니다. FoodVision Mini Hugging Face Space에서 .gitattributes 파일 예시를 볼 수 있습니다.\n\ngit add .gitattributes 실행\n\n나머지 foodvision_mini 앱 파일들을 추가하고 다음 명령으로 커밋합니다.\n\ngit add *\ngit commit -m \"first commit\"\n\nHugging Face에 파일을 푸시(업로드)합니다.\n\ngit push\n\n빌드가 완료될 때까지 3~5분 정도 기다리면(나중의 빌드는 더 빠름) 앱이 라이브 상태가 됩니다!\n\n모든 것이 올바르게 작동했다면, 여기 있는 것과 같은 FoodVision Mini Gradio 데모의 라이브 실행 예제를 볼 수 있을 것입니다: https://huggingface.co/spaces/mrdbourke/foodvision_mini\n또한 https://hf.space/embed/[사용자_이름]/[SPACE_이름]/+ 형식의 링크와 함께 IPython.display.IFrame을 사용하여 iframe으로 FoodVision Mini Gradio 데모를 노트북에 직접 포함시킬 수도 있습니다.\n\n# IPython은 Python을 대화형으로 사용할 수 있도록 돕는 라이브러리입니다.\nfrom IPython.display import IFrame\n\n# FoodVision Mini Gradio 데모 포함\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#foodvision-big-만들기",
    "href": "09_pytorch_model_deployment.html#foodvision-big-만들기",
    "title": "09 - PyTorch 모델 배포",
    "section": "10. FoodVision Big 만들기",
    "text": "10. FoodVision Big 만들기\n우리는 지난 몇 섹션과 챕터에 걸쳐 FoodVision Mini를 실현하기 위해 노력해 왔습니다.\n그리고 이제 라이브 데모에서 작동하는 것을 보았는데, 한 단계 더 나아가 보는 건 어떨까요?\n어떻게 요?\n바로 FoodVision Big입니다!\nFoodVision Mini는 Food101 데이터셋(각 1000개의 이미지가 있는 101개 음식 클래스)의 피자, 스테이크, 초밥 이미지로 훈련되었으므로, 101개 클래스 전체에 대해 모델을 훈련하여 FoodVision Big을 만들어 보는 건 어떨까요!\n3개 클래스에서 101개 클래스로 나아갑니다!\n피자, 스테이크, 초밥에서 피자, 스테이크, 초밥, 핫도그, 애플 파이, 당근 케이크, 초콜릿 케이크, 감자튀김, 마늘 빵, 라면, 나초, 타코 등등까지!\n어떻게 요?\n우리는 이미 모든 단계를 갖추고 있으며, EffNetB2 모델을 약간 수정하고 다른 데이터셋을 준비하기만 하면 됩니다.\n마일스톤 프로젝트 3을 마무리하기 위해, FoodVision Mini(3개 클래스)와 유사하지만 FoodVision Big(101개 클래스)을 위한 Gradio 데모를 다시 만들어 보겠습니다.\n\nFoodVision Mini는 피자, 스테이크, 초밥의 세 가지 음식 클래스로 작동합니다. 그리고 FoodVision Big은 Food101 데이터셋의 모든 클래스인 101개 음식 클래스로 작동하도록 한 단계 더 발전시켰습니다.\n\n10.1 FoodVision Big을 위한 모델 및 트랜스폼 생성\nFoodVision Mini를 만들 때 EffNetB2 모델이 속도와 성능 사이에서 좋은 균형을 이루는 것을 보았습니다(빠른 속도로 잘 수행됨).\n따라서 FoodVision Big에도 동일한 모델을 계속 사용하겠습니다.\n섹션 3.1에서 만든 create_effnetb2_model() 함수를 사용하고, Food101에는 101개의 클래스가 있으므로 num_classes=101 매개변수를 전달하여 Food101용 EffNetB2 특성 추출기를 만들 수 있습니다.\n\n# Food101의 101개 클래스에 적합한 EffNetB2 모델 생성\neffnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\n좋습니다!\n이제 모델 요약을 확인해 보겠습니다.\n\nfrom torchinfo import summary\n\n# # 101개 출력 클래스를 갖는 Food101용 EffNetB2 특성 추출기 요약 가져오기 (전체 출력을 보려면 주석 해제)\n# summary(effnetb2_food101, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n\n\n좋네요!\nFoodVision Mini를 위한 EffNetB2 모델과 마찬가지로 기본 레이어는 고정되었고(ImageNet에서 사전 훈련됨), 외부 레이어(classifier 레이어)는 훈련 가능하며 Food101의 101개 클래스를 위해 [batch_size, 101]의 출력 모양을 가지고 있습니다.\n이제 평소보다 상당히 많은 양의 데이터를 처리할 예정이므로, 훈련 데이터를 보강하기 위해 트랜스폼(effnetb2_transforms)에 약간의 데이터 증강(data augmentation)을 추가하는 건 어떨까요?\n\n참고: 데이터 증강은 오버피팅을 방지하기 위해 훈련 데이터셋의 다양성을 인위적으로 증가시키기 위해 입력 훈련 샘플의 모양을 변경(예: 이미지 회전 또는 약간 기울이기)하는 기술입니다. 데이터 증강에 대한 자세한 내용은 04. PyTorch 사용자 정의 데이터셋 섹션 6에서 확인할 수 있습니다.\n\n훈련 이미지 변환을 위해 PyTorch 팀이 컴퓨터 비전 레시피에서 사용한 것과 동일한 데이터 증강 기술인 torchvision.transforms.TrivialAugmentWide()와 effnetb2_transforms를 결합하여 torchvision.transforms 파이프라인을 구성해 보겠습니다.\n\n# Food101 훈련 데이터 트랜스폼 생성 (훈련 이미지에 대해서만 데이터 증강 수행)\nfood101_train_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.TrivialAugmentWide(),\n    effnetb2_transforms,\n])\n\n최고네요!\n이제 (훈련 데이터를 위한) food101_train_transforms와 (테스트/추론 데이터를 위한) effnetb2_transforms를 비교해 보겠습니다.\n\nprint(f\"훈련 트랜스폼:\\n{food101_train_transforms}\\n\") \nprint(f\"테스트 트랜스폼:\\n{effnetb2_transforms}\")\n\n\n\n10.2 FoodVision Big을 위한 데이터 가져오기\nFoodVision Mini를 위해 전체 Food101 데이터셋의 사용자 정의 데이터 분할을 만들었습니다.\n전체 Food101 데이터셋을 얻으려면 torchvision.datasets.Food101()을 사용할 수 있습니다.\n먼저 이미지를 저장할 data/ 디렉토리 경로를 설정하겠습니다.\n그런 다음 food101_train_transforms 및 effnetb2_transforms를 사용하여 각각의 데이터셋을 변환하도록 훈련 및 테스트 데이터셋 분할을 다운로드하고 변환하겠습니다.\n\n참고: Google Colab을 사용하는 경우 아래 셀을 실행하여 PyTorch에서 Food101 이미지를 완전히 다운로드하는 데 약 3~5분 정도 걸립니다.\n다운로드되는 이미지가 100,000개가 넘기 때문입니다(101개 클래스 x 클래스당 1000개 이미지). Google Colab 런타임을 재시작하고 이 셀로 돌아오면 이미지를 다시 다운로드해야 합니다. 또는 이 노트북을 로컬에서 실행하는 경우 이미지는 torchvision.datasets.Food101()의 root 매개변수에 지정된 디렉토리에 캐시되어 저장됩니다.\n\n\nfrom torchvision import datasets\n\n# 데이터 디렉토리 설정\nfrom pathlib import Path\ndata_dir = Path(\"data\")\n\n# 훈련 데이터 가져오기 (~750개 이미지 x 101개 음식 클래스)\ntrain_data = datasets.Food101(root=data_dir, # 데이터를 다운로드할 경로\n                              split=\"train\", # 가져올 데이터셋 분할\n                              transform=food101_train_transforms, # 훈련 데이터에 대해 데이터 증강 수행\n                              download=True) # 다운로드할까요?\n\n# 테스트 데이터 가져오기 (~250개 이미지 x 101개 음식 클래스)\ntest_data = datasets.Food101(root=data_dir,\n                             split=\"test\",\n                             transform=effnetb2_transforms, # 테스트 데이터에 대해 일반 EffNetB2 트랜스폼 수행\n                             download=True)\n\n데이터 다운로드 완료!\n이제 train_data.classes를 사용하여 모든 클래스 이름 리스트를 얻을 수 있습니다.\n\n# Food101 클래스 이름 가져오기\nfood101_class_names = train_data.classes\n\n# 처음 10개 확인\nfood101_class_names[:10]\n\n호호! 정말 맛있어 보이는 음식들이네요(비록 “beignets”에 대해서는 들어본 적이 없지만… 업데이트: 구글 검색을 해보니 beignets도 맛있어 보입니다).\n과정 GitHub의 extras/food101_class_names.txt에서 Food101 클래스 이름 전체 리스트를 확인할 수 있습니다.\n\n\n10.3 더 빠른 실험을 위해 Food101 데이터셋의 서브셋 생성\n이는 선택 사항입니다.\nFood101 데이터셋의 또 다른 서브셋을 만들 필요는 없으며, 101,000개 이미지 전체에 대해 모델을 훈련하고 평가할 수도 있습니다.\n하지만 훈련 속도를 유지하기 위해 훈련 및 테스트 데이터셋의 20% 분할을 만들겠습니다.\n우리의 목표는 단 20%의 데이터만으로 원본 Food101 논문의 최고 결과를 뛰어넘을 수 있는지 확인하는 것입니다.\n우리가 사용했거나 사용할 데이터셋을 요약하면 다음과 같습니다.\n\n\n\n\n\n\n\n\n\n\n\n노트북\n프로젝트 이름\n데이터셋\n클래스 수\n훈련 이미지\n테스트 이미지\n\n\n\n\n04, 05, 06, 07, 08\nFoodVision Mini (10% 데이터)\nFood101 사용자 정의 분할\n3 (피자, 스테이크, 초밥)\n225\n75\n\n\n07, 08, 09\nFoodVision Mini (20% 데이터)\nFood101 사용자 정의 분할\n3 (피자, 스테이크, 초밥)\n450\n150\n\n\n09 (현재)\nFoodVision Big (20% 데이터)\nFood101 사용자 정의 분할\n101 (모든 Food101 클래스)\n15150\n5050\n\n\n확장\nFoodVision Big\nFood101 모든 데이터\n101\n75750\n25250\n\n\n\n경향이 보이시나요?\n시간이 지남에 따라 모델 크기가 서서히 증가한 것처럼, 실험에 사용해 온 데이터셋의 크기도 증가했습니다.\n\n참고: 20%의 데이터로 원본 Food101 논문의 결과를 진정으로 뛰어넘으려면, 훈련 데이터의 20%로 모델을 훈련시킨 다음 우리가 만든 분할이 아닌 전체 테스트 세트에서 모델을 평가해야 합니다. 이는 여러분이 시도해 볼 수 있는 확장 과제로 남겨두겠습니다. 또한 Food101 훈련 데이터셋 전체에 대해 모델을 훈련해 보시기를 권장합니다.\n\nFoodVision Big(20% 데이터) 분할을 만들기 위해, 주어진 데이터셋을 특정 비율로 나누는 split_dataset()이라는 함수를 만들겠습니다.\nlengths 매개변수를 사용하여 주어진 크기의 분할을 생성하는 torch.utils.data.random_split()을 사용할 수 있습니다.\nlengths 매개변수는 원하는 분할 길이 리스트를 받으며 리스트의 총합은 데이터셋의 전체 길이와 같아야 합니다.\n예를 들어 크기가 100인 데이터셋의 경우, lengths=[20, 80]을 전달하여 20%와 80%의 분할을 받을 수 있습니다.\n우리는 함수가 두 개의 분할, 즉 타겟 길이(예: 훈련 데이터의 20%)를 가진 분할과 나머지 길이(예: 나머지 80%의 훈련 데이터)를 가진 분할을 반환하기를 원합니다.\n마지막으로 재현성을 위해 generator 매개변수를 torch.manual_seed() 값으로 설정하겠습니다.\n\ndef split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n    \"\"\"split_size와 seed를 기반으로 주어진 데이터셋을 두 가지 비율로 무작위 분할합니다.\n\n    인자:\n        dataset (torchvision.datasets): PyTorch 데이터셋, 일반적으로 torchvision.datasets에서 가져온 것.\n        split_size (float, optional): 데이터셋을 얼마나 분할할까요? \n            예: split_size=0.2는 20% 분할과 80% 분할이 있음을 의미함. 기본값은 0.2.\n        seed (int, optional): 무작위 생성기를 위한 시드. 기본값은 42.\n\n    반환값:\n        tuple: (random_split_1, random_split_2) 여기서 random_split_1은 split_size*len(dataset) 크기이고, \n            random_split_2는 (1-split_size)*len(dataset) 크기임.\n    \"\"\"\n    # 원본 데이터셋 길이를 기반으로 분할 길이 생성\n    length_1 = int(len(dataset) * split_size) # 원하는 길이\n    length_2 = len(dataset) - length_1 # 나머지 길이\n        \n    # 정보 출력\n    print(f\"[INFO] {len(dataset)} 길이의 데이터셋을 {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%) 크기의 분할로 나누는 중\")\n    \n    # 주어진 무작위 시드로 분할 생성\n    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n                                                                   lengths=[length_1, length_2],\n                                                                   generator=torch.manual_seed(seed)) # 재현 가능한 분할을 위해 무작위 시드 설정\n    return random_split_1, random_split_2\n\n데이터셋 분할 함수가 생성되었습니다!\n이제 Food101의 20% 훈련 및 테스트 데이터셋 분할을 생성하여 테스트해 보겠습니다.\n\n# Food101의 훈련용 20% 분할 생성\ntrain_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n                                                 split_size=0.2)\n\n# Food101의 테스트용 20% 분할 생성\ntest_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n                                                split_size=0.2)\n\nlen(train_data_food101_20_percent), len(test_data_food101_20_percent)\n\n훌륭합니다!\n\n\n10.4 Food101 데이터셋을 DataLoader로 변환\n이제 torch.utils.data.DataLoader()를 사용하여 Food101 20% 데이터셋 분할을 DataLoader로 변환해 보겠습니다.\n훈련 데이터에 대해서만 shuffle=True로 설정하고 두 데이터셋 모두 배치 크기를 32로 설정하겠습니다.\nCPU 코어 수가 사용 가능하다면 num_workers를 4로, 그렇지 않다면 2로 설정하겠습니다(단, num_workers의 값은 매우 실험적이며 사용 중인 하드웨어에 따라 달라질 수 있으므로 PyTorch 포럼에 이에 대한 활발한 토론 스레드가 있습니다).\n\nimport os\nimport torch\n\nBATCH_SIZE = 32\nNUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # 이 값은 매우 실험적이며 사용 가능한 하드웨어에 따라 달라집니다. Google Colab은 일반적으로 2개의 CPU를 제공합니다.\n\n# Food101 20% 훈련 DataLoader 생성\ntrain_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n                                                                  batch_size=BATCH_SIZE,\n                                                                  shuffle=True,\n                                                                  num_workers=NUM_WORKERS)\n# Food101 20% 테스트 DataLoader 생성\ntest_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n                                                                 batch_size=BATCH_SIZE,\n                                                                 shuffle=False,\n                                                                 num_workers=NUM_WORKERS)\n\n\n\n10.5 FoodVision Big 모델 훈련\nFoodVision Big 모델과 DataLoader가 준비되었습니다!\n이제 훈련할 시간입니다.\ntorch.optim.Adam()과 학습률 1e-3을 사용하여 옵티마이저를 만들겠습니다.\n클래스가 매우 많기 때문에, torchvision의 최첨단 훈련 레시피에 따라 label_smoothing=0.1인 torch.nn.CrossEntropyLoss()를 사용하여 손실 함수를 설정하겠습니다.\n레이블 스무딩(label smoothing)이란 무엇일까요?\n레이블 스무딩은 모델이 특정 레이블에 부여하는 값을 줄이고 이를 다른 레이블에 분산시키는 규제(regularization) 기술입니다(규제는 오버피팅을 방지하는 과정을 설명하는 또 다른 단어입니다).\n본질적으로, 모델이 단일 레이블에 대해 지나치게 확신하는 대신 레이블 스무딩은 다른 레이블에 0이 아닌 값을 부여하여 일반화에 도움을 줍니다.\n예를 들어 레이블 스무딩이 없는 모델이 5개 클래스에 대해 다음과 같은 출력을 가졌다고 가정해 보겠습니다.\n[0, 0, 0.99, 0.01, 0]\n레이블 스무딩이 있는 모델은 다음과 같은 출력을 가질 수 있습니다.\n[0.01, 0.01, 0.96, 0.01, 0.01]\n모델은 여전히 클래스 3에 대한 예측을 확신하고 있지만, 다른 레이블에 작은 값을 부여함으로써 모델이 적어도 다른 옵션을 고려하도록 강제합니다.\n마지막으로, 작업을 빠르게 진행하기 위해 05. PyTorch 모듈화 섹션 4에서 만든 engine.train() 함수를 사용하여 5 에포크 동안 모델을 훈련할 것이며, 원본 Food101 논문의 테스트 세트 정확도 결과인 56.4%를 뛰어넘는 것을 목표로 합니다.\n지금까지 가장 큰 모델을 훈련해 봅시다!\n\n참고: 아래 셀을 실행하는 데 Google Colab에서 약 15~20분 정도 걸립니다. 지금까지 사용한 데이터 중 가장 많은 양(훈련 이미지 15,150개, 테스트 이미지 5,050개)으로 가장 큰 모델을 훈련하기 때문입니다. 이것이 우리가 앞서 전체 Food101 데이터셋의 20%를 분할하기로 결정한 이유입니다(훈련에 한 시간 이상 걸리지 않도록 하기 위해).\n\n\nfrom going_modular.going_modular import engine\n\n# 옵티마이저 설정\noptimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n                             lr=1e-3)\n\n# 손실 함수 설정\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # 클래스가 매우 많으므로 약간의 레이블 스무딩 추가\n\n# 20%의 데이터로 원본 Food101 논문의 결과인 56.4% 이상의 테스트 데이터셋 정확도를 달성하고자 함\nset_seeds()    \neffnetb2_food101_results = engine.train(model=effnetb2_food101,\n                                        train_dataloader=train_dataloader_food101_20_percent,\n                                        test_dataloader=test_dataloader_food101_20_percent,\n                                        optimizer=optimizer,\n                                        loss_fn=loss_fn,\n                                        epochs=5,\n                                        device=device)\n\n우와!!!!\n훈련 데이터의 20%만으로 원본 Food101 논문의 결과인 56.4% 정확도를 뛰어넘은 것 같습니다(비록 테스트 데이터의 20%에 대해서만 평가했지만, 결과를 완전히 재현하려면 테스트 데이터의 100%에 대해 평가할 수도 있습니다).\n이것이 바로 전이 학습의 힘입니다!\n\n\n10.6 FoodVision Big 모델의 손실 곡선 검사\n우리의 FoodVision Big 손실 곡선을 시각화해 보겠습니다.\nhelper_functions.py의 plot_loss_curves() 함수를 사용하여 그렇게 할 수 있습니다.\n\nfrom helper_functions import plot_loss_curves\n\n# FoodVision Big의 손실 곡선 확인\nplot_loss_curves(effnetb2_food101_results)\n\n좋네요!!!\n우리의 규제 기술(데이터 증강 및 레이블 스무딩)이 모델의 오버피팅을 방지하는 데 도움이 된 것 같습니다(훈련 손실이 여전히 테스트 손실보다 높음). 이는 우리 모델이 학습할 수 있는 용량이 조금 더 남아 있으며 추가 훈련을 통해 더 개선될 수 있음을 나타냅니다.\n\n\n10.7 FoodVision Big 저장 및 불러오기\n이제 지금까지 가장 큰 모델을 훈련했으므로 나중에 다시 불러올 수 있도록 저장하겠습니다.\n\nfrom going_modular.going_modular import utils\n\n# 모델 경로 생성\neffnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n\n# FoodVision Big 모델 저장\nutils.save_model(model=effnetb2_food101,\n                 target_dir=\"models\",\n                 model_name=effnetb2_food101_model_path)\n\n모델 저장 완료!\n다음으로 넘어가기 전에 모델을 다시 불러올 수 있는지 확인하겠습니다.\n먼저 create_effnetb2_model(num_classes=101)(모든 Food101 클래스를 위해 101개 클래스 설정)로 모델 인스턴스를 생성하여 이를 수행합니다.\n그런 다음 torch.nn.Module.load_state_dict() 및 torch.load()를 사용하여 저장된 state_dict()를 불러옵니다.\n\n# Food101 호환 EffNetB2 인스턴스 생성\nloaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\n# 저장된 모델의 state_dict() 불러오기\nloaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))\n\n\n\n10.8 FoodVision Big 모델 크기 확인\n우리의 FoodVision Big 모델은 FoodVision Mini의 3개 클래스에 비해 101개 클래스를 분류할 수 있으며, 이는 33.6배 증가한 수치입니다!\n이것이 모델 크기에 어떤 영향을 미칠까요?\n알아봅시다.\n\nfrom pathlib import Path\n\n# 모델 크기를 바이트 단위로 가져온 다음 메가바이트로 변환\npretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # 나눗셈을 통해 바이트를 메가바이트로 변환(대략) \nprint(f\"사전 훈련된 EffNetB2 특성 추출기 Food101 모델 크기: {pretrained_effnetb2_food101_model_size} MB\")\n\n흠, 클래스 수가 크게 증가했음에도 불구하고 모델 크기는 거의 동일하게 유지된 것으로 보입니다(FoodVision Big은 30MB, FoodVision Mini는 29MB).\n이는 FoodVision Big을 위한 모든 추가 파라미터가 오직 마지막 레이어(분류 헤드)에만 있기 때문입니다.\n모든 기본 레이어는 FoodVision Big과 FoodVision Mini 사이에 동일합니다.\n위로 돌아가서 모델 요약을 비교하면 더 자세한 내용을 알 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n모델\n출력 모양 (클래스 수)\n훈련 가능한 파라미터\n총 파라미터\n모델 크기 (MB)\n\n\n\n\nFoodVision Mini (EffNetB2 특성 추출기)\n3\n4,227\n7,705,221\n29\n\n\nFoodVision Big (EffNetB2 특성 추출기)\n101\n142,309\n7,843,303\n30",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#foodvision-big-모델을-배포-가능한-앱으로-변환",
    "href": "09_pytorch_model_deployment.html#foodvision-big-모델을-배포-가능한-앱으로-변환",
    "title": "09 - PyTorch 모델 배포",
    "section": "11. FoodVision Big 모델을 배포 가능한 앱으로 변환",
    "text": "11. FoodVision Big 모델을 배포 가능한 앱으로 변환\nFood101 데이터셋의 20%로 훈련되고 저장된 EffNetB2 모델이 있습니다.\n모델이 폴더에만 평생 있게 하는 대신 배포해 봅시다!\nFoodVision Mini 모델을 배포한 것과 동일한 방식으로, Hugging Face Spaces에서 Gradio 데모로 FoodVision Big 모델을 배포하겠습니다.\n시작하기 위해, FoodVision Big 데모 파일을 저장할 demos/foodvision_big/ 디렉토리와 데모 테스트를 위한 예시 이미지를 담을 demos/foodvision_big/examples 디렉토리를 생성하겠습니다.\n완료되면 다음과 같은 파일 구조를 갖게 될 것입니다.\ndemos/\n  foodvision_big/\n    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n    app.py\n    class_names.txt\n    examples/\n      example_1.jpg\n    model.py\n    requirements.txt\n여기서: * 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth는 훈련된 PyTorch 모델 파일입니다. * app.py에는 FoodVision Big Gradio 앱이 들어 있습니다. * class_names.txt에는 FoodVision Big을 위한 모든 클래스 이름이 들어 있습니다. * examples/에는 Gradio 앱과 함께 사용할 예시 이미지가 들어 있습니다. * model.py에는 모델 정의와 모델과 관련된 모든 트랜스폼이 들어 있습니다. * requirements.txt에는 torch, torchvision, gradio와 같이 앱을 실행하기 위한 의존성이 들어 있습니다.\n\nfrom pathlib import Path\n\n# FoodVision Big 데모 경로 생성\nfoodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n\n# FoodVision Big 데모 디렉토리 생성\nfoodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n\n# FoodVision Big 데모 예시 디렉토리 생성\n(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)\n\n\n11.1 예시 이미지 다운로드 및 examples 디렉토리로 이동\n예시 이미지로는 믿음직한 pizza-dad 이미지(피자를 먹고 있는 제 아버지의 사진)를 사용하겠습니다.\n과정 GitHub에서 !wget 명령어를 통해 다운로드한 다음, !mv(“move”의 약자) 명령어를 사용하여 demos/foodvision_big/examples로 이동하겠습니다.\n하는 김에 훈련된 Food101 EffNetB2 모델도 models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth에서 demos/foodvision_big으로 이동시키겠습니다.\n\n# 예시 이미지 다운로드 및 이동\n!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n\n# 훈련된 모델을 FoodVision Big 데모 폴더로 이동 (모델이 이미 이동된 경우 오류 발생)\n!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big\n\n\n\n11.2 Food101 클래스 이름을 파일로 저장 (class_names.txt)\nFood101 데이터셋에는 클래스가 매우 많기 때문에, 이를 app.py 파일에 리스트로 저장하는 대신 .txt 파일에 저장하고 필요할 때 읽어오도록 하겠습니다.\n먼저 food101_class_names를 확인하여 어떻게 생겼는지 다시 한번 상기해 보겠습니다.\n\n# 처음 10개 Food101 클래스 이름 확인\nfood101_class_names[:10]\n\n좋습니다. 이제 demos/foodvision_big/class_names.txt 경로를 먼저 생성한 다음 Python의 open()으로 파일을 열고 각 클래스마다 줄바꿈을 하여 기록하겠습니다.\n이상적으로 클래스 이름은 다음과 같이 저장되기를 원합니다.\napple_pie\nbaby_back_ribs\nbaklava\nbeef_carpaccio\nbeef_tartare\n...\n\n# Food101 클래스 이름 경로 생성\nfoodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n\n# Food101 클래스 이름 리스트를 파일로 기록\nwith open(foodvision_big_class_names_path, \"w\") as f:\n    print(f\"[INFO] Food101 클래스 이름을 {foodvision_big_class_names_path}에 저장 중\")\n    f.write(\"\\n\".join(food101_class_names)) # 각 클래스 사이에 줄바꿈 추가\n\n훌륭합니다. 이제 다시 읽어올 수 있는지 확인해 보겠습니다.\nPython의 open()을 읽기 모드(\"r\")로 사용한 다음, readlines() 메서드를 사용하여 class_names.txt 파일의 각 줄을 읽어오겠습니다.\n그리고 리스트 컴프리헨션(list comprehension)과 strip()을 사용하여 각 줄의 줄바꿈 문자를 제거하고 클래스 이름을 리스트로 저장할 수 있습니다.\n\n# Food101 클래스 이름 파일을 열고 각 줄을 리스트로 읽어오기\nwith open(foodvision_big_class_names_path, \"r\") as f:\n    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n    \n# 다시 로드된 처음 5개 클래스 이름 확인\nfood101_class_names_loaded[:5]\n\n\n\n11.3 FoodVision Big 모델을 Python 스크립트로 변환 (model.py)\nFoodVision Mini 데모와 마찬가지로, EffNetB2 특성 추출기 모델을 인스턴스화하고 필요한 트랜스폼을 함께 제공할 수 있는 스크립트를 만들어 보겠습니다.\n\n%%writefile demos/foodvision_big/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"EfficientNetB2 특성 추출기 모델과 트랜스폼을 생성합니다.\n\n    인자:\n        num_classes (int, optional): 분류 헤드의 클래스 수. \n            기본값은 3.\n        seed (int, optional): 무작위 시드 값. 기본값은 42.\n\n    반환값:\n        model (torch.nn.Module): EffNetB2 특성 추출기 모델. \n        transforms (torchvision.transforms): EffNetB2 이미지 트랜스폼.\n    \"\"\"\n    # 사전 훈련된 EffNetB2 가중치, 트랜스폼 및 모델 생성\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 기본 모델의 모든 레이어 고정\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 재현성을 위해 무작위 시드와 함께 분류 헤드 변경\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n\n\n\n11.4 FoodVision Big Gradio 앱을 Python 스크립트로 변환 (app.py)\nFoodVision Big용 model.py 스크립트를 얻었으니 이제 FoodVision Big용 app.py 스크립트를 만들어 보겠습니다.\n이는 다시 한번 FoodVision Mini용 app.py 스크립트와 거의 동일하지만 다음 사항들을 변경하겠습니다.\n\n임포트 및 클래스 이름 설정 - class_names 변수는 피자, 스테이크, 초밥이 아닌 모든 Food101 클래스에 대한 리스트가 됩니다. 이들은 demos/foodvision_big/class_names.txt를 통해 접근할 수 있습니다.\n모델 및 트랜스폼 준비 - model은 num_classes=3이 아닌 num_classes=101을 갖게 됩니다. 또한 가중치를 \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"(FoodVision Big 모델 경로)에서 불러오도록 하겠습니다.\n예측 함수 - 이 부분은 FoodVision Mini의 app.py와 동일하게 유지됩니다.\nGradio 앱 - Gradio 인터페이스는 FoodVision Big의 세부 사항을 반영하기 위해 서로 다른 title, description 및 article 매개변수를 갖게 됩니다.\n\n또한 %%writefile 매직 명령어를 사용하여 demos/foodvision_big/app.py에 저장하도록 하겠습니다.\n\n%%writefile demos/foodvision_big/app.py\n### 1. 임포트 및 클래스 이름 설정 ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# 클래스 이름 설정\nwith open(\"class_names.txt\", \"r\") as f: # class_names.txt에서 읽어오기\n    class_names = [food_name.strip() for food_name in  f.readlines()]\n    \n### 2. 모델 및 트랜스폼 준비 ###    \n\n# 모델 생성\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=101, # len(class_names)를 사용할 수도 있습니다.\n)\n\n# 저장된 가중치 불러오기\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # CPU로 불러오기\n    )\n)\n\n### 3. 예측 함수 ###\n\n# 예측 함수 생성\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"img에 대해 변환 및 예측을 수행하고 예측 결과와 소요 시간을 반환합니다.\n    \"\"\"\n    # 타이머 시작\n    start_time = timer()\n    \n    # 타겟 이미지 변환 및 배치 차원 추가\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # 모델을 평가 모드로 설정하고 추론 모드 활성화\n    effnetb2.eval()\n    with torch.inference_mode():\n        # 변환된 이미지를 모델에 통과시키고 예측 로짓을 예측 확률로 변환\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # 각 예측 클래스에 대한 예측 레이블 및 예측 확률 딕셔너리 생성 (이는 Gradio의 출력 매개변수에 필요한 형식입니다)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # 예측 시간 계산\n    pred_time = round(timer() - start_time, 5)\n    \n    # 예측 딕셔너리와 예측 시간 반환 \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio 앱 ###\n\n# 제목, 설명 및 기사 문자열 생성\ntitle = \"FoodVision Big 🍔👁\"\ndescription = \"음식 이미지를 [101가지 클래스](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt)로 분류하는 EfficientNetB2 특성 추출기 컴퓨터 비전 모델입니다.\"\narticle = \"[09. PyTorch 모델 배포](https://www.learnpytorch.io/09_pytorch_model_deployment/)에서 생성되었습니다.\"\n\n# \"examples/\" 디렉토리에서 예시 리스트 생성\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Gradio 인터페이스 생성 \ndemo = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=[\n        gr.Label(num_top_classes=5, label=\"Predictions\"),\n        gr.Number(label=\"Prediction time (s)\"),\n    ],\n    examples=example_list,\n    title=title,\n    description=description,\n    article=article,\n)\n\n# 앱 실행!\ndemo.launch()\n\n\n\n11.5 FoodVision Big을 위한 요구 사항 파일 생성 (requirements.txt)\n이제 우리 FoodVision Big 앱에 어떤 의존성이 필요한지 Hugging Face Space에 알려주기 위해 requirements.txt 파일만 있으면 됩니다.\n\n%%writefile demos/foodvision_big/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n\n\n\n11.6 FoodVision Big 앱 파일 다운로드\nHugging Face에 FoodVision Big 앱을 배포하는 데 필요한 모든 파일을 얻었으니 이제 이들을 함께 압축하고 다운로드해 보겠습니다.\n위의 FoodVision Mini 앱에서 사용했던 것과 동일한 과정을 섹션 9.1: FoodVision Mini 앱 파일 다운로드에서 사용하겠습니다.\n\n# foodvision_big 폴더를 압축하되 특정 파일은 제외\n!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# 압축된 FoodVision Big 앱 다운로드 (Google Colab에서 실행 중인 경우)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_big.zip\")\nexcept:\n    print(\"Google Colab에서 실행 중이 아님, google.colab.files.download()를 사용할 수 없습니다.\")\n\n\n\n11.7 FoodVision Big 앱을 HuggingFace Spaces에 배포하기\nB, E, A, Utiful!\n전체 과정 중 가장 큰 모델을 실현할 시간입니다!\nFoodVision Big Gradio 데모를 Hugging Face Spaces에 배포하여 대화형으로 테스트하고 다른 사람들이 우리의 머신러닝 노력의 마법을 경험하게 해봅시다!\n\n참고: Hugging Face Spaces에 파일을 업로드하는 몇 가지 방법이 있습니다. 다음 단계는 파일을 추적하기 위해 Hugging Face를 git 저장소로 취급합니다. 하지만 웹 인터페이스 또는 huggingface_hub 라이브러리를 통해 Hugging Face Spaces에 직접 업로드할 수도 있습니다.\n\n다행히도 우리는 이미 FoodVision Mini를 통해 수행하는 단계를 거쳤으므로, 이제 FoodVision Big에 맞게 수정하기만 하면 됩니다.\n\nHugging Face 계정에 가입합니다.\n프로필로 이동한 후 “New Space”를 클릭하여 새 Hugging Face Space를 시작합니다.\n\n참고: Hugging Face의 Space는 “코드 저장소(code repository)” 또는 줄여서 “저장소(repo)”라고도 불립니다.\n\nSpace에 이름을 지정합니다. 예를 들어, 제 것은 mrdbourke/foodvision_big이며 여기에서 볼 수 있습니다: https://huggingface.co/spaces/mrdbourke/foodvision_big\n라이선스를 선택합니다(저는 MIT를 사용했습니다).\nSpace SDK(software development kit)로 Gradio를 선택합니다.\n\n참고: Streamlit과 같은 다른 옵션도 사용할 수 있지만 우리 앱은 Gradio로 구축되었으므로 Gradio를 고수하겠습니다.\n\nSpace를 공개(public)로 할지 비공개(private)로 할지 선택합니다(저는 제 Space를 다른 사람들이 사용할 수 있도록 공개를 선택했습니다).\n“Create Space”를 클릭합니다.\n터미널 또는 명령 프롬프트에서 git clone https://huggingface.co/spaces/[사용자_이름]/[SPACE_이름]을 실행하여 저장소를 로컬로 복제합니다.\n\n참고: “Files and versions” 탭에서 파일을 업로드하여 추가할 수도 있습니다.\n\n다운로드한 foodvision_big 폴더의 내용을 복제된 저장소 폴더로 복사하거나 이동합니다.\n대용량 파일(예: 10MB 이상의 파일 또는 우리의 경우 PyTorch 모델 파일)을 업로드하고 추적하려면 Git LFS(Git Large File Storage)를 설치해야 합니다.\nGit LFS를 설치한 후 git lfs install을 실행하여 활성화할 수 있습니다.\nfoodvision_big 디렉토리에서 git lfs track \"*.file_extension\"으로 10MB 이상의 파일을 추적합니다.\n\ngit lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" 명령으로 EffNetB2 PyTorch 모델 파일을 추적합니다.\n참고: 이미지를 업로드할 때 오류가 발생하면 이미지도 git lfs로 추적해야 할 수 있습니다. 예: git lfs track \"examples/04-pizza-dad.jpg\"\n\n.gitattributes(HuggingFace에서 복제할 때 자동으로 생성됨, 이 파일은 큰 파일들이 Git LFS로 추적되도록 보장함)를 추적합니다. FoodVision Big Hugging Face Space에서 .gitattributes 파일 예시를 볼 수 있습니다.\n\ngit add .gitattributes 실행\n\n나머지 foodvision_big 앱 파일들을 추가하고 다음 명령으로 커밋합니다.\n\ngit add *\ngit commit -m \"first commit\"\n\nHugging Face에 파일을 푸시(업로드)합니다.\n\ngit push\n\n빌드가 완료될 때까지 3~5분 정도 기다리면(나중의 빌드는 더 빠름) 앱이 라이브 상태가 됩니다!\n\n모든 것이 올바르게 작동했다면, 우리 FoodVision Big Gradio 데모가 분류 준비를 마쳤을 것입니다!\n제 버전은 여기에서 보실 수 있습니다: https://huggingface.co/spaces/mrdbourke/foodvision_big/\n또는 https://hf.space/embed/[사용자_이름]/[SPACE_이름]/+ 형식의 링크와 함께 IPython.display.IFrame을 사용하여 iframe으로 FoodVision Big Gradio 데모를 노트북에 직접 포함시킬 수도 있습니다.\n\n# IPython은 Python을 대화형으로 사용할 수 있도록 돕는 라이브러리입니다.\nfrom IPython.display import IFrame\n\n# FoodVision Big Gradio 데모를 iframe으로 포함\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)\n\n얼마나 멋진가요!?!\n직선을 예측하기 위해 PyTorch 모델을 구축하는 것에서 시작하여… 이제 전 세계 사람들이 접근할 수 있는 컴퓨터 비전 모델을 구축하고 있습니다!",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#주요-요점",
    "href": "09_pytorch_model_deployment.html#주요-요점",
    "title": "09 - PyTorch 모델 배포",
    "section": "주요 요점",
    "text": "주요 요점\n\n배포는 훈련만큼이나 중요합니다. 잘 작동하는 모델을 얻고 나면 첫 번째 질문은 ’이를 어떻게 배포하여 다른 사람들이 접근할 수 있게 만들 것인가?’여야 합니다. 배포를 통해 개인 훈련 및 테스트 세트가 아닌 실제 세상에서 모델을 테스트할 수 있습니다.\n머신러닝 모델 배포를 위한 세 가지 질문:\n\n모델의 가장 이상적인 유스케이스는 무엇인가(얼마나 잘 그리고 얼마나 빨리 작동하는가)?\n모델이 어디로 갈 것인가(온디바이스 또는 클라우드)?\n모델이 어떻게 작동할 것인가(예측이 온라인인가 오프라인인가)?\n\n배포 옵션은 풍부합니다. 하지만 간단하게 시작하는 것이 가장 좋습니다. 현재 가장 좋은 방법 중 하나(이러한 것들은 항상 바뀌기 때문에 현재라고 말합니다)는 Gradio를 사용하여 데모를 만들고 Hugging Face Spaces에 호스팅하는 것입니다. 간단하게 시작하고 필요할 때 확장하세요.\n실험을 멈추지 마세요. 머신러닝 모델 요구 사항은 시간이 지남에 따라 변할 가능성이 높으므로 단일 모델을 배포하는 것이 마지막 단계는 아닙니다. 데이터셋이 변하는 것을 발견할 수 있으므로 모델을 업데이트해야 합니다. 또는 새로운 연구 결과가 발표되어 사용할 더 나은 아키텍처가 생길 수도 있습니다.\n\n따라서 하나의 모델을 배포하는 것은 훌륭한 단계이지만, 시간이 지남에 따라 업데이트하고 싶어질 것입니다.\n\n머신러닝 모델 배포는 MLOps(Machine Learning Operations) 엔지니어링 실무의 일부입니다. MLOps는 DevOps(Development Operations)의 확장이며 모델 훈련과 관련된 모든 엔지니어링 부분(데이터 수집 및 저장, 데이터 전처리, 모델 배포, 모델 모니터링, 버전 관리 등)을 포함합니다. 빠르게 진화하는 분야이지만 더 많이 배울 수 있는 탄탄한 리소스들이 있으며, 그중 상당수가 PyTorch 추가 리소스에 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#연습-문제",
    "href": "09_pytorch_model_deployment.html#연습-문제",
    "title": "09 - PyTorch 모델 배포",
    "section": "연습 문제",
    "text": "연습 문제\n모든 연습 문제는 위의 코드를 연습하는 데 중점을 둡니다.\n각 섹션을 참조하거나 링크된 리소스를 따라 완료할 수 있어야 합니다.\n리소스:\n\n09 장 연습 문제 템플릿 노트북.\n09 장 연습 문제 모범 답안 노트북 답안을 보기 전에 직접 풀어보세요.\n\nYouTube에서 솔루션에 대한 라이브 비디오 연습을 시청하세요 (오류 등 포함).\n\n\n\nGPU(device=\"cuda\")를 사용하여 테스트 데이터셋에서 두 특성 추출기 모델 모두에 대해 예측을 수행하고 시간을 측정합니다. 모델의 GPU 대 CPU 예측 시간을 비교해 보세요. 그 간격이 좁혀지나요? 즉, GPU에서 예측을 수행하면 ViT 특성 추출기 예측 시간이 EffNetB2 특성 추출기 예측 시간에 더 가까워지나요?\n\n섹션 5. 훈련된 모델로 예측하고 시간 측정하기 및 섹션 6. 모델 결과, 예측 시간 및 크기 비교에서 이 단계들을 수행하기 위한 코드를 찾을 수 있습 니다.\n\nViT 특성 추출기는 EffNetB2보다 더 많은 학습 용량(더 많은 파라미터 때문)을 가지고 있는 것으로 보이는데, 전체 Food101 데이 터셋의 더 큰 20% 분할에서는 어떻게 작동하나요?\n\n섹션 10. FoodVision Big 만들기에서 EffNetB2로 했던 것과 똑같이, 20% Food101 데이터셋에서 5 에포크 동안 ViT 특성 추출기를 훈련시키세요.\n\n연습 문제 2의 ViT 특성 추출기를 사용하여 20% Food101 테스트 데이터셋 전체에 대해 예측을 수행하고 “가장 틀린” 예측을 찾 으세요.\n\n예측 확률은 가장 높지만 예측 레이블이 틀린 것이 “가장 틀린” 예측이 됩니다.\n왜 모델이 이러한 예측을 틀렸다고 생각하는지 한두 문장으로 적어보세요.\n\n20% 버전이 아닌 Food101 테스트 데이터셋 전체에 대해 ViT 특성 추출기를 평가하면 성능이 어떤가요?\n\n원본 Food101 논문의 최고 결과인 56.4% 정확도를 뛰어넘나요?\n\nPaperswithcode.com으로 이동하여 Food101 데이터셋에서 현재 가장 성능이 좋은 모델을 찾으세요.\n\n어떤 모델 아키텍처를 사용하나요?\n\n배포된 FoodVision 모델의 잠재적인 실패 지점 1~3개와 잠재적인 해결책을 적어보세요.\n\n예를 들어, 누군가 음식이 아닌 사진을 FoodVision Mini 모델에 업로드하면 어떻게 될까요?\n\ntorchvision.datasets에서 임의의 데이터셋을 선택하고, torchvision.models의 모델(이미 만든 모델 중 하나인 EffNetB2 또는 ViT를 사용할 수 있음)을 사 용하여 5 에포크 동안 특성 추출기 모델을 훈련시킨 다음, Hugging Face Spaces에 Gradio 앱으로 배포하세요.\n\n훈련 시간이 너무 오래 걸리지 않도록 더 작은 데이터셋을 선택하거나 일부만 사용하고 싶을 수 있습니다.\n여러분이 배포한 모델을 보고 싶습니다! Discord나 과정 GitHub Discussion 페이지에 공유해 주세요.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#추가-학습-자료",
    "href": "09_pytorch_model_deployment.html#추가-학습-자료",
    "title": "09 - PyTorch 모델 배포",
    "section": "추가 학습 자료",
    "text": "추가 학습 자료\n\n머신러닝 모델 배포는 일반적으로 순수 머신러닝 과제라기보다는 엔지니어링 과제입니다. 자세한 내용은 PyTorch 추가 리소스 머신러닝 엔지니어링 섹션을 참조하세요.\n\n그곳에서 Chip Huyen의 책 Designing Machine Learning Systems (특히 모델 배포에 관한 7장) 및 Goku Mohandas의 Made with ML MLOps 과정과 같은 리소스를 찾을 수 있습니다.\n\n자신만의 프로젝트를 점점 더 많이 구축함에 따라 Git(및 잠재적으로 GitHub)을 상당히 자주 사용하게 될 것입니다. 둘 다에 대해 더 자세히 알아보려면 freeCodeCamp YouTube 채널의 초보자를 위한 Git 및 GitHub - 집중 과정 동영상을 추천합니다.\n우리는 Gradio로 가능한 기능의 극히 일부만 살펴보았습니다. 더 자세한 내용은 전체 문서를 확인해 보시길 권장하며, 특히 다음 내용을 추천합니다.\n\n모든 다양한 종류의 입력 및 출력 구성 요소.\n더 고급 워크플로우를 위한 Gradio Blocks API.\nHugging Face와 함께 Gradio를 사용하는 방법에 대한 Hugging Face 과정 챕터.\n\n에지 장치는 모바일 폰에만 국한되지 않고 라즈베리 파이와 같은 소형 컴퓨터를 포함하며, PyTorch 팀은 라즈베리 파이에 PyTorch 모델을 배포하는 것에 대한 훌륭한 블로그 게시물 튜토리얼을 제공합니다.\nAI 및 머신러닝 기반 애플리케이션 개발에 대한 훌륭한 가이드는 Google의 People + AI 가이드북을 참조하세요. 제가 가장 좋아하는 섹션 중 하나는 올바른 기대치 설정에 관한 섹션입니다.\n\n저는 Machine Learning Monthly 2021년 4월호 (머신러닝 분야의 최신 소식을 담아 매달 보내드리는 뉴스레터)에서 Apple, Microsoft 등의 가이드를 포함하여 이러한 종류의 리소스를 더 많이 다루었습니다.\n\nCPU에서 모델 실행 속도를 높이고 싶다면, TorchScript, ONNX (Open Neural Network Exchange), OpenVINO에 대해 알아두어야 합니다. 순수 PyTorch 모델에서 ONNX/OpenVINO 모델로 전환하면 성능이 2배 이상 향상되는 것을 보았습니다.\n모델을 배포 및 확장 가능한 API로 변환하려면 TorchServe 라이브러리를 참조하세요.\n브라우저에서 머신러닝 모델을 배포하는 것(에지 배포의 한 형태)이 여러 가지 이점(네트워크 전송 지연 없음)을 제공하는 이유에 대한 훌륭한 예시와 근거는 Jo Kristian Bergum의 기사 Moving ML Inference from the Cloud to the Edge를 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>09 - PyTorch 모델 배포</span>"
    ]
  }
]