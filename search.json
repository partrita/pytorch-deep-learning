[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "íŒŒì´í† ì¹˜ ë”¥ëŸ¬ë‹ ì…ë¬¸",
    "section": "",
    "text": "1 ë”¥ëŸ¬ë‹ì„ ìœ„í•œ PyTorch ë°°ìš°ê¸°\nZero to Mastery ë”¥ëŸ¬ë‹ì„ ìœ„í•œ PyTorch ë°°ìš°ê¸° ê³¼ì •ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ì¸í„°ë„·ì—ì„œ PyTorchë¥¼ ë°°ìš°ê¸°ì— ë‘ ë²ˆì§¸ë¡œ ì¢‹ì€ ê³³ì…ë‹ˆë‹¤(ì²« ë²ˆì§¸ëŠ” PyTorch ì„¤ëª…ì„œì…ë‹ˆë‹¤).",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ë”¥ëŸ¬ë‹ì„ ìœ„í•œ PyTorch ë°°ìš°ê¸°</span>"
    ]
  },
  {
    "objectID": "index.html#ì´-í˜ì´ì§€ì˜-ë‚´ìš©",
    "href": "index.html#ì´-í˜ì´ì§€ì˜-ë‚´ìš©",
    "title": "íŒŒì´í† ì¹˜ ë”¥ëŸ¬ë‹ ì…ë¬¸",
    "section": "1.1 ì´ í˜ì´ì§€ì˜ ë‚´ìš©",
    "text": "1.1 ì´ í˜ì´ì§€ì˜ ë‚´ìš©\n\nê³¼ì • ìë£Œ/ê°œìš”\nì´ ê³¼ì •ì— ëŒ€í•˜ì—¬\nìƒíƒœ (ê³¼ì • ì œì‘ ì§„í–‰ ìƒí™©)\në¡œê·¸ (ê³¼ì • ìë£Œ ì œì‘ ê³¼ì • ë¡œê·¸)",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ë”¥ëŸ¬ë‹ì„ ìœ„í•œ PyTorch ë°°ìš°ê¸°</span>"
    ]
  },
  {
    "objectID": "index.html#ê³¼ì •-ìë£Œê°œìš”",
    "href": "index.html#ê³¼ì •-ìë£Œê°œìš”",
    "title": "íŒŒì´í† ì¹˜ ë”¥ëŸ¬ë‹ ì…ë¬¸",
    "section": "1.2 ê³¼ì • ìë£Œ/ê°œìš”",
    "text": "1.2 ê³¼ì • ìë£Œ/ê°œìš”\n\nğŸ“– ì˜¨ë¼ì¸ ì±… ë²„ì „: ëª¨ë“  ê³¼ì • ìë£ŒëŠ” learnpytorch.ioì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” ì˜¨ë¼ì¸ ì±…ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.\nğŸ¥ YouTubeì˜ ì²« 5ê°œ ì„¹ì…˜: ì²« 25ì‹œê°„ ë¶„ëŸ‰ì˜ ìë£Œë¥¼ ì‹œì²­í•˜ì—¬ í•˜ë£¨ ë§Œì— Pytorchë¥¼ ë°°ìš°ì‹­ì‹œì˜¤.\nğŸ”¬ ê³¼ì • ì´ˆì : ì½”ë“œ, ì½”ë“œ, ì½”ë“œ, ì‹¤í—˜, ì‹¤í—˜, ì‹¤í—˜.\nğŸƒâ€â™‚ï¸ êµìˆ˜ ìŠ¤íƒ€ì¼: https://sive.rs/kimo.\nğŸ¤” ì§ˆë¬¸í•˜ê¸°: ê¸°ì¡´ ì§ˆë¬¸/ìì‹ ì˜ ì§ˆë¬¸ì„ ìœ„í•´ GitHub í† ë¡  í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>ë”¥ëŸ¬ë‹ì„ ìœ„í•œ PyTorch ë°°ìš°ê¸°</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html",
    "href": "00_pytorch_fundamentals.html",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "",
    "text": "2.1 PyTorchë€ ë¬´ì—‡ì¸ê°€ìš”?\nPyTorch is an open source machine learning and deep learning framework.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#pytorchëŠ”-ì–´ë””ì—-ì‚¬ìš©ë˜ë‚˜ìš”",
    "href": "00_pytorch_fundamentals.html#pytorchëŠ”-ì–´ë””ì—-ì‚¬ìš©ë˜ë‚˜ìš”",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.2 PyTorchëŠ” ì–´ë””ì— ì‚¬ìš©ë˜ë‚˜ìš”?",
    "text": "2.2 PyTorchëŠ” ì–´ë””ì— ì‚¬ìš©ë˜ë‚˜ìš”?\nPyTorch allows you to manipulate and process data and write machine learning algorithms using Python code.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ëˆ„ê°€-pytorchë¥¼-ì‚¬ìš©í•˜ë‚˜ìš”",
    "href": "00_pytorch_fundamentals.html#ëˆ„ê°€-pytorchë¥¼-ì‚¬ìš©í•˜ë‚˜ìš”",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.3 ëˆ„ê°€ PyTorchë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?",
    "text": "2.3 ëˆ„ê°€ PyTorchë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?\nì„¸ê³„ ìµœëŒ€ ê¸°ìˆ  ê¸°ì—… ì¤‘ ìƒë‹¹ìˆ˜ê°€ such as Meta (Facebook), Tesla and Microsoft as well as artificial intelligence research companies such as OpenAI use PyTorch to power research and bring machine learning to their products.\n\n\n\npytorch being used across industry and research\n\n\nFor example, Andrej Karpathy (head of AI at Tesla) has given several talks (PyTorch DevCon 2019, Tesla AI Day 2021) about how Tesla use PyTorch to power their self-driving computer vision models.\nPyTorch is also used in other industries such as argiculture to power computer vision on tractors.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ì™œ-pytorchë¥¼-ì‚¬ìš©í•´ì•¼-í•˜ë‚˜ìš”",
    "href": "00_pytorch_fundamentals.html#ì™œ-pytorchë¥¼-ì‚¬ìš©í•´ì•¼-í•˜ë‚˜ìš”",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.4 ì™œ PyTorchë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?",
    "text": "2.4 ì™œ PyTorchë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?\nMachine learning researchers love using PyTorch. And as of February 2022, PyTorch is the most used deep learning framework on Papers With Code, a website for tracking machine learning research papers and the code repositories attached with them.\nPyTorch also helps take care of many things such as GPU acceleration (making your code run faster) behind the scenes.\nSo you can focus on manipulating data and writing algorithms and PyTorch will make sure it runs fast.\nAnd if companies such as Tesla and Meta (Facebook) use it to build models they deploy to power hundreds of applications, drive thousands of cars and deliver content to billions of people, itâ€™s clearly capable on the development front too.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ì´-ëª¨ë“ˆì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "00_pytorch_fundamentals.html#ì´-ëª¨ë“ˆì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.5 ì´ ëª¨ë“ˆì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "2.5 ì´ ëª¨ë“ˆì—ì„œ ë‹¤ë£° ë‚´ìš©\nì´ ê³¼ì •ì€ ì—¬ëŸ¬ ì„¹ì…˜(ë…¸íŠ¸ë¶)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\nEach notebook covers important ideas and concepts within PyTorch.\nSubsequent notebooks build upon knowledge from the previous one (numbering starts at 00, 01, 02 and goes to whatever it ends up going to).\nThis notebook deals with the basic building block of machine learning and deep learning, the tensor.\nSpecifically, weâ€™re going to cover:\n\n\n\nTopic\nContents\n\n\n\n\ní…ì„œ ì†Œê°œ\ní…ì„œs are the basic building block of all of machine learning and deep learning.\n\n\ní…ì„œ ìƒì„±í•˜ê¸°\ní…ì„œs can represent almost any kind of data (images, words, tables of numbers).\n\n\ní…ì„œì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\nIf you can put information into a tensor, youâ€™ll want to get it out too.\n\n\nManipulating tensors\nMachine learning algorithms (like neural networks) involve manipulating tensors in many different ways such as adding, multiplying, combining.\n\n\nDealing with tensor shapes\nOne of the most common issues in machine learning is dealing with shape mismatches (trying to mixed wrong shaped tensors with other tensors).\n\n\nIndexing on tensors\nIf youâ€™ve indexed on a Python list or NumPy array, itâ€™s very similar with tensors, except they can have far more dimensions.\n\n\nMixing PyTorch tensors and NumPy\nPyTorch plays with tensors (torch.í…ì„œ), NumPy likes arrays (np.ndarray) sometimes youâ€™ll want to mix and match these.\n\n\nReproducibility\nMachine learning is very experimental and since it uses a lot of randomness to work, sometimes youâ€™ll want that randomness to not be so random.\n\n\nRunning tensors on GPU\nGPUs (Graphics Processing Units) make your code faster, PyTorch makes it easy to run your code on GPUs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "href": "00_pytorch_fundamentals.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.6 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³",
    "text": "2.6 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³\nAll of the materials for this course live on GitHub.\nAnd if you run into trouble, you can ask a question on the Discussions page there too.\nThereâ€™s also the PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#pytorch-ì„í¬íŠ¸í•˜ê¸°",
    "href": "00_pytorch_fundamentals.html#pytorch-ì„í¬íŠ¸í•˜ê¸°",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.7 PyTorch ì„í¬íŠ¸í•˜ê¸°",
    "text": "2.7 PyTorch ì„í¬íŠ¸í•˜ê¸°\n\nì°¸ê³ : Before running any of the code in this notebook, you should have gone through the PyTorch setup steps.\nHowever, if youâ€™re running on Google Colab, everything should work (Google Colab comes with PyTorch and other libraries installed).\n\nLetâ€™s start by importing PyTorch and checking the version weâ€™re using.\n\nimport torch\ntorch.__version__\n\n'1.10.0+cu111'\n\n\nWonderful, it looks like weâ€™ve got PyTorch 1.10.0 (as of Decemeber 2021). This means if youâ€™re going through these materials, youâ€™ll see most compatability with PyTorch 1.10.0, however if your version number is far higher than that, you might notice some inconsistencies.\nAnd if you do have any issues, please post on the GitHub Discussions page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#í…ì„œ-ì†Œê°œ",
    "href": "00_pytorch_fundamentals.html#í…ì„œ-ì†Œê°œ",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.8 í…ì„œ ì†Œê°œ",
    "text": "2.8 í…ì„œ ì†Œê°œ\nNow weâ€™ve got PyTorch imported, itâ€™s time to learn about tensors.\ní…ì„œs are the fundamental building block of machine learning.\nTheir job is to represent data in a numerical way.\nFor example, you could represent an image as a tensor with shape [3, 224, 224] which would mean [colour_channels, height, width], as in the image has 3 colour channels (red, green, blue), a height of 224 pixels and a width of 224 pixels.\n\n\n\nexample of going from an input image to a tensor representation of the image, image gets broken down into 3 colour channels as well as numbers to represent the height and width\n\n\nIn tensor-speak (the language used to describe tensors), the tensor would have three dimensions, one for colour_channels, height and width.\nBut weâ€™re getting ahead of ourselves.\nLetâ€™s learn more about tensors by coding them.\n\n2.8.1 í…ì„œ ìƒì„±í•˜ê¸°\nPyTorch loves tensors. So much so thereâ€™s a whole documentation page dedicated to the torch.í…ì„œ class.\nYour first piece of homework is to read through the documentation on torch.í…ì„œ for 10-minutes. But you can get to that later.\nLetâ€™s code.\nThe first thing weâ€™re going to create is a scalar.\nA scalar is a single number and in tensor-speak itâ€™s a zero dimension tensor.\n\nì°¸ê³ : Thatâ€™s a trend for this course. Weâ€™ll focus on writing specific code. But often Iâ€™ll set exercises which involve reading and getting familiar with the PyTorch documentaiton. Because after all, once youâ€™re finished this course, youâ€™ll no doubt want to learn more. And the documentaiton is somewhere youâ€™ll be finding yourself quite often.\n\n\n# ìŠ¤ì¹¼ë¼\nscalar = torch.tensor(7)\nscalar\n\ntensor(7)\n\n\nSee how the above printed out tensor(7)?\nThat means although scalar is a single number, itâ€™s of type torch.í…ì„œ.\nWe can check the dimensions of a tensor using the ndim attribute.\n\nscalar.ndim\n\n0\n\n\nWhat if we wanted to retrieve the number from the tensor?\nAs in, turn it from torch.í…ì„œ to a Python integer?\nTo do we can use the item() method.\n\n# Get the Python number within a tensor (only works with one-element tensors)\nscalar.item()\n\n7\n\n\nOkay, now letâ€™s see a vector.\nA vector is a single dimension tensor but can contain many numbers.\nAs in, you could have a vector [3, 2] to describe [bedrooms, bathrooms] in your house. Or you could have [3, 2, 2] to describe [bedrooms, bathrooms, car_parks] in your house.\nThe important trend here is that a vector is flexible in what it can represent (the same with tensors).\n\n# ë²¡í„°\nvector = torch.tensor([7, 7])\nvector\n\ntensor([7, 7])\n\n\nWonderful, vector now contains two 7â€™s, my favourite number.\nHow many dimensions do you think itâ€™ll have?\n\n# Check the number of dimensions of vector\nvector.ndim\n\n1\n\n\nHmm, thatâ€™s strange, vector contains two numbers but only has a single dimension.\nIâ€™ll let you in on a trick.\nYou can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side.\nHow many square brackets does vector have?\nAnother important concept for tensors is their shape attribute. The shape tells you how the elements inside them are arranged.\nLetâ€™s check out the shape of vector.\n\n# Check shape of vector\nvector.shape\n\ntorch.Size([2])\n\n\nThe above returns torch.Size([2]) which means our vector has a shape of [2]. This is because of the two elements we placed inside the square brackets ([7, 7]).\nLetâ€™s now see a matrix.\n\n# í–‰ë ¬\nMATRIX = torch.tensor([[7, 8], \n                       [9, 10]])\nMATRIX\n\ntensor([[ 7,  8],\n        [ 9, 10]])\n\n\nWow! More numbers! Matrices are as flexible as vectors, except theyâ€™ve got an extra dimension.\n\n# Check number of dimensions\nMATRIX.ndim\n\n2\n\n\nMATRIX has two dimensions (did you count the number of square brakcets on the outside of one side?).\nWhat shape do you think it will have?\n\nMATRIX.shape\n\ntorch.Size([2, 2])\n\n\nWe get the output torch.Size([2, 2]) because MATRIX is two elements deep and two elements wide.\nHow about we create a tensor?\n\n# í…ì„œ\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n\ntensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])\n\n\nWoah! What a nice looking tensor.\nI want to stress that tensors can represent almost anything.\nThe one we just created could be the sales numbers for a steak and almond butter store (two of my favourite foods).\n\n\n\na simple tensor in google sheets showing day of week, steak sales and almond butter sales\n\n\nHow many dimensions do you think it has? (hint: use the square bracket counting trick)\n\n# Check number of dimensions for TENSOR\nTENSOR.ndim\n\n3\n\n\nAnd what about its shape?\n\n# Check shape of TENSOR\nTENSOR.shape\n\ntorch.Size([1, 3, 3])\n\n\nAlright, it outputs torch.Size([1, 3, 3]).\nThe dimensions go outer to inner.\nThat means thereâ€™s 1 dimension of 3 by 3.\n\n\n\nexample of different tensor dimensions\n\n\n\nì°¸ê³ : You mightâ€™ve noticed me using lowercase letters for scalar and vector and uppercase letters for MATRIX and TENSOR. This was on purpose. In practice, youâ€™ll often see scalars and vectors denoted as lowercase letters such as y or a. And matrices and tensors denoted as uppercase letters such as X or W.\nYou also might notice the names matrix and tensor used interchangably. This is common. Since in PyTorch youâ€™re often dealing with torch.í…ì„œâ€™s (hence the tensor name), however, the shape and dimensions of whatâ€™s inside will dictate what it actually is.\n\nLetâ€™s summarise.\n\n\n\n\n\n\n\n\n\nName\nWhat is it?\nNumber of dimensions\nLower or upper (usually/example)\n\n\n\n\nscalar\na single number\n0\nLower (a)\n\n\nvector\na number with direction (e.g.Â wind speed with direction) but can also have many other numbers\n1\nLower (y)\n\n\nmatrix\na 2-dimensional array of numbers\n2\nUpper (Q)\n\n\ntensor\nan n-dimensional array of numbers\ncan be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector\nUpper (X)\n\n\n\n\n\n\nscalar vector matrix tensor and what they look like\n\n\n\n\n2.8.2 ë¬´ì‘ìœ„ í…ì„œ\nWeâ€™ve established tensors represent some form of data.\nAnd machine learning models such as neural networks manipulate and seek patterns within tensors.\nBut when building machine learning models with PyTorch, itâ€™s rare youâ€™ll create tenors by hand (like what weâ€™ve being doing).\nInstead, a machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.\nIn essence:\nStart with random numbers -&gt; look at data -&gt; update random numbers -&gt; look at data -&gt; update random numbers...\nAs a data scientist, you can define how the machine learning model starts (initialization), looks at data (representation) and updates (optimization) its random numbers.\nWeâ€™ll get hands on with these steps later on.\nFor now, letâ€™s see how to create a tensor of random numbers.\nWe can do so using torch.rand() and passing in the size parameter.\n\n# Create a random tensor of size (3, 4)\nrandom_tensor = torch.rand(size=(3, 4))\nrandom_tensor, random_tensor.dtype\n\n(tensor([[0.4090, 0.2527, 0.8699, 0.2002],\n         [0.8421, 0.1428, 0.1431, 0.0111],\n         [0.2281, 0.0345, 0.6734, 0.3866]]), torch.float32)\n\n\nThe flexibility of torch.rand() is that we can adjust the size to be whatever we want.\nFor example, say you wanted a random tensor in the common image shape of [224, 224, 3] ([height, width, color_channels]).\n\n# Create a random tensor of size (224, 224, 3)\nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n\n(torch.Size([224, 224, 3]), 3)\n\n\n\n\n2.8.3 0ê³¼ 1\nSometimes youâ€™ll just want to fill tensors with zeros or ones.\nThis happens a lot with masking (like masking some of the values in one tensor with zeros to let a model know not to learn them).\nLetâ€™s create a tensor full of zeros with torch.zeros()\nAgain, the size parameter comes into play.\n\n# Create a tensor of all zeros\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n\n(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]), torch.float32)\n\n\nWe can do the same to create a tensor of all ones except using torch.ones() instead.\n\n# Create a tensor of all ones\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n\n(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]), torch.float32)\n\n\n\n\n2.8.4 ë²”ìœ„ ë° ìœ ì‚¬ í…ì„œ ìƒì„±\nSometimes you might want a range of numbers, such as 1 to 10 or 0 to 100.\nYou can use torch.arange(start, end, step) to do so.\nWhere: * start = start of range (e.g.Â 0) * end = end of range (e.g.Â 10) * step = how many steps in between each value (e.g.Â 1)\n\nì°¸ê³ : In Python, you can use range() to create a range. However in PyTorch, torch.range() is deprecated and may show an error in the future.\n\n\n# Use torch.arange(), torch.range() is deprecated \nzero_to_ten_deprecated = torch.range(0, 10) # ì°¸ê³ : this may return an error in the future\n\n# Create a range of values 0 to 10\nzero_to_ten = torch.arange(start=0, end=10, step=1)\nzero_to_ten\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  \n\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nSometimes you might want one tensor of a certain type with the same shape as another tensor.\nFor example, a tensor of all zeros with the same shape as a previous tensor.\nTo do so you can use torch.zeros_like(input) or torch.ones_like(input) which return a tensor filled with zeros or ones in the same shape as the input respectively.\n\n# Can also create a tensor of zeros similar to another tensor\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n\n2.8.5 í…ì„œ datatypes\nThere are many different tensor datatypes available in PyTorch.\nSome are specific for CPU and some are better for GPU.\nGetting to know which is which can take some time.\nGenerally if you see torch.cuda anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).\nThe most common type (and generally the default) is torch.float32 or torch.float.\nThis is referred to as â€œ32-bit floating pointâ€.\nBut thereâ€™s also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\nAnd to confuse things even more thereâ€™s also 8-bit, 16-bit, 32-bit and 64-bit integers.\nPlus more!\n\nì°¸ê³ : An integer is a flat round number like 7 whereas a float has a decimal 7.0.\n\nThe reason for all of these is to do with precision in computing.\nPrecision is the amount of detail used to describe a number.\nThe higher the precision value (8, 16, 32), the more detail and hence data used to express a number.\nThis matters in deep learning and numerical computing because youâ€™re making so many operations, the more detail you have to calculate on, the more compute you have to use.\nSo lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).\n\nResources: * See the PyTorch documentation for a list of all available tensor datatypes. * Read the Wikipedia page for an overview of what precision in computing is.\n\nLetâ€™s see how to create some tensors with specific datatypes. We can do so using the dtype parameter.\n\n# Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations perfromed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n\n(torch.Size([3]), torch.float32, device(type='cpu'))\n\n\nAside from shape issues (tensor shapes donâ€™t match up), two of the other most common issues youâ€™ll come across in PyTorch are datatype and device issues.\nFor example, one of tensors is torch.float32 and the other is torch.float16 (PyTorch often likes tensors to be the same format).\nOr one of your tensors is on the CPU and the other is on the GPU (PyTorch likes calculations between tensors to be on the same device).\nWeâ€™ll see more of this device talk later on.\nFor now letâ€™s create a tensor with dtype=torch.float16.\n\nfloat_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n\ntorch.float16",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#í…ì„œì—ì„œ-ì •ë³´-ê°€ì ¸ì˜¤ê¸°",
    "href": "00_pytorch_fundamentals.html#í…ì„œì—ì„œ-ì •ë³´-ê°€ì ¸ì˜¤ê¸°",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.9 í…ì„œì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°",
    "text": "2.9 í…ì„œì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\nOnce youâ€™ve created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.\nWeâ€™ve seen these before but three of the most common attributes youâ€™ll want to find out about tensors are: * shape - what shape is the tensor? (some operations require specific shape rules) * dtype - what datatype are the elements within the tensor stored in? * device - what device is the tensor stored on? (usually GPU or CPU)\nLetâ€™s create a random tensor and find out details about it.\n\n# Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n\ntensor([[0.7799, 0.8140, 0.0893, 0.2062],\n        [0.7525, 0.3845, 0.8207, 0.4587],\n        [0.9277, 0.8166, 0.9052, 0.0953]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\nì°¸ê³ : When you run into issues in PyTorch, itâ€™s very often one to do with one of the three attributes above. So when the error messages show up, sing yourself a little song called â€œwhat, what, whereâ€: * â€œwhat shape are my tensors? what datatype are they and where are they stored? what shape, what datatype, where where whereâ€",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#í…ì„œ-ì¡°ì‘-í…ì„œ-ì—°ì‚°",
    "href": "00_pytorch_fundamentals.html#í…ì„œ-ì¡°ì‘-í…ì„œ-ì—°ì‚°",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.10 í…ì„œ ì¡°ì‘ (í…ì„œ ì—°ì‚°)",
    "text": "2.10 í…ì„œ ì¡°ì‘ (í…ì„œ ì—°ì‚°)\nIn deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\nA model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\nThese operations are often a wonderful dance between: * Addition * Subtraction * Multiplication (element-wise) * Division * í–‰ë ¬ multiplication\nAnd thatâ€™s it. Sure there are a few more here and there but these are the basic building blocks of neural networks.\nStacking these building blocks in the right way, you can create the most sophisticated of neural networks (just like lego!).\n\n2.10.1 ê¸°ë³¸ ì—°ì‚°\nLetâ€™s start with a few of the fundamental operations, addition (+), subtraction (-), mutliplication (*).\nThey work just as you think they would.\n\n# Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n\ntensor([11, 12, 13])\n\n\n\n# Multiply it by 10\ntensor * 10\n\ntensor([10, 20, 30])\n\n\nNotice how the tensor values above didnâ€™t end up being tensor([110, 120, 130]), this is because the values inside the tensor donâ€™t change unless theyâ€™re reassigned.\n\n# í…ì„œs don't change unless reassigned\ntensor\n\ntensor([1, 2, 3])\n\n\nLetâ€™s subtract a number and this time weâ€™ll reassign the tensor variable.\n\n# Subtract and reassign\ntensor = tensor - 10\ntensor\n\ntensor([-9, -8, -7])\n\n\n\n# Add and reassign\ntensor = tensor + 10\ntensor\n\ntensor([1, 2, 3])\n\n\nPyTorch also has a bunch of built-in functions like torch.mul() (short for multiplcation) and torch.add() to perform basic operations.\n\n# Can also use torch functions\ntorch.multiply(tensor, 10)\n\ntensor([10, 20, 30])\n\n\n\n# Original tensor is still unchanged \ntensor\n\ntensor([1, 2, 3])\n\n\nHowever, itâ€™s more common to use the operator symbols like * instead of torch.mul()\n\n# Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n\ntensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n\n\n\n\n2.10.2 í–‰ë ¬ multiplication (is all you need)\nOne of the most common operations in machine learning and deep learning algorithms (like neural networks) is matrix multiplication.\nPyTorch implements matrix multiplication functionality in the torch.matmul() method.\nThe main two rules for matrix multiplication to remember are: 1. The inner dimensions must match: * (3, 2) @ (3, 2) wonâ€™t work * (2, 3) @ (3, 2) will work * (3, 2) @ (2, 3) will work 2. The resulting matrix has the shape of the outer dimensions: * (2, 3) @ (3, 2) -&gt; (2, 2) * (3, 2) @ (2, 3) -&gt; (3, 3)\n\nì°¸ê³ : â€œ@â€ in Python is the symbol for matrix multiplication.\n\n\në¦¬ì†ŒìŠ¤: You can see all of the rules for matrix multiplication using torch.matmul() in the PyTorch documentation.\n\nLetâ€™s create a tensor and perform element-wise multiplication and matrix multiplication on it.\n\nimport torch\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n\ntorch.Size([3])\n\n\nThe difference between element-wise multiplication and matrix multiplication is the addition of values.\nFor our tensor variable with values [1, 2, 3]:\n\n\n\n\n\n\n\n\nOperation\nCalculation\nCode\n\n\n\n\nElement-wise multiplication\n[1*1, 2*2, 3*3] = [1, 4, 9]\ntensor * tensor\n\n\ní–‰ë ¬ multiplication\n[1*1 + 2*2 + 3*3] = [14]\ntensor.matmul(tensor)\n\n\n\n\n# Element-wise matrix mutlication\ntensor * tensor\n\ntensor([1, 4, 9])\n\n\n\n# í–‰ë ¬ multiplication\ntorch.matmul(tensor, tensor)\n\ntensor(14)\n\n\n\n# Can also use the \"@\" symbol for matrix multiplication, though not recommended\ntensor @ tensor\n\ntensor(14)\n\n\nYou can do matrix multiplication by hand but itâ€™s not recommended.\nThe in-built torch.matmul() method is faster.\n\n%%time\n# í–‰ë ¬ multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n\nCPU times: user 146 Âµs, sys: 38 Âµs, total: 184 Âµs\nWall time: 227 Âµs\n\n\n\n%%time\ntorch.matmul(tensor, tensor)\n\nCPU times: user 27 Âµs, sys: 7 Âµs, total: 34 Âµs\nWall time: 36.7 Âµs\n\n\ntensor(14)",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ë”¥ëŸ¬ë‹ì—ì„œ-ê°€ì¥-í”í•œ-ì˜¤ë¥˜-ì¤‘-í•˜ë‚˜-í˜•íƒœ-ì˜¤ë¥˜",
    "href": "00_pytorch_fundamentals.html#ë”¥ëŸ¬ë‹ì—ì„œ-ê°€ì¥-í”í•œ-ì˜¤ë¥˜-ì¤‘-í•˜ë‚˜-í˜•íƒœ-ì˜¤ë¥˜",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.11 ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ í”í•œ ì˜¤ë¥˜ ì¤‘ í•˜ë‚˜ (í˜•íƒœ ì˜¤ë¥˜)",
    "text": "2.11 ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ í”í•œ ì˜¤ë¥˜ ì¤‘ í•˜ë‚˜ (í˜•íƒœ ì˜¤ë¥˜)\nBecause much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors youâ€™ll run into in deep learning is shape mismatches.\n\n# Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-37-aceec990e652&gt; in &lt;module&gt;()\n      8                          [9, 12]], dtype=torch.float32)\n      9 \n---&gt; 10 torch.matmul(tensor_A, tensor_B) # (this will error)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)\n\n\n\nWe can make matrix multiplication work between tensor_A and tensor_B by making their inner dimensions match.\nOne of the ways to do this is with a transpose (switch the dimensions of a given tensor).\nYou can perform transposes in PyTorch using either: * torch.transpose(input, dim0, dim1) - where input is the desired tensor to transpose and dim0 and dim1 are the dimensions to be swapped. * tensor.T - where tensor is the desired tensor to transpose.\nLetâ€™s try the latter.\n\n# View tensor_A and tensor_B\nprint(tensor_A)\nprint(tensor_B)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7., 10.],\n        [ 8., 11.],\n        [ 9., 12.]])\n\n\n\n# View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n\ntensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n\n\n\n# The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n\nOriginal shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n\n\nYou can also use torch.mm() which is a short for torch.matmul().\n\n# torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\n\nWithout the transpose, the rules of matrix mulitplication arenâ€™t fulfilled and we get an error like above.\nHow about a visual?\n\n\n\nvisual demo of matrix multiplication\n\n\nYou can create your own matrix multiplication visuals like this at http://matrixmultiplication.xyz/.\n\nì°¸ê³ : A matrix multiplication like this is also referred to as the dot product of two matrices.\n\nNeural networks are full of matrix multiplications and dot products.\nThe torch.nn.Linear() module (weâ€™ll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input x and a weights matrix A.\n\\[\ny = x\\cdot{A^T} + b\n\\]\nWhere: * x is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other). * A is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the â€œTâ€, thatâ€™s because the weights matrix gets transposed). * ì°¸ê³ : You might also often see W or another letter like X used to showcase the weights matrix. * b is the bias term used to slightly offset the weights and inputs. * y is the output (a manipulation of the input in the hopes to discover patterns in it).\nThis is a linear function (you may have seen something like \\(y = mx+b\\) in high school or elsewhere), and can be used to draw a straight line!\nLetâ€™s play around with a linear layer.\nTry changing the values of in_features and out_features below and see what happens.\nDo you notice anything to do with the shapes?\n\n# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix mutliplcation\nlinear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n\nInput shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n\n\n\nì§ˆë¬¸: What happens if you change in_features from 2 to 3 above? Does it error? How could you change the shape of the input (x) to accomodate to the error? Hint: what did we have to do to tensor_B above?\n\nIf youâ€™ve never done it before, matrix multiplication can be a confusing topic at first.\nBut after youâ€™ve played around with it a few times and even cracked open a few neural networks, youâ€™ll notice itâ€™s everywhere.\nRemember, matrix multiplication is all you need.\n\n\n\nmatrix multiplication is all you need\n\n\nWhen you start digging into neural network layers and building your own, youâ€™ll find matrix multiplications everywhere. Source: https://marksaroufim.substack.com/p/working-class-deep-learner\n\n2.11.1 ìµœì†Œ, ìµœëŒ€, í‰ê· , í•©ê³„ ë“± ì°¾ê¸° (ì§‘ê³„)\nNow weâ€™ve seen a few ways to manipulate tensors, letâ€™s run through a few ways to aggregate them (go from more values to less values).\nFirst weâ€™ll create a tensor and then find the max, min, mean and sum of it.\n\n# Create a tensor\nx = torch.arange(0, 100, 10)\nx\n\ntensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n\n\nNow letâ€™s perform some aggregation.\n\nprint(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n\nMinimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n\n\n\nì°¸ê³ : You may find some methods such as torch.mean() require tensors to be in torch.float32 (the most common) or another specific datatype, otherwise the operation will fail.\n\nYou can also do the same as above with torch methods.\n\ntorch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n\n(tensor(90), tensor(0), tensor(45.), tensor(450))\n\n\n\n\n2.11.2 ìœ„ì¹˜ë³„ ìµœì†Œ/ìµœëŒ€\nYou can also find the index of a tensor where the max or minimum occurs with torch.argmax() and torch.argmin() respectively.\nThis is helpful incase you just want the position where the highest (or lowest) value is and not the actual value itself (weâ€™ll see this in a later section when using the softmax activation function).\n\n# Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"í…ì„œ: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n\nTensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n\n\n\n\n2.11.3 í…ì„œ ë°ì´í„° íƒ€ì… ë³€ê²½\nAs mentioned, a common issue with deep learning operations is having your tensors in different datatypes.\nIf one tensor is in torch.float64 and another is in torch.float32, you might run into some errors.\nBut thereâ€™s a fix.\nYou can change the datatypes of tensors using torch.í…ì„œ.type(dtype=None) where the dtype parameter is the datatype youâ€™d like to use.\nFirst weâ€™ll create a tensor and check itâ€™s datatype (the default is torch.float32).\n\n# Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n\ntorch.float32\n\n\nNow weâ€™ll create another tensor the same as before but change its datatype to torch.float16.\n\n# Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n\ntensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)\n\n\nAnd we can do something similar to make a torch.int8 tensor.\n\n# Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n\ntensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)\n\n\n\nì°¸ê³ : Different datatypes can be confusing to begin with. But think of it like this, the lower the number (e.g.Â 32, 16, 8), the less precise a computer stores the value. And with a lower amount of storage, this generally results in faster computation and a smaller overall model. Mobile-based neural networks often operate with 8-bit integers, smaller and faster to run but less accurate than their float32 counterparts. For more on this, Iâ€™d read up about precision in computing.\n\n\nExercise: So far weâ€™ve covered a fair few tensor methods but thereâ€™s a bunch more in the torch.í…ì„œ documentation, Iâ€™d recommend spending 10-minutes scrolling through and looking into any that catch your eye. Click on them and then write them out in code yourself to see what happens.\n\n\n\n2.11.4 Reshaping, stacking, squeezing and unsqueezing\nOften times youâ€™ll want to reshape or change the dimensions of your tensors without actually changing the values inside them.\nTo do so, some popular methods are:\n\n\n\nMethod\nOne-line description\n\n\n\n\ntorch.reshape(input, shape)\nReshapes input to shape (if compatible), can also use torch.í…ì„œ.reshape().\n\n\ntorch.í…ì„œ.view(shape)\nReturns a view of the original tensor in a different shape but shares the same data as the original tensor.\n\n\ntorch.stack(tensors, dim=0)\nConcatenates a sequence of tensors along a new dimension (dim), all tensors must be same size.\n\n\ntorch.squeeze(input)\nSqueezes input to remove all the dimenions with value 1.\n\n\ntorch.unsqueeze(input, dim)\nReturns input with a dimension value of 1 added at dim.\n\n\ntorch.permute(input, dims)\nReturns a view of the original input with its dimensions permuted (rearranged) to dims.\n\n\n\nWhy do any of these?\nBecause deep learning models (neural networks) are all about manipulating tensors in some way. And because of the rules of matrix multiplication, if youâ€™ve got shape mismatches, youâ€™ll run into errors. These methods help you make the right elements of your tensors are mixing with the right elements of other tensors.\nLetâ€™s try them out.\nFirst, weâ€™ll create a tensor.\n\n# Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n\n(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))\n\n\nNow letâ€™s add an extra dimension with torch.reshape().\n\n# Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\nWe can also change the view with torch.view().\n\n# Change view (keeps same data as original but changes view)\n# See more: https://stackoverflow.com/a/54507446/7900723\nz = x.view(1, 7)\nz, z.shape\n\n(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))\n\n\nRemember though, changing the view of a tensor with torch.view() really only creates a new view of the same tensor.\nSo changing the view changes the original tensor too.\n\n# Changing z changes x\nz[:, 0] = 5\nz, x\n\n(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))\n\n\nIf we wanted to stack our new tensor on top of itself four times, we could do so with torch.stack().\n\n# Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n\ntensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])\n\n\nHow about removing all single dimensions from a tensor?\nTo do so you can use torch.squeeze() (I remember this as squeezing the tensor to only have dimensions over 1).\n\nprint(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n\nPrevious tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n\n\nAnd to do the reverse of torch.squeeze() you can use torch.unsqueeze() to add a dimension value of 1 at a specific index.\n\nprint(f\"Previous tensor: {x_squeezed}\")\nprint(f\"Previous shape: {x_squeezed.shape}\")\n\n## Add an extra dimension with unsqueeze\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"\\nNew tensor: {x_unsqueezed}\")\nprint(f\"New shape: {x_unsqueezed.shape}\")\n\nPrevious tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nPrevious shape: torch.Size([7])\n\nNew tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nNew shape: torch.Size([1, 7])\n\n\nYou can also rearrange the order of axes values with torch.permute(input, dims), where the input gets turned into a view with new dims.\n\n# Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n\nPrevious shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n\n\n\nNote: Because permuting returns a view (shares the same data as the original), the values in the permuted tensor will be the same as the original tensor and if you change the values in the view, it will change the values of the original.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#indexing-selecting-data-from-tensors",
    "href": "00_pytorch_fundamentals.html#indexing-selecting-data-from-tensors",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.12 Indexing (selecting data from tensors)",
    "text": "2.12 Indexing (selecting data from tensors)\nSometimes youâ€™ll want to select specific data from tensors (for example, only the first column or second row).\nTo do so, you can use indexing.\nIf youâ€™ve ever done indexing on Python lists or NumPy arrays, indexing in PyTorch with tensors is very similar.\n\n# Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n\n(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]), torch.Size([1, 3, 3]))\n\n\nIndexing values goes outer dimension -&gt; inner dimension (check out the square brackets).\n\n# Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n\nFirst square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n\n\nYou can also use : to specify â€œall values in this dimensionâ€ and then use a comma (,) to add another dimension.\n\n# Get all values of 0th dimension and the 0 index of 1st dimension\nx[:, 0]\n\ntensor([[1, 2, 3]])\n\n\n\n# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n\ntensor([[2, 5, 8]])\n\n\n\n# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\nx[:, 1, 1]\n\ntensor([5])\n\n\n\n# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \nx[0, 0, :] # same as x[0][0]\n\ntensor([1, 2, 3])\n\n\nIndexing can be quite confusing to begin with, especially with larger tensors (I still have to try indexing multiple times to get it right). But with a bit of practice and following the data explorerâ€™s motto (visualize, visualize, visualize), youâ€™ll start to get the hang of it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#pytorch-tensors-numpy",
    "href": "00_pytorch_fundamentals.html#pytorch-tensors-numpy",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.13 PyTorch tensors & NumPy",
    "text": "2.13 PyTorch tensors & NumPy\nSince NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.\nThe two main methods youâ€™ll want to use for NumPy to PyTorch (and back again) are: * torch.from_numpy(ndarray) - NumPy array -&gt; PyTorch tensor. * torch.í…ì„œ.numpy() - PyTorch tensor -&gt; NumPy array.\nLetâ€™s try them out.\n\n# NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n\n(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\n\nì°¸ê³ : By default, NumPy arrays are created with the datatype float64 and if you convert it to a PyTorch tensor, itâ€™ll keep the same datatype (as above).\nHowever, many PyTorch calculations default to using float32.\nSo if you want to convert your NumPy array (float64) -&gt; PyTorch tensor (float64) -&gt; PyTorch tensor (float32), you can use tensor = torch.from_numpy(array).type(torch.float32).\n\nBecause we reassigned tensor above, if you change the tensor, the array stays the same.\n\n# Change the array, keep the tensor\narray = array + 1\narray, tensor\n\n(array([2., 3., 4., 5., 6., 7., 8.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))\n\n\nAnd if you want to go from PyTorch tensor to NumPy array, you can call tensor.numpy().\n\n# í…ì„œ to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n\n(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))\n\n\nAnd the same rule applies as above, if you change the original tensor, the new numpy_tensor stays the same.\n\n# Change the tensor, keep the array the same\ntensor = tensor + 1\ntensor, numpy_tensor\n\n(tensor([2., 2., 2., 2., 2., 2., 2.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ì¬í˜„ì„±-ë¬´ì‘ìœ„ì„±-ì œì–´í•˜ê¸°",
    "href": "00_pytorch_fundamentals.html#ì¬í˜„ì„±-ë¬´ì‘ìœ„ì„±-ì œì–´í•˜ê¸°",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.14 ì¬í˜„ì„± (ë¬´ì‘ìœ„ì„± ì œì–´í•˜ê¸°)",
    "text": "2.14 ì¬í˜„ì„± (ë¬´ì‘ìœ„ì„± ì œì–´í•˜ê¸°)\nAs you learn more about neural networks and machine learning, youâ€™ll start to discover how much randomness plays a part.\nWell, pseudorandomness that is. Because after all, as theyâ€™re designed, a computer is fundamentally deterministic (each step is predictable) so the randomness they create are simulated randomness (though there is debate on this too, but since Iâ€™m not a computer scientist, Iâ€™ll let you find out more yourself).\nHow does this relate to neural networks and deep learning then?\nWeâ€™ve discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we havenâ€™t discussed yet) to better describe patterns in data.\nIn short:\nstart with random numbers -&gt; tensor operations -&gt; try to make better (again and again and again)\nAlthough randomness is nice and powerful, sometimes youâ€™d like there to be a little less randomness.\nWhy?\nSo you can perform repeatable experiments.\nFor example, you create an algorithm capable of achieving X performance.\nAnd then your friend tries it out to verify youâ€™re not crazy.\nHow could they do such a thing?\nThatâ€™s where reproducibility comes in.\nIn other words, can you get the same (or very similar) results on your computer running the same code as I get on mine?\nLetâ€™s see a brief example of reproducibility in PyTorch.\nWeâ€™ll start by creating two random tensors, since theyâ€™re random, youâ€™d expect them to be different right?\n\nimport torch\n\n# Create two random tensors\nrandom_tensor_A = torch.rand(3, 4)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"í…ì„œ A:\\n{random_tensor_A}\\n\")\nprint(f\"í…ì„œ B:\\n{random_tensor_B}\\n\")\nprint(f\"Does í…ì„œ A equal í…ì„œ B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n\nTensor A:\ntensor([[0.8016, 0.3649, 0.6286, 0.9663],\n        [0.7687, 0.4566, 0.5745, 0.9200],\n        [0.3230, 0.8613, 0.0919, 0.3102]])\n\nTensor B:\ntensor([[0.9536, 0.6002, 0.0351, 0.6826],\n        [0.3743, 0.5220, 0.1336, 0.9666],\n        [0.9754, 0.8474, 0.8988, 0.1105]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n\n\ntensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])\n\n\nJust as you mightâ€™ve expected, the tensors come out with different values.\nBut what if you wanted to created two random tensors with the same values.\nAs in, the tensors would still contain random values but they would be of the same flavour.\nThatâ€™s where torch.manual_seed(seed) comes in, where seed is an integer (like 42 but it could be anything) that flavours the randomness.\nLetâ€™s try it out by creating some more flavoured random tensors.\n\nimport torch\nimport random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"í…ì„œ C:\\n{random_tensor_C}\\n\")\nprint(f\"í…ì„œ D:\\n{random_tensor_D}\\n\")\nprint(f\"Does í…ì„œ C equal í…ì„œ D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n\nTensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n\n\ntensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])\n\n\nNice!\nIt looks like setting the seed worked.\n\në¦¬ì†ŒìŠ¤: What weâ€™ve just covered only scratches the surface of reproducibility in PyTorch. For more, on reproducbility in general and random seeds, Iâ€™d checkout: * The PyTorch reproducibility documentation (a good exericse would be to read through this for 10-minutes and even if you donâ€™t understand it now, being aware of it is important). * The Wikipedia random seed page (thisâ€™ll give a good overview of random seeds and pseudorandomness in general).",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#gpuì—ì„œ-í…ì„œ-ì‹¤í–‰í•˜ê¸°-ë°-ê³„ì‚°-ê°€ì†í™”",
    "href": "00_pytorch_fundamentals.html#gpuì—ì„œ-í…ì„œ-ì‹¤í–‰í•˜ê¸°-ë°-ê³„ì‚°-ê°€ì†í™”",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.15 GPUì—ì„œ í…ì„œ ì‹¤í–‰í•˜ê¸° (ë° ê³„ì‚° ê°€ì†í™”)",
    "text": "2.15 GPUì—ì„œ í…ì„œ ì‹¤í–‰í•˜ê¸° (ë° ê³„ì‚° ê°€ì†í™”)\nDeep learning algorithms require a lot of numerical operations.\nAnd by default these operations are often done on a CPU (computer processing unit).\nHowever, thereâ€™s another common piece of hardware called a GPU (graphics processing unit), which is often much faster at performing the specific types of operations neural networks need (matrix multiplications) than CPUs.\nYour computer might have one.\nIf so, you should look to use it whenever you can to train neural networks because chances are itâ€™ll speed up the training time dramatically.\nThere are a few ways to first get access to a GPU and secondly get PyTorch to use the GPU.\n\nì°¸ê³ : When I reference â€œGPUâ€ throughout this course, Iâ€™m referencing a Nvidia GPU with CUDA enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing & not just graphics) unless otherwise specified.\n\n\n2.15.1 1. Getting a GPU\nYou may already know whatâ€™s going on when I say GPU. But if not, there are a few ways to get access to one.\n\n\n\nMethod\nDifficulty to setup\nPros\nCons\nHow to setup\n\n\n\n\nGoogle Colab\nEasy\nFree to use, almost zero setup required, can share work with others as easy as a link\nDoesnâ€™t save your data outputs, limited compute, subject to timeouts\nFollow the Google Colab Guide\n\n\nUse your own\nMedium\nRun everything locally on your own machine\nGPUs arenâ€™t free, require upfront cost\nFollow the PyTorch installation guidelines\n\n\nCloud computing (AWS, GCP, Azure)\nMedium-Hard\nSmall upfront cost, access to almost infinite compute\nCan get expensive if running continually, takes some time to setup right\nFollow the PyTorch installation guidelines\n\n\n\nThere are more options for using GPUs but the above three will suffice for now.\nPersonally, I use a combination of Google Colab and my own personal computer for small scale experiments (and creating this course) and go to cloud resources when I need more compute power.\n\në¦¬ì†ŒìŠ¤: If youâ€™re looking to purchase a GPU of your own but not sure what to get, Tim Dettmers has an excellent guide.\n\nTo check if youâ€™ve got access to a Nvidia GPU, you can run !nvidia-smi where the ! (also called bang) means â€œrun this on the command lineâ€.\n\n!nvidia-smi\n\nThu Feb 10 02:09:18 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\nIf you donâ€™t have a Nvidia GPU accessible, the above will output something like:\nNVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\nIn that case, go back up and follow the install steps.\nIf you do have a GPU, the line above will output something like:\nWed Jan 19 22:09:08 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n2.15.2 2. Getting PyTorch to run on the GPU\nOnce youâ€™ve got a GPU ready to access, the next step is getting PyTorch to use for storing data (tensors) and computing on data (performing operations on tensors).\nTo do so, you can use the torch.cuda package.\nRather than talk about it, letâ€™s try it out.\nYou can test if PyTorch has access to a GPU using torch.cuda.is_available().\n\n# Check for GPU\nimport torch\ntorch.cuda.is_available()\n\nTrue\n\n\nIf the above outputs True, PyTorch can see and use the GPU, if it outputs False, it canâ€™t see the GPU and in that case, youâ€™ll have to go back through the installation steps.\nNow, letâ€™s say you wanted to setup your code so it ran on CPU or the GPU if it was available.\nThat way, if you or someone decides to run your code, itâ€™ll work regardless of the computing device theyâ€™re using.\nLetâ€™s create a device variable to store what kind of device is available.\n\n# Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\nIf the above output \"cuda\" it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output \"cpu\", our PyTorch code will stick with the CPU.\n\nì°¸ê³ : In PyTorch, itâ€™s best practice to write device agnostic code. This means code thatâ€™ll run on CPU (always available) or GPU (if available).\n\nIf you want to do faster computing you can use a GPU but if you want to do much faster computing, you can use multiple GPUs.\nYou can count the number of GPUs PyTorch has access to using torch.cuda.device_count().\n\n# Count number of devices\ntorch.cuda.device_count()\n\n1\n\n\nKnowing the number of GPUs PyTorch has access to is helpful incase you wanted to run a specific process on one GPU and another process on another (PyTorch also has features to let you run a process across all GPUs).\n\n\n2.15.3 3. Putting tensors (and models) on the GPU\nYou can put tensors (and models, weâ€™ll see this later) on a specific device by calling to(device) on them. Where device is the target device youâ€™d like the tensor (or model) to go to.\nWhy do this?\nGPUs offer far faster numerical computing than CPUs do and if a GPU isnâ€™t available, because of our device agnostic code (see above), itâ€™ll run on the CPU.\n\nì°¸ê³ : Putting a tensor on GPU using to(device) (e.g.Â some_tensor.to(device)) returns a copy of that tensor, e.g.Â the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:\nsome_tensor = some_tensor.to(device)\n\nLetâ€™s try creating a tensor and putting it on the GPU (if itâ€™s available).\n\n# Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# í…ì„œ not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n\ntensor([1, 2, 3]) cpu\n\n\ntensor([1, 2, 3], device='cuda:0')\n\n\nIf you have a GPU available, the above code will output something like:\ntensor([1, 2, 3]) cpu\ntensor([1, 2, 3], device='cuda:0')\nNotice the second tensor has device='cuda:0', this means itâ€™s stored on the 0th GPU available (GPUs are 0 indexed, if two GPUs were available, theyâ€™d be 'cuda:0' and 'cuda:1' respectively, up to 'cuda:n').\n\n\n2.15.4 4. Moving tensors back to the CPU\nWhat if we wanted to move the tensor back to CPU?\nFor example, youâ€™ll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).\nLetâ€™s try using the torch.í…ì„œ.numpy() method on our tensor_on_gpu.\n\n# If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-75-53175578f49e&gt; in &lt;module&gt;()\n      1 # If tensor is on GPU, can't transform it to NumPy (this will error)\n----&gt; 2 tensor_on_gpu.numpy()\n\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n\n\n\nInstead, to get a tensor back to CPU and usable with NumPy we can use í…ì„œ.cpu().\nThis copies the tensor to CPU memory so itâ€™s usable with CPUs.\n\n# Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n\narray([1, 2, 3])\n\n\nThe above returns a copy of the GPU tensor in CPU memory so the original tensor is still on GPU.\n\ntensor_on_gpu\n\ntensor([1, 2, 3], device='cuda:0')",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "00_pytorch_fundamentals.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.16 ì—°ìŠµ ë¬¸ì œ",
    "text": "2.16 ì—°ìŠµ ë¬¸ì œ\n\nDocumentation reading - A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework youâ€™re using. Weâ€™ll be using the PyTorch documentation a lot throughout the rest of this course. So Iâ€™d recommend spending 10-minutes reading the following (itâ€™s okay if you donâ€™t get some things for now, the focus is not yet full understanding, itâ€™s awareness):\n\n\nThe documentation on torch.í…ì„œ.\nThe documentation on torch.cuda.\n\n\nCreate a random tensor with shape (7, 7).\nPerform a matrix multiplication on the tensor from 2 with another random tensor with shape (1, 7) (hint: you may have to transpose the second tensor).\nSet the random seed to 0 and do 2 & 3 over again. The output should be:\n\n(tensor([[1.8542],\n         [1.9611],\n         [2.2884],\n         [3.0481],\n         [1.7067],\n         [2.5290],\n         [1.7989]]), torch.Size([7, 1]))\n\nSpeaking of random seeds, we saw how to set it with torch.manual_seed() but is there a GPU equivalent? (hint: youâ€™ll need to look into the documentation for torch.cuda for this one)\n\n\nIf there is, set the GPU random seed to 1234.\n\n\nCreate two random tensors of shape (2, 3) and send them both to the GPU (youâ€™ll need access to a GPU for this). Set torch.manual_seed(1234) when creating the tensors (this doesnâ€™t have to be the GPU random seed). The output should be something like:\n\nDevice: cuda\n(tensor([[0.0290, 0.4019, 0.2598],\n         [0.3666, 0.0583, 0.7006]], device='cuda:0'),\n tensor([[0.0518, 0.4681, 0.6738],\n         [0.3315, 0.7837, 0.5631]], device='cuda:0'))\n\nPerform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors).\nFind the maximum and minimum values of the output of 7.\nFind the maximum and minimum index values of the output of 7.\nMake a random tensor with shape (1, 1, 1, 10) and then create a new tensor with all the 1 dimensions removed to be left with a tensor of shape (10). Set the seed to 7 when you create it and print out the first tensor and itâ€™s shape as well as the second tensor and itâ€™s shape. The output should look something like:\n\ntensor([[[[0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297,\n           0.3653, 0.8513]]]]) torch.Size([1, 1, 1, 10])\ntensor([0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297, 0.3653,\n        0.8513]) torch.Size([10])\n\në¦¬ì†ŒìŠ¤: To complete these exercises, see the exercises notebooks templates and potential solutions on the course GitHub.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "00_pytorch_fundamentals.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "00_pytorch_fundamentals.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "2Â  00 - PyTorch ê¸°ì´ˆ",
    "section": "2.17 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "2.17 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nSpend 1-hour going through the PyTorch basics tutorial (Iâ€™d recommend the Quickstart and í…ì„œs sections).\nTo learn more on how a tensor can represent data, see this video: Whatâ€™s a tensor?",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>00 - PyTorch ê¸°ì´ˆ</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html",
    "href": "01_pytorch_workflow.html",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "",
    "text": "3.1 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nThe essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discoverd patterns to predict the future.\nThere are many ways to do this and many new ways are being discovered all the time.\nBut letâ€™s start small.\nHow about we start with a straight line?\nAnd we see if we can build a model with PyTorch to that straight line.\nIn this module weâ€™re going to cover a standard PyTorch workflow (it can be chopped and changed as necessary but it covers the main outline of steps).\nFor now, weâ€™ll use this workflow to predict a simple straight line but the workflow steps can be repeated and changed depending on the problem youâ€™re working on.\nSpecifically, weâ€™re going to cover:",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "01_pytorch_workflow.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "",
    "text": "Topic\nContents\n\n\n\n\n1. ë°ì´í„° ì¤€ë¹„í•˜ê¸°\nData can be almost anything but to get started weâ€™re going to create a simple straight line\n\n\n2. ëª¨ë¸ êµ¬ì¶•í•˜ê¸°\nHere weâ€™ll create a model to learn patterns in the data, weâ€™ll also choose a loss function, optimizer and build a training loop.\n\n\n3. ë°ì´í„°ì— ëª¨ë¸ ë§ì¶”ê¸° (í›ˆë ¨)\nWeâ€™ve got data and a model, now letâ€™s let the model (try to) find patterns in the (training) data.\n\n\n4. ì˜ˆì¸¡ ë° ëª¨ë¸ í‰ê°€ (ì¶”ë¡ )\nOur modelâ€™s found patterns in the data, letâ€™s compare its findings to the actual (testing) data.\n\n\n5. ëª¨ë¸ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°\nYou may want to use your model elsewhere, or come back to it later, here weâ€™ll cover that.\n\n\n6. ì „ì²´ ê³¼ì • í•©ì¹˜ê¸°\nLetâ€™s take all of the above and combine it.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "href": "01_pytorch_workflow.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.2 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³",
    "text": "3.2 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³\nAll of the materials for this course are available on GitHub.\nAnd if you run into trouble, you can ask a question on the Discussions page there too.\nThereâ€™s also the PyTorch developer forums, a very helpful place for all things PyTorch.\nLetâ€™s start by putting what weâ€™re covering into a dictionary to reference later.\n\nwhat_were_covering = {1: \"data (prepare and load)\",\n    2: \"build model\",\n    3: \"fitting the model to data (training)\",\n    4: \"making predictions and evaluating a model (inference)\",\n    5: \"saving and loading a model\",\n    6: \"putting it all together\"\n}\n\nAnd now letâ€™s import what weâ€™ll need for this module.\nWeâ€™re going to get torch, torch.nn (nn stands for neural network and this package contains the building blocks for creating neural networks in PyTorch) and matplotlib.\n\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.11.0'",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#data-preparing-and-loading",
    "href": "01_pytorch_workflow.html#data-preparing-and-loading",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.3 1. Data (preparing and loading)",
    "text": "3.3 1. Data (preparing and loading)\nI want to stress that â€œdataâ€ in machine learning can be almost anything you can imagine. A table of numbers (like a big Excel spreadsheet), images of any kind, videos (YouTube has lots of data!), audio files like songs or podcasts, protein structures, text and more.\n\n\n\nmachine learning is a game of two parts: 1. turn your data into a representative set of numbers and 2. build or pick a model to learn the representation as best as possible\n\n\në¨¸ì‹ ëŸ¬ë‹ì€ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤: 1. ë°ì´í„°ê°€ ë¬´ì—‡ì´ë“  ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤(í‘œí˜„). 2. í•´ë‹¹ í‘œí˜„ì„ ê°€ì¥ ì˜ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ êµ¬ì¶•í•©ë‹ˆë‹¤.\nSometimes one and two can be done at the same time.\nBut what if you donâ€™t have data?\nWell, thatâ€™s where weâ€™re at now.\nNo data.\nBut we can create some.\nLetâ€™s create our data as a straight line.\nWeâ€™ll use linear regression to create the data with known parameters (things that can be learned by a model) and then weâ€™ll use PyTorch to see if we can build model to estimate these parameters using gradient descent.\nDonâ€™t worry if the terms above donâ€™t mean much now, weâ€™ll see them in action and Iâ€™ll put extra resources below where you can learn more.\n\n# Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\nBeautiful! Now weâ€™re going to move towards building a model that can learn the relationship between X (features) and y (labels).\n\n3.3.1 ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ê¸°\nWeâ€™ve got some data.\nBut before we build a model we need to split it up.\nOne of most important steps in a machine learning project is creating a training and test set (and when required, a validation set).\nEach split of the dataset serves a specific purpose:\n\n\n\n\n\n\n\n\n\nSplit\nPurpose\nAmount of total data\nHow often is it used?\n\n\n\n\nTraining set\nThe model learns from this data (like the course materials you study during the semester).\n~60-80%\nAlways\n\n\nValidation set\nThe model gets tuned on this data (like the practice exam you take before the final exam).\n~10-20%\nOften but not always\n\n\nTesting set\nThe model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester).\n~10-20%\nAlways\n\n\n\nFor now, weâ€™ll just use a training and test set, this means weâ€™ll have a dataset for our model to learn on as well as be evaluated on.\nWe can create them by splitting our X and y tensors.\n\nì°¸ê³ : When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.\n\n\n# Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\nWonderful, weâ€™ve got 40 samples for training (X_train & y_train) and 10 samples for testing (X_test & y_test).\nThe model we create is going to try and learn the relationship between X_train & y_train and then we will evaluate what it learns on X_test and y_test.\nBut right now our data is just numbers on a page.\nLetâ€™s create a function to visualize it.\n\ndef plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n  \n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n\n\nplot_predictions();\n\n\n\n\n\n\n\n\nEpic!\nNow instead of just being numbers on a page, our data is a straight line.\n\nì°¸ê³ : Nowâ€™s a good time to introduce you to the data explorerâ€™s mottoâ€¦ â€œvisualize, visualize, visualize!â€\nThink of this whenever youâ€™re working with data and turning it into numbers, if you can visualize something, it can do wonders for understanding.\nMachines love numbers and we humans like numbers too but we also like to look at things.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#build-model",
    "href": "01_pytorch_workflow.html#build-model",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.4 2. Build model",
    "text": "3.4 2. Build model\nNow weâ€™ve got some data, letâ€™s build a model to use the blue dots to predict the green dots.\nWeâ€™re going to jump right in.\nWeâ€™ll write the code first and then explain everything.\nLetâ€™s replicate a standard linear regression model using pure PyTorch.\n\n# Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n            requires_grad=True, # &lt;- can we update this value with gradient descent?\n            dtype=torch.float # &lt;- PyTorch loves float32 by default\n        ))\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n            requires_grad=True, # &lt;- can we update this value with gradient descent?\n            dtype=torch.float # &lt;- PyTorch loves float32 by default\n        ))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n\nAlright thereâ€™s a fair bit going on above but letâ€™s break it down bit by bit.\n\në¦¬ì†ŒìŠ¤: Weâ€™ll be using Python classes to create bits and pieces for building neural networks. If youâ€™re unfamiliar with Python class notation, Iâ€™d recommend reading Real Pythonâ€™s Object Orientating programming in Python 3 guide a few times.\n\n\n3.4.1 PyTorch model building essentials\nPyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.\nThey are torch.nn, torch.optim, torch.utils.data.Dataset and torch.utils.data.DataLoader. For now, weâ€™ll focus on the first two and get to the other two later (though you may be able to guess what they do).\n\n\n\nPyTorch module\nWhat does it do?\n\n\n\n\ntorch.nn\nContains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way).\n\n\ntorch.nn.Parameter\nStores tensors that can be used with nn.Module. If requires_grad=True gradients (used for updating model parameters via gradient descent) are calculated automatically, this is often referred to as â€œautogradâ€.\n\n\ntorch.nn.Module\nThe base class for all neural network modules, all the building blocks for neural networks are subclasses. If youâ€™re building a neural network in PyTorch, your models should subclass nn.Module. Requires a forward() method be implemented.\n\n\ntorch.optim\nContains various optimization algorithms (these tell the model parameters stored in nn.Parameter how to best change to improve gradient descent and in turn reduce the loss).\n\n\ndef forward()\nAll nn.Module subclasses require a forward() method, this defines the computation that will take place on the data passed to the particular nn.Module (e.g.Â the linear regression formula above).\n\n\n\nIf the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from torch.nn, * nn.Module contains the larger building blocks (layers) * nn.Parameter contains the smaller parameters like weights and biases (put these together to make nn.Module(s)) * foward() tells the larger blocks how to make calculations on inputs (tensors full of data) within nn.Module(s) * torch.optim contains optimization methods on how to improve the parameters within nn.Parameter to better represent input data\n Basic building blocks of creating a PyTorch model by subclassing nn.Module. For ojbects that subclass nn.Module, the forward() method must be defined.\n\në¦¬ì†ŒìŠ¤: See more of these essential modules and their uses cases in the PyTorch Cheat Sheet.\n\n\n\n3.4.2 Checking the contents of a PyTorch model\nNow weâ€™ve got these out of the way, letâ€™s create a model instance with the class weâ€™ve made and check its parameters using .parameters().\n\n# Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n\n[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]\n\n\nWe can also get the state (what the model contains) of the model using .state_dict().\n\n# List named parameters \nmodel_0.state_dict()\n\nOrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n\n\nNotice how the values for weights and bias from model_0.state_dict() come out as random float tensors?\nThis is because we initialized them above using torch.randn().\nEssentially we want to start from random parameters and get the model to update them towards parameters that fit our data best (the hardcoded weight and bias values we set when creating our straight line data).\n\nExercise: Try changing the torch.manual_seed() value two cells above, see what happens to the weights and bias values.\n\nBecause our model starts with random values, right now itâ€™ll have poor predictive power.\n\n\n3.4.3 Making predictions using torch.inference_mode()\nTo check this we can pass it the test data X_test to see how closely it predicts y_test.\nWhen we pass data to our model, itâ€™ll go through the modelâ€™s forward() method and produce a result using the computation weâ€™ve defined.\nLetâ€™s make some predictions.\n\n# Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# ì°¸ê³ : in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n\nHmm?\nYou probably noticed we used torch.inference_mode() as a context manager (thatâ€™s what the with torch.inference_mode(): is) to make the predictions.\nAs the name suggests, torch.inference_mode() is used when using a model for inference (making predictions).\ntorch.inference_mode() turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make forward-passes (data going through the forward() method) faster.\n\nì°¸ê³ : In older PyTorch code, you may also see torch.no_grad() being used for inference. While torch.inference_mode() and torch.no_grad() do similar things, torch.inference_mode() is newer, potentially faster and preferred. See this Tweet from PyTorch for more.\n\nWeâ€™ve made some predictions, letâ€™s see what they look like.\n\n# Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n\nNumber of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n\n\nNotice how thereâ€™s one prediction value per testing sample.\nThis is because of the kind of data weâ€™re using. For our straight line, one X value maps to one y value.\nHowever, machine learning models are very flexible. You could have 100 X values mapping to one, two, three or 10 y values. It all depends on what youâ€™re working on.\nOur predictions are still numbers on a page, letâ€™s visualize them with our plot_predictions() function we created above.\n\nplot_predictions(predictions=y_preds)\n\n\n\n\n\n\n\n\n\ny_test - y_preds\n\ntensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])\n\n\nWoah! Those predictions look pretty badâ€¦\nThis make sense though when you remember our model is just using random parameter values to make predictions.\nIt hasnâ€™t even looked at the blue dots to try to predict the green dots.\nTime to change that.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#train-model",
    "href": "01_pytorch_workflow.html#train-model",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.5 3. Train model",
    "text": "3.5 3. Train model\nRight now our model is making predictions using random parameters to make calculations, itâ€™s basically guessing (randomly).\nTo fix that, we can update its internal parameters (I also refer to parameters as patterns), the weights and bias values we set randomly using nn.Parameter() and torch.randn() to be something that better represents the data.\nWe could hard code this (since we know the default values weight=0.7 and bias=0.3) but whereâ€™s the fun in that?\nMuch of the time you wonâ€™t know what the ideal parameters are for a model.\nInstead, itâ€™s much more fun to write code to see if the model can try and figure them out itself.\n\n3.5.1 Creating a loss function and optimizer in PyTorch\nFor our model to update its parameters on its own, weâ€™ll need to add a few more things to our recipe.\nAnd thatâ€™s a loss function as well as an optimizer.\nThe roles of these are:\n\n\n\nFunction\nWhat does it do?\nWhere does it live in PyTorch?\nCommon values\n\n\n\n\nLoss function\nMeasures how wrong your models predictions (e.g.Â y_preds) are compared to the truth labels (e.g.Â y_test). Lower the better.\nPyTorch has plenty of built-in loss functions in torch.nn.\nMean absolute error (MAE) for regression problems (torch.nn.L1Loss()). Binary cross entropy for binary classification problems (torch.nn.BCELoss()).\n\n\nOptimizer\nTells your model how to update its internal parameters to best lower the loss.\nYou can find various optimization function implementations in torch.optim.\nStochastic gradient descent (torch.optim.SGD()). Adam optimizer (torch.optim.Adam()).\n\n\n\nLetâ€™s create a loss function and an optimizer we can use to help improve our model.\nDepending on what kind of problem youâ€™re working on will depend on what loss function and what optimizer you use.\nHowever, there are some common values, that are known to work well such as the SGD (stochastic gradient descent) or Adam optimizer. And the MAE (mean absolute error) loss function for regression problems (predicting a number) or binary cross entropy loss function for classification problems (predicting one thing or another).\nFor our problem, since weâ€™re predicting a number, letâ€™s use MAE (which is under torch.nn.L1Loss()) in PyTorch as our loss function.\n Mean absolute error (MAE, in PyTorch: torch.nn.L1Loss) measures the absolute difference between two points (predictions and labels) and then takes the mean across all examples.\nAnd weâ€™ll use SGD, torch.optim.SGD(params, lr) where:\n\nparams is the target model parameters youâ€™d like to optimize (e.g.Â the weights and bias values we randomly set before).\nlr is the learning rate youâ€™d like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparamter (because itâ€™s set by a machine learning engineer). Common starting values for the learning rate are 0.01, 0.001, 0.0001, however, these can also be adjusted over time (this is called learning rate scheduling).\n\nWoah, thatâ€™s a lot, letâ€™s see it in code.\n\n# Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n\n\n\n3.5.2 Creating an optimization loop in PyTorch\nWoohoo! Now weâ€™ve got a loss function and an optimizer, itâ€™s now time to create a training loop (and testing loop).\nThe training loop involves the model going through the training data and learning the relationships between the features and labels.\nThe testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never seeâ€™s the testing data during training).\nEach of these is called a â€œloopâ€ because we want our model to look (loop through) at each sample in each dataset.\nTo create these weâ€™re going to write a Python for loop in the theme of the unofficial PyTorch optimization loop song (thereâ€™s a video version too).\n The unoffical PyTorch optimization loops song, a fun way to remember the steps in a PyTorch training (and testing) loop.\nThere will be a fair bit of code but nothing we canâ€™t handle.\n\n\n3.5.3 PyTorch training loop\nFor the training loop, weâ€™ll build the following steps:\n\n\n\n\n\n\n\n\n\nNumber\nStep name\nWhat does it do?\nCode example\n\n\n\n\n1\nForward pass\nThe model goes through all of the training data once, performing its forward() function calculations.\nmodel(x_train)\n\n\n2\nCalculate the loss\nThe modelâ€™s outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are.\nloss = loss_fn(y_pred, y_train)\n\n\n3\nZero gradients\nThe optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step.\noptimizer.zero_grad()\n\n\n4\nPerform backpropagation on the loss\nComputes the gradient of the loss with respect for every model parameter to be updated (each parameter with requires_grad=True). This is known as backpropagation, hence â€œbackwardsâ€.\nloss.backward()\n\n\n5\nUpdate the optimizer (gradient descent)\nUpdate the parameters with requires_grad=True with respect to the loss gradients in order to improve them.\noptimizer.step()\n\n\n\n\n\n\npytorch training loop annotated\n\n\n\nì°¸ê³ : The above is just one example of how the steps could be ordered or described. With experience youâ€™ll find making PyTorch training loops can be quite flexible.\nAnd on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb: * Calculate the loss (loss = ...) before performing backpropagation on it (loss.backward()). * Zero gradients (optimizer.zero_grad()) before computing the gradients of the loss with respect to every model parameter (loss.backward()). * Step the optimizer (optimizer.step()) after performing backpropagation on the loss (loss.backward()).\n\nFor resources to help understand whatâ€™s happening behind the scenes with backpropagation and gradient descent, see the extra-curriculum section.\n\n\n3.5.4 PyTorch testing loop\nAs for the testing loop (evaluating our model), the typical steps include:\n\n\n\n\n\n\n\n\n\nNumber\nStep name\nWhat does it do?\nCode example\n\n\n\n\n1\nForward pass\nThe model goes through all of the training data once, performing its forward() function calculations.\nmodel(x_test)\n\n\n2\nCalculate the loss\nThe modelâ€™s outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are.\nloss = loss_fn(y_pred, y_test)\n\n\n3\nCalulate evaluation metrics (optional)\nAlongisde the loss value you may want to calculate other evaluation metrics such as accuracy on the test set.\nCustom functions\n\n\n\nNotice the testing loop doesnâ€™t contain performing backpropagation (loss.backward()) or stepping the optimizer (optimizer.step()), this is because no parameters in the model are being changed during testing, theyâ€™ve already been calculated. For testing, weâ€™re only interested in the output of the forward pass through the model.\n\n\n\npytorch annotated testing loop\n\n\nLetâ€™s put all of the above together and train our model for 100 epochs (forward passes through the data) and weâ€™ll evaluate it every 10 epochs.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n\nEpoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \nEpoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 \n\n\nOh would you look at that! Looks like our loss is going down with every epoch, letâ€™s plot it to find out.\n\n# Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\nNice! The loss curves show the loss going down over time. Remember, loss is the measure of how wrong your model is, so the lower the better.\nBut why did the loss go down?\nWell, thanks to our loss function and optimizer, the modelâ€™s internal parameters (weights and bias) were updated to better reflect the underlying patterns in the data.\nLetâ€™s inspect our modelâ€™s .state_dict() to see see how close our model gets to the original values we set for weights and bias.\n\n# Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.5784])), ('bias', tensor([0.3513]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n\n\nWow! How cool is that?\nOur model got very close to calculate the exact original values for weight and bias (and it would probably get even closer if we trained it for longer).\n\nExercise: Try changing the epochs value above to 200, what happens to the loss curves and the weights and bias parameter values of the model?\n\nItâ€™d likely never guess them perfectly (especially when using more complicated datasets) but thatâ€™s okay, often you can do very cool things with a close approximation.\nThis is the whole idea of machine learning and deep learning, there are some ideal values that describe our data and rather than figuring them out by hand, we can train a model to figure them out programmatically.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#making-predictions-with-a-trained-pytorch-model-inference",
    "href": "01_pytorch_workflow.html#making-predictions-with-a-trained-pytorch-model-inference",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.6 4. Making predictions with a trained PyTorch model (inference)",
    "text": "3.6 4. Making predictions with a trained PyTorch model (inference)\nOnce youâ€™ve trained a model, youâ€™ll likely want to make predictions with it.\nWeâ€™ve already seen a glimpse of this in the training and testing code above, the steps to do it outside of the training/testing loop are similar.\nThere are three things to remember when making predictions (also called performing inference) with a PyTorch model:\n\nSet the model in evaluation mode (model.eval()).\nMake the predictions using the inference mode context manager (with torch.inference_mode(): ...).\nAll predictions should be made with objects on the same device (e.g.Â data and model on GPU only or data and model on CPU only).\n\nThe first two items make sure all helpful calculations and settings PyTorch uses behind the scenes during training but arenâ€™t necessary for inference are turned off (this results in faster computation). And the third ensures that you wonâ€™t run into cross-device errors.\n\n# 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n\ntensor([[0.8141],\n        [0.8256],\n        [0.8372],\n        [0.8488],\n        [0.8603],\n        [0.8719],\n        [0.8835],\n        [0.8950],\n        [0.9066],\n        [0.9182]])\n\n\nNice! Weâ€™ve made some predictions with our trained model, now how do they look?\n\nplot_predictions(predictions=y_preds)\n\n\n\n\n\n\n\n\nWoohoo! Those red dots are looking far closer than they were before!\nLetâ€™s get onto saving an reloading a model in PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#saving-and-loading-a-pytorch-model",
    "href": "01_pytorch_workflow.html#saving-and-loading-a-pytorch-model",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.7 5. Saving and loading a PyTorch model",
    "text": "3.7 5. Saving and loading a PyTorch model\nIf youâ€™ve trained a PyTorch model, chances are youâ€™ll want to save it and export it somewhere.\nAs in, you might train it on Google Colab or your local machine with a GPU but youâ€™d like to now export it to some sort of application where others can use it.\nOr maybe youâ€™d like to save your progress on a model and come back and load it back later.\nFor saving and loading models in PyTorch, there are three main methods you should be aware of (all of below have been taken from the PyTorch saving and loading models guide):\n\n\n\nPyTorch method\nWhat does it do?\n\n\n\n\ntorch.save\nSaves a serialzed object to disk using Pythonâ€™s pickle utility. Models, tensors and various other Python objects like dictionaries can be saved using torch.save.\n\n\ntorch.load\nUses pickleâ€™s unpickling features to deserialize and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc).\n\n\ntorch.nn.Module.load_state_dict\nLoads a modelâ€™s parameter dictionary (model.state_dict()) using a saved state_dict() object.\n\n\n\n\nì°¸ê³ : As stated in Pythonâ€™s pickle documentation, the pickle module is not secure. That means you should only ever unpickle (load) data you trust. That goes for loading PyTorch models as well. Only ever use saved PyTorch models from sources you trust.\n\n\n3.7.1 Saving a PyTorch modelâ€™s state_dict()\nThe recommended way for saving and loading a model for inference (making preditions) is by saving and loading a modelâ€™s state_dict().\nLetâ€™s see how we can do that in a few steps:\n\nWeâ€™ll create a directory for saving models to called models using Pythonâ€™s pathlib module.\nWeâ€™ll create a file path to save the model to.\nWeâ€™ll call torch.save(obj, f) where obj is the target modelâ€™s state_dict() and f is the filename of where to save the model.\n\n\nì°¸ê³ : Itâ€™s common convention for PyTorch saved models or objects to end with .pt or .pth, like saved_model_01.pth.\n\n\nfrom pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_0.pth\n\n\n\n# Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n\n-rw-rw-r-- 1 daniel daniel 1063 May 10 10:26 models/01_pytorch_workflow_model_0.pth\n\n\n\n\n3.7.2 Loading a saved PyTorch modelâ€™s state_dict()\nSince weâ€™ve now got a saved model state_dict() at models/01_pytorch_workflow_model_0.pth we can now load it in using torch.nn.Module.load_state_dict(torch.load(f)) where f is the filepath of our saved model state_dict().\nWhy call torch.load() inside torch.nn.Module.load_state_dict()?\nBecause we only saved the modelâ€™s state_dict() which is a dictionary of learned parameters and not the entire model, we first have to load the state_dict() with torch.load() and then pass that state_dict() to a new instance of our model (which is a subclass of nn.Module).\nWhy not save the entire model?\nSaving the entire model rather than just the state_dict() is more intuitive, however, to quote the PyTorch documentation (italics mine):\n\nThe disadvantage of this approach (saving the whole model) is that the serialized data is bound to the specific classes and the exact directory structure used when the model is savedâ€¦\nBecause of this, your code can break in various ways when used in other projects or after refactors.\n\nSo instead, weâ€™re using the flexible method of saving and loading just the state_dict(), which again is basically a dictionary of model parameters.\nLetâ€™s test it out by created another instance of LinearRegressionModel(), which is a subclass of torch.nn.Module and will hence have the in-built method load_state_dict().\n\n# Instantiate a new instance of our model (this will be instantiated with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n&lt;All keys matched successfully&gt;\n\n\nExcellent! It looks like things matched up.\nNow to test our loaded model, letâ€™s perform inference with it (make predictions) on the test data.\nRemember the rules for performing inference with PyTorch models?\nIf not, hereâ€™s a refresher:\n\n\nPyTorch inference rules\n\n\n\nSet the model in evaluation mode (model.eval()).\n\n\nMake the predictions using the inference mode context manager (with torch.inference_mode(): â€¦).\n\n\nAll predictions should be made with objects on the same device (e.g.Â data and model on GPU only or data and model on CPU only).\n\n\n\n\n# 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n\nNow weâ€™ve made some predictions with the loaded model, letâ€™s see if theyâ€™re the same as the previous predictions.\n\n# Compare previous model predictions with loaded model predictions (these should be the same)\ny_preds == loaded_model_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])\n\n\nNice!\nIt looks like the loaded model predictions are the same as the previous model predictions (predictions made prior to saving). This indicates our model is saving and loading as expected.\n\nì°¸ê³ : There are more methods to save and load PyTorch models but Iâ€™ll leave these for extra-curriculum and further reading. See the PyTorch guide for saving and loading models for more.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#ì „ì²´-ê³¼ì •-í•©ì¹˜ê¸°",
    "href": "01_pytorch_workflow.html#ì „ì²´-ê³¼ì •-í•©ì¹˜ê¸°",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.8 6. ì „ì²´ ê³¼ì • í•©ì¹˜ê¸°",
    "text": "3.8 6. ì „ì²´ ê³¼ì • í•©ì¹˜ê¸°\nWeâ€™ve covered a fair bit of ground so far.\nBut once youâ€™ve had some practice, youâ€™ll be performing the above steps like dancing down the street.\nSpeaking of practice, letâ€™s put everything weâ€™ve done so far together.\nExcept this time weâ€™ll make our code device agnostic (so if thereâ€™s a GPU available, itâ€™ll use it and if not, it will default to the CPU).\nThereâ€™ll be far less commentary in this section than above since what weâ€™re going to go through has already been covered.\nWeâ€™ll start by importing the standard libraries we need.\n\nì°¸ê³ : If youâ€™re using Google Colab, to setup a GPU, go to Runtime -&gt; Change runtime type -&gt; Hardware acceleration -&gt; GPU. If you do this, it will reset the Colab runtime and you will lose saved variables.\n\n\n# Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n\n'1.11.0'\n\n\nNow letâ€™s start making our code device agnostic by setting device=\"cuda\" if itâ€™s available, otherwise itâ€™ll default to device=\"cpu\".\n\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nUsing device: cuda\n\n\nIf youâ€™ve got access to a GPU, the above shouldâ€™ve printed out:\nUsing device: cuda\nOtherwise, youâ€™ll be using a CPU for the following computations. This is fine for our small dataset but it will take longer for larger datasets.\n\n3.8.1 6.1 Data\nLetâ€™s create some data just like before.\nFirst, weâ€™ll hard-code some weight and bias values.\nThen weâ€™ll make a range of numbers between 0 and 1, these will be our X values.\nFinally, weâ€™ll use the X values, as well as the weight and bias values to create y using the linear regression formula (y = weight * X + bias).\n\n# Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:10], y[:10]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))\n\n\nWonderful!\nNow weâ€™ve got some data, letâ€™s split it into training and test sets.\nWeâ€™ll use an 80/20 split with 80% training data and 20% testing data.\n\n# Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n\n(40, 40, 10, 10)\n\n\nExcellent, letâ€™s visualize them to make sure they look okay.\n\n# ì°¸ê³ : If you've reset your runtime, this function won't work, \n# you'll have to rerun the cell above where it's instantiated.\nplot_predictions(X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n3.8.2 6.2 Building a PyTorch linear model\nWeâ€™ve got some data, now itâ€™s time to make a model.\nWeâ€™ll create the same style of model as before except this time, instead of defining the weight and bias parameters of our model manually using nn.Parameter(), weâ€™ll use nn.Linear(in_features, out_features) to do it for us.\nWhere in_features is the number of dimensions your input data has and out_features is the number of dimensions youâ€™d like it to be output to.\nIn our case, both of these are 1 since our data has 1 input feature (X) per label (y).\n Creating a linear regression model using nn.Parameter versus using nn.Linear. There are plenty more examples of where the torch.nn module has pre-built computations, including many popular and useful neural network layers.\n\n# Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n\n(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))\n\n\nNotice the outputs of model_1.state_dict(), the nn.Linear() layer created a random weight and bias parameter for us.\nNow letâ€™s put our model on the GPU (if itâ€™s available).\nWe can change the device our PyTorch objects are on using .to(device).\nFirst letâ€™s check the modelâ€™s current device.\n\n# Check model device\nnext(model_1.parameters()).device\n\ndevice(type='cpu')\n\n\nWonderful, looks like the modelâ€™s on the CPU by default.\nLetâ€™s change it to be on the GPU (if itâ€™s available).\n\n# Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n\ndevice(type='cuda', index=0)\n\n\nNice! Because of our device agnostic code, the above cell will work regardless of whether a GPU is available or not.\nIf you do have access to a CUDA-enabled GPU, you should see an output of something like:\ndevice(type='cuda', index=0)\n\n\n3.8.3 6.3 Training\nTime to build a training and testing loop.\nFirst weâ€™ll need a loss function and an optimizer.\nLetâ€™s use the same functions we used earlier, nn.L1Loss() and torch.optim.SGD().\nWeâ€™ll have to pass the new modelâ€™s parameters (model.parameters()) to the optimizer for it to adjust them during training.\nThe learning rate of 0.1 worked well before too so letâ€™s use that again.\n\n# Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n\nBeautiful, loss function and optimizer ready, now letâ€™s train and evaluate our model using a training and testing loop.\nThe only different thing weâ€™ll be doing in this step compared to the previous training loop is putting the data on the target device.\nWeâ€™ve already put our model on the target device using model_1.to(device).\nAnd we can do the same with the data.\nThat way if the model is on the GPU, the data is on the GPU (and vice versa).\nLetâ€™s step things up a notch this time and set epochs=1000.\nIf you need a reminder of the PyTorch training loop steps, see below.\n\n\nPyTorch training loop steps\n\n\n\nForward pass - The model goes through all of the training data once, performing its forward() function calculations (model(x_train)).\n\n\nCalculate the loss - The modelâ€™s outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (loss = loss_fn(y_pred, y_train).\n\n\nZero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (optimizer.zero_grad()).\n\n\nPerform backpropagation on the loss - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with requires_grad=True). This is known as backpropagation, hence â€œbackwardsâ€ (loss.backward()).\n\n\nStep the optimizer (gradient descent) - Update the parameters with requires_grad=True with respect to the loss gradients in order to improve them (optimizer.step()).\n\n\n\n\ntorch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n\nEpoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089\nEpoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\n\n\n\nì°¸ê³ : Due to the random nature of machine learning, you will likely get slightly different results (different loss and prediction values) depending on whether your model was trained on CPU or GPU. This is true even if you use the same random seed on either device. If the difference is large, you may want to look for errors, however, if it is small (ideally it is), you can ignore it.\n\nNice! That loss looks pretty low.\nLetâ€™s check the parameters our model has learned and compare them to the original parameters we hard-coded.\n\n# Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n\nThe model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='cuda:0')),\n             ('linear_layer.bias', tensor([0.3025], device='cuda:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n\n\nHo ho! Now thatâ€™s pretty darn close to a perfect model.\nRemember though, in practice, itâ€™s rare that youâ€™ll know the perfect parameters ahead of time.\nAnd if you knew the parameters your model had to learn ahead of time, what would be the fun of machine learning?\nPlus, in many real-world machine learning problems, the number of parameters can well exceed tens of millions.\nI donâ€™t know about you but Iâ€™d rather write code for a computer to figure those out rather than doing it by hand.\n\n\n3.8.4 6.4 Making predictions\nNow weâ€™ve got a trained model, letâ€™s turn on itâ€™s evaluation mode and make some predictions.\n\n# Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n\ntensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='cuda:0')\n\n\nIf youâ€™re making predictions with data on the GPU, you might notice the output of the above has device='cuda:0' towards the end. That means the data is on CUDA device 0 (the first GPU your system has access to due to zero-indexing), if you end up using multiple GPUs in the future, this number may be higher.\nNow letâ€™s plot our modelâ€™s predictions.\n\nì°¸ê³ : Many data science libraries such as pandas, matplotlib and NumPy arenâ€™t capable of using data that is stored on GPU. So you might run into some issues when trying to use a function from one of these libraries with tensor data not stored on the CPU. To fix this, you can call .cpu() on your target tensor to return a copy of your target tensor on the CPU.\n\n\n# plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n\n\n\n\n\n\n\n\nWoah! Look at those red dots, they line up almost perfectly with the green dots. I guess the extra epochs helped.\n\n\n3.8.5 6.5 ëª¨ë¸ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°\nWeâ€™re happy with our models predictions, so letâ€™s save it to file so it can be used later.\n\nfrom pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n\nSaving model to: models/01_pytorch_workflow_model_1.pth\n\n\nAnd just to make sure everything worked well, letâ€™s load it back in.\nWeâ€™ll: * Create a new instance of the LinearRegressionModelV2() class * Load in the model state dict using torch.nn.Module.load_state_dict() * Send the new instance of the model to the target device (to ensure our code is device-agnostic)\n\n# Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n\nLoaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\ncuda:0\n\n\nNow we can evaluate the loaded model to see if its predictions line up with the predictions made prior to saving.\n\n# Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n\ntensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='cuda:0')\n\n\nEverything adds up! Nice!\nWell, weâ€™ve come a long way. Youâ€™ve now built and trained your first two neural network models in PyTorch!\nTime to practice your skills.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "01_pytorch_workflow.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.9 ì—°ìŠµ ë¬¸ì œ",
    "text": "3.9 ì—°ìŠµ ë¬¸ì œ\nAll exercises have been inspired from code throughout the notebook.\nThere is one exercise per major section.\nYou should be able to complete them by referencing their specific section.\n\nì°¸ê³ : For all exercises, your code should be device agnostic (meaning it could run on CPU or GPU if itâ€™s available).\n\n\nCreate a straight line dataset using the linear regression formula (weight * X + bias).\n\n\nSet weight=0.3 and bias=0.9 there should be at least 100 datapoints total.\nSplit the data into 80% training, 20% testing.\nPlot the training and testing data so it becomes visual.\n\n\nBuild a PyTorch model by subclassing nn.Module.\n\n\nInside should be a randomly initialized nn.Parameter() with requires_grad=True, one for weights and one for bias.\nImplement the forward() method to compute the linear regression function you used to create the dataset in 1.\nOnce youâ€™ve constructed the model, make an instance of it and check its state_dict().\nì°¸ê³ : If youâ€™d like to use nn.Linear() instead of nn.Parameter() you can.\n\n\nCreate a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively.\n\n\nSet the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\nWrite a training loop to perform the appropriate training steps for 300 epochs.\nThe training loop should test the model on the test dataset every 20 epochs.\n\n\nMake predictions with the trained model on the test data.\n\n\nVisualize these predictions against the original training and testing data (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).\n\n\nSave your trained modelâ€™s state_dict() to file.\n\n\nCreate a new instance of your model class you made in 2. and load in the state_dict() you just saved to it.\nPerform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.\n\n\në¦¬ì†ŒìŠ¤: See the exercises notebooks templates and solutions on the course GitHub.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "01_pytorch_workflow.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "01_pytorch_workflow.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "3Â  01 - PyTorch ì›Œí¬í”Œë¡œìš°",
    "section": "3.10 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "3.10 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nListen to The Unofficial PyTorch Optimization Loop Song (to help remember the steps in a PyTorch training/testing loop).\nRead What is torch.nn, really? by Jeremy Howard for a deeper understanding of how one of the most important modules in PyTorch works.\nSpend 10-minutes scrolling through and checking out the PyTorch documentation cheatsheet for all of the different PyTorch modules you might come across.\nSpend 10-minutes reading the loading and saving documentation on the PyTorch website to become more familiar with the different saving and loading options in PyTorch.\nSpend 1-2 hours read/watching the following for an overview of the internals of gradient descent and backpropagation, the two main algorithms that have been working in the background to help our model learn.\nWikipedia page for gradient descent\nGradient Descent Algorithm â€” a deep dive by Robert Kwiatkowski\nGradient descent, how neural networks learn video by 3Blue1Brown\nWhat is backpropagation really doing? video by 3Blue1Brown\nBackpropagation Wikipedia Page",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>01 - PyTorch ì›Œí¬í”Œë¡œìš°</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html",
    "href": "02_pytorch_classification.html",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "",
    "text": "4.1 What is a classification problem?\nA classification problem involves predicting whether something is one thing or another.\nFor example, you might want to:\nClassification, along with regression (predicting a number, covered in notebook 01) is one of the most common types of machine learning problems.\nIn this notebook, weâ€™re going to work through a couple of different classification problems with PyTorch.\nIn other words, taking a set of inputs and predicting what class those set of inputs belong to.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#what-is-a-classification-problem",
    "href": "02_pytorch_classification.html#what-is-a-classification-problem",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "",
    "text": "Problem type\nWhat is it?\nExample\n\n\n\n\nBinary classification\nTarget can be one of two options, e.g.Â yes or no\nPredict whether or not someone has heart disease based on their health parameters.\n\n\nMulti-class classification\nTarget can be one of more than two options\nDecide whether a photo of is of food, a person or a dog.\n\n\nMulti-label classification\nTarget can be assigned more than one option\nPredict what categories should be assigned to a Wikipedia article (e.g.Â mathematics, science & philosohpy).\n\n\n\n\n\n\nvarious different classification in machine learning such as binary classification, multiclass classification and multilabel classification",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "02_pytorch_classification.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.2 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "4.2 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nIn this notebook weâ€™re going to reiterate over the PyTorch workflow we coverd in notebook 01.\n\n\n\na pytorch workflow flowchart\n\n\nExcept instead of trying to predict a straight line (predicting a number, also called a regression problem), weâ€™ll be working on a classification problem.\nSpecifically, weâ€™re going to cover:\n\n\n\n\n\n\n\nTopic\nContents\n\n\n\n\n0. Architecture of a classification neural network\nNeural networks can come in almost any shape or size, but they typically follow a similar floor plan.\n\n\n1. Getting binary classification data ready\nData can be almost anything but to get started weâ€™re going to create a simple binary classification dataset.\n\n\n2. Building a PyTorch classification model\nHere weâ€™ll create a model to learn patterns in the data, weâ€™ll also choose a loss function, optimizer and build a training loop specific to classification.\n\n\n3. ë°ì´í„°ì— ëª¨ë¸ ë§ì¶”ê¸° (í›ˆë ¨)\nWeâ€™ve got data and a model, now letâ€™s let the model (try to) find patterns in the (training) data.\n\n\n4. ì˜ˆì¸¡ ë° ëª¨ë¸ í‰ê°€ (ì¶”ë¡ )\nOur modelâ€™s found patterns in the data, letâ€™s compare its findings to the actual (testing) data.\n\n\n5. Improving a model (from a model perspective)\nWeâ€™ve trained an evaluated a model but itâ€™s not working, letâ€™s try a few things to improve it.\n\n\n6. Non-linearity\nSo far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines?\n\n\n7. Replicating non-linear functions\nWe used non-linear functions to help model non-linear data, but what do these look like?\n\n\n8. ì „ì²´ ê³¼ì • í•©ì¹˜ê¸° with multi-class classification\nLetâ€™s put everything weâ€™ve done so far for binary classification together with a multi-class classification problem.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "href": "02_pytorch_classification.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.3 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³",
    "text": "4.3 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³\nAll of the materials for this course live on GitHub.\nAnd if you run into trouble, you can ask a question on the Discussions page there too.\nThereâ€™s also the PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#architecture-of-a-classification-neural-network",
    "href": "02_pytorch_classification.html#architecture-of-a-classification-neural-network",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.4 0. Architecture of a classification neural network",
    "text": "4.4 0. Architecture of a classification neural network\nBefore we get into writing code, letâ€™s look at the general architecture of a classification neural network.\n\n\n\nHyperparameter\nBinary Classification\nMulticlass classification\n\n\n\n\nInput layer shape (in_features)\nSame as number of features (e.g.Â 5 for age, sex, height, weight, smoking status in heart disease prediction)\nSame as binary classification\n\n\nHidden layer(s)\nProblem specific, minimum = 1, maximum = unlimited\nSame as binary classification\n\n\nNeurons per hidden layer\nProblem specific, generally 10 to 512\nSame as binary classification\n\n\nOutput layer shape (out_features)\n1 (one class or the other)\n1 per class (e.g.Â 3 for food, person or dog photo)\n\n\nHidden layer activation\nUsually ReLU (rectified linear unit) but can be many others\nSame as binary classification\n\n\nOutput activation\nSigmoid (torch.sigmoid in PyTorch)\nSoftmax (torch.softmax in PyTorch)\n\n\nLoss function\nBinary crossentropy (torch.nn.BCELoss in PyTorch)\nCross entropy (torch.nn.CrossEntropyLoss in PyTorch)\n\n\nOptimizer\nSGD (stochastic gradient descent), Adam (see torch.optim for more options)\nSame as binary classification\n\n\n\nOf course, this ingredient list of classification neural network components will vary depending on the problem youâ€™re working on.\nBut itâ€™s more than enough to get started.\nWeâ€™re going to gets hands-on with this setup throughout this notebook.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#make-classification-data-and-get-it-ready",
    "href": "02_pytorch_classification.html#make-classification-data-and-get-it-ready",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.5 1. Make classification data and get it ready",
    "text": "4.5 1. Make classification data and get it ready\nLetâ€™s begin by making some data.\nWeâ€™ll use the make_circles() method from Scikit-Learn to generate two circles with different coloured dots.\n\nfrom sklearn.datasets import make_circles\n\n\n# Make 1000 samples \nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03, # a little bit of noise to the dots\n                    random_state=42) # keep random state so we get the same values\n\nAlright, now letâ€™s view the first 5 X and y values.\n\nprint(f\"First 5 X features:\\n{X[:5]}\")\nprint(f\"\\nFirst 5 y labels:\\n{y[:5]}\")\n\nFirst 5 X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst 5 y labels:\n[1 1 1 1 0]\n\n\nLooks like thereâ€™s two X values per one y value.\nLetâ€™s keep following the data explorerâ€™s motto of visualize, visualize, visualize and put them into a pandas DataFrame.\n\n# Make DataFrame of circle data\nimport pandas as pd\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n    \"X2\": X[:, 1],\n    \"label\": y\n})\ncircles.head(10)\n\n\n    \n      \n\n\n\n\n\n\nX1\nX2\nlabel\n\n\n\n\n0\n0.754246\n0.231481\n1\n\n\n1\n-0.756159\n0.153259\n1\n\n\n2\n-0.815392\n0.173282\n1\n\n\n3\n-0.393731\n0.692883\n1\n\n\n4\n0.442208\n-0.896723\n0\n\n\n5\n-0.479646\n0.676435\n1\n\n\n6\n-0.013648\n0.803349\n1\n\n\n7\n0.771513\n0.147760\n1\n\n\n8\n-0.169322\n-0.793456\n1\n\n\n9\n-0.121486\n1.021509\n0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n\n\nIt looks like each pair of X features (X1 and X2) has a label (y) value of either 0 or 1.\nThis tells us that our problem is binary classification since thereâ€™s only two options (0 or 1).\nHow many values of each class is there?\n\n# Check different labels\ncircles.label.value_counts()\n\n1    500\n0    500\nName: label, dtype: int64\n\n\n500 each, nice and balanced.\nLetâ€™s plot them.\n\n# Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0], \n            y=X[:, 1], \n            c=y, \n            cmap=plt.cm.RdYlBu);\n\n\n\n\n\n\n\n\nAlrighty, looks like weâ€™ve got a problem to solve.\nLetâ€™s find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).\n\nì°¸ê³ : This dataset is often whatâ€™s considered a toy problem (a problem thatâ€™s used to try and test things out on) in machine learning.\nBut it represents the major key of classification, you have some kind of data represented as numerical values and youâ€™d like to build a model thatâ€™s able to classify it, in our case, separate it into red or blue dots.\n\n\n4.5.1 1.1 Input and output shapes\nOne of the most common errors in deep learning is shape errors.\nMismatching the shapes of tensors and tensor operations with result in errors in your models.\nWeâ€™re going to see plenty of these throughout the course.\nAnd thereâ€™s no surefire way to making sure they wonâ€™t happen, they will.\nWhat you can do instead is continaully familiarize yourself with the shape of the data youâ€™re working with.\nI like referring to it as input and output shapes.\nAsk yourself:\nâ€œWhat shapes are my inputs and what shapes are my outputs?â€\nLetâ€™s find out.\n\n# Check the shapes of our features and labels\nX.shape, y.shape\n\n((1000, 2), (1000,))\n\n\nLooks like weâ€™ve got a match on the first dimension of each.\nThereâ€™s 1000 X and 1000 y.\nBut whatâ€™s the second dimension on X?\nIt often helps to view the values and shapes of a single sample (features and labels).\nDoing so will help you understand what input and output shapes youâ€™d be expecting from your model.\n\n# View the first example of features and labels\nX_sample = X[0]\ny_sample = y[0]\nprint(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\nprint(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")\n\nValues for one sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for one sample of X: (2,) and the same for y: ()\n\n\nThis tells us the second dimension for X means it has two features (vector) where as y has a single feature (scalar).\nWe have two inputs for one output.\n\n\n4.5.2 1.2 Turn data into tensors and create train and test splits\nWeâ€™ve investigated the input and output shapes of our data, now letâ€™s prepare it for being used with PyTorch and for modelling.\nSpecifically, weâ€™ll need to: 1. Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors). 2. Split our data into training and test sets (weâ€™ll train a model on the training set to learn the patterns between X and y and then evaluate those learned patterns on the test dataset).\n\n# Turn data into tensors\n# Otherwise this causes issues with computations later on\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# View the first five samples\nX[:5], y[:5]\n\n(tensor([[ 0.7542,  0.2315],\n         [-0.7562,  0.1533],\n         [-0.8154,  0.1733],\n         [-0.3937,  0.6929],\n         [ 0.4422, -0.8967]]), tensor([1., 1., 1., 1., 0.]))\n\n\nNow our data is in tensor format, letâ€™s split it into training and test sets.\nTo do so, letâ€™s use the helpful function train_test_split() from Scikit-Learn.\nWeâ€™ll use test_size=0.2 (80% training, 20% testing) and because the split happens randomly across the data, letâ€™s use random_state=42 so the split is reproducible.\n\n# Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, # 20% test, 80% train\n                                                    random_state=42) # make the random split reproducible\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n\n(800, 200, 800, 200)\n\n\nNice! Looks like weâ€™ve now got 800 training samples and 200 testing samples.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#ëª¨ë¸-êµ¬ì¶•í•˜ê¸°",
    "href": "02_pytorch_classification.html#ëª¨ë¸-êµ¬ì¶•í•˜ê¸°",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.6 2. ëª¨ë¸ êµ¬ì¶•í•˜ê¸°",
    "text": "4.6 2. ëª¨ë¸ êµ¬ì¶•í•˜ê¸°\nWeâ€™ve got some data ready, now itâ€™s time to build a model.\nWeâ€™ll break it down into a few parts.\n\nSetting up device agnostic code (so our model can run on CPU or GPU if itâ€™s available).\nConstructing a model by subclassing nn.Module.\nDefining a loss function and optimizer.\nCreating a training loop (thisâ€™ll be in the next section).\n\nThe good news is weâ€™ve been through all of the above steps before in notebook 01.\nExcept now weâ€™ll be adjusting them so they work with a classification dataset.\nLetâ€™s start by importing PyTorch and torch.nn as well as setting up device agnostic code.\n\n# Standard PyTorch imports\nimport torch\nfrom torch import nn\n\n# Make device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\nExcellent, now device is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if itâ€™s available.\nHow about we create a model?\nWeâ€™ll want a model capable of handling our X data as inputs and producing something in the shape of our y data as ouputs.\nIn other words, given X (features) we want our model to predict y (label).\nThis setup where you have features and labels is referred to as supervised learning. Because your data is telling your model what the outputs should be given a certain input.\nTo create such a model itâ€™ll need to handle the input and output shapes of X and y.\nRemember how I said input and output shapes are important? Here weâ€™ll see why.\nLetâ€™s create a model class that: 1. Subclasses nn.Module (almost all PyTorch models are subclasses of nn.Module). 2. Creates 2 nn.Linear layers in the constructor capable of handling the input and output shapes of X and y. 3. Defines a forward() method containing the forward pass computation of the model. 4. Instantiates the model class and sends it to the target device.\n\n# 1. Construct a model class that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n    \n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n\nCircleModelV0(\n  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n)\n\n\nWhatâ€™s going on here?\nWeâ€™ve seen a few of these steps before.\nThe only major change is whatâ€™s happening between self.layer_1 and self.layer_2.\nself.layer_1 takes 2 input features in_features=2 and produces 5 output features out_features=5.\nThis is known as having 5 hidden units or neurons.\nThis layer turns the input data from having 2 features to 5 features.\nWhy do this?\nThis allows the model to learn patterns from 5 numbers rather than just 2 numbers, potentially leading to better outputs.\nI say potentially because sometimes it doesnâ€™t work.\nThe number of hidden units you can use in neural network layers is a hyperparameter (a value you can set yourself) and thereâ€™s no set in stone value you have to use.\nGenerally more is better but thereâ€™s also such a thing as too much. The amount you choose will depend on your model type and dataset youâ€™re working with.\nSince our dataset is small and simple, weâ€™ll keep it small.\nThe only rule with hidden units is that the next layer, in our case, self.layer_2 has to take the same in_features as the previous layer out_features.\nThatâ€™s why self.layer_2 has in_features=5, it takes the out_features=5 from self.layer_1 and performs a linear computation on them, turning them into out_features=1 (the same shape as y).\n A visual example of what a similar classificiation neural network to the one weâ€™ve just built looks like. Try create one of your own on the TensorFlow Playground website.\nYou can also do the same as above using nn.Sequential.\nnn.Sequential performs a forward pass computation of the input data through the layers in the order they appear.\n\n# Replicate CircleModelV0 with nn.Sequential\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),\n    nn.Linear(in_features=5, out_features=1)\n).to(device)\n\nmodel_0\n\nSequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): Linear(in_features=5, out_features=1, bias=True)\n)\n\n\nWoah, that looks much simpler than subclassing nn.Module, why not just always use nn.Sequential?\nnn.Sequential is fantastic for straight-forward computations, however, as the namespace says, it always runs in sequential order.\nSo if youâ€™d something else to happen (rather than just straight-forward sequential computation) youâ€™ll want to define your own custom nn.Module subclass.\nNow weâ€™ve got a model, letâ€™s see what happens when we pass some data through it.\n\n# Make predictions with the model\nuntrained_preds = model_0(X_test.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\nprint(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n\nLength of predictions: 200, Shape: torch.Size([200, 1])\nLength of test samples: 200, Shape: torch.Size([200])\n\nFirst 10 predictions:\ntensor([[0.7311],\n        [0.7607],\n        [0.4336],\n        [0.8163],\n        [0.0846],\n        [0.1053],\n        [0.4653],\n        [0.3110],\n        [0.4488],\n        [0.7589]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nFirst 10 test labels:\ntensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n\n\nHmm, it seems thereâ€™s the same amount of predictions as there is test labels but the predictions donâ€™t look like theyâ€™re in the same form or shape as the test labels.\nWeâ€™ve got a couple steps we can do to fix this, weâ€™ll see these later on.\n\n4.6.1 2.1 Setup loss function and optimizer\nWeâ€™ve setup a loss (also called a criterion or cost function) and optimizer before in notebook 01.\nBut different problem types require different loss functions.\nFor example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.\nAnd for a binary classification problem (like ours), youâ€™ll often use binary cross entropy as the loss function.\nHowever, the same optimizer function can often be used across different problem spaces.\nFor example, the stochastic gradient descent optimizer (SGD, torch.optim.SGD()) can be used for a range of problems, so can too the Adam optimizer (torch.optim.Adam()).\n\n\n\nLoss function/Optimizer\nProblem type\nPyTorch Code\n\n\n\n\nStochastic Gradient Descent (SGD) optimizer\nClassification, regression, many others.\ntorch.optim.SGD()\n\n\nAdam Optimizer\nClassification, regression, many others.\ntorch.optim.Adam()\n\n\nBinary cross entropy loss\nBinary classification\ntorch.nn.BCELossWithLogits or torch.nn.BCELoss\n\n\nCross entropy loss\nMutli-class classification\ntorch.nn.CrossEntropyLoss\n\n\nMean absolute error (MAE) or L1 Loss\nRegression\ntorch.nn.L1Loss\n\n\nMean squared error (MSE) or L2 Loss\nRegression\ntorch.nn.MSELoss\n\n\n\nTable of various loss functions and optimizers, there are more but these some common ones youâ€™ll see.\nSince weâ€™re working with a binary classification problem, letâ€™s use a binary cross entropy loss function.\n\nì°¸ê³ : Recall a loss function is what measures how wrong your model predictions are, the higher the loss, the worse your model.\nAlso, PyTorch documentation often refers to loss functions as â€œloss criterionâ€ or â€œcriterionâ€, these are all different ways of describing the same thing.\n\nPyTorch has two binary cross entropy implementations: 1. torch.nn.BCELoss() - Creates a loss function that measures the binary cross entropy between the target (label) and input (features). 2. torch.nn.BCEWithLogitsLoss() - This is the same as above except it has a sigmoid layer (nn.Sigmoid) built-in (weâ€™ll see what this means soon).\nWhich one should you use?\nThe documentation for torch.nn.BCEWithLogitsLoss() states that itâ€™s more numerically stable than using torch.nn.BCELoss() after a nn.Sigmoid layer.\nSo generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of nn.Sigmoid and torch.nn.BCELoss() but that is beyond the scope of this notebook.\nKnowing this, letâ€™s create a loss function and an optimizer.\nFor the optimizer weâ€™ll use torch.optim.SGD() to optimize the model parameters with learning rate 0.1.\n\nì°¸ê³ : Thereâ€™s a discussion on the PyTorch forums about the use of nn.BCELoss vs.Â nn.BCEWithLogitsLoss. It can be confusing at first but as with many things, it becomes easier with practice.\n\n\n# Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n\nNow letâ€™s also create an evaluation metric.\nAn evaluation metric can be used to offer another perspective on how your model is going.\nIf a loss function measures how wrong your model is, I like to think of evaluation metrics as measuring how right it is.\nOf course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.\nAfter all, when evaluating your models itâ€™s good to look at things from multiple points of view.\nThere are several evaluation metrics that can be used for classification problems but letâ€™s start out with accuracy.\nAccuracy can be measured by dividing the total number of correct predictions over the total number of predictions.\nFor example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.\nLetâ€™s write a function to do so.\n\n# Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n\nExcellent! We can now use this function whilst training our model to measure itâ€™s performance alongside the loss.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#train-model",
    "href": "02_pytorch_classification.html#train-model",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.7 3. Train model",
    "text": "4.7 3. Train model\nOkay, now weâ€™ve got a loss function and optimizer ready to go, letâ€™s train a model.\nDo you remember the steps in a PyTorch training loop?\nIf not, hereâ€™s a reminder.\nSteps in training:\n\n\nPyTorch training loop steps\n\n\n\nForward pass - The model goes through all of the training data once, performing its forward() function calculations (model(x_train)).\n\n\nCalculate the loss - The modelâ€™s outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are (loss = loss_fn(y_pred, y_train).\n\n\nZero gradients - The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step (optimizer.zero_grad()).\n\n\nPerform backpropagation on the loss - Computes the gradient of the loss with respect for every model parameter to be updated (each parameter with requires_grad=True). This is known as backpropagation, hence â€œbackwardsâ€ (loss.backward()).\n\n\nStep the optimizer (gradient descent) - Update the parameters with requires_grad=True with respect to the loss gradients in order to improve them (optimizer.step()).\n\n\n\n\n4.7.1 3.1 Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)\nBefore we the training loop steps, letâ€™s see what comes out of our model during the forward pass (the forward pass is defined by the foward() method).\nTo do so, letâ€™s pass the model some data.\n\n# View the frist 5 outputs of the forward pass on the test data\ny_logits = model_0(X_test.to(device))[:5]\ny_logits\n\ntensor([[0.7311],\n        [0.7607],\n        [0.4336],\n        [0.8163],\n        [0.0846]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\n\nSince our model hasnâ€™t been trained, these outputs are basically random.\nBut what are they?\nTheyâ€™re the output of our forward() method.\nWhich implements two layers of nn.Linear() which internally calls the following equation:\n\\[\n\\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias}\n\\]\nThe raw outputs (unmodified) of this equation (\\(\\mathbf{y}\\)) and in turn, the raw outputs of our model are often referred to as logits.\nThatâ€™s what our model is outputing above when it takes in the input data (\\(x\\) in the equation or X_test in the code), logits.\nHowever, these numbers are hard to interpret.\nWeâ€™d like some numbers that are comparable to our truth labels.\nTo get our modelâ€™s raw outputs (logits) into such a form, we can use the sigmoid activation function.\nLetâ€™s try it out.\n\n# Use sigmoid on model logits\ny_pred_probs = torch.sigmoid(y_logits)\ny_pred_probs\n\ntensor([[0.6751],\n        [0.6815],\n        [0.6067],\n        [0.6935],\n        [0.5211]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)\n\n\nOkay, it seems like the outputs now have some kind of consistency (even though theyâ€™re still random).\nTheyâ€™re now in the form of prediction probabilities (I usually refer to these as y_pred_probs), in other words, the values are now how much the model thinks the data point belongs to one class or another.\nIn our case, since weâ€™re dealing with binary classification, our ideal outputs are 0 or 1.\nSo these values can be viewed as a decision boundary.\nThe closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.\nMore specificially: * If y_pred_probs &gt;= 0.5, y=1 (class 1) * If y_pred_probs &lt; 0.5, y=0 (class 0)\nTo turn our prediction probabilities in prediction labels, we can round the outputs of the sigmoid activation function.\n\n# Find the predicted labels (round the prediction probabilities)\ny_preds = torch.round(y_pred_probs)\n\n# In full\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# Check for equality\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# Get rid of extra dimension\ny_preds.squeeze()\n\ntensor([True, True, True, True, True], device='cuda:0')\n\n\ntensor([1., 1., 1., 1., 1.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)\n\n\nExcellent! Now it looks like our modelâ€™s predictions are in the same form as our truth labels (y_test).\n\ny_test[:5]\n\ntensor([1., 0., 1., 0., 1.])\n\n\nThis means weâ€™ll be able to compare our models predictions to the test labels to see how well itâ€™s going.\nTo recap, we converted our modelâ€™s raw outputs (logits) to predicition probabilities using a sigmoid activation function.\nAnd then converted the prediction probabilities to prediction labels by rounding them.\n\nì°¸ê³ : The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, weâ€™ll be looking at using the softmax activation function (this will come later on).\nAnd the use of the sigmoid activation function is not required when passing our modelâ€™s raw outputs to the nn.BCEWithLogitsLoss (the â€œlogitsâ€ in logits loss is because it works on the modelâ€™s raw logits output), this is because it has a sigmoid function built-in.\n\n\n\n4.7.2 3.2 Building a training and testing loop\nAlright, weâ€™ve discussed how to take our raw model outputs and convert them to prediction labels, now letâ€™s build a training loop.\nLetâ€™s start by training for 100 epochs and outputing the modelâ€™s progress every 10 epochs.\n\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 100\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train()\n\n    # 1. Forward pass (model outputs raw logits)\n    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls\n  \n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n    #                y_train) \n    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 0.72095, Accuracy: 50.00% | Test loss: 0.72767, Test acc: 50.50%\nEpoch: 10 | Loss: 0.70546, Accuracy: 53.87% | Test loss: 0.71203, Test acc: 52.50%\nEpoch: 20 | Loss: 0.70011, Accuracy: 52.25% | Test loss: 0.70601, Test acc: 49.50%\nEpoch: 30 | Loss: 0.69792, Accuracy: 51.50% | Test loss: 0.70315, Test acc: 49.50%\nEpoch: 40 | Loss: 0.69682, Accuracy: 51.12% | Test loss: 0.70148, Test acc: 48.50%\nEpoch: 50 | Loss: 0.69613, Accuracy: 50.75% | Test loss: 0.70034, Test acc: 49.50%\nEpoch: 60 | Loss: 0.69565, Accuracy: 51.00% | Test loss: 0.69948, Test acc: 49.50%\nEpoch: 70 | Loss: 0.69527, Accuracy: 50.75% | Test loss: 0.69880, Test acc: 50.00%\nEpoch: 80 | Loss: 0.69497, Accuracy: 50.62% | Test loss: 0.69825, Test acc: 49.50%\nEpoch: 90 | Loss: 0.69472, Accuracy: 50.62% | Test loss: 0.69779, Test acc: 49.50%\n\n\nHmm, what do you notice about the performance of our model?\nIt looks like it went through the training and testing steps fine but the results donâ€™t seem to have moved too much.\nThe accuracy barely moves above 50% on each data split.\nAnd because weâ€™re working with a balanced binary classification problem, it means our model is performing as good as random guessing (with 500 samples of class 0 and class 1 a model predicting class 1 every single time would achieve 50% accuracy).",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#make-predictions-and-evaluate-the-model",
    "href": "02_pytorch_classification.html#make-predictions-and-evaluate-the-model",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.8 4. Make predictions and evaluate the model",
    "text": "4.8 4. Make predictions and evaluate the model\nFrom the metrics it looks like our model is random guessing.\nHow could we investigate this further?\nIâ€™ve got an idea.\nThe data explorerâ€™s motto!\nâ€œVisualize, visualize, visualize!â€\nLetâ€™s make a plot of our modelâ€™s predictions, the data itâ€™s trying to predict on and the decision boundary itâ€™s creating for whether something is class 0 or class 1.\nTo do so, weâ€™ll write some code to download and import the helper_functions.py script from the Learn PyTorch for Deep Learning repo.\nIt contains a helpful function called plot_decision_boundary() which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.\nWeâ€™ll also import plot_predictions() which we wrote in notebook 01 to use later.\n\nimport requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n\nDownloading helper_functions.py\n\n\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n\n\n\n\n\n\n\n\nOh wow, it seems like weâ€™ve found the cause of modelâ€™s performance issue.\nItâ€™s currently trying to split the red and blue dots using a straight lineâ€¦\nThat explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.\nIn machine learning terms, our model is underfitting, meaning itâ€™s not learning predictive patterns from the data.\nHow could we improve this?",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#improving-a-model-from-a-model-perspective",
    "href": "02_pytorch_classification.html#improving-a-model-from-a-model-perspective",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.9 5. Improving a model (from a model perspective)",
    "text": "4.9 5. Improving a model (from a model perspective)\nLetâ€™s try to fix our modelâ€™s underfitting problem.\nFocusing specifically on the model (not the data), there are a few ways we could do this.\n\n\n\nModel improvement technique*\nWhat does it do?\n\n\n\n\nAdd more layers\nEach layer potentially increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network deeper.\n\n\nAdd more hidden units\nSimilar to the above, more hidden units per layer means a potential increase in learning capabilities of the model, more hidden units is often referred to as making your neural network wider.\n\n\nFitting for longer (more epochs)\nYour model might learn more if it had more opportunities to look at the data.\n\n\nChanging the activation functions\nSome data just canâ€™t be fit with only straight lines (like what weâ€™ve seen), using non-linear activation functions can help with this (hint, hint).\n\n\nChange the learning rate\nLess model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesnâ€™t learn enough.\n\n\nChange the loss function\nAgain, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function wonâ€™t work with a multi-class classification problem.\n\n\nUse transfer learning\nTake a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in notebook 06.\n\n\n\n\nì°¸ê³ : *because you can adjust all of these by hand, theyâ€™re referred to as hyperparameters.\nAnd this is also where machine learningâ€™s half art half science comes in, thereâ€™s no real way to know here what the best combination of values is for your project, best to follow the data scientistâ€™s motto of â€œexperiment, experiment, experimentâ€.\n\nLetâ€™s see what happens if we add an extra layer to our model, fit for longer (epochs=1000 instead of epochs=100) and increase the number of hidden units from 5 to 10.\nWeâ€™ll follow the same steps we did above but with a few changed hyperparameters.\n\nclass CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        \n    def forward(self, x): # note: always make sure forward is spelt correctly!\n        # Creating a model like this is the same as below, though below\n        # generally benefits from speedups where possible.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n\nCircleModelV1(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\nNow weâ€™ve got a model, weâ€™ll recreate a loss function and optimizer instance, using the same settings as before.\n\n# loss_fn = nn.BCELoss() # Requires sigmoid on input\nloss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\noptimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n\nBeautiful, model, optimizer and loss function ready, letâ€™s make a training loop.\nThis time weâ€™ll train for longer (epochs=1000 vs epochs=100) and see if it improves our model.\n\ntorch.manual_seed(42)\n\nepochs = 1000 # Train for longer\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    # 1. Forward pass\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels\n\n    # 2. Calculate loss/accuracy\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%\nEpoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%\nEpoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%\nEpoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%\nEpoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%\nEpoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%\nEpoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\n\n\nWhat? Our model trained for longer and with an extra layer but it still looks like it didnâ€™t learn any patterns better than random guessing.\nLetâ€™s visualize.\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_1, X_test, y_test)\n\n\n\n\n\n\n\n\nHmmm.\nOur model is still drawing a straight line between the red and blue dots.\nIf our model is drawing a straight line, could it model linear data? Like we did in notebook 01?\n\n4.9.1 5.1 Preparing data to see if our model can model a straight line\nLetâ€™s create some linear data to see if our modelâ€™s able to model it and weâ€™re not just using a model that canâ€™t learn anything.\n\n# Create some data (same as notebook 01)\nweight = 0.7\nbias = 0.3\nstart = 0\nend = 1\nstep = 0.01\n\n# Create data\nX_regression = torch.arange(start, end, step).unsqueeze(dim=1)\ny_regression = weight * X_regression + bias # linear regression formula\n\n# Check the data\nprint(len(X_regression))\nX_regression[:5], y_regression[:5]\n\n100\n\n\n(tensor([[0.0000],\n         [0.0100],\n         [0.0200],\n         [0.0300],\n         [0.0400]]), tensor([[0.3000],\n         [0.3070],\n         [0.3140],\n         [0.3210],\n         [0.3280]]))\n\n\nWonderful, now letâ€™s split our data into training and test sets.\n\n# Create train and test splits\ntrain_split = int(0.8 * len(X_regression)) # 80% of data used for training set\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# Check the lengths of each split\nprint(len(X_train_regression), \n    len(y_train_regression), \n    len(X_test_regression), \n    len(y_test_regression))\n\n80 80 20 20\n\n\nBeautiful, letâ€™s see how the data looks.\nTo do so, weâ€™ll use the plot_predictions() function we created in notebook 01.\nItâ€™s contained within the helper_functions.py script on the Learn PyTorch for Deep Learning repo which we downloaded above.\n\nplot_predictions(train_data=X_train_regression,\n    train_labels=y_train_regression,\n    test_data=X_test_regression,\n    test_labels=y_test_regression\n);\n\n\n\n\n\n\n\n\n\n\n4.9.2 5.2 Adjusting model_1 to fit a straight line\nNow weâ€™ve got some data, letâ€™s recreate model_1 but with a loss function suited to our regression data.\n\n# Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n\nSequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)\n\n\nWeâ€™ll setup the loss function to be nn.L1Loss() (the same as mean absolute error) and the optimizer to be torch.optim.SGD().\n\n# Loss and optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n\nNow letâ€™s train the model using the regular training loop steps for epochs=1000 (just like model_1).\n\nì°¸ê³ : Weâ€™ve been writing similar training loop code over and over again. Iâ€™ve made it that way on purpose though, to keep practicing. However, do you have ideas how we could functionize this? That would save a fair bit of coding in the future. Potentially there could be a function for training and a function for testing.\n\n\n# Train the model\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put data to target device\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\nfor epoch in range(epochs):\n    ### Training \n    # 1. Forward pass\n    y_pred = model_2(X_train_regression)\n    \n    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_2.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_pred = model_2(X_test_regression)\n      # 2. Calculate the loss \n      test_loss = loss_fn(test_pred, y_test_regression)\n\n    # Print out what's happening\n    if epoch % 100 == 0: \n        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")\n\nEpoch: 0 | Train loss: 0.75986, Test loss: 0.54143\nEpoch: 100 | Train loss: 0.09309, Test loss: 0.02901\nEpoch: 200 | Train loss: 0.07376, Test loss: 0.02850\nEpoch: 300 | Train loss: 0.06745, Test loss: 0.00615\nEpoch: 400 | Train loss: 0.06107, Test loss: 0.02004\nEpoch: 500 | Train loss: 0.05698, Test loss: 0.01061\nEpoch: 600 | Train loss: 0.04857, Test loss: 0.01326\nEpoch: 700 | Train loss: 0.06109, Test loss: 0.02127\nEpoch: 800 | Train loss: 0.05599, Test loss: 0.01426\nEpoch: 900 | Train loss: 0.05571, Test loss: 0.00603\n\n\nOkay, unlike model_1 on the classification data, it looks like model_2â€™s loss is actually going down.\nLetâ€™s plot its predictions to see if thatâ€™s so.\nAnd remember, since our model and data are using the target device, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib canâ€™t handle data on the GPU.\nTo handle that, weâ€™ll send all of our data to the CPU using .cpu() when we pass it to plot_predictions().\n\n# Turn on evaluation mode\nmodel_2.eval()\n\n# Make predictions (inference)\nwith torch.inference_mode():\n    y_preds = model_2(X_test_regression)\n\n# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n# (try removing .cpu() from one of the below and see what happens)\nplot_predictions(train_data=X_train_regression.cpu(),\n                 train_labels=y_train_regression.cpu(),\n                 test_data=X_test_regression.cpu(),\n                 test_labels=y_test_regression.cpu(),\n                 predictions=y_preds.cpu());\n\n\n\n\n\n\n\n\nAlright, it looks like our model is able to do far better than random guessing on straight lines.\nThis is a good thing.\nIt means our model at least has some capacity to learn.\n\nì°¸ê³ : A helpful troubleshooting step when building deep learning models is to start as small as possible to see if the model works before scaling it up.\nThis could mean starting with a simple neural network (not many layers, not many hidden neurons) and a small dataset (like the one weâ€™ve made) and then overfitting (making the model perform too well) on that small example before increasing the amount data or the model size/design to reduce overfitting.\n\nSo what could it be?\nLetâ€™s find out.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#the-missing-piece-non-linearity",
    "href": "02_pytorch_classification.html#the-missing-piece-non-linearity",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.10 6. The missing piece: non-linearity",
    "text": "4.10 6. The missing piece: non-linearity\nWeâ€™ve seen our model can draw straight (linear) lines, thanks to its linear layers.\nBut how about we give it the capacity to draw non-straight (non-linear) lines?\nHow?\nLetâ€™s find out.\n\n4.10.1 6.1 Recreating non-linear data (red and blue circles)\nFirst, letâ€™s recreate the data to start off fresh. Weâ€™ll use the same setup as before.\n\n# Make and plot data\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\nn_samples = 1000\n\nX, y = make_circles(n_samples=1000,\n    noise=0.03,\n    random_state=42,\n)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);\n\n\n\n\n\n\n\n\nNice! Now letâ€™s split it into training and test sets using 80% of the data for training and 20% for testing.\n\n# Convert to tensors and split into train and test sets\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n# Turn data into tensors\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42\n)\n\nX_train[:5], y_train[:5]\n\n(tensor([[ 0.6579, -0.4651],\n         [ 0.6319, -0.7347],\n         [-1.0086, -0.1240],\n         [-0.9666, -0.2256],\n         [-0.1666,  0.7994]]), tensor([1., 0., 0., 0., 1.]))\n\n\n\n\n4.10.2 6.2 ëª¨ë¸ êµ¬ì¶•í•˜ê¸° with non-linearity\nNow here comes the fun part.\nWhat kind of pattern do you think you could draw with unlimited straight (linear) and non-straight (non-linear) lines?\nI bet you could get pretty creative.\nSo far our neural networks have only been using linear (straight) line functions.\nBut the data weâ€™ve been working with is non-linear (circles).\nWhat do you think will happen when we introduce the capability for our model to use non-linear actviation functions?\nWell letâ€™s see.\nPyTorch has a bunch of ready-made non-linear activation functions that do similiar but different things.\nOne of the most common and best performing is ReLU (rectified linear-unit, torch.nn.ReLU()).\nRather than talk about it, letâ€™s put it in our neural network between the hidden layers in the forward pass and see what happens.\n\n# Build model with non-linear activation function\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- add in ReLU activation function\n        # Can also put sigmoid in the model \n        # This would mean you don't need to use it on the predictions\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # Intersperse the ReLU activation function between layers\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n\nCircleModelV2(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)\n\n\n A visual example of what a similar classificiation neural network to the one weâ€™ve just built (suing ReLU activation) looks like. Try create one of your own on the TensorFlow Playground website.\n\nQuestion: Where should I put the non-linear activation functions when constructing a neural network?\nA rule of thumb is to put them in between hidden layers and just after the output layer, however, there is no set in stone option. As you learn more about neural networks and deep learning youâ€™ll find a bunch of different ways of putting things together. In the meantine, best to experiment, experiment, experiment.\n\nNow weâ€™ve got a model ready to go, letâ€™s create a binary classification loss function as well as an optimizer.\n\n# Setup loss and optimizer \nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n\nWonderful!\n\n\n4.10.3 6.3 Training a model with non-linearity\nYou know the drill, model, loss function, optimizer ready to go, letâ€™s create a training and testing loop.\n\n# Fit the model\ntorch.manual_seed(42)\nepochs = 1000\n\n# Put all data on target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    # 1. Forward pass\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n    \n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n    \n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_3.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_3(X_test).squeeze()\n      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n      # 2. Calcuate loss and accuracy\n      test_loss = loss_fn(test_logits, y_test)\n      test_acc = accuracy_fn(y_true=y_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n\nEpoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\n\n\nHo ho! Thatâ€™s looking far better!\n\n\n4.10.4 6.4 Evaluating a model trained with non-linear activation functions\nRemember how our circle data is non-linear? Well, letâ€™s see how our models predictions look now the modelâ€™s been trained with non-linear activation functions.\n\n# Make predictions\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # want preds in same format as truth labels\n\n(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),\n tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))\n\n\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity\n\n\n\n\n\n\n\n\nNice! Not perfect but still far better than before.\nPotentially you could try a few tricks to improve the test accuracy of the model? (hint: head back to section 5 for tips on improving the model)",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#replicating-non-linear-activation-functions",
    "href": "02_pytorch_classification.html#replicating-non-linear-activation-functions",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.11 7. Replicating non-linear activation functions",
    "text": "4.11 7. Replicating non-linear activation functions\nWe saw before how adding non-linear activation functions to our model can helped it to model non-linear data.\n\nì°¸ê³ : Much of the data youâ€™ll encounter in the wild is non-linear (or a combination of linear and non-linear). Right now weâ€™ve been working with dots on a 2D plot. But imagine if you had images of plants youâ€™d like to classify, thereâ€™s a lot of different plant shapes. Or text from Wikipedia youâ€™d like to summarize, thereâ€™s lots of different ways words can be put together (linear and non-linear patterns).\n\nBut what does a non-linear activation look like?\nHow about we replicate some and what they do?\nLetâ€™s start by creating a small amount of data.\n\n# Create a toy tensor (similar to the data going into our model(s))\nA = torch.arange(-10, 10, 1, dtype=torch.float32)\nA\n\ntensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])\n\n\nWonderful, now letâ€™s plot it.\n\n# Visualize the toy tensor\nplt.plot(A);\n\n\n\n\n\n\n\n\nA straight line, nice.\nNow letâ€™s see how the ReLU activation function influences it.\nAnd instead of using PyTorchâ€™s ReLU (torch.nn.ReLU), weâ€™ll recreate it ourselves.\nThe ReLU function turns all negatives to 0 and leaves the positive values as they are.\n\n# Create ReLU function by hand \ndef relu(x):\n  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n\n# Pass toy tensor through ReLU function\nrelu(A)\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n        8., 9.])\n\n\nIt looks like our ReLU function worked, all of the negative values are zeros.\nLetâ€™s plot them.\n\n# Plot ReLU activated toy tensor\nplt.plot(relu(A));\n\n\n\n\n\n\n\n\nNice! That looks exactly like the shape of the ReLU function on the Wikipedia page for ReLU.\nHow about we try the sigmoid function weâ€™ve been using?\nThe sigmoid function formula goes like so:\n\\[ out_i = \\frac{1}{1+e^{-input_i}} \\]\nOr using \\(x\\) as input:\n\\[ S(x) = \\frac{1}{1+e^{-x_i}} \\]\nWhere \\(S\\) stands for sigmoid, \\(e\\) stands for exponential (torch.exp()) and \\(i\\) stands for a particular element in a tensor.\nLetâ€™s build a function to replicate the sigmoid function with PyTorch.\n\n# Create a custom sigmoid function\ndef sigmoid(x):\n  return 1 / (1 + torch.exp(-x))\n\n# Test custom sigmoid on toy tensor\nsigmoid(A)\n\ntensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n        9.9966e-01, 9.9988e-01])\n\n\nWoah, those values look a lot like prediction probabilities weâ€™ve seen earlier, letâ€™s see what they look like visualized.\n\n# Plot sigmoid activated toy tensor\nplt.plot(sigmoid(A));\n\n\n\n\n\n\n\n\nLooking good! Weâ€™ve gone from a straight line to a curved line.\nNow thereâ€™s plenty more non-linear activation functions that exist in PyTorch that we havenâ€™t tried.\nBut these two are two of the most common.\nAnd the point remains, what patterns could you draw using an unlimited amount of linear (straight) and non-linear (not straight) lines?\nAlmost anything right?\nThatâ€™s exactly what our model is doing when we combine linear and non-linear functions.\nInstead of telling our model what to do, we give it tools to figure out how to best discover patterns in the data.\nAnd those tools are linear and non-linear functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#putting-things-together-by-building-a-multi-class-pytorch-model",
    "href": "02_pytorch_classification.html#putting-things-together-by-building-a-multi-class-pytorch-model",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.12 8. Putting things together by building a multi-class PyTorch model",
    "text": "4.12 8. Putting things together by building a multi-class PyTorch model\nWeâ€™ve covered a fair bit.\nBut now letâ€™s put it all together using a multi-class classification problem.\nRecall a binary classification problem deals with classifying something as one of two options (e.g.Â a photo as a cat photo or a dog photo) where as a multi-class classification problem deals with classifying something from a list of more than two options (e.g.Â classifying a photo as a cat a dog or a chicken).\n Example of binary vs.Â multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular ImageNet-1k dataset is used as a computer vision benchmark and has 1000 classes.\n\n4.12.1 8.1 Creating mutli-class classification data\nTo begin a multi-class classification problem, letâ€™s create some multi-class data.\nTo do so, we can leverage Scikit-Learnâ€™s make_blobs() method.\nThis method will create however many classes (using the centers parameter) we want.\nSpecifically, letâ€™s do the following:\n\nCreate some multi-class data with make_blobs().\nTurn the data into tensors (the default of make_blobs() is to use NumPy arrays).\nSplit the data into training and test sets using train_test_split().\nVisualize the data.\n\n\n# Import dependencies\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n# Set the hyperparameters for data creation\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. Create multi-class data\nX_blob, y_blob = make_blobs(n_samples=1000,\n    n_features=NUM_FEATURES, # X features\n    centers=NUM_CLASSES, # y labels \n    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n    random_state=RANDOM_SEED\n)\n\n# 2. Turn data into tensors\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\nprint(X_blob[:5], y_blob[:5])\n\n# 3. Split into train and test sets\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n    y_blob,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\n\n# 4. Plot data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);\n\ntensor([[-8.4134,  6.9352],\n        [-5.7665, -6.4312],\n        [-6.0421, -6.7661],\n        [ 3.9508,  0.6984],\n        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])\n\n\n\n\n\n\n\n\n\nNice! Looks like weâ€™ve got some multi-class data ready to go.\nLetâ€™s build a model to separate the coloured blobs.\n\nQuestion: Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?\n\n\n\n4.12.2 8.2 Building a multi-class classification model in PyTorch\nWeâ€™ve created a few models in PyTorch so far.\nYou might also be starting to get an idea of how flexible neural networks are.\nHow about we build one similar to model_3 but this still capable of handling multi-class data?\nTo do so, letâ€™s create a subclass of nn.Module that takes in three hyperparameters: * input_features - the number of X features coming into the model. * output_features - the ideal numbers of output features weâ€™d like (this will be equivalent to NUM_CLASSES or the number of classes in your multi-class classification problem). * hidden_units - the number of hidden neurons weâ€™d like each hidden layer to use.\nSince weâ€™re putting things together, letâ€™s setup some device agnostic code (we donâ€™t have to do this again in the same notebook, itâ€™s only a reminder).\nThen weâ€™ll create the model class using the hyperparameters above.\n\n# Create device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nfrom torch import nn\n\n# Build model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model.\n            out_features (int): Number of output features of the model\n              (how many classes there are).\n            hidden_units (int): Number of hidden units between layers, default 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n        )\n    \n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n\nBlobModel(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=2, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=8, bias=True)\n    (2): Linear(in_features=8, out_features=4, bias=True)\n  )\n)\n\n\nExcellent! Our multi-class model is ready to go, letâ€™s create a loss function and optimizer for it.\n\n\n4.12.3 8.3 Creating a loss function and optimizer for a multi-class PyTorch model\nSince weâ€™re working on a multi-class classification problem, weâ€™ll use the nn.CrossEntropyLoss() method as our loss function.\nAnd weâ€™ll stick with using SGD with a learning rate of 0.1 for optimizing our model_4 parameters.\n\n# Create loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n\n\n\n4.12.4 8.4 Getting prediction probabilities for a multi-class PyTorch model\nAlright, weâ€™ve got a loss function and optimizer ready, and weâ€™re ready to train our model but before we do letâ€™s do a single forward pass with our model to see if it works.\n\n# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\nmodel_4(X_blob_train.to(device))[:5]\n\ntensor([[-1.2711, -0.6494, -1.4740, -0.7044],\n        [ 0.2210, -1.5439,  0.0420,  1.1531],\n        [ 2.8698,  0.9143,  3.3169,  1.4027],\n        [ 1.9576,  0.3125,  2.2244,  1.1324],\n        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\nWhatâ€™s coming out here?\nIt looks like we get one value per feature of each sample.\nLetâ€™s check the shape to confirm.\n\n# How many elements in a single prediction sample?\nmodel_4(X_blob_train.to(device))[0].shape, NUM_CLASSES \n\n(torch.Size([4]), 4)\n\n\nWonderful, our model is predicting one value for each class that we have.\nDo you remember what the raw outputs of our model are called?\nHint: it rhymes with â€œfrog splitsâ€ (no animals were harmed in the creation of these materials).\nIf you guessed logits, youâ€™d be correct.\nSo right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?\nAs in, how do we go from logits -&gt; prediction probabilities -&gt; prediction labels just like we did with the binary classification problem?\nThatâ€™s where the softmax activation function comes into play.\nThe softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.\nIf this doesnâ€™t make sense, letâ€™s see in code.\n\n# Make prediction logits with model\ny_logits = model_4(X_test.to(device))\n\n# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1) \nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n\ntensor([[ 0.2341, -0.3357,  0.2307,  0.2534],\n        [ 0.1198, -0.3702,  0.0998,  0.1887],\n        [ 0.3790, -0.2037,  0.4095,  0.2689],\n        [ 0.1936, -0.3733,  0.1807,  0.2496],\n        [ 0.1338, -0.1378,  0.1487,  0.0247]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\ntensor([[0.2792, 0.1579, 0.2782, 0.2846],\n        [0.2729, 0.1672, 0.2675, 0.2924],\n        [0.2869, 0.1602, 0.2958, 0.2570],\n        [0.2769, 0.1571, 0.2733, 0.2928],\n        [0.2722, 0.2075, 0.2763, 0.2441]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\nHmm, whatâ€™s happened here?\nIt may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasnâ€™t been trained and is predicting using random patterns) but thereâ€™s a very specific thing different about each sample.\nAfter passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).\nLetâ€™s check.\n\n# Sum the first sample output of the softmax activation function \ntorch.sum(y_pred_probs[0])\n\ntensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)\n\n\nThese prediction probablities are essentially saying how much the model thinks the target X sample (the input) maps to each class.\nSince thereâ€™s one value for each class in y_pred_probs, the index of the highest value is the class the model thinks the specific data sample most belongs to.\nWe can check which index has the highest value using torch.argmax().\n\n# Which class does the model think is *most* likely at the index 0 sample?\nprint(y_pred_probs[0])\nprint(torch.argmax(y_pred_probs[0]))\n\ntensor([0.2792, 0.1579, 0.2782, 0.2846], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\ntensor(3, device='cuda:0')\n\n\nYou can see the output of torch.argmax() returns 3, so for the features (X) of the sample at index 0, the model is predicting that the most likely class value (y) is 3.\nOf course, right now this is just random guessing so itâ€™s got a 25% chance of being right (since thereâ€™s four classes). But we can improve those chances by training the model.\n\nì°¸ê³ : To summarize the above, a modelâ€™s raw output is referred to as logits.\nFor a multi-class classification problem, to turn the logits into prediction probabilities, you use the softmax activation function (torch.softmax).\nThe index of the value with the highest prediction probability is the class number the model thinks is most likely given the input features for that sample (although this is a prediction, it doesnâ€™t mean it will be correct).\n\n\n\n4.12.5 8.5 Creating a training and testing loop for a multi-class PyTorch model\nAlright, now weâ€™ve got all of the preparation steps out of the way, letâ€™s write a training and testing loop to improve and evaluation our model.\nWeâ€™ve done many of these steps before so much of this will be practice.\nThe only difference is that weâ€™ll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).\nLetâ€™s train the model for epochs=100 and evaluate it every 10 epochs.\n\n# Fit the model\ntorch.manual_seed(42)\n\n# Set number of epochs\nepochs = 100\n\n# Put data to target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_4.train()\n\n    # 1. Forward pass\n    y_logits = model_4(X_blob_train) # model outputs raw logits \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels\n    # print(y_logits)\n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. Calculate test loss and accuracy\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") \n\nEpoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%\nEpoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%\nEpoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%\nEpoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%\nEpoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%\nEpoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%\nEpoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%\nEpoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%\nEpoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%\nEpoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%\n\n\n\n\n4.12.6 8.6 Making and evaluating predictions with a PyTorch multi-class model\nIt looks like our trained model is performaning pretty well.\nBut to make sure of this, letâ€™s make some predictions and visualize them.\n\n# Make predictions\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\n# View the first 10 predictions\ny_logits[:10]\n\ntensor([[  4.3377,  10.3539, -14.8948,  -9.7642],\n        [  5.0142, -12.0371,   3.3860,  10.6699],\n        [ -5.5885, -13.3448,  20.9894,  12.7711],\n        [  1.8400,   7.5599,  -8.6016,  -6.9942],\n        [  8.0726,   3.2906, -14.5998,  -3.6186],\n        [  5.5844, -14.9521,   5.0168,  13.2890],\n        [ -5.9739, -10.1913,  18.8655,   9.9179],\n        [  7.0755,  -0.7601,  -9.5531,   0.1736],\n        [ -5.5918, -18.5990,  25.5309,  17.5799],\n        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')\n\n\nAlright, looks like our modelâ€™s predictions are still in logit form.\nThough to evaluate them, theyâ€™ll have to be in the same form as our labels (y_blob_test) which are in integer form.\nLetâ€™s convert our modelâ€™s prediction logits to prediction probabilities (using torch.softmax()) then to prediction labels (by taking the argmax() of each sample).\n\nì°¸ê³ : Itâ€™s possible to skip the torch.softmax() function and go straight from predicted logits -&gt; predicted labels by calling torch.argmax() directly on the logits.\nFor example, y_preds = torch.argmax(y_logits, dim=1), this saves a computation step (no torch.softmax()) but results in no prediction probabilities being available to use.\n\n\n# Turn predicted logits in prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1)\n\n# Turn prediction probabilities into prediction labels\ny_preds = y_pred_probs.argmax(dim=1)\n\n# Compare first 10 model preds and test labels\nprint(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\nprint(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n\nPredictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nLabels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nTest accuracy: 99.5%\n\n\nNice! Our model predictions are now in the same form as our test labels.\nLetâ€™s visualize them with plot_decision_boundary(), remember because our data is on the GPU, weâ€™ll have to move it to the CPU for use with matplotlib (plot_decision_boundary() does this automatically for us).\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#more-classification-evaluation-metrics",
    "href": "02_pytorch_classification.html#more-classification-evaluation-metrics",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.13 9. More classification evaluation metrics",
    "text": "4.13 9. More classification evaluation metrics\nSo far weâ€™ve only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).\nThese are some of the most common methods youâ€™ll come across and are a good starting point.\nHowever, you may want to evaluate you classification model using more metrics such as the following:\n\n\n\nMetric name/Evaluation method\nDefintion\nCode\n\n\n\n\nAccuracy\nOut of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct.\ntorchmetrics.Accuracy() or sklearn.metrics.accuracy_score()\n\n\nPrecision\nProportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it shouldâ€™ve been 0).\ntorchmetrics.Precision() or sklearn.metrics.precision_score()\n\n\nRecall\nProportion of true positives over total number of true positives and false negatives (model predicts 0 when it shouldâ€™ve been 1). Higher recall leads to less false negatives.\ntorchmetrics.Recall() or sklearn.metrics.recall_score()\n\n\nF1-score\nCombines precision and recall into one metric. 1 is best, 0 is worst.\ntorchmetrics.F1Score() or sklearn.metrics.f1_score()\n\n\nConfusion matrix\nCompares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).\ntorchmetrics.ConfusionMatrix or sklearn.metrics.plot_confusion_matrix()\n\n\nClassification report\nCollection of some of the main classification metrics such as precision, recall and f1-score.\nsklearn.metrics.classification_report()\n\n\n\nScikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and youâ€™re looking for a PyTorch-like version, check out TorchMetrics, especially the TorchMetrics classification section.\nLetâ€™s try the torchmetrics.Accuracy metric out.\n\n!pip -q install torchmetrics\n\nfrom torchmetrics import Accuracy\n\n# Setup metric and make sure it's on the target device\ntorchmetrics_accuracy = Accuracy().to(device)\n\n# Calculate accuracy\ntorchmetrics_accuracy(y_preds, y_blob_test)\n\n     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409 kB 5.2 MB/s eta 0:00:01\n\n\ntensor(0.9950, device='cuda:0')",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "02_pytorch_classification.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.14 ì—°ìŠµ ë¬¸ì œ",
    "text": "4.14 ì—°ìŠµ ë¬¸ì œ\nAll of the exercises are focused on practicing the code in the sections above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agonistic code.\nResources: * Exercise template notebook for 02 * Example solutions notebook for 02 (try the exercises before looking at this)\n\nMake a binary classification dataset with Scikit-Learnâ€™s make_moons() function.\n\n\nFor consistency, the dataset should have 1000 samples and a random_state=42.\nTurn the data into PyTorch tensors. Split the data into training and test sets using train_test_split with 80% training and 20% testing.\n\n\nBuild a model by subclassing nn.Module that incorporates non-linear activation functions and is capable of fitting the data you created in 1.\n\n\nFeel free to use any combination of PyTorch layers (linear and non-linear) you want.\n\n\nSetup a binary classification compatible loss function and optimizer to use when training the model.\nCreate a training and testing loop to fit the model you created in 2 to the data you created in 1.\n\n\nTo measure model accuray, you can create your own accuracy function or use the accuracy function in TorchMetrics.\nTrain the model for long enough for it to reach over 96% accuracy.\nThe training loop should output progress every 10 epochs of the modelâ€™s training and test set loss and accuracy.\n\n\nMake predictions with your trained model and plot them using the plot_decision_boundary() function created in this notebook.\nReplicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.\n\n\nFeel free to reference the ML cheatsheet website for the formula.\n\n\nCreate a multi-class dataset using the spirals data creation function from CS231n (see below for the code).\n\n\nConstruct a model capable of fitting the data (you may need a combination of linear and non-linear layers).\nBuild a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).\nMake a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like).\nPlot the decision boundaries on the spirals dataset from your model predictions, the plot_decision_boundary() function should work for this dataset too.\n\n# Code for creating a spiral dataset from CS231n\nimport numpy as np\nN = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# lets visualize the data\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "02_pytorch_classification.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "02_pytorch_classification.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "4Â  02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜",
    "section": "4.15 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "4.15 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nWrite down 3 problems where you think machine classification could be useful (these can be anything, get creative as you like, for example, classifying credit card transactions as fraud or not fraud based on the purchase amount and purchase location features).\nResearch the concept of â€œmomentumâ€ in gradient-based optimizers (like SGD or Adam), what does it mean?\nSpend 10-minutes reading the Wikipedia page for different activation functions, how many of these can you line up with PyTorchâ€™s activation functions?\nResearch when accuracy might be a poor metric to use (hint: read â€œBeyond Accuracyâ€ by by Will Koehrsen for ideas).\nWatch: For an idea of whatâ€™s happening within our neural networks and what theyâ€™re doing to learn, watch MITâ€™s Introduction to Deep Learning video.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>02 - PyTorch ì‹ ê²½ë§ ë¶„ë¥˜</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html",
    "href": "03_pytorch_computer_vision.html",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "",
    "text": "5.1 Where does computer vision get used?\nComputer vision is the art of teaching a computer to see.\nFor example, it could involve building a model to classify whether a photo is of a cat or a dog (binary classification).\nOr whether a photo is of a cat, dog or chicken (multi-class classification).\nOr identifying where a car appears in a video frame (object detection).\nOr figuring out where different objects in an image can be separated (panoptic segmentation).\nExample computer vision problems for binary classification, multiclass classification, object detection and segmentation.\nIf you use a smartphone, youâ€™ve already used computer vision.\nCamera and photo apps use computer vision to enhance and sort images.\nModern cars use computer vision to avoid other cars and stay within lane lines.\nManufacturers use computer vision to identify defects in various products.\nSecurity cameras use computer vision to detect potential intruders.\nIn essence, anything that can described in a visual sense can be a potential computer vision problem.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "03_pytorch_computer_vision.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.2 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "5.2 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nWeâ€™re going to apply the PyTorch Workflow weâ€™ve been learning in the past couple of sections to computer vision.\n\n\n\na PyTorch workflow with a computer vision focus\n\n\nSpecifically, weâ€™re going to cover:\n\n\n\nTopic\nContents\n\n\n\n\n0. Computer vision libraries in PyTorch\nPyTorch has a bunch of built-in helpful computer vision libraries, letâ€™s check them out.\n\n\n1. Load data\nTo practice computer vision, weâ€™ll start with some images of different pieces of clothing from FashionMNIST.\n\n\n2. Prepare data\nWeâ€™ve got some images, letâ€™s load them in with a PyTorch DataLoader so we can use them with our training loop.\n\n\n3. Model 0: Building a baseline model\nHere weâ€™ll create a multi-class classification model to learn patterns in the data, weâ€™ll also choose a loss function, optimizer and build a training loop.\n\n\n4. Making predictions and evaluting model 0\nLetâ€™s make some predictions with our baseline model and evaluate them.\n\n\n5. Setup device agnostic code for future models\nItâ€™s best practice to write device-agnostic code, so letâ€™s set it up.\n\n\n6. Model 1: Adding non-linearity\nExperimenting is a large part of machine learning, letâ€™s try and improve upon our baseline model by adding non-linear layers.\n\n\n7. Model 2: Convolutional Neural Network (CNN)\nTime to get computer vision specific and introduce the powerful convolutional neural network architecture.\n\n\n8. Comparing our models\nWeâ€™ve built three different models, letâ€™s compare them.\n\n\n9. Evaluating our best model\nLetâ€™s make some predictons on random images and evaluate our best model.\n\n\n10. Making a confusion matrix\nA confusion matrix is a great way to evaluate a classification model, letâ€™s see how we can make one.\n\n\n11. Saving and loading the best performing model\nSince we might want to use our model for later, letâ€™s save it and make sure it loads back in correctly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "href": "03_pytorch_computer_vision.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.3 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³",
    "text": "5.3 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³\nAll of the materials for this course live on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page there too.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#computer-vision-libraries-in-pytorch",
    "href": "03_pytorch_computer_vision.html#computer-vision-libraries-in-pytorch",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.4 0. Computer vision libraries in PyTorch",
    "text": "5.4 0. Computer vision libraries in PyTorch\nBefore we get started writing code, letâ€™s talk about some PyTorch computer vision libraries you should be aware of.\n\n\n\nPyTorch module\nWhat does it do?\n\n\n\n\ntorchvision\nContains datasets, model architectures and image transformations often used for computer vision problems.\n\n\ntorchvision.datasets\nHere youâ€™ll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains a series of base classes for making custom datasets.\n\n\ntorchvision.models\nThis module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.\n\n\ntorchvision.transforms\nOften images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.\n\n\ntorch.utils.data.Dataset\nBase dataset class for PyTorch.\n\n\ntorch.utils.data.DataLoader\nCreates a Python iterable over a dataset (created with torch.utils.data.Dataset).\n\n\n\n\nì°¸ê³ : The torch.utils.data.Dataset and torch.utils.data.DataLoader classes arenâ€™t only for computer vision in PyTorch, they are capable of dealing with many different types of data.\n\nNow weâ€™ve covered some of the most important PyTorch computer vision libraries, letâ€™s import the relevant dependencies.\n\n# Import PyTorch\nimport torch\nfrom torch import nn\n\n# Import torchvision \nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Import matplotlib for visualization\nimport matplotlib.pyplot as plt\n\n# Check versions\n# ì°¸ê³ : your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\nprint(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")\n\nPyTorch version: 1.11.0\ntorchvision version: 0.12.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#getting-a-dataset",
    "href": "03_pytorch_computer_vision.html#getting-a-dataset",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.5 1. Getting a dataset",
    "text": "5.5 1. Getting a dataset\nTo begin working on a computer vision problem, letâ€™s get a computer vision dataset.\nWeâ€™re going to start with FashionMNIST.\nMNIST stands for Modified National Institute of Standards and Technology.\nThe original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.\nFashionMNIST, made by Zalando Research, is a similar setup.\nExcept it contains grayscale images of 10 different kinds of clothing.\n torchvision.datasets contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), itâ€™s a multi-class classification problem.\nLater, weâ€™ll be building a computer vision neural network to identify the different styles of clothing in these images.\nPyTorch has a bunch of common computer vision datasets stored in torchvision.datasets.\nIncluding FashionMNIST in torchvision.datasets.FashionMNIST().\nTo download it, we provide the following parameters: * root: str - which folder do you want to download the data to? * train: Bool - do you want the training or test split? * download: Bool - should the data be downloaded? * transform: torchvision.transforms - what transformations would you like to do on the data? * target_transform - you can transform the targets (labels) if you like too.\nMany other datasets in torchvision have these parameter options.\n\n# Setup training data\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=ToTensor()\n)\n\nLetâ€™s check out the first sample of the training data.\n\n# See first training sample\nimage, label = train_data[0]\nimage, label\n\n(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n           0.0157, 0.0000, 0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0471, 0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n           0.3020, 0.5098, 0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n           0.5529, 0.3451, 0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n           0.4824, 0.7686, 0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n           0.8745, 0.9608, 0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n           0.8627, 0.9529, 0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n           0.8863, 0.7725, 0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n           0.9608, 0.4667, 0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n           0.8510, 0.8196, 0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n           0.8549, 1.0000, 0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n           0.8784, 0.9569, 0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n           0.9137, 0.9333, 0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n           0.8627, 0.9098, 0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n           0.8706, 0.8941, 0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n           0.8745, 0.8784, 0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n           0.8627, 0.8667, 0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n           0.7098, 0.8039, 0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n           0.6549, 0.6941, 0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n           0.7529, 0.8471, 0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n           0.3882, 0.2275, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 9)\n\n\n\n5.5.1 1.1 Input and output shapes of a computer vision model\nWeâ€™ve got a big tensor of values (the image) leading to a single value for the target (the label).\nLetâ€™s see the image shape.\n\n# What's the shape of the image?\nimage.shape\n\ntorch.Size([1, 28, 28])\n\n\nThe shape of the image tensor is [1, 28, 28] or more specifically:\n[color_channels=1, height=28, width=28]\nHaving color_channels=1 means the image is grayscale.\n Various problems will have various input and output shapes. But the premise reamins: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.\nIf color_channels=3, the image comes in pixel values for red, green and blue (this is also known a the RGB color model).\nThe order of our current tensor is often referred to as CHW (Color Channels, Height, Width).\nThereâ€™s debate on whether images should be represented as CHW (color channels first) or HWC (color channels last).\n\nì°¸ê³ : Youâ€™ll also see NCHW and NHWC formats where N stands for number of images. For example if you have a batch_size=32, your tensor shape may be [32, 1, 28, 28]. Weâ€™ll cover batch sizes later.\n\nPyTorch generally accepts NCHW (channels first) as the default for many operators.\nHowever, PyTorch also explains that NHWC (channels last) performs better and is considered best practice.\nFor now, since our dataset and models are relatively small, this wonâ€™t make too much of a difference.\nBut keep it in mind for when youâ€™re working on larger image datasets and using convolutional neural networks (weâ€™ll see these later).\nLetâ€™s check out more shapes of our data.\n\n# How many samples are there? \nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n\n(60000, 60000, 10000, 10000)\n\n\nSo weâ€™ve got 60,000 training samples and 10,000 testing samples.\nWhat classes are there?\nWe can find these via the .classes attribute.\n\n# See classes\nclass_names = train_data.classes\nclass_names\n\n['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']\n\n\nSweet! It looks like weâ€™re dealing with 10 different kinds of clothes.\nBecause weâ€™re working with 10 different classes, it means our problem is multi-class classification.\nLetâ€™s get visual.\n\n\n5.5.2 1.2 Visualizing our data\n\nimport matplotlib.pyplot as plt\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\nplt.title(label);\n\nImage shape: torch.Size([1, 28, 28])\n\n\n\n\n\n\n\n\n\nWe can turn the image into grayscale using the cmap parameter of plt.imshow().\n\nplt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(class_names[label]);\n\n\n\n\n\n\n\n\nBeautiful, well as beautiful as a pixelated grayscale ankle boot can get.\nLetâ€™s view a few more.\n\n# Plot more images\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows * cols + 1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n\n\n\n\n\n\n\n\nHmmm, this dataset doesnâ€™t look too aesthetic.\nBut the principles weâ€™re going to learn on how to build a model for it will be similar across a wide range of computer vision problems.\nIn essence, taking pixel values and building a model to find patterns in them to use on future pixel values.\nPlus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?\nYou probably could.\nBut I think coding a model in PyTorch would be faster.\n\nQuestion: Do you think the above data can be model with only straight (linear) lines? Or do you think youâ€™d also need non-straight (non-linear) lines?",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#prepare-dataloader",
    "href": "03_pytorch_computer_vision.html#prepare-dataloader",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.6 2. Prepare DataLoader",
    "text": "5.6 2. Prepare DataLoader\nNow weâ€™ve got a dataset ready to go.\nThe next step is to prepare it with a torch.utils.data.DataLoader or DataLoader for short.\nThe DataLoader does what you think it might do.\nIt helps load data into a model.\nFor training and for inference.\nIt turns a large Dataset into a Python iterable of smaller chunks.\nThese smaller chunks are called batches or mini-batches and can be set by the batch_size parameter.\nWhy do this?\nBecause itâ€™s more computationally efficient.\nIn an ideal world you could do the forward pass and backward pass across all of your data at once.\nBut once you start using really large datasets, unless youâ€™ve got infinite computing power, itâ€™s easier to break them up into batches.\nIt also gives your model more opportunities to improve.\nWith mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\nWhatâ€™s a good batch size?\n32 is a good place to start for a fair amount of problems.\nBut since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g.Â 32, 64, 128, 256, 512).\n Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.\nLetâ€™s create DataLoaderâ€™s for our training and test sets.\n\nfrom torch.utils.data import DataLoader\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 32\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True # shuffle data every epoch?\n)\n\ntest_dataloader = DataLoader(test_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False # don't necessarily have to shuffle the testing data\n)\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n\nDataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193a8a90&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7f9e193b0700&gt;)\nLength of train dataloader: 1875 batches of 32\nLength of test dataloader: 313 batches of 32\n\n\n\n# Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n\n(torch.Size([32, 1, 28, 28]), torch.Size([32]))\n\n\nAnd we can see that the data remains unchanged by checking a single sample.\n\n# Show a sample\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n\nImage size: torch.Size([1, 28, 28])\nLabel: 6, label size: torch.Size([])",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#model-0-build-a-baseline-model",
    "href": "03_pytorch_computer_vision.html#model-0-build-a-baseline-model",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.7 3. Model 0: Build a baseline model",
    "text": "5.7 3. Model 0: Build a baseline model\nData loaded and prepared!\nTime to build a baseline model by subclassing nn.Module.\nA baseline model is one of the simplest models you can imagine.\nYou use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.\nOur baseline will consist of two nn.Linear() layers.\nWeâ€™ve done this in a previous section but thereâ€™s going to one slight difference.\nBecause weâ€™re working with image data, weâ€™re going to use a different layer to start things off.\nAnd thatâ€™s the nn.Flatten() layer.\nnn.Flatten() compresses the dimensions of a tensor into a single vector.\nThis is easier to understand when you see it.\n\n# Create a flatten layer\nflatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n\n# Get a single sample\nx = train_features_batch[0]\n\n# Flatten the sample\noutput = flatten_model(x) # perform forward pass\n\n# Print out what happened\nprint(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n\n# Try uncommenting below and see what happens\n#print(x)\n#print(output)\n\nShape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]\nShape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]\n\n\nThe nn.Flatten() layer took our shape from [color_channels, height, width] to [color_channels, height*width].\nWhy do this?\nBecause weâ€™ve now turned our pixel data from height and width dimensions into one long feature vector.\nAnd nn.Linear() layers like their inputs to be in the form of feature vectors.\nLetâ€™s create our first model using nn.Flatten() as the first layer.\n\nfrom torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # neural networks like their inputs in vector form\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n\nWonderful!\nWeâ€™ve got a baseline model class we can use, now letâ€™s instantiate a model.\nWeâ€™ll need to set the following parameters: * input_shape=784 - this is how many features youâ€™ve got going in the model, in our case, itâ€™s one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features). * hidden_units=10 - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small weâ€™ll start with 10. * output_shape=len(class_names) - since weâ€™re working with a multi-class classification problem, we need an output neuron per class in our dataset.\nLetâ€™s create an instance of our model and send to the CPU for now (weâ€™ll run a small test for running model_0 on CPU vs.Â a similar model on GPU soon).\n\ntorch.manual_seed(42)\n\n# Need to setup model with input parameters\nmodel_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n    hidden_units=10, # how many units in the hidden layer\n    output_shape=len(class_names) # one for every class\n)\nmodel_0.to(\"cpu\") # keep model on CPU to begin with \n\nFashionMNISTModelV0(\n  (layer_stack): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n\n\n\n5.7.1 3.1 Setup loss, optimizer and evaluation metrics\nSince weâ€™re working on a classification problem, letâ€™s bring in our helper_functions.py script and subsequently the accuracy_fn() we defined in notebook 02.\n\nì°¸ê³ : Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the TorchMetrics package.\n\n\nimport requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # ì°¸ê³ : you need the \"raw\" GitHub URL for this to work\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nhelper_functions.py already exists, skipping download\n\n\n\n# Import accuracy metric\nfrom helper_functions import accuracy_fn # ì°¸ê³ : could also use torchmetrics.Accuracy()\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n\n\n\n5.7.2 3.2 Creating a function to time our experiments\nLoss function and optimizer ready!\nItâ€™s time to start training a model.\nBut how about we do a little experiment while we train.\nI mean, letâ€™s make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\nWeâ€™ll train this model on the CPU but the next one on the GPU and see what happens.\nOur timing function will import the timeit.default_timer() function from the Python timeit module.\n\nfrom timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n    \"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n\n\n\n5.7.3 3.3 Creating a training loop and training a model on batches of data\nBeautiful!\nLooks like weâ€™ve got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.\nLetâ€™s now create a training loop and a testing loop to train and evaluate our model.\nWeâ€™ll be using the same steps as the previous notebook(s), though since our data is now in batch form, weâ€™ll add another loop to loop through our data batches.\nOur data batches are contained within our DataLoaders, train_dataloader and test_dataloader for the training and test data splits respectively.\nA batch is BATCH_SIZE samples of X (features) and y (labels), since weâ€™re using BATCH_SIZE=32, our batches have 32 samples of images and targets.\nAnd since weâ€™re computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.\nThis means weâ€™ll have to divide our loss and accuracy values by the number of batches in each datasetâ€™s respective dataloader.\nLetâ€™s step through it: 1. Loop through epochs. 2. Loop through training batches, perform training steps, calculate the train loss per batch. 3. Loop through testing batches, perform testing steps, calculate the test loss per batch. 4. Print out whatâ€™s happening. 5. Time it all (for fun).\nA fair few steps butâ€¦\nâ€¦if in doubt, code it out.\n\n# Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# Set the seed and start the timer\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Set the number of epochs (we'll keep this small for faster training times)\nepochs = 3\n\n# Create training and testing loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate loss (per batch)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulatively add up the loss per epoch \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Print out how many samples have been seen\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n\n    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n    train_loss /= len(train_dataloader)\n    \n    ### Testing\n    # Setup variables for accumulatively adding up loss and accuracy \n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X)\n           \n            # 2. Calculate loss (accumatively)\n            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n\n            # 3. Calculate accuracy (preds need to be same as y_true)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # Calculations on test metrics need to happen inside torch.inference_mode()\n        # Divide total test loss by length of test dataloader (per batch)\n        test_loss /= len(test_dataloader)\n\n        # Divide total accuracy by length of test dataloader (per batch)\n        test_acc /= len(test_dataloader)\n\n    ## Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n\n# Calculate training time      \ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n\n\n\n\nEpoch: 0\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n\nEpoch: 1\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n\nEpoch: 2\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n\nTrain time on cpu: 14.975 seconds\n\n\nNice! Looks like our baseline model did fairly well.\nIt didnâ€™t take too long to train either, even just on the CPU, I wonder if itâ€™ll speed up on the GPU?\nLetâ€™s write some code to evaluate our model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#make-predictions-and-get-model-0-results",
    "href": "03_pytorch_computer_vision.html#make-predictions-and-get-model-0-results",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.8 4. Make predictions and get Model 0 results",
    "text": "5.8 4. Make predictions and get Model 0 results\nSince weâ€™re going to be building a few models, itâ€™s a good idea to write some code to evaluate them all in similar ways.\nNamely, letâ€™s create a function that takes in a trained model, a DataLoader, a loss function and an accuracy function.\nThe function will use the model to make predictions on the data in the DataLoader and then we can evaluate those predictions using the loss function and accuracy function.\n\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Make predictions with the model\n            y_pred = model(X)\n            \n            # Accumulate the loss and accuracy values per batch\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # Scale loss and acc to find the average loss/acc per batch\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 0 results on test dataset\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n\n{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}\n\n\nLooking good!\nWe can use this dictionary to compare the baseline model results to other models later on.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one",
    "href": "03_pytorch_computer_vision.html#setup-device-agnostic-code-for-using-a-gpu-if-there-is-one",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.9 5. Setup device agnostic-code (for using a GPU if there is one)",
    "text": "5.9 5. Setup device agnostic-code (for using a GPU if there is one)\nWeâ€™ve seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.\n\nì°¸ê³ : Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.\n\nNow letâ€™s setup some device-agnostic code for our models and data to run on GPU if itâ€™s available.\nIf youâ€™re running this notebook on Google Colab, and you donâ€™t a GPU turned on yet, itâ€™s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU. If you do this, your runtime will likely reset and youâ€™ll have to run all of the cells above by going Runtime -&gt; Run before.\n\n# Setup device agnostic code\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\nBeautiful!\nLetâ€™s build another model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#model-1-building-a-better-model-with-non-linearity",
    "href": "03_pytorch_computer_vision.html#model-1-building-a-better-model-with-non-linearity",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.10 6. Model 1: Building a better model with non-linearity",
    "text": "5.10 6. Model 1: Building a better model with non-linearity\nWe learned about the power of non-linearity in notebook 02.\nSeeing the data weâ€™ve been working with, do you think it needs non-linear functions?\nAnd remember, linear means straight and non-linear means non-straight.\nLetâ€™s find out.\nWeâ€™ll do so by recreating a similar model to before, except this time weâ€™ll put non-linear functions (nn.ReLU()) in between each linear layer.\n\n# Create a model with non-linear and linear layers\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n\nThat looks good.\nNow letâ€™s instantiate it with the same settings we used before.\nWeâ€™ll need input_shape=784 (equal to the number of features of our image data), hidden_units=10 (starting small and the same as our baseline model) and output_shape=len(class_names) (one output unit per class).\n\nì°¸ê³ : Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again.\n\n\ntorch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n    hidden_units=10,\n    output_shape=len(class_names) # number of output classes desired\n).to(device) # send model to GPU if it's available\nnext(model_1.parameters()).device # check model device\n\ndevice(type='cuda', index=0)\n\n\n\n5.10.1 6.1 Setup loss, optimizer and evaluation metrics\nAs usual, weâ€™ll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but weâ€™ll stick with accuracy for now).\n\nfrom helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n\n\n\n5.10.2 6.2 Functionizing training and test loops\nSo far weâ€™ve been writing train and test loops over and over.\nLetâ€™s write them again but this time weâ€™ll put them in functions so they can be called again and again.\nAnd because weâ€™re using device-agnostic code now, weâ€™ll be sure to call .to(device) on our feature (X) and target (y) tensors.\nFor the training loop weâ€™ll create a function called train_step() which takes in a model, a DataLoader a loss function and an optimizer.\nThe testing loop will be similar but itâ€™ll be called test_step() and itâ€™ll take in a model, a DataLoader, a loss function and an evaluation function.\n\nì°¸ê³ : Since these are functions, you can customize them in any way you like. What weâ€™re making here can be considered barebones training and testing functions for our specific classification use case.\n\n\ndef train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    for batch, (X, y) in enumerate(data_loader):\n        # Send data to GPU\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.eval() # put model in eval mode\n    # Turn on inference context manager\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred = model(X)\n            \n            # 2. Calculate loss and accuracy\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels\n            )\n        \n        # Adjust metrics and print out\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n\nWoohoo!\nNow weâ€™ve got some functions for training and testing our model, letâ€™s run them.\nWeâ€™ll do so inside another loop for each epoch.\nThat way for each epoch weâ€™re going a training and a testing step.\n\nì°¸ê³ : You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.\n\nLetâ€™s also time things to see how long our code takes to run on the GPU.\n\ntorch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n\n\n\n\nEpoch: 0\n---------\nTrain loss: 1.09199 | Train accuracy: 61.34%\nTest loss: 0.95636 | Test accuracy: 65.00%\n\nEpoch: 1\n---------\nTrain loss: 0.78101 | Train accuracy: 71.93%\nTest loss: 0.72227 | Test accuracy: 73.91%\n\nEpoch: 2\n---------\nTrain loss: 0.67027 | Train accuracy: 75.94%\nTest loss: 0.68500 | Test accuracy: 75.02%\n\nTrain time on cuda: 16.943 seconds\n\n\nExcellent!\nOur model trained but the training time took longer?\n\nì°¸ê³ : The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU youâ€™re using. Read on for a more explained answer.\n\n\nQuestion: â€œI used a a GPU but my model didnâ€™t train faster, why might that be?â€\nAnswer: Well, one reason could be because your dataset and model are both so small (like the dataset and model weâ€™re working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.\nThereâ€™s a small bottleneck between copying data from the CPU memory (default) to the GPU memory.\nSo for smaller models and datasets, the CPU might actually be the optimal place to compute on.\nBut for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.\nHowever, this is largely dependant on the hardware youâ€™re using. With practice, you will get used to where the best place to train your models is.\n\nLetâ€™s evaluate our trained model_1 using our eval_model() function and see how it went.\n\ntorch.manual_seed(42)\n\n# ì°¸ê³ : This will error due to `eval_model()` not using device agnostic code \nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_1084458/2906876561.py in &lt;module&gt;\n      2 \n      3 # Note: This will error due to `eval_model()` not using device agnostic code\n----&gt; 4 model_1_results = eval_model(model=model_1, \n      5     data_loader=test_dataloader,\n      6     loss_fn=loss_fn,\n\n/tmp/ipykernel_1084458/2300884397.py in eval_model(model, data_loader, loss_fn, accuracy_fn)\n     20         for X, y in data_loader:\n     21             # Make predictions with the model\n---&gt; 22             y_pred = model(X)\n     23 \n     24             # Accumulate the loss and accuracy values per batch\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n/tmp/ipykernel_1084458/3744982926.py in forward(self, x)\n     12 \n     13     def forward(self, x: torch.Tensor):\n---&gt; 14         return self.layer_stack(x)\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/container.py in forward(self, input)\n    139     def forward(self, input):\n    140         for module in self:\n--&gt; 141             input = module(input)\n    142         return input\n    143 \n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/code/pytorch/env/lib/python3.9/site-packages/torch/nn/modules/linear.py in forward(self, input)\n    101 \n    102     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 103         return F.linear(input, self.weight, self.bias)\n    104 \n    105     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\n\n\nOh no!\nIt looks like our eval_model() function errors out with:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n\nItâ€™s because weâ€™ve setup our data and model to use device-agnostic code but not our evaluation function.\nHow about we fix that by passing a target device parameter to our eval_model() function?\nThen weâ€™ll try calculating the results again.\n\n# Move values to device\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n    \"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 1 results with device-agnostic code \nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n\n{'model_name': 'FashionMNISTModelV1',\n 'model_loss': 0.6850008964538574,\n 'model_acc': 75.01996805111821}\n\n\n\n# Check baseline results\nmodel_0_results\n\n{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}\n\n\nWoah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.\nThatâ€™s a thing to note in machine learning, sometimes the thing you thought should work doesnâ€™t.\nAnd then the thing you thought might not work does.\nItâ€™s part science, part art.\nFrom the looks of things, it seems like our model is overfitting on the training data.\nOverfitting means our model is learning the training data well but those patterns arenâ€™t generalizing to the testing data.\nTwo of the main to fix overfitting include: 1. Using a smaller or different model (some models fit certain kinds of data better than others). 2. Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).\nThere are more, but Iâ€™m going to leave that as a challenge for you to explore.\nTry searching online, â€œways to prevent overfitting in machine learningâ€ and see what comes up.\nIn the meantime, letâ€™s take a look at number 1: using a different model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#model-2-building-a-convolutional-neural-network-cnn",
    "href": "03_pytorch_computer_vision.html#model-2-building-a-convolutional-neural-network-cnn",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.11 7. Model 2: Building a Convolutional Neural Network (CNN)",
    "text": "5.11 7. Model 2: Building a Convolutional Neural Network (CNN)\nAlright, time to step things up a notch.\nItâ€™s time to create a Convolutional Neural Network (CNN or ConvNet).\nCNNâ€™s are known for their capabilities to find patterns in visual data.\nAnd since weâ€™re dealing with visual data, letâ€™s see if using a CNN model can improve upon our baseline.\nThe CNN model weâ€™re going to be using is known as TinyVGG from the CNN Explainer website.\nIt follows the typical structure of a convolutional neural network:\nInput layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer\nWhere the contents of [Convolutional layer -&gt; activation layer -&gt; pooling layer] can be upscaled and repeated multiple times, depending on requirements.\n\n5.11.1 What model should I use?\n\nQuestion: Wait, you say CNNâ€™s are good for images, are there any other model types I should be aware of?\n\nGood question.\nThis table is a good general guide for which model to use (though there are exceptions).\n\n\n\nProblem type\nModel to use (generally)\nCode example\n\n\n\n\nStructured data (Excel spreadsheets, row and column data)\nGradient boosted models, Random Forests, XGBoost\nsklearn.ensemble, XGBoost library\n\n\nUnstructured data (images, audio, language)\nConvolutional Neural Networks, Transformers\ntorchvision.models, HuggingFace Transformers\n\n\n\n\nì°¸ê³ : The table above is only for reference, the model you end up using will be highly dependant on the problem youâ€™re working on and the constraints you have (amount of data, latency requirements).\n\nEnough talking about models, letâ€™s now build a CNN that replicates the model on the CNN Explainer website.\n\n\n\nTinyVGG architecture, as setup by CNN explainer website\n\n\nTo do so, weâ€™ll leverage the nn.Conv2d() and nn.MaxPool2d() layers from torch.nn.\n\n# Create a convolutional neural network \nclass FashionMNISTModelV2(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n\nFashionMNISTModelV2(\n  (block_1): Sequential(\n    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=490, out_features=10, bias=True)\n  )\n)\n\n\nNice!\nOur biggest model yet!\nWhat weâ€™ve done is a common practice in machine learning.\nFind a model architecture somewhere and replicate it with code.\n\n\n5.11.2 7.1 Stepping through nn.Conv2d()\nWe could start using our model above and see what happens but letâ€™s first step through the two new layers weâ€™ve added: * nn.Conv2d(), also known as a convolutional layer. * nn.MaxPool2d(), also known as a max pooling layer.\n\nQuestion: What does the â€œ2dâ€ in nn.Conv2d() stand for?\nThe 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, thereâ€™s color channel dimension but each of the color channel dimensions have two dimensions too: height and width.\nFor other dimensional data (such as 1D for text or 3D for 3D objects) thereâ€™s also nn.Conv1d() and nn.Conv3d().\n\nTo test the layers out, letâ€™s create some toy data just like the data used on CNN Explainer.\n\ntorch.manual_seed(42)\n\n# Create sample batch of random numbers with same size as image batch\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # get a single image for testing\nprint(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"Single image pixel values:\\n{test_image}\")\n\nImage batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nSingle image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nSingle image pixel values:\ntensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n         ...,\n         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n\n        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n         ...,\n         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n\n        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n         ...,\n         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n\n\nLetâ€™s create an example nn.Conv2d() with various parameters: * in_channels (int) - Number of channels in the input image. * out_channels (int) - Number of channels produced by the convolution. * kernel_size (int or tuple) - Size of the convolving kernel/filter. * stride (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1. * padding (int, tuple, str) - Padding added to all four sides of input. Default: 0.\n\n\n\nexample of going through the different parameters of a Conv2d layer\n\n\nExample of what happens when you change the hyperparameters of a nn.Conv2d() layer.\n\ntorch.manual_seed(42)\n\n# Create a convolutional layer with same dimensions as TinyVGG \n# (try changing any of the parameters and see what happens)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # also try using \"valid\" or \"same\" here \n\n# Pass the data through the convolutional layer\nconv_layer(test_image) # ì°¸ê³ : If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) \n\ntensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n         ...,\n         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n\n        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n         ...,\n         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n\n        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n         ...,\n         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n\n        ...,\n\n        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n         ...,\n         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n\n        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n         ...,\n         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n\n        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n         ...,\n         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n       grad_fn=&lt;SqueezeBackward1&gt;)\n\n\nIf we try to pass a single image in, we get a shape mismatch error:\n\nRuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead\nì°¸ê³ : If youâ€™re running PyTorch 1.11.0+, this error wonâ€™t occur.\n\nThis is because our nn.Conv2d() layer expects a 4-dimensional tensor as input with size (N, C, H, W) or [batch_size, color_channels, height, width].\nRight now our single image test_image only has a shape of [color_channels, height, width] or [3, 64, 64].\nWe can fix this for a single image using test_image.unsqueeze(dim=0) to add an extra dimension for N.\n\n# Add extra dimension to test image\ntest_image.unsqueeze(dim=0).shape\n\ntorch.Size([1, 3, 64, 64])\n\n\n\n# Pass test image with extra dimension through conv_layer\nconv_layer(test_image.unsqueeze(dim=0)).shape\n\ntorch.Size([1, 10, 62, 62])\n\n\nHmm, notice what happens to our shape (the same shape as the first layer of TinyVGG on CNN Explainer), we get different channel sizes as well as different pixel sizes.\nWhat if we changed the values of conv_layer?\n\ntorch.manual_seed(42)\n# Create a new conv_layer with different values (try setting these to whatever you like)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n\ntorch.Size([1, 10, 30, 30])\n\n\nWoah, we get another shape change.\nNow our image is of shape [1, 10, 30, 30] (it will be different if you use different values) or [batch_size=1, color_channels=10, height=30, width=30].\nWhatâ€™s going on here?\nBehind the scenes, our nn.Conv2d() is compressing the information stored in the image.\nIt does this by performing operations on the input (our test image) against its internal parameters.\nThe goal of this is similar to all of the other neural networks weâ€™ve been building.\nData goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.\nThe only difference is how the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer forward() method.\nIf we check out our conv_layer_2.state_dict() weâ€™ll find a similar weight and bias setup as weâ€™ve seen before.\n\n# Check out the conv_layer_2 internal parameters\nprint(conv_layer_2.state_dict())\n\nOrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n\n         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n\n         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n\n\n        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n\n         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n\n         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n\n\n        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n\n         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n\n         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n\n\n        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n\n         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n\n         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n\n\n        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n\n         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n\n         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n\n\n        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n\n         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n\n         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n\n\n        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n\n         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n\n         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n\n\n        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n\n         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n\n         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n\n\n        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n\n         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n\n         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n\n\n        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n\n         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n\n         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n        -0.0797,  0.1121]))])\n\n\nLook at that! A bunch of random numbers for a weight and bias tensor.\nThe shapes of these are manipulated by the inputs we passed to nn.Conv2d() when we set it up.\nLetâ€™s check them out.\n\n# Get shapes of weight and bias tensors within conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n\nconv_layer_2 weight shape: \ntorch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n\nconv_layer_2 bias shape: \ntorch.Size([10]) -&gt; [out_channels=10]\n\n\n\nQuestion: What should we set the parameters of our nn.Conv2d() layers?\nThatâ€™s a good one. But similar to many other things in machine learning, the values of these arenâ€™t set in stone (and recall, because these values are ones we can set ourselves, theyâ€™re referred to as â€œhyperparametersâ€).\nThe best way to find out is to try out different values and see how they effect your modelâ€™s performance.\nOr even better, find a working example on a problem similar to yours (like weâ€™ve done with TinyVGG) and copy it.\n\nWeâ€™re working with a different of layer here to what weâ€™ve seen before.\nBut the premise remains the same: start with random numbers and update them to better represent the data.\n\n\n5.11.3 7.2 Stepping through nn.MaxPool2d()\nNow letâ€™s check out what happens when we move data through nn.MaxPool2d().\n\n# Print out original image shape without and with unsqueezed dimension\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Create a sample nn.MaxPoo2d() layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass data through just the conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pass data through the max pool layer\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n\nTest image original shape: torch.Size([3, 64, 64])\nTest image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\nShape after going through conv_layer(): torch.Size([1, 10, 62, 62])\nShape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n\n\nNotice the change in the shapes of whatâ€™s happening in and out of a nn.MaxPool2d() layer.\nThe kernel_size of the nn.MaxPool2d() layer will effects the size of the output shape.\nIn our case, the shape halves from a 62x62 image to 31x31 image.\nLetâ€™s see this work with a smaller tensor.\n\ntorch.manual_seed(42)\n# Create a random tensor with a similiar number of dimensions to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n\nRandom tensor:\ntensor([[[[0.3367, 0.1288],\n          [0.2345, 0.2303]]]])\nRandom tensor shape: torch.Size([1, 1, 2, 2])\n\nMax pool tensor:\ntensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor\nMax pool tensor shape: torch.Size([1, 1, 1, 1])\n\n\nNotice the final two dimensions between random_tensor and max_pool_tensor, they go from [2, 2] to [1, 1].\nIn essence, they get halved.\nAnd the change would be different for different values of kernel_size for nn.MaxPool2d().\nAlso notice the value leftover in max_pool_tensor is the maximum value from random_tensor.\nWhatâ€™s happening here?\nThis is another important piece of the puzzle of neural networks.\nEssentially, every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space.\nIn other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being smaller in size than the original values.\nFrom an artificial intelligence perspective, you could consider the whole goal of a neural network to compress information.\n\n\n\neach layer of a neural network compresses the original input data into a smaller representation that is (hopefully) capable of making predictions on future input data\n\n\nThis means, that from the point of view of a neural network, intelligence is compression.\nThis is the idea of the use of a nn.MaxPool2d() layer: take the maximum value from a portion of a tensor and disregard the rest.\nIn essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.\nIt is the same story for a nn.Conv2d() layer.\nExcept instead of just taking the maximum, the nn.Conv2d() performs a conovlutional operation on the data (see this in action on the CNN Explainer webpage).\n\nExercise: What do you think the nn.AvgPool2d() layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.\n\n\nì¶”ê°€ í•™ìŠµ ìë£Œ: Lookup â€œmost common convolutional neural networksâ€, what architectures do you find? Are any of them contained within the torchvision.models library? What do you think you could do with these?\n\n\n\n5.11.4 7.3 Setup a loss function and optimizer for model_2\nWeâ€™ve stepped through the layers in our first CNN enough.\nBut remember, if something still isnâ€™t clear, try starting small.\nPick a single layer of a model, pass some data through it and see what happens.\nNow itâ€™s time to move forward and get to training!\nLetâ€™s setup a loss function and an optimizer.\nWeâ€™ll use the functions as before, nn.CrossEntropyLoss() as the loss function (since weâ€™re working with multi-class classification data).\nAnd torch.optim.SGD() as the optimizer to optimize model_2.parameters() with a learning rate of 0.1.\n\n# Setup loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n\n\n\n5.11.5 7.4 Training and testing model_2 using our training and test functions\nLoss and optimizer ready!\nTime to train and test.\nWeâ€™ll use our train_step() and test_step() functions we created before.\nWeâ€™ll also measure the time to compare it to our other models.\n\ntorch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# Train and test model \nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n\n\n\n\nEpoch: 0\n---------\nTrain loss: 0.59411 | Train accuracy: 78.41%\nTest loss: 0.39967 | Test accuracy: 85.70%\n\nEpoch: 1\n---------\nTrain loss: 0.36450 | Train accuracy: 86.81%\nTest loss: 0.34607 | Test accuracy: 87.48%\n\nEpoch: 2\n---------\nTrain loss: 0.32553 | Train accuracy: 88.33%\nTest loss: 0.32664 | Test accuracy: 88.23%\n\nTrain time on cuda: 21.099 seconds\n\n\nWoah! Looks like the convolutional and max pooling layers helped improve performance a little.\nLetâ€™s evaluate model_2â€™s results with our eval_model() function.\n\n# Get model_2 results \nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results\n\n{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.32664385437965393,\n 'model_acc': 88.22883386581469}",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#compare-model-results-and-training-time",
    "href": "03_pytorch_computer_vision.html#compare-model-results-and-training-time",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.12 8. Compare model results and training time",
    "text": "5.12 8. Compare model results and training time\nWeâ€™ve trained three different models.\n\nmodel_0 - our baseline model with two nn.Linear() layers.\nmodel_1 - the same setup as our baseline model except with nn.ReLU() layers in between the nn.Linear() layers.\nmodel_2 - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.\n\nThis is a regular practice in machine learning.\nBuilding multiple models and performing multiple training experiments to see which performs best.\nLetâ€™s combine our model results dictionaries into a DataFrame and find out.\n\nimport pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n\n\n\n\n\n\n\n\nmodel_name\nmodel_loss\nmodel_acc\n\n\n\n\n0\nFashionMNISTModelV0\n0.476639\n83.426518\n\n\n1\nFashionMNISTModelV1\n0.685001\n75.019968\n\n\n2\nFashionMNISTModelV2\n0.326644\n88.228834\n\n\n\n\n\n\n\nNice!\nWe can add the training time values too.\n\n# Add training times to results comparison\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n\n\n\n\n\n\n\n\nmodel_name\nmodel_loss\nmodel_acc\ntraining_time\n\n\n\n\n0\nFashionMNISTModelV0\n0.476639\n83.426518\n14.974887\n\n\n1\nFashionMNISTModelV1\n0.685001\n75.019968\n16.942553\n\n\n2\nFashionMNISTModelV2\n0.326644\n88.228834\n21.098929\n\n\n\n\n\n\n\nIt looks like our CNN (FashionMNISTModelV2) model performed the best (lowest loss, highest accuracy) but had the longest training time.\nAnd our baseline model (FashionMNISTModelV0) performed better than model_1 (FashionMNISTModelV1) but took longer to train (this is likely because we used a CPU to train model_0 but a GPU to train model_1).\nThe tradeoffs here are known as the performance-speed tradeoff.\nGenerally, you get better performance out of a larger, more complex model (like we did with model_2).\nHowever, this performance increase often comes at a sacrifice of training speed and inference speed.\n\nì°¸ê³ : The training times you get will be very dependant on the hardware you use.\nGenerally, the more CPU cores you have, the faster your models will train on CPU. And similar for GPUs.\nNewer hardware (in terms of age) will also often train models faster due to incorporating technology advances.\n\nHow about we get visual?\n\n# Visualize our model results\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#make-and-evaluate-random-predictions-with-best-model",
    "href": "03_pytorch_computer_vision.html#make-and-evaluate-random-predictions-with-best-model",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.13 9. Make and evaluate random predictions with best model",
    "text": "5.13 9. Make and evaluate random predictions with best model\nAlright, weâ€™ve compared our models to each other, letâ€™s further evaluate our best performing model, model_2.\nTo do so, letâ€™s create a function make_predictions() where we can pass the model and some data for it to predict on.\n\ndef make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n\n\nimport random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# View the first test sample shape and label\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n\nTest sample image shape: torch.Size([1, 28, 28])\nTest sample label: 5 (Sandal)\n\n\nAnd now we can use our make_predictions() function to predict on test_samples.\n\n# Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n\ntensor([[2.3550e-07, 1.7185e-08, 4.6618e-07, 6.1371e-08, 5.1185e-08, 9.9957e-01,\n         3.7702e-07, 1.5924e-05, 3.7681e-05, 3.7831e-04],\n        [7.3275e-02, 6.7410e-01, 3.7231e-03, 8.8129e-02, 1.0114e-01, 6.9186e-05,\n         5.8674e-02, 4.2595e-04, 3.8635e-04, 7.1354e-05]])\n\n\nExcellent!\nAnd now we can go from prediction probabilities to prediction labels by taking the torch.argmax() of the output of the torch.softmax() activation function.\n\n# Turn the prediction probabilities into prediction labels by taking the argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n\ntensor([5, 1, 7, 4, 3, 0, 4, 7, 1])\n\n\n\n# Are our predictions in the same form as our test labels? \ntest_labels, pred_classes\n\n([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))\n\n\nNow our predicted classes are in the same format as our test labels, we can compare.\nSince weâ€™re dealing with image data, letâ€™s stay true to the data explorerâ€™s motto.\nâ€œVisualize, visualize, visualize!â€\n\n# Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n\n\n\n\n\n\n\n\nWell, well, well, doesnâ€™t that look good!\nNot bad for a couple dozen lines of PyTorch code!",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#making-a-confusion-matrix-for-further-prediction-evaluation",
    "href": "03_pytorch_computer_vision.html#making-a-confusion-matrix-for-further-prediction-evaluation",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.14 10. Making a confusion matrix for further prediction evaluation",
    "text": "5.14 10. Making a confusion matrix for further prediction evaluation\nThere are many different evaluation metrics we can use for classification problems.\nOne of the most visual is a confusion matrix.\nA confusion matrix shows you where your classification model got confused between predicitons and true labels.\nTo make a confusion matrix, weâ€™ll go through three steps: 1. Make predictions with our trained model, model_2 (a confusion matrix compares predictions to true labels). 2. Make a confusion matrix using torch.ConfusionMatrix. 3. Plot the confusion matrix using mlxtend.plotting.plot_confusion_matrix().\nLetâ€™s start by making predictions with our trained model.\n\n# Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# 1. Make predictions with trained model\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenate list of predictions into a tensor\ny_pred_tensor = torch.cat(y_preds)\n\n\n\n\nWonderful!\nNow weâ€™ve got predictions, letâ€™s go through steps 2 & 3: 2. Make a confusion matrix using torchmetrics.ConfusionMatrix. 3. Plot the confusion matrix using mlxtend.plotting.plot_confusion_matrix().\nFirst weâ€™ll need to make sure weâ€™ve got torchmetrics and mlxtend installed (these two libraries will help us make and visual a confusion matrix).\n\nì°¸ê³ : If youâ€™re using Google Colab, the default version of mlxtend installed is 0.14.0 (as of March 2022), however, for the parameters of the plot_confusion_matrix() function weâ€™d like use, we need 0.19.0 or higher.\n\n\n# See if torchmetrics exists, if not, install it\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- ì°¸ê³ : If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n\nmlxtend version: 0.19.0\n\n\nTo plot the confusion matrix, we need to make sure weâ€™ve got and mlxtend version of 0.19.0 or higher.\n\n# Import mlxtend upgraded version\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n\n0.19.0\n\n\ntorchmetrics and mlxtend installed, letâ€™s make a confusion matrix!\nFirst weâ€™ll create a torchmetrics.ConfusionMatrix instance telling it how many classes weâ€™re dealing with by setting num_classes=len(class_names).\nThen weâ€™ll create a confusion matrix (in tensor format) by passing our instance our modelâ€™s predictions (preds=y_pred_tensor) and targets (target=test_data.targets).\nFinally we can plot our confision matrix using the plot_confusion_matrix() function from mlxtend.plotting.\n\nfrom torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Setup confusion matrix instance and compare predictions to targets\nconfmat = ConfusionMatrix(num_classes=len(class_names))\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Plot the confusion matrix\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);\n\n\n\n\n\n\n\n\nWoah! Doesnâ€™t that look good?\nWe can see our model does fairly well since most of the dark squares are down the diagonal from top left to bottom right (and ideal model will have only values in these squares and 0 everywhere else).\nThe model gets most â€œconfusedâ€ on classes that are similar, for example predicting â€œPulloverâ€ for images that are actually labelled â€œShirtâ€.\nAnd the same for predicting â€œShirtâ€ for classes that are actually labelled â€œT-shirt/topâ€.\nThis kind of information is often more helpful that a single accuracy metric because it tells use where a model is getting things wrong.\nIt also hints at why the model may be getting certain things wrong.\nItâ€™s understandable the model sometimes predicts â€œShirtâ€ for images labelled â€œT-shirt/topâ€.\nWe can use this kind of information to further inspect our models and data to see how it could be improved.\n\nExercise: Use the trained model_2 to make predictions on the test FashionMNIST dataset. Then plot some predictions where the model was wrong alongside what the label of the image shouldâ€™ve been. After visualing these predictions do you think itâ€™s more of a modelling error or a data error? As in, could the model do better or are the labels of the data too close to each other (e.g.Â a â€œShirtâ€ label is too close to â€œT-shirt/topâ€)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#save-and-load-best-performing-model",
    "href": "03_pytorch_computer_vision.html#save-and-load-best-performing-model",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.15 11. Save and load best performing model",
    "text": "5.15 11. Save and load best performing model\nLetâ€™s finish this section off by saving and loading in our best performing model.\nRecall from notebook 01 we can save and load a PyTorch model using a combination of: * torch.save - a function to save a whole PyTorch model or a modelâ€™s state_dict(). * torch.load - a function to load in a saved PyTorch object. * torch.nn.Module.load_state_dict() - a function to load a saved state_dict() into an existing model instance.\nYou can see more of these three in the PyTorch saving and loading models documentation.\nFor now, letâ€™s save our model_2â€™s state_dict() then load it back in and evaluate it to make sure the save and load went correctly.\n\nfrom pathlib import Path\n\n# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # create parent directories if needed\n                 exist_ok=True # if models directory already exists, don't error\n)\n\n# Create model save path\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n           f=MODEL_SAVE_PATH)\n\nSaving model to: models/03_pytorch_computer_vision_model_2.pth\n\n\nNow weâ€™ve got a saved model state_dict() we can load it back in using a combination of load_state_dict() and torch.load().\nSince weâ€™re using load_state_dict(), weâ€™ll need to create a new instance of FashionMNISTModelV2() with the same input parameters as our saved model state_dict().\n\n# Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n# ì°¸ê³ : loading model will error if the shapes here aren't the same as the saved version\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # try changing this to 128 and seeing what happens \n                                    output_shape=10) \n\n# Load in the saved state_dict()\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# Send model to GPU\nloaded_model_2 = loaded_model_2.to(device)\n\nAnd now weâ€™ve got a loaded model we can evaluate it with eval_model() to make sure its parameters work similarly to model_2 prior to saving.\n\n# Evaluate loaded model\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n\n{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.32664385437965393,\n 'model_acc': 88.22883386581469}\n\n\nDo these results look the same as model_2_results?\n\nmodel_2_results\n\n{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.32664385437965393,\n 'model_acc': 88.22883386581469}\n\n\nWe can find out if two tensors are close to each other using torch.isclose() and passing in a tolerance level of closeness via the parameters atol (absolute tolerance) and rtol (relative tolerance).\nIf our modelâ€™s results are close, the output of torch.isclose() should be true.\n\n# Check to see if results are close to each other (if they are very far away, there may be an error)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # absolute tolerance\n              rtol=0.0001) # relative tolerance\n\ntensor(True)",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "03_pytorch_computer_vision.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.16 ì—°ìŠµ ë¬¸ì œ",
    "text": "5.16 ì—°ìŠµ ë¬¸ì œ\nAll of the exercises are focused on practicing the code in the sections above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 03 * Example solutions notebook for 03 (try the exercises before looking at this)\n\nWhat are 3 areas in industry where computer vision is currently being used?\nSearch â€œwhat is overfitting in machine learningâ€ and write down a sentence about what you find.\nSearch â€œways to prevent overfitting in machine learningâ€, write down 3 of the things you find and a sentence about each. ì°¸ê³ : there are lots of these, so donâ€™t worry too much about all of them, just pick 3 and start with those.\nSpend 20-minutes reading and clicking through the CNN Explainer website.\n\nUpload your own example image using the â€œuploadâ€ button and see what happens in each layer of a CNN as your image passes through it.\n\nLoad the torchvision.datasets.MNIST() train and test datasets.\nVisualize at least 5 different samples of the MNIST training dataset.\nTurn the MNIST train and test datasets into dataloaders using torch.utils.data.DataLoader, set the batch_size=32.\nRecreate model_2 used in this notebook (the same model from the CNN Explainer website, also known as TinyVGG) capable of fitting on the MNIST dataset.\nTrain the model you built in exercise 8. on CPU and GPU and see how long it takes on each.\nMake predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label.\nPlot a confusion matrix comparing your modelâ€™s predictions to the truth labels.\nCreate a random tensor of shape [1, 3, 64, 64] and pass it through a nn.Conv2d() layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the kernel_size parameter goes up and down?\nUse a model similar to the trained model_2 from this notebook to make predictions on the test torchvision.datasets.FashionMNIST dataset.\n\nThen plot some predictions where the model was wrong alongside what the label of the image shouldâ€™ve been.\nAfter visualing these predictions do you think itâ€™s more of a modelling error or a data error?\nAs in, could the model do better or are the labels of the data too close to each other (e.g.Â a â€œShirtâ€ label is too close to â€œT-shirt/topâ€)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "03_pytorch_computer_vision.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "03_pytorch_computer_vision.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "5Â  03 - PyTorch ì»´í“¨í„° ë¹„ì „",
    "section": "5.17 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "5.17 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nWatch: MITâ€™s Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks.\nSpend 10-minutes clicking thorugh the different options of the PyTorch vision library, what different modules are available?\nLookup â€œmost common convolutional neural networksâ€, what architectures do you find? Are any of them contained within the torchvision.models library? What do you think you could do with these?\nFor a large number of pretrained PyTorch computer vision models as well as many different extensions to PyTorchâ€™s computer vision functionalities check out the PyTorch Image Models library timm (Torch Image Models) by Ross Wightman.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>03 - PyTorch ì»´í“¨í„° ë¹„ì „</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html",
    "href": "04_pytorch_custom_datasets.html",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "",
    "text": "6.1 What is a custom dataset?\nIn the last notebook, notebook 03, we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).\nThe steps we took are similar across many different problems in machine learning.\nFind a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.\nPyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, youâ€™ll often want to use your own custom dataset.\nA custom dataset is a collection of data relating to a specific problem youâ€™re working on.\nIn essence, a custom dataset can be comprised of almost anything.\nFor example, if we were building a food image classification app like Nutrify, our custom dataset might be images of food.\nOr if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.\nOr if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.\nOr if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.\nPyTorch includes many existing functions to load in various custom datasets in the TorchVision, TorchText, TorchAudio and TorchRec domain libraries.\nBut sometimes these existing functions may not be enough.\nIn that case, we can always subclass torch.utils.data.Dataset and customize it to our liking.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "04_pytorch_custom_datasets.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.2 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "6.2 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nWeâ€™re going to be applying the PyTorch Workflow we covered in notebook 01 and notebook 02 to a computer vision problem.\nBut instead of using an in-built PyTorch dataset, weâ€™re going to be using our own dataset of pizza, steak and sushi images.\nThe goal will be to load these images and then build a model to train and predict on them.\n\nWhat weâ€™re going to build. Weâ€™ll use torchvision.datasets as well as our own custom Dataset class to load in images of food and then weâ€™ll build a PyTorch computer vision model to hopefully be able to classify them.\nSpecifically, weâ€™re going to cover:\n\n\n\n\n\n\n\nTopic\nContents\n\n\n\n\n0. Importing PyTorch and setting up device-agnostic code\nLetâ€™s get PyTorch loaded and then follow best practice to setup our code to be device-agnostic.\n\n\n1. Get data\nWeâ€™re going to be using our own custom dataset of pizza, steak and sushi images.\n\n\n2. Become one with the data (data preparation)\nAt the beginning of any new machine learning problem, itâ€™s paramount to understand the data youâ€™re working with. Here weâ€™ll take some steps to figure out what data we have.\n\n\n3. Transforming data\nOften, the data you get wonâ€™t be 100% ready to use with a machine learning model, here weâ€™ll look at some steps we can take to transform our images so theyâ€™re ready to be used with a model.\n\n\n4. Loading data with ImageFolder (option 1)\nPyTorch has many in-built data loading functions for common types of data. ImageFolder is helpful if our images are in standard image classification format.\n\n\n5. Loading image data with a custom Dataset\nWhat if PyTorch didnâ€™t have an in-built function to load data with? This is where we can build our own custom subclass of torch.utils.data.Dataset.\n\n\n6. Other forms of transforms (data augmentation)\nData augmentation is a common technique for expanding the diversity of your training data. Here weâ€™ll explore some of torchvisionâ€™s in-built data augmentation functions.\n\n\n7. Model 0: TinyVGG without data augmentation\nBy this stage, weâ€™ll have our data ready, letâ€™s build a model capable of fitting it. Weâ€™ll also create some training and testing functions for training and evaluating our model.\n\n\n8. Exploring loss curves\nLoss curves are a great way to see how your model is training/improving over time. Theyâ€™re also a good way to see if your model is underfitting or overfitting.\n\n\n9. Model 1: TinyVGG with data augmentation\nBy now, weâ€™ve tried a model without, how about we try one with data augmentation?\n\n\n10. Compare model results\nLetâ€™s compare our different modelsâ€™ loss curves and see which performed better and discuss some options for improving performance.\n\n\n11. Making a prediction on a custom image\nOur model is trained to on a dataset of pizza, steak and sushi images. In this section weâ€™ll cover how to use our trained model to predict on an image outside of our existing dataset.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "href": "04_pytorch_custom_datasets.html#ë„ì›€ì„-ë°›ì„-ìˆ˜-ìˆëŠ”-ê³³",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.3 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³",
    "text": "6.3 ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³\nAll of the materials for this course live on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page there too.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#importing-pytorch-and-setting-up-device-agnostic-code",
    "href": "04_pytorch_custom_datasets.html#importing-pytorch-and-setting-up-device-agnostic-code",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.4 0. Importing PyTorch and setting up device-agnostic code",
    "text": "6.4 0. Importing PyTorch and setting up device-agnostic code\n\nimport torch\nfrom torch import nn\n\n# ì°¸ê³ : this notebook requires torch &gt;= 1.10.0\ntorch.__version__\n\n'1.11.0'\n\n\nAnd now letâ€™s follow best practice and setup device-agnostic code.\n\nì°¸ê³ : If youâ€™re using Google Colab, and you donâ€™t have a GPU turned on yet, itâ€™s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU. If you do this, your runtime will likely reset and youâ€™ll have to run all of the cells above by going Runtime -&gt; Run before.\n\n\n# Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#get-data",
    "href": "04_pytorch_custom_datasets.html#get-data",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.5 1. Get data",
    "text": "6.5 1. Get data\nFirst thingâ€™s first we need some data.\nAnd like any good cooking show, some data has already been prepared for us.\nWeâ€™re going to start small.\nBecause weâ€™re not looking to train the biggest model or use the biggest dataset yet.\nMachine learning is an iterative process, start small, get something working and increase when necessary.\nThe data weâ€™re going to be using is a subset of the Food101 dataset.\nFood101 is popular computer vision benchmark as it contains 1000 images of 101 different kinds of foods, totaling 101,000 images (75,750 train and 25,250 test).\nCan you think of 101 different foods?\nCan you think of a computer program to classify 101 foods?\nI can.\nA machine learning model!\nSpecifically, a PyTorch computer vision model like we covered in notebook 03.\nInstead of 101 food classes though, weâ€™re going to start with 3: pizza, steak and sushi.\nAnd instead of 1,000 images per class, weâ€™re going to start with a random 10% (start small, increase when necessary).\nIf youâ€™d like to see where the data came from you see the following resources: * Original Food101 dataset and paper website. * torchvision.datasets.Food101 - the version of the data I downloaded for this notebook. * extras/04_custom_data_creation.ipynb - a notebook I used to format the Food101 dataset to use for this notebook. * data/pizza_steak_sushi.zip - the zip archive of pizza, steak and sushi images from Food101, created with the notebook linked above.\nLetâ€™s write some code to download the formatted data from GitHub.\n\nì°¸ê³ : The dataset weâ€™re about to use has been pre-formatted for what weâ€™d like to use it for. However, youâ€™ll often have to format your own datasets for whatever problem youâ€™re working on. This is a regular practice in the machine learning world.\n\n\nimport requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\ndata/pizza_steak_sushi directory exists.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#become-one-with-the-data-data-preparation",
    "href": "04_pytorch_custom_datasets.html#become-one-with-the-data-data-preparation",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.6 2. Become one with the data (data preparation)",
    "text": "6.6 2. Become one with the data (data preparation)\nDataset downloaded!\nTime to become one with it.\nThis is another important step before building a model.\nAs Abraham Lossfunction saidâ€¦\n\nData preparation is paramount. Before building a model, become one with the data. Ask: What am I trying to do here? Source: @mrdbourke Twitter.\nWhatâ€™s inspecting the data and becoming one with it?\nBefore starting a project or building any kind of model, itâ€™s important to know what data youâ€™re working with.\nIn our case, we have images of pizza, steak and sushi in standard image classification format.\nImage classification format contains separate classes of images in seperate directories titled with a particular class name.\nFor example, all images of pizza are contained in the pizza/ directory.\nThis format is popular across many different image classification benchmarks, including ImageNet (of the most popular computer vision benchmark datasets).\nYou can see an example of the storage format below, the images numbers are arbitrary.\npizza_steak_sushi/ &lt;- overall dataset folder\n    train/ &lt;- training images\n        pizza/ &lt;- class name as folder name\n            image01.jpeg\n            image02.jpeg\n            ...\n        steak/\n            image24.jpeg\n            image25.jpeg\n            ...\n        sushi/\n            image37.jpeg\n            ...\n    test/ &lt;- testing images\n        pizza/\n            image101.jpeg\n            image102.jpeg\n            ...\n        steak/\n            image154.jpeg\n            image155.jpeg\n            ...\n        sushi/\n            image167.jpeg\n            ...\nThe goal will be to take this data storage structure and turn it into a dataset usable with PyTorch.\n\nì°¸ê³ : The structure of the data you work with will vary depending on the problem youâ€™re working on. But the premise still remains: become one with the data, then find a way to best turn it into a dataset compatible with PyTorch.\n\nWe can inspect whatâ€™s in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.\nTo do so, weâ€™ll use Pythonâ€™s in-built os.walk().\n\nimport os\ndef walk_through_dir(dir_path):\n  \"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  \n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n\n\nwalk_through_dir(image_path)\n\nThere are 2 directories and 0 images in 'data\\pizza_steak_sushi'.\nThere are 3 directories and 0 images in 'data\\pizza_steak_sushi\\test'.\nThere are 0 directories and 25 images in 'data\\pizza_steak_sushi\\test\\pizza'.\nThere are 0 directories and 19 images in 'data\\pizza_steak_sushi\\test\\steak'.\nThere are 0 directories and 31 images in 'data\\pizza_steak_sushi\\test\\sushi'.\nThere are 3 directories and 0 images in 'data\\pizza_steak_sushi\\train'.\nThere are 0 directories and 78 images in 'data\\pizza_steak_sushi\\train\\pizza'.\nThere are 0 directories and 75 images in 'data\\pizza_steak_sushi\\train\\steak'.\nThere are 0 directories and 72 images in 'data\\pizza_steak_sushi\\train\\sushi'.\n\n\nExcellent!\nIt looks like weâ€™ve got about 75 images per training class and 25 images per testing class.\nThat should be enough to get started.\nRemember, these images are subsets of the original Food101 dataset.\nYou can see how they were created in the data creation notebook.\nWhile weâ€™re at it, letâ€™s setup our training and testing paths.\n\n# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n\n(WindowsPath('data/pizza_steak_sushi/train'),\n WindowsPath('data/pizza_steak_sushi/test'))\n\n\n\n6.6.1 2.1 Visualize an image\nOkay, weâ€™ve seen how our directory structure is formatted.\nNow in the spirit of the data explorer, itâ€™s time to visualize, visualize, visualize!\nLetâ€™s write some code to: 1. Get all of the image paths using pathlib.Path.glob() to find all of the files ending in .jpg. 2. Pick a random image path using Pythonâ€™s random.choice(). 3. Get the image class name using pathlib.Path.parent.stem. 4. And since weâ€™re working with images, weâ€™ll open the random image path using PIL.Image.open() (PIL stands for Python Image Library). 5. Weâ€™ll then show the image and print some metadata.\n\nimport random\nfrom PIL import Image\n\n# Set seed\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n\nRandom image path: data\\pizza_steak_sushi\\test\\sushi\\2394442.jpg\nImage class: sushi\nImage height: 408\nImage width: 512\n\n\n\n\n\n\n\n\n\nWe can do the same with matplotlib.pyplot.imshow(), except we have to convert the image to a NumPy array first.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#transforming-data",
    "href": "04_pytorch_custom_datasets.html#transforming-data",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.7 3. Transforming data",
    "text": "6.7 3. Transforming data\nNow what if we wanted to load our image data into PyTorch?\nBefore we can use our image data with PyTorch we need to:\n\nTurn it into tensors (numerical representations of our images).\nTurn it into a torch.utils.data.Dataset and subsequently a torch.utils.data.DataLoader, weâ€™ll call these Dataset and DataLoader for short.\n\nThere are several different kinds of pre-built datasets and dataset loaders for PyTorch, depending on the problem youâ€™re working on.\n\n\n\nProblem space\nPre-built Datasets and Functions\n\n\n\n\nVision\ntorchvision.datasets\n\n\nAudio\ntorchaudio.datasets\n\n\nText\ntorchtext.datasets\n\n\nRecommendation system\ntorchrec.datasets\n\n\n\nSince weâ€™re working with a vision problem, weâ€™ll be looking at torchvision.datasets for our data loading functions as well as torchvision.transforms for preparing our data.\nLetâ€™s import some base libraries.\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n\n6.7.1 3.1 Transforming data with torchvision.transforms\nWeâ€™ve got folders of images but before we can use them with PyTorch, we need to convert them into tensors.\nOne of the ways we can do this is by using the torchvision.transforms module.\ntorchvision.transforms contains many pre-built methods for formatting images, turning them into tensors and even manipulating them for data augmentation (the practice of altering data to make it harder for a model to learn, weâ€™ll see this later on) purposes .\nTo get experience with torchvision.transforms, letâ€™s write a series of transform steps that: 1. Resize the images using transforms.Resize() (from about 512x512 to 64x64, the same shape as the images on the CNN Explainer website). 2. Flip our images randomly on the horizontal using transforms.RandomHorizontalFlip() (this could be considered a form of data augmentation because it will artificially change our image data). 3. Turn our images from a PIL image to a PyTorch tensor using transforms.ToTensor().\nWe can compile all of these steps using torchvision.transforms.Compose().\n\n# Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n\nNow weâ€™ve got a composition of transforms, letâ€™s write a function to try them out on various images.\n\ndef plot_transformed_images(image_paths, transform, n=3, seed=42):\n    \"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # ì°¸ê³ : permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNice!\nWeâ€™ve now got a way to convert our images to tensors using torchvision.transforms.\nWe also manipulate their size and orientation if needed (some models prefer images of different sizes and shapes).\nGenerally, the larger the shape of the image, the more information a model can recover.\nFor example, an image of size [256, 256, 3] will have 16x more pixels than an image of size [64, 64, 3] ((256*256*3)/(64*64*3)=16).\nHowever, the tradeoff is that more pixels requires more computations.\n\nExercise: Try commenting out one of the transforms in data_transform and running the plotting function plot_transformed_images() again, what happens?",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#option-1-loading-image-data-using-imagefolder",
    "href": "04_pytorch_custom_datasets.html#option-1-loading-image-data-using-imagefolder",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.8 4. Option 1: Loading Image Data Using ImageFolder",
    "text": "6.8 4. Option 1: Loading Image Data Using ImageFolder\nAlright, time to turn our image data into a Dataset capable of being used with PyTorch.\nSince our data is in standard image classification format, we can use the class torchvision.datasets.ImageFolder.\nWhere we can pass it the file path of a target image directory as well as a series of transforms weâ€™d like to perform on our images.\nLetâ€™s test it out on our data folders train_dir and test_dir passing in transform=data_transform to turn our images into tensors.\n\n# Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n\nTrain data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data\\pizza_steak_sushi\\train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data\\pizza_steak_sushi\\test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\n\n\nBeautiful!\nIt looks like PyTorch has registered our Datasetâ€™s.\nLetâ€™s inspect them by checking out the classes and class_to_idx attributes as well as the lengths of our training and test sets.\n\n# Get class names as a list\nclass_names = train_data.classes\nclass_names\n\n['pizza', 'steak', 'sushi']\n\n\n\n# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n\n{'pizza': 0, 'steak': 1, 'sushi': 2}\n\n\n\n# Check the lengths\nlen(train_data), len(test_data)\n\n(225, 75)\n\n\nNice! Looks like weâ€™ll be able to use these to reference for later.\nHow about our images and labels?\nHow do they look?\nWe can index on our train_data and test_data Datasetâ€™s to find samples and their target labels.\n\nimg, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n\nImage tensor:\ntensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],\n         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],\n         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],\n         ...,\n         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],\n\n        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],\n         [0.0706, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],\n         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],\n         ...,\n         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],\n         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],\n         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],\n\n        [[0.0941, 0.0902, 0.0902,  ..., 0.0196, 0.0196, 0.0196],\n         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],\n         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],\n         ...,\n         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1804],\n         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],\n         [0.1059, 0.1020, 0.1059,  ..., 0.1843, 0.1804, 0.1765]]])\nImage shape: torch.Size([3, 64, 64])\nImage datatype: torch.float32\nImage label: 0\nLabel datatype: &lt;class 'int'&gt;\n\n\nOur images are now in the form of a tensor (with shape [3, 64, 64]) and the labels are in the form of an integer relating to a specific class (as referenced by the class_to_idx attribute).\nHow about we plot a single image tensor using matplotlib?\nWeâ€™ll first have to to permute (rearrange the order of its dimensions) so itâ€™s compatible.\nRight now our image dimensions are in the format CHW (color channels, height, width) but matplotlib prefers HWC (height, width, color channels).\n\n# Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n\nOriginal shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nImage permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]\n\n\n\n\n\n\n\n\n\nNotice the image is now more pixelated (less quality).\nThis is due to it being resized from 512x512 to 64x64 pixels.\nThe intuition here is that if you think the image is harder to recognize whatâ€™s going on, chances are a model will find it harder to understand too.\n\n6.8.1 4.1 Turn loaded images into DataLoaderâ€™s\nWeâ€™ve got our images as PyTorch Datasetâ€™s but now letâ€™s turn them into DataLoaderâ€™s.\nWeâ€™ll do so using torch.utils.data.DataLoader.\nTurning our Datasetâ€™s into DataLoaderâ€™s makes them iterable so a model can go through learn the relationships between samples and targets (features and labels).\nTo keep things simple, weâ€™ll use a batch_size=1 and num_workers=1.\nWhatâ€™s num_workers?\nGood question.\nIt defines how many subprocesses will be created to load your data.\nThink of it like this, the higher value num_workers is set to, the more compute power PyTorch will use to load your data.\nPersonally, I usually set it to the total number of CPUs on my machine via Pythonâ€™s os.cpu_count().\nThis ensures the DataLoader recruits as many cores as possible to load data.\n\nì°¸ê³ : There are more parameters you can get familiar with using torch.utils.data.DataLoader in the PyTorch documentation.\n\n\n# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94fa0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94dc0&gt;)\n\n\nWonderful!\nNow our data is iterable.\nLetâ€™s try it out and check the shapes.\n\nimg, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n\nImage shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n\n\nWe could now use these DataLoaderâ€™s with a training and testing loop to train a model.\nBut before we do, letâ€™s look at another option to load images (or almost any other kind of data).",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#option-2-loading-image-data-with-a-custom-dataset",
    "href": "04_pytorch_custom_datasets.html#option-2-loading-image-data-with-a-custom-dataset",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.9 5. Option 2: Loading Image Data with a Custom Dataset",
    "text": "6.9 5. Option 2: Loading Image Data with a Custom Dataset\nWhat if a pre-built Dataset creator like torchvision.datasets.ImageFolder() didnâ€™t exist?\nOr one for your specific problem didnâ€™t exist?\nWell, you could build your own.\nBut wait, what are the pros and cons of creating your own custom way to load Datasetâ€™s?\n\n\n\n\n\n\n\nPros of creating a custom Dataset\nCons of creating a custom Dataset\n\n\n\n\nCan create a Dataset out of almost anything.\nEven though you could create a Dataset out of almost anything, it doesnâ€™t mean it will work.\n\n\nNot limited to PyTorch pre-built Dataset functions.\nUsing a custom Dataset often results in writing more code, which could be prone to errors or performance issues.\n\n\n\nTo see this in action, letâ€™s work towards replicating torchvision.datasets.ImageFolder() by subclassing torch.utils.data.Dataset (the base class for all Datasetâ€™s in PyTorch).\nWeâ€™ll start by importing the modules we need: * Pythonâ€™s os for dealing with directories (our data is stored in directories). * Pythonâ€™s pathlib for dealing with filepaths (each of our images has a unique filepath). * torch for all things PyTorch. * PILâ€™s Image class for loading images. * torch.utils.data.Dataset to subclass and create our own custom Dataset. * torchvision.transforms to turn our images into tensors. * Various types from Pythonâ€™s typing module to add type hints to our code.\n\nì°¸ê³ : You can customize the following steps for your own dataset. The premise remains: write code to load your data in the format youâ€™d like it.\n\n\nimport os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n\nRemember how our instances of torchvision.datasets.ImageFolder() allowed us to use the classes and class_to_idx attributes?\n\n# Instance of torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n\n(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})\n\n\n\n6.9.1 5.1 Creating a helper function to get class names\nLetâ€™s write a helper function capable of creating a list of class names and a dictionary of class names and their indexes given a directory path.\nTo do so, weâ€™ll: 1. Get the class names using os.scandir() to traverse a target directory (ideally the directory is in standard image classification format). 2. Raise an error if the class names arenâ€™t found (if this happens, there might be something wrong with the directory structure). 3. Turn the class names into a dictionary of numerical labels, one for each class.\nLetâ€™s see a small example of step 1 before we write the full function.\n\n# Setup path for target directory\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Get the class names from the target directory\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n\nTarget directory: data\\pizza_steak_sushi\\train\nClass names found: ['pizza', 'steak', 'sushi']\n\n\nExcellent!\nHow about we turn it into a full function?\n\n# Make function to find classes in target directory\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folder names in a target directory.\n    \n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    \n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Crearte a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n\nLooking good!\nNow letâ€™s test out our find_classes() function.\n\nfind_classes(train_dir)\n\n(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})\n\n\nWoohoo! Looking good!\n\n\n6.9.2 5.2 Create a custom Dataset to replicate ImageFolder\nNow weâ€™re ready to build our own custom Dataset.\nWeâ€™ll build one to replicate the functionality of torchvision.datasets.ImageFolder().\nThis will be good practice, plus, itâ€™ll reveal a few of the required steps to make your own custom Dataset.\nItâ€™ll be a fair bit of a codeâ€¦ but nothing we canâ€™t handle!\nLetâ€™s break it down: 1. Subclass torch.utils.data.Dataset. 2. Initialize our subclass with a targ_dir parameter (the target data directory) and transform parameter (so we have the option to transform our data if needed). 3. Create several attributes for paths (the paths of our target images), transform (the transforms we might like to use, this can be None), classes and class_to_idx (from our find_classes() function). 4. Create a function to load images from file and return them, this could be using PIL or torchvision.io (for input/output of vision data). 5. Overwrite the __len__ method of torch.utils.data.Dataset to return the number of samples in the Dataset, this is recommended but not required. This is so you can call len(Dataset). 6. Overwrite the __getitem__ method of torch.utils.data.Dataset to return a single sample from the Dataset, this is required.\nLetâ€™s do it!\n\n# Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n\nWoah! A whole bunch of code to load in our images.\nThis is one of the downsides of creating your own custom Datasetâ€™s.\nHowever, now weâ€™ve written it once, we could move it into a .py file such as data_loader.py along with some other helpful data functions and reuse it later on.\nBefore we test out our new ImageFolderCustom class, letâ€™s create some transforms to prepare our images.\n\n# Augment train data\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n\nNow comes the moment of truth!\nLetâ€™s turn our training images (contained in train_dir) and our testing images (contained in test_dir) into Datasetâ€™s using our own ImageFolderCustom class.\n\ntrain_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n\n(&lt;__main__.ImageFolderCustom at 0x1fd2cf886d0&gt;,\n &lt;__main__.ImageFolderCustom at 0x1fd2cf5a0d0&gt;)\n\n\nHmmâ€¦ no errors, did it work?\nLetâ€™s try calling len() on our new Datasetâ€™s and find the classes and class_to_idx attributes.\n\nlen(train_data_custom), len(test_data_custom)\n\n(225, 75)\n\n\n\ntrain_data_custom.classes\n\n['pizza', 'steak', 'sushi']\n\n\n\ntrain_data_custom.class_to_idx\n\n{'pizza': 0, 'steak': 1, 'sushi': 2}\n\n\nlen(test_data_custom) == len(test_data) and len(test_data_custom) == len(test_data) Yes!!!\nIt looks like it worked.\nWe could check for equality with the Datasetâ€™s made by the torchvision.datasets.ImageFolder() class too.\n\n# Check for equality amongst our custom Dataset and ImageFolder Dataset\nprint((len(train_data_custom) == len(train_data)) & (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n\nTrue\nTrue\nTrue\n\n\nHo ho!\nLook at us go!\nThree Trueâ€™s!\nYou canâ€™t get much better than that.\nHow about we take it up a notch and plot some random images to test our __getitem__ override?\n\n\n6.9.3 5.3 Create a function to display random images\nYou know what time it is!\nTime to put on our data explorerâ€™s hat and visualize, visualize, visualize!\nLetâ€™s create a helper function called display_random_images() that helps us visualize images in our Dataset's.\nSpecifically, itâ€™ll: 1. Take in a Dataset and a number of other parameters such as classes (the names of our target classes), the number of images to display (n) and a random seed. 2. To prevent the display getting out of hand, weâ€™ll cap n at 10 images. 3. Set the random seed for reproducible plots (if seed is set). 4. Get a list of random sample indexes (we can use Pythonâ€™s random.sample() for this) to plot. 5. Setup a matplotlib plot. 6. Loop through the random sample indexes found in step 4 and plot them with matplotlib. 7. Make sure the sample images are of shape HWC (height, width, color channels) so we can plot them.\n\n# 1. Take in a Dataset as well as a list of class names\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n\nWhat a good looking function!\nLetâ€™s test it out first with the Dataset we created with torchvision.datasets.ImageFolder().\n\n# Display random images from ImageFolder created Dataset\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n\n\n\n\n\n\n\n\nAnd now with the Dataset we created with our own ImageFolderCustom.\n\n# Display random images from ImageFolderCustom Dataset\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n\nFor display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\n\n\n\n\n\n\n\n\n\nNice!!!\nLooks like our ImageFolderCustom is working just as weâ€™d like it to.\n\n\n6.9.4 5.4 Turn custom loaded images into DataLoaderâ€™s\nWeâ€™ve got a way to turn our raw images into Datasetâ€™s (features mapped to labels or Xâ€™s mapped to yâ€™s) through our ImageFolderCustom class.\nNow how could we turn our custom Datasetâ€™s into DataLoaderâ€™s?\nIf you guessed by using torch.utils.data.DataLoader(), youâ€™d be right!\nBecause our custom Datasetâ€™s subclass torch.utils.data.Dataset, we can use them directly with torch.utils.data.DataLoader().\nAnd we can do using very similar steps to before except this time weâ€™ll be using our custom created Datasetâ€™s.\n\n# Turn train and test custom Dataset's into DataLoader's\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cfabf10&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cfabb80&gt;)\n\n\nDo the shapes of the samples look the same?\n\n# Get image and label from custom DataLoader\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n\nImage shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n\n\nThey sure do!\nLetâ€™s now take a lot at some other forms of data transforms.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#other-forms-of-transforms-data-augmentation",
    "href": "04_pytorch_custom_datasets.html#other-forms-of-transforms-data-augmentation",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.10 6. Other forms of transforms (data augmentation)",
    "text": "6.10 6. Other forms of transforms (data augmentation)\nWeâ€™ve seen a couple of transforms on our data already but thereâ€™s plenty more.\nYou can see them all in the torchvision.transforms documentation.\nThe purpose of tranforms is to alter your images in some way.\nThat may be turning your images into a tensor (as weâ€™ve seen before).\nOr cropping it or randomly erasing a portion or randomly rotating them.\nDoing this kinds of transforms is often referred to as data augmentation.\nData augmentation is the process of altering your data in such a way that you artificially increase the diversity of your training set.\nTraining a model on this artificially altered dataset hopefully results in a model that is capable of better generalization (the patterns it learns are more robust to future unseen examples).\nYou can see many different examples of data augmentation performed on images using torchvision.transforms in PyTorchâ€™s Illustration of Transforms example.\nBut letâ€™s try one out ourselves.\nMachine learning is all about harnessing the power of randomness and research shows that random transforms (like transforms.RandAugment() and transforms.TrivialAugmentWide()) generally perform better than hand-picked transforms.\nThe idea behind TrivialAugment isâ€¦ well, trivial.\nYou have a set of transforms and you randomly pick a number of them to perform on an image and at a random magnitude between a given range (a higher magnitude means more instense).\nThe PyTorch team even used TrivialAugment it to train their latest state-of-the-art vision models.\n\n\n\ntrivial augment data augmentation being used for PyTorch state of the art training\n\n\nTrivialAugment was one of the ingredients used in a recent state of the art training upgrade to various PyTorch vision models.\nHow about we test it out on some of our own images?\nThe main parameter to pay attention to in transforms.TrivialAugmentWide() is num_magnitude_bins=31.\nIt defines how much of a range an intensity value will be picked to apply a certain transform, 0 being no range and 31 being maximum range (highest chance for highest intensity).\nWe can incorporate transforms.TrivialAugmentWide() into transforms.Compose().\n\nfrom torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 & 1\n])\n\n# Don't need to perform augmentation on the test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n\n\nì°¸ê³ : You usually donâ€™t perform data augmentation on the test set. The idea of data augmentation is to to artificially increase the diversity of the training set to better predict on the testing set.\nHowever, you do need to make sure your test set images are transformed to tensors. We size the test images to the same size as our training images too, however, inference can be done on different size images if necessary (though this may alter performance).\n\nBeautiful, now weâ€™ve got a training transform (with data augmentation) and test transform (without data augmentation).\nLetâ€™s test our data augmentation out!\n\n# Get all image paths\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Plot random images\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry running the cell above a few times and seeing how the original image changes as it goes through the transform.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#model-0-tinyvgg-without-data-augmentation",
    "href": "04_pytorch_custom_datasets.html#model-0-tinyvgg-without-data-augmentation",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.11 7. Model 0: TinyVGG without data augmentation",
    "text": "6.11 7. Model 0: TinyVGG without data augmentation\nAlright, weâ€™ve seen how to turn our data from images in folders to transformed tensors.\nNow letâ€™s construct a computer vision model to see if we can classify if an image is of pizza, steak or sushi.\nTo begin, weâ€™ll start with a simple transform, only resizing the images to (64, 64) and turning them into tensors.\n\n6.11.1 7.1 Creating transforms and loading data for Model 0\n\n# Create simple transform\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n\nExcellent, now weâ€™ve got a simple transform, letâ€™s: 1. Load the data, turning each of our training and test folders first into a Dataset with torchvision.datasets.ImageFolder() 2. Then into a DataLoader using torch.utils.data.DataLoader(). * Weâ€™ll set the batch_size=32 and num_workers to as many CPUs on our machine (this will depend on what machine youâ€™re using).\n\n# 1. Load and transform data\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Turn data into DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Setup batch size and number of workers \nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Create DataLoader's\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n\nCreating DataLoader's with batch size 32 and 16 workers.\n\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2ce5c4f0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd1d0e10d0&gt;)\n\n\nDataLoaderâ€™s created!\nLetâ€™s build a model.\n\n\n6.11.2 7.2 Create TinyVGG model class\nIn notebook 03, we used the TinyVGG model from the CNN Explainer website.\nLetâ€™s recreate the same model, except this time weâ€™ll be using color images instead of grayscale (in_channels=3 instead of in_channels=1 for RGB pixels).\n\nclass TinyVGG(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)\n\n\n\nì°¸ê³ : One of the ways to speed up deep learning models computing on a GPU is to leverage operator fusion.\nThis means in the forward() method in our model above, instead of calling a layer block and reassigning x every time, we call each block in succession (see the final line of the forward() method in the model above for an example).\nThis saves the time spent reassigning x (memory heavy) and focuses on only computing on x.\nSee Making Deep Learning Go Brrrr From First Principles by Horace He for more ways on how to speed up machine learning models.\n\nNow thatâ€™s a nice looking model!\nHow about we test it out with a forward pass on a single image?\n\n\n6.11.3 7.3 Try a forward pass on a single image (to test the model)\nA good way to test a model is to do a forward pass on a single piece of data.\nItâ€™s also handy way to test the input and output shapes of our different layers.\nTo do a forward pass on a single image, letâ€™s: 1. Get a batch of images and labels from the DataLoader. 2. Get a single image from the batch and unsqueeze() the image so it has a batch size of 1 (so its shape fits the model). 3. Perform inference on a single image (making sure to send the image to the target device). 4. Print out whatâ€™s happening and convert the modelâ€™s raw output logits to prediction probabilities with torch.softmax() (since weâ€™re working with multi-class data) and convert the prediction probabilities to prediction labels with torch.argmax().\n\n# 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n\nSingle image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[0.0578, 0.0635, 0.0352]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3352, 0.3371, 0.3277]], device='cuda:0')\n\nOutput prediction label:\ntensor([1], device='cuda:0')\n\nActual label:\n2\n\n\nWonderful, it looks like our model is outputting what weâ€™d expect it to output.\nYou can run the cell above a few times and each time have a different image be predicted on.\nAnd youâ€™ll probably notice the predictions are often wrong.\nThis is to be expected because the model hasnâ€™t been trained yet and itâ€™s essentially guessing using random weights.\n\n\n6.11.4 7.4 Use torchinfo to get an idea of the shapes going through our model\nPrinting out our model with print(model) gives us an idea of whatâ€™s going on with our model.\nAnd we can print out the shapes of our data throughout the forward() method.\nHowever, a helpful way to get information from our model is to use torchinfo.\ntorchinfo comes with a summary() method that takes a PyTorch model as well as an input_shape and returns what happens as a tensor moves through your model.\n\nì°¸ê³ : If youâ€™re using Google Colab, youâ€™ll need to install torchinfo.\n\n\n# Install torchinfo if it's not available, import it if it is\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size \n\nCollecting torchinfo\n  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.6.5\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  --                        --\nâ”œâ”€Sequential: 1-1                        [1, 10, 32, 32]           --\nâ”‚    â””â”€Conv2d: 2-1                       [1, 10, 64, 64]           280\nâ”‚    â””â”€ReLU: 2-2                         [1, 10, 64, 64]           --\nâ”‚    â””â”€Conv2d: 2-3                       [1, 10, 64, 64]           910\nâ”‚    â””â”€ReLU: 2-4                         [1, 10, 64, 64]           --\nâ”‚    â””â”€MaxPool2d: 2-5                    [1, 10, 32, 32]           --\nâ”œâ”€Sequential: 1-2                        [1, 10, 16, 16]           --\nâ”‚    â””â”€Conv2d: 2-6                       [1, 10, 32, 32]           910\nâ”‚    â””â”€ReLU: 2-7                         [1, 10, 32, 32]           --\nâ”‚    â””â”€Conv2d: 2-8                       [1, 10, 32, 32]           910\nâ”‚    â””â”€ReLU: 2-9                         [1, 10, 32, 32]           --\nâ”‚    â””â”€MaxPool2d: 2-10                   [1, 10, 16, 16]           --\nâ”œâ”€Sequential: 1-3                        [1, 3]                    --\nâ”‚    â””â”€Flatten: 2-11                     [1, 2560]                 --\nâ”‚    â””â”€Linear: 2-12                      [1, 3]                    7,683\n==========================================================================================\nTotal params: 10,693\nTrainable params: 10,693\nNon-trainable params: 0\nTotal mult-adds (M): 6.75\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.91\n==========================================================================================\n\n\nNice!\nThe output of torchinfo.summary() gives us a whole bunch of information about our model.\nSuch as Total params, the total number of parameters in our model, the Estimated Total Size (MB) which is the size of our model.\nYou can also see the change in input and output shapes as data of a certain input_size moves through our model.\nRight now, our parameter numbers and total model size is low.\nThis because weâ€™re starting with a small model.\nAnd if we need to increase its size later, we can.\n\n\n6.11.5 7.5 Create train & test loop functions\nWeâ€™ve got data and weâ€™ve got a model.\nNow letâ€™s make some training and test loop functions to train our model on the training data and evaluate our model on the testing data.\nAnd to make sure we can use these the training and testing loops again, weâ€™ll functionize them.\nSpecifically, weâ€™re going to make three functions: 1. train_step() - takes in a model, a DataLoader, a loss function and an optimizer and trains the model on the DataLoader. 2. test_step() - takes in a model, a DataLoader and a loss function and evaluates the model on the DataLoader. 3. train() - performs 1. and 2. together for a given number of epochs and returns a results dictionary.\n\nì°¸ê³ : We covered the steps in a PyTorch opimization loop in notebook 01, as well as theUnofficial PyTorch Optimization Loop Song and weâ€™ve built similar functions in notebook 03.\n\nLetâ€™s start by building train_step().\nBecause weâ€™re dealing with batches in the DataLoaderâ€™s, weâ€™ll accumulate the model loss and accuracy values during training (by adding them up for each batch) and then adjust them at the end before we return them.\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\nWoohoo! train_step() function done.\nNow letâ€™s do the same for the test_step() function.\nThe main difference here will be the test_step() wonâ€™t take in an optimizer and therefore wonâ€™t perform gradient descent.\nBut since weâ€™ll be doing inference, weâ€™ll make sure to turn on the torch.inference_mode() context manager for making predictions.\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n\nExcellent!\n\n\n6.11.6 7.6 Creating a train() function to combine train_step() and test_step()\nNow we need a way to put our train_step() and test_step() functions together.\nTo do so, weâ€™ll package them up in a train() function.\nThis function will train the model as well as evaluate it.\nSpecificially, itâ€™ll: 1. Take in a model, a DataLoader for training and test sets, an optimizer, a loss function and how many epochs to perform each train and test step for. 2. Create an empty results dictionary for train_loss, train_acc, test_loss and test_acc values (we can fill this up as training goes on). 3. Loop through the training and test step functions for a number of epochs. 4. Print out whatâ€™s happening at the end of each epoch. 5. Update the empty results dictionary with the updated metrics each epoch. 6. Return the filled\nTo keep track of the number of epochs weâ€™ve been through, letâ€™s import tqdm from tqdm.auto (tqdm is one of the most popular progress bar libraries for Python and tqdm.auto automatically decides what kind of progress bar is best for your computing environment, e.g.Â Jupyter Notebook vs.Â Python script).\n\nfrom tqdm.auto import tqdm\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n\n\n\n6.11.7 7.7 Train and Evaluate Model 0\nAlright, alright, alright weâ€™ve got all of the ingredients we need to train and evaluate our model.\nTime to put our TinyVGG model, DataLoaderâ€™s and train() function together to see if we can build a model capable of discerning between pizza, steak and sushi!\nLetâ€™s recreate model_0 (we donâ€™t need to but we will for completeness) then call our train() function passing in the necessary parameters.\nTo keep our experiments quick, weâ€™ll train our model for 5 epochs (though you could increase this if you want).\nAs for an optimizer and loss function, weâ€™ll use torch.nn.CrossEntropyLoss() (since weâ€™re working with multi-class classification data) and torch.optim.Adam() with a learning rate of 1e-3 respecitvely.\nTo see how long things take, weâ€™ll import Pythonâ€™s timeit.default_timer() method to calculate the training time.\n\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n\n\n\nEpoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1360 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0847 | train_acc: 0.4258 | test_loss: 1.1620 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.1157 | train_acc: 0.2930 | test_loss: 1.1695 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.0955 | train_acc: 0.4141 | test_loss: 1.1380 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.0985 | train_acc: 0.2930 | test_loss: 1.1420 | test_acc: 0.1979\nTotal training time: 65.122 seconds\n\n\nHmmâ€¦\nIt looks like our model performed pretty poorly.\nBut thatâ€™s okay for now, weâ€™ll keep persevering.\nWhat are some ways you could potentially improve it?\n\nì°¸ê³ : Check out the Improving a model (from a model perspective) section in notebook 02 for ideas on improving our TinyVGG model.\n\n\n\n6.11.8 7.8 Plot the loss curves of Model 0\nFrom the print outs of our model_0 training, it didnâ€™t look like it did too well.\nBut we can further evaluate it by plotting the modelâ€™s loss curves.\nLoss curves show the modelâ€™s results over time.\nAnd theyâ€™re a great way to see how your model performs on different datasets (e.g.Â training and test).\nLetâ€™s create a function to plot the values in our model_0_results dictionary.\n\n# Check the model_0_results keys\nmodel_0_results.keys()\n\ndict_keys(['train_loss', 'train_acc', 'test_loss', 'test_acc'])\n\n\nWeâ€™ll need to extract each of these keys and turn them into a plot.\n\ndef plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n\nOkay, letâ€™s test our plot_loss_curves() function out.\n\nplot_loss_curves(model_0_results)\n\n\n\n\n\n\n\n\nWoah.\nLooks like things are all over the placeâ€¦\nBut we kind of knew that because our modelâ€™s print out results during training didnâ€™t show much promise.\nYou could try training the model for longer and see what happens when you plot a loss curve over a longer time horizon.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#what-should-an-ideal-loss-curve-look-like",
    "href": "04_pytorch_custom_datasets.html#what-should-an-ideal-loss-curve-look-like",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.12 8. What should an ideal loss curve look like?",
    "text": "6.12 8. What should an ideal loss curve look like?\nLooking at training and test loss curves is a great way to see if your model is overfitting.\nAn overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.\nIf your training loss is far lower than your test loss, your model is overfitting.\nAs in, itâ€™s learning the patterns in the training too well and those patterns arenâ€™t generalizing to the test data.\nThe other side is when your training and test loss are not as low as youâ€™d like, this is considered underfitting.\nThe ideal position for a training and test loss curve is for them to line up closely with each other.\n\nLeft: If your training and test loss curves arenâ€™t as low as youâ€™d like, this is considered underfitting. Middle:* When your test/validation loss is higher than your training loss this is considered overfitting. Right: The ideal scenario is when your training and test loss curves line up over time. This means your model is generalizing well. There are more combinations and different things loss curves can do, for more on these, see Googleâ€™s Interpreting Loss Curves guide.*\n\n6.12.1 8.1 How to deal with overfitting\nSince the main problem with overfitting is that youâ€™re model is fitting the training data too well, youâ€™ll want to use techniques to â€œreign it inâ€.\nA common technique of preventing overfitting is known as regularization.\nI like to think of this as â€œmaking our models more regularâ€, as in, capable of fitting more kinds of data.\nLetâ€™s discuss a few methods to prevent overfitting.\n\n\n\nMethod to prevent overfitting\nWhat is it?\n\n\n\n\nGet more data\nHaving more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples.\n\n\nSimplify your model\nIf the current model is already overfitting the training data, it may be too complicated of a model. This means itâ€™s learning the patterns of the data too well and isnâ€™t able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer.\n\n\nUse data augmentation\nData augmentation manipulates the training data in a way so thatâ€™s harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data.\n\n\nUse transfer learning\nTransfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.\n\n\nUse dropout layers\nDropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model but also making the remaining connections better. See torch.nn.Dropout() for more.\n\n\nUse learning rate decay\nThe idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to convergence, the smaller youâ€™ll want your weight updates to be.\n\n\nUse early stopping\nEarly stopping stops model training before it begins to overfit. As in, say the modelâ€™s loss has stopped decreasing for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model weights that had the lowest loss (10 epochs prior).\n\n\n\nThere are more methods for dealing with overfitting but these are some of the main ones.\nAs you start to build more and more deep models, youâ€™ll find because deep learnings are so good at learning patterns in data, dealing with overfitting is one of the primary problems of deep learning.\n\n\n6.12.2 8.2 How to deal with underfitting\nWhen a model is underfitting it is considered to have poor predictive power on the training and test sets.\nIn essence, an underfitting model will fail to reduce the loss values to a desired level.\nRight now, looking at our current loss curves, Iâ€™d considered our TinyVGG model, model_0, to be underfitting the data.\nThe main idea behind dealing with underfitting is to increase your modelâ€™s predictive power.\nThere are several ways to do this.\n\n\n\n\n\n\n\nMethod to prevent underfitting\nWhat is it?\n\n\n\n\nAdd more layers/units to your model\nIf your model is underfitting, it may not have enough capability to learn the required patterns/weights/representations of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden layers/units within those layers.\n\n\nTweak the learning rate\nPerhaps your modelâ€™s learning rate is too high to begin with. And itâ€™s trying to update its weights each epoch too much, in turn not learning anything. In this case, you might lower the learning rate and see what happens.\n\n\nUse transfer learning\nTransfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a previously working model and adjusting them to your own problem.\n\n\nTrain for longer\nSometimes a model just needs more time to learn representations of data. If you find in your smaller experiments your model isnâ€™t learning anything, perhaps leaving it train for a more epochs may result in better performance.\n\n\nUse less regularization\nPerhaps your model is underfitting because youâ€™re trying to prevent overfitting too much. Holding back on regularization techniques can help your model fit the data better.\n\n\n\n\n\n6.12.3 8.3 The balance between overfitting and underfitting\nNone of the methods discussed above are silver bullets, meaning, they donâ€™t always work.\nAnd preventing overfitting and underfitting is possibly the most active area of machine learning research.\nSince everone wants their models to fit better (less underfitting) but not so good they donâ€™t generalize well and perform in the real world (less overfitting).\nThereâ€™s a fine line between overfitting and underfitting.\nBecause too much of each can cause the other.\nTransfer learning is perhaps one of the most powerful techniques when it comes to dealing with both overfitting and underfitting on your own problems.\nRather than handcraft different overfitting and underfitting techniques, transfer learning enables you to take an already working model in a similar problem space to yours (say one from paperswithcode.com/sota or Hugging Face models) and apply it to your own dataset.\nWeâ€™ll see the power of transfer learning in a later notebook.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#model-1-tinyvgg-with-data-augmentation",
    "href": "04_pytorch_custom_datasets.html#model-1-tinyvgg-with-data-augmentation",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.13 9. Model 1: TinyVGG with Data Augmentation",
    "text": "6.13 9. Model 1: TinyVGG with Data Augmentation\nTime to try out another model!\nThis time, letâ€™s load in the data and use data augmentation to see if it improves our results in anyway.\nFirst, weâ€™ll compose a training transform to include transforms.TrivialAugmentWide() as well as resize and turn our images into tensors.\nWeâ€™ll do the same for a testing transform except without the data augmentation.\n\n6.13.1 9.1 Create transform with data augmentation\n\n# Create training transform with TrivialAugment\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# Create testing transform (no data augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n\nWonderful!\nNow letâ€™s turn our images into Datasetâ€™s using torchvision.datasets.ImageFolder() and then into DataLoaderâ€™s with torch.utils.data.DataLoader().\n\n\n6.13.2 9.2 Create train and test Datasetâ€™s and DataLoaderâ€™s\nWeâ€™ll make sure the train Dataset uses the train_transform_trivial_augment and the test Dataset uses the test_transform.\n\n# Turn image folders into Datasets\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n\n(Dataset ImageFolder\n     Number of datapoints: 225\n     Root location: data\\pizza_steak_sushi\\train\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n                ToTensor()\n            ),\n Dataset ImageFolder\n     Number of datapoints: 75\n     Root location: data\\pizza_steak_sushi\\test\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                ToTensor()\n            ))\n\n\nAnd weâ€™ll make DataLoaderâ€™s with a batch_size=32 and with num_workers set to the number of CPUs available on our machine (we can get this using Pythonâ€™s os.cpu_count()).\n\n# Turn Datasets into DataLoader's\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x1fd2d0531c0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x1fd2cf94dc0&gt;)\n\n\n\n\n6.13.3 9.3 Construct and train Model 1\nData loaded!\nNow to build our next model, model_1, we can reuse our TinyVGG class from before.\nWeâ€™ll make sure to send it to the target device.\n\n# Create model_1 and send it to the target device\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)\n\n\nModel ready!\nTime to train!\nSince weâ€™ve already got functions for the training loop (train_step()) and testing loop (test_step()) and a function to put them together in train(), letâ€™s reuse those.\nWeâ€™ll use the same setup as model_0 with only the train_dataloader parameter varying: * Train for 5 epochs. * Use train_dataloader=train_dataloader_augmented as the training data in train(). * Use torch.nn.CrossEntropyLoss() as the loss function (since weâ€™re working with multi-class classification). * Use torch.optim.Adam() with lr=0.001 as the learning rate as the optimizer.\n\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n\n\n\n\nEpoch: 1 | train_loss: 1.1074 | train_acc: 0.2461 | test_loss: 1.1058 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0791 | train_acc: 0.4258 | test_loss: 1.1382 | test_acc: 0.2604\nEpoch: 3 | train_loss: 1.0804 | train_acc: 0.4258 | test_loss: 1.1683 | test_acc: 0.2604\nEpoch: 4 | train_loss: 1.1287 | train_acc: 0.3047 | test_loss: 1.1626 | test_acc: 0.2604\nEpoch: 5 | train_loss: 1.0884 | train_acc: 0.4258 | test_loss: 1.1481 | test_acc: 0.2604\nTotal training time: 67.543 seconds\n\n\nHmmâ€¦\nIt doesnâ€™t look like our model performed very well again.\nLetâ€™s check out its loss curves.\n\n\n6.13.4 9.4 Plot the loss curves of Model 1\nSince weâ€™ve got the results of model_1 saved in a results dictionary, model_1_results, we can plot them using plot_loss_curves().\n\nplot_loss_curves(model_1_results)\n\n\n\n\n\n\n\n\nWowâ€¦\nThese donâ€™t look very good eitherâ€¦\nIs our model underfitting or overfitting?\nOr both?\nIdeally weâ€™d like it have higher accuracy and lower loss right?\nWhat are some methods you could try to use to achieve these?",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#compare-model-results",
    "href": "04_pytorch_custom_datasets.html#compare-model-results",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.14 10. Compare model results",
    "text": "6.14 10. Compare model results\nEven though our models our performing quite poorly, we can still write code to compare them.\nLetâ€™s first turn our model results in pandas DataFrames.\n\nimport pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n\n\n\n\n\n\n\n\ntrain_loss\ntrain_acc\ntest_loss\ntest_acc\n\n\n\n\n0\n1.107832\n0.257812\n1.136025\n0.260417\n\n\n1\n1.084726\n0.425781\n1.161953\n0.197917\n\n\n2\n1.115656\n0.292969\n1.169479\n0.197917\n\n\n3\n1.095543\n0.414062\n1.137993\n0.197917\n\n\n4\n1.098464\n0.292969\n1.142002\n0.197917\n\n\n\n\n\n\n\nAnd now we can write some plotting code using matplotlib to visualize the results of model_0 and model_1 together.\n\n# Setup a plot \nplt.figure(figsize=(15, 10))\n\n# Get number of epochs\nepochs = range(len(model_0_df))\n\n# Plot train loss\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test loss\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot train accuracy\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test accuracy\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n\n\n\n\n\n\n\n\nIt looks like our models both performed equally poorly and were kind of sporadic (the metrics go up and down sharply).\nIf you built model_2, what would you do differently to try and improve performance?",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#make-a-prediction-on-a-custom-image",
    "href": "04_pytorch_custom_datasets.html#make-a-prediction-on-a-custom-image",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.15 11. Make a prediction on a custom image",
    "text": "6.15 11. Make a prediction on a custom image\nIf youâ€™ve trained a model on a certain dataset, chances are youâ€™d like to make a prediction on on your own custom data.\nIn our case, since weâ€™ve trained a model on pizza, steak and sushi images, how could we use our model to make a prediction on one of our own images?\nTo do so, we can load an image and then preprocess it in a way that matches the type of data our model was trained on.\nIn other words, weâ€™ll have to convert our own custom image to a tensor and make sure itâ€™s in the right datatype before passing it to our model.\nLetâ€™s start by downloading a custom image.\nSince our model predicts whether an image contains pizza, steak or sushi, letâ€™s download a photo of my Dad giving two thumbs up to a big pizza from the Learn PyTorch for Deep Learning GitHub.\nWe download the image using Pythonâ€™s requests module.\n\nì°¸ê³ : If youâ€™re using Google Colab, you can also upload an image to the current session by going to the left hand side menu -&gt; Files -&gt; Upload to session storage. Beware though, this image will delete when your Google Colab session ends.\n\n\n# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\ndata\\04-pizza-dad.jpeg already exists, skipping download.\n\n\n\n6.15.1 11.1 Loading in a custom image with PyTorch\nExcellent!\nLooks like weâ€™ve got a custom image downloaded and ready to go at data/04-pizza-dad.jpeg.\nTime to load it in.\nPyTorchâ€™s torchvision has several input and output (â€œIOâ€ or â€œioâ€ for short) methods for reading and writing images and video in torchvision.io.\nSince we want to load in an image, weâ€™ll use torchvision.io.read_image().\nThis method will read a JPEG or PNG image and turn it into a 3 dimensional RGB or grayscale torch.Tensor with values of datatype uint8 in range [0, 255].\nLetâ€™s try it out.\n\nimport torchvision\n\n# Read in custom image\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n\nCustom image tensor:\ntensor([[[154, 175, 181,  ...,  21,  18,  14],\n         [146, 167, 180,  ...,  21,  18,  15],\n         [124, 146, 171,  ...,  18,  17,  15],\n         ...,\n         [ 72,  59,  45,  ..., 152, 150, 148],\n         [ 64,  55,  41,  ..., 150, 147, 144],\n         [ 64,  60,  46,  ..., 149, 146, 143]],\n\n        [[171, 189, 193,  ...,  22,  19,  15],\n         [163, 181, 194,  ...,  22,  19,  16],\n         [141, 163, 185,  ...,  19,  18,  16],\n         ...,\n         [ 55,  42,  28,  ..., 106, 104, 102],\n         [ 47,  38,  24,  ..., 108, 105, 102],\n         [ 47,  43,  29,  ..., 107, 104, 101]],\n\n        [[117, 138, 145,  ...,  17,  14,  10],\n         [109, 130, 145,  ...,  17,  14,  11],\n         [ 87, 111, 136,  ...,  14,  13,  11],\n         ...,\n         [ 35,  22,   8,  ...,  54,  52,  50],\n         [ 27,  18,   4,  ...,  50,  47,  44],\n         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.uint8\n\n\nNice! Looks like our image is in tensor format, however, is this image format compatible with our model?\nOur custom_image tensor is of datatype torch.uint8 and its values are between [0, 255].\nBut our model takes image tensors of datatype torch.float32 and with values between [0, 1].\nSo before we use our custom image with our model, weâ€™ll need to convert it to the same format as the data our model is trained on.\nIf we donâ€™t do this, our model will error.\n\n# Try to make a prediction on image in uint8 format (this will error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [65], in &lt;cell line: 3&gt;()\n      2 model_1.eval()\n      3 with torch.inference_mode():\n----&gt; 4     model_1(custom_image_uint8.to(device))\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447, in Conv2d.forward(self, input)\n    446 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 447     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n    439 if self.padding_mode != 'zeros':\n    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    441                     weight, bias, self.stride,\n    442                     _pair(0), self.dilation, self.groups)\n--&gt; 443 return F.conv2d(input, weight, bias, self.stride,\n    444                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same\n\n\n\nIf we try to make a prediction on an image in a different datatype to what our model was trained on, we get an error like the following:\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same\n\nLetâ€™s fix this by converting our custom image to the same datatype as what our model was trained on (torch.float32).\n\n# Load in custom image and convert the tensor values to float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divide the image pixel values by 255 to get them between [0, 1]\ncustom_image = custom_image / 255. \n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n\nCustom image tensor:\ntensor([[[0.6039, 0.6863, 0.7098,  ..., 0.0824, 0.0706, 0.0549],\n         [0.5725, 0.6549, 0.7059,  ..., 0.0824, 0.0706, 0.0588],\n         [0.4863, 0.5725, 0.6706,  ..., 0.0706, 0.0667, 0.0588],\n         ...,\n         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],\n         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],\n         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],\n\n        [[0.6706, 0.7412, 0.7569,  ..., 0.0863, 0.0745, 0.0588],\n         [0.6392, 0.7098, 0.7608,  ..., 0.0863, 0.0745, 0.0627],\n         [0.5529, 0.6392, 0.7255,  ..., 0.0745, 0.0706, 0.0627],\n         ...,\n         [0.2157, 0.1647, 0.1098,  ..., 0.4157, 0.4078, 0.4000],\n         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4118, 0.4000],\n         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],\n\n        [[0.4588, 0.5412, 0.5686,  ..., 0.0667, 0.0549, 0.0392],\n         [0.4275, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],\n         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],\n         ...,\n         [0.1373, 0.0863, 0.0314,  ..., 0.2118, 0.2039, 0.1961],\n         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1843, 0.1725],\n         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.float32\n\n\n\n\n6.15.2 11.2 Predicting on custom images with a trained PyTorch model\nBeautiful, it looks like our image data is now in the same format our model was trained on.\nExcept for one thingâ€¦\nItâ€™s shape.\nOur model was trained on images with shape [3, 64, 64], whereas our custom image is currently [3, 4032, 3024].\nHow could we make sure our custom image is the same shape as the images our model was trained on?\nAre there any torchvision.transforms that could help?\nBefore we answer that question, letâ€™s plot the image with matplotlib to make sure it looks okay, remember weâ€™ll have to permute the dimensions from CHW to HWC to suit matplotlibâ€™s requirements.\n\n# Plot custom image\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n\n\n\n\n\n\n\n\nTwo thumbs up!\nNow how could we get our image to be the same size as the images our model was trained on?\nOne way to do so is with torchvision.transforms.Resize().\nLetâ€™s compose a transform pipeline to do so.\n\n# Create transform pipleine to resize image\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transform target image\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Print out original shape and new shape\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n\nOriginal shape: torch.Size([3, 4032, 3024])\nNew shape: torch.Size([3, 64, 64])\n\n\nWoohoo!\nLetâ€™s finally make a prediction on our own custom image.\n\nmodel_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [69], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447, in Conv2d.forward(self, input)\n    446 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 447     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443, in Conv2d._conv_forward(self, input, weight, bias)\n    439 if self.padding_mode != 'zeros':\n    440     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    441                     weight, bias, self.stride,\n    442                     _pair(0), self.dilation, self.groups)\n--&gt; 443 return F.conv2d(input, weight, bias, self.stride,\n    444                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)\n\n\n\nOh my goodnessâ€¦\nDespite our preparations our custom image and model are on different devices.\nAnd we get the error:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)\n\nLetâ€™s fix that by putting our custom_image_transformed on the target device.\n\nmodel_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [70], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed.to(device))\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [45], in TinyVGG.forward(self, x)\n     42 x = self.conv_block_2(x)\n     43 # print(x.shape)\n---&gt; 44 x = self.classifier(x)\n     45 # print(x.shape)\n     46 return x\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:141, in Sequential.forward(self, input)\n    139 def forward(self, input):\n    140     for module in self:\n--&gt; 141         input = module(input)\n    142     return input\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110, in Module._call_impl(self, *input, **kwargs)\n   1106 # If we don't have any hooks, we want to skip the rest of the logic in\n   1107 # this function, and just call forward.\n   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110     return forward_call(*input, **kwargs)\n   1111 # Do not call functions when jit is used\n   1112 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~\\mambaforge\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103, in Linear.forward(self, input)\n    102 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 103     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)\n\n\n\nWhat now?\nIt looks like weâ€™re getting a shape error.\nWhy might this be?\nWe converted our custom image to be the same size as the images our model was trained onâ€¦\nOh waitâ€¦\nThereâ€™s one dimension we forgot about.\nThe batch size.\nOur model expects image tensors with a batch size dimension at the start (NCHW where N is the batch size).\nExcept our custom image is currently only CHW.\nWe can add a batch size dimension using torch.unsqueeze(dim=0) to add an extra dimension our image and finally make a prediction.\nEssentially weâ€™ll be telling our model to predict on a single image (an image with a batch_size of 1).\n\nmodel_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n\nCustom image transformed shape: torch.Size([3, 64, 64])\nUnsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n\n\nYes!!!\nIt looks like it worked!\n\nì°¸ê³ : What weâ€™ve just gone through are three of the classical and most common deep learning and PyTorch issues: 1. Wrong datatypes - our model expects torch.float32 where our original custom image was uint8. 2. Wrong device - our model was on the target device (in our case, the GPU) whereas our target data hadnâ€™t been moved to the target device yet. 3. Wrong shapes - our model expected an input image of shape [N, C, H, W] or [batch_size, color_channels, height, width] whereas our custom image tensor was of shape [color_channels, height, width].\nKeep in mind, these errors arenâ€™t just for predicting on custom images.\nThey will be present with almost every kind of data type (text, audio, structured data) and problem you work with.\n\nNow letâ€™s take a look at our modelâ€™s predictions.\n\ncustom_image_pred\n\ntensor([[ 0.1161,  0.0213, -0.1422]], device='cuda:0')\n\n\nAlright, these are still in logit form (the raw outputs of a model are called logits).\nLetâ€™s convert them from logits -&gt; prediction probabilities -&gt; prediction labels.\n\n# Print out prediction logits\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convert prediction probabilities -&gt; prediction labels\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n\nPrediction logits: tensor([[ 0.1161,  0.0213, -0.1422]], device='cuda:0')\nPrediction probabilities: tensor([[0.3729, 0.3392, 0.2880]], device='cuda:0')\nPrediction label: tensor([0], device='cuda:0')\n\n\nAlright!\nLooking good.\nBut of course our prediction label is still in index/tensor form.\nWe can convert it to a string class name prediction by indexing on the class_names list.\n\n# Find the predicted label\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n\n'pizza'\n\n\nWow.\nIt looks like the model gets the prediction right, even though it was performing poorly based on our evaluation metrics.\n\nì°¸ê³ : The model in its current form will predict â€œpizzaâ€, â€œsteakâ€ or â€œsushiâ€ no matter what image itâ€™s given. If you wanted your model to predict on a different class, youâ€™d have to train it to do so.\n\nBut if we check the custom_image_pred_probs, weâ€™ll notice that the model gives almost equal weight (the values are similar) to every class.\n\n# The values of the prediction probabilities are quite similar\ncustom_image_pred_probs\n\ntensor([[0.3729, 0.3392, 0.2880]], device='cuda:0')\n\n\nHaving prediction probabilities this similar could mean a couple of things: 1. The model is trying to predict all three classes at the same time (there may be an image containing pizza, steak and sushi). 2. The model doesnâ€™t really know what it wants to predict and is in turn just assigning similar values to each of the classes.\nOur case is number 2, since our model is poorly trained, it is basically guessing the prediction.\n\n\n6.15.3 11.3 Putting custom image prediction together: building a function\nDoing all of the above steps every time youâ€™d like to make a prediction on a custom image would quickly become tedious.\nSo letâ€™s put them all together in a function we can easily use over and over again.\nSpecifically, letâ€™s make a function that: 1. Takes in a target image path and converts to the right datatype for our model (torch.float32). 2. Makes sure the target image pixel values are in the range [0, 1]. 3. Transforms the target image if necessary. 4. Makes sure the model is on the target device. 5. Makes a prediction on the target image with a trained model (ensuring the image is the right size and on the same device as the model). 6. Converts the modelâ€™s output logits to prediction probabilities. 7. Converts the prediction probabilities to prediction labels. 8. Plots the target image alongside the model prediction and prediction probability.\nA fair few steps but weâ€™ve got this!\n\ndef pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n\nWhat a nice looking function, letâ€™s test it out.\n\n# Pred on our custom image\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n\n\n\n\n\n\n\n\nTwo thumbs up again!\nLooks like our model got the prediction right just by guessing.\nThis wonâ€™t always be the case with other images thoughâ€¦\nThe image is pixelated too because we resized it to [64, 64] using custom_image_transform.\n\nExercise: Try making a prediction with one of your own images of pizza, steak or sushi and see what happens.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#main-takeaways",
    "href": "04_pytorch_custom_datasets.html#main-takeaways",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.16 Main takeaways",
    "text": "6.16 Main takeaways\nWeâ€™ve covered a fair bit in this module.\nLetâ€™s summarise it with a few dot points.\n\nPyTorch has many in-built functions to deal with all kinds of data, from vision to text to audio to recommendation systems.\nIf PyTorchâ€™s built-in data loading functions donâ€™t suit your requirements, you can write code to create your own custom datasets by subclassing torch.utils.data.Dataset.\ntorch.utils.data.DataLoaderâ€™s in PyTorch help turn your Datasetâ€™s into iterables that can be used when training and testing a model.\nA lot of machine learning is dealing with the balance between overfitting and underfitting (we discussed different methods for each above, so a good exercise would be to research more and writing code to try out the different techniques).\nPredicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:\n\nWrong datatypes - Your model expected torch.float32 when your data is torch.uint8.\nWrong data shapes - Your model expected [batch_size, color_channels, height, width] when your data is [color_channels, height, width].\nWrong devices - Your model is on the GPU but your data is on the CPU.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "04_pytorch_custom_datasets.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.17 ì—°ìŠµ ë¬¸ì œ",
    "text": "6.17 ì—°ìŠµ ë¬¸ì œ\nAll of the exercises are focused on practicing the code in the sections above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 04 * Example solutions notebook for 04 (try the exercises before looking at this)\n\nOur models are underperforming (not fitting the data well). What are 3 methods for preventing underfitting? Write them down and explain each with a sentence.\nRecreate the data loading functions we built in sections 1, 2, 3 and 4. You should have train and test DataLoaderâ€™s ready to use.\nRecreate model_0 we built in section 7.\nCreate training and testing functions for model_0.\nTry training the model you made in exercise 3 for 5, 20 and 50 epochs, what happens to the results?\n\nUse torch.optim.Adam() with a learning rate of 0.001 as the optimizer.\n\nDouble the number of hidden units in your model and train it for 20 epochs, what happens to the results?\nDouble the data youâ€™re using with your model and train it for 20 epochs, what happens to the results?\n\nì°¸ê³ : You can use the custom data creation notebook to scale up your Food101 dataset.\nYou can also find the already formatted double data (20% instead of 10% subset) dataset on GitHub, you will need to write download code like in exercise 2 to get it into this notebook.\n\nMake a prediction on your own custom image of pizza/steak/sushi (you could even download one from the internet) and share your prediction.\n\nDoes the model you trained in exercise 7 get it right?\nIf not, what do you think you could do to improve it?",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "04_pytorch_custom_datasets.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "04_pytorch_custom_datasets.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "6Â  04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹",
    "section": "6.18 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "6.18 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nTo practice your knowledge of PyTorch Datasetâ€™s and DataLoaderâ€™s through PyTorch datasets and dataloaders tutorial notebook.\nSpend 10-minutes reading the PyTorch torchvision.transforms documentation.\n\nYou can see demos of transforms in action in the illustrations of transforms tutorial.\n\nSpend 10-minutes reading the PyTorch torchvision.datasets documentation.\n\nWhat are some datasets that stand out to you?\nHow could you try building a model on these?\n\nTorchData is currently in beta (as of April 2022), itâ€™ll be a future way of loading data in PyTorch, but you can start to check it out now.\nTo speed up deep learning models, you can do a few tricks to improve compute, memory and overhead computations, for more read the post Making Deep Learning Go Brrrr From First Principles by Horace He.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>04 - PyTorch ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html",
    "href": "05_pytorch_going_modular.html",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "",
    "text": "7.1 What is cell mode?\nThis notebook is part 1/2 of section 05. Going Modular.\nFor reference, the two parts are: 1. 05. Going Modular: Part 1 (cell mode) - this notebook is run as a traditional Jupyter Notebook/Google Colab notebook and is a condensed version of notebook 04. 2. 05. Going Modular: Part 2 (script mode) - this notebook is the same as number 1 but with added functionality to turn each of the major sections into Python scripts, such as, data_setup.py and train.py.\nWhy two parts?\nBecause sometimes the best way to learn something is to see how it differs from something else.\nIf you run each notebook side-by-side youâ€™ll see how they differ and thatâ€™s where the key learnings are.\nA cell mode notebook is a regular notebook run exactly how weâ€™ve been running them through the course.\nSome cells contain text and others contain code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#whats-the-difference-between-this-notebook-part-1-and-the-script-mode-notebook-part-2",
    "href": "05_pytorch_going_modular.html#whats-the-difference-between-this-notebook-part-1-and-the-script-mode-notebook-part-2",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.2 Whatâ€™s the difference between this notebook (Part 1) and the script mode notebook (Part 2)?",
    "text": "7.2 Whatâ€™s the difference between this notebook (Part 1) and the script mode notebook (Part 2)?\nThis notebook, 05. PyTorch Going Modular: Part 1 (cell mode), runs a cleaned up version of the most useful code from section 04. PyTorch Custom Datasets.\nRunning this notebook end-to-end will result in recreating the image classification model we built in notebook 04 (TinyVGG) trained on images of pizza, steak and sushi.\nThe main difference between this notebook (Part 1) and Part 2 is that each section in Part 2 (script mode) has an extra subsection (e.g.Â 2.1, 3.1, 4.1) for turning cell code into script code.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#where-can-you-get-help",
    "href": "05_pytorch_going_modular.html#where-can-you-get-help",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.3 Where can you get help?",
    "text": "7.3 Where can you get help?\nYou can find the book version of this section 05. PyTorch Going Modular on learnpytorch.io.\nThe rest of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#running-a-notebook-in-cell-mode",
    "href": "05_pytorch_going_modular.html#running-a-notebook-in-cell-mode",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.4 0. Running a notebook in cell mode",
    "text": "7.4 0. Running a notebook in cell mode\nAs discussed, weâ€™re going to be running this notebook normally.\nOne cell at a time.\nThe code is from notebook 04, however, it has been condensed down to its core functionality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#get-data",
    "href": "05_pytorch_going_modular.html#get-data",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.5 1. Get data",
    "text": "7.5 1. Get data\nWeâ€™re going to start by downloading the same data we used in notebook 04, the pizza_steak_sushi dataset with images of pizza, steak and sushi.\n\nimport os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n    \n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")\n\ndata/pizza_steak_sushi directory exists.\nDownloading pizza, steak, sushi data...\nUnzipping pizza, steak, sushi data...\n\n\n\n# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n\n(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#create-datasets-and-dataloaders",
    "href": "05_pytorch_going_modular.html#create-datasets-and-dataloaders",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.6 2. Create Datasets and DataLoaders",
    "text": "7.6 2. Create Datasets and DataLoaders\nNow weâ€™ll turn the image dataset into PyTorch Datasetâ€™s and DataLoaderâ€™s.\n\nfrom torchvision import datasets, transforms\n\n# Create simple transform\ndata_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n\n# Use ImageFolder to create dataset(s)\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n\nTrain data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data/pizza_steak_sushi/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data/pizza_steak_sushi/test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               ToTensor()\n           )\n\n\n\n# Get class names as a list\nclass_names = train_data.classes\nclass_names\n\n['pizza', 'steak', 'sushi']\n\n\n\n# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n\n{'pizza': 0, 'steak': 1, 'sushi': 2}\n\n\n\n# Check the lengths\nlen(train_data), len(test_data)\n\n(225, 75)\n\n\n\n# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7f853747bbe0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f853747b550&gt;)\n\n\n\n# Check out single image size/shape\nimg, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n\nImage shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#making-a-model-tinyvgg",
    "href": "05_pytorch_going_modular.html#making-a-model-tinyvgg",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.7 3. Making a model (TinyVGG)",
    "text": "7.7 3. Making a model (TinyVGG)\nWeâ€™re going to use the same model we used in notebook 04: TinyVGG from the CNN Explainer website.\nThe only change here from notebook 04 is that a docstring has been added using Googleâ€™s Style Guide for Python.\n\nimport torch\n\nfrom torch import nn\n\nclass TinyVGG(nn.Module):\n  \"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n  \n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, # how big is the square that's going over the image?\n                    stride=1, # default\n                    padding=0), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2) # default stride value is same as kernel_size\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n    \n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.block_2(self.block_1(x))) # &lt;- leverage the benefits of operator fusion\n\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n\nTinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=1690, out_features=3, bias=True)\n  )\n)\n\n\nTo test our model letâ€™s do a single forward pass (pass a sample batch from the training set through our model).\n\n# 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n\nSingle image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[ 0.0208, -0.0019,  0.0095]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3371, 0.3295, 0.3333]], device='cuda:0')\n\nOutput prediction label:\ntensor([0], device='cuda:0')\n\nActual label:\n0",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#creating-train_step-and-test_step-functions-and-train-to-combine-them",
    "href": "05_pytorch_going_modular.html#creating-train_step-and-test_step-functions-and-train-to-combine-them",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.8 4. Creating train_step() and test_step() functions and train() to combine them",
    "text": "7.8 4. Creating train_step() and test_step() functions and train() to combine them\nRather than writing them again, we can reuse the train_step() and test_step() functions from notebook 04.\nThe same goes for the train() function we created.\nThe only difference here is that these functions have had docstrings added to them in Googleâ€™s Python Functions and Methods Style Guide.\nLetâ€™s start by making train_step().\n\nfrom typing import Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n    \n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n  \n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n  \n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\nNow weâ€™ll do test_step().\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n    \n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n  \n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n  \n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n  \n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n          \n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n          \n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\nAnd weâ€™ll combine train_step() and test_step() into train().\n\nfrom typing import Dict, List\n\nfrom tqdm.auto import tqdm\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List[float]]:\n  \"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n  \n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n      \n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#creating-a-function-to-save-the-model",
    "href": "05_pytorch_going_modular.html#creating-a-function-to-save-the-model",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.9 5. Creating a function to save the model",
    "text": "7.9 5. Creating a function to save the model\nLetâ€™s setup a function to save our model to a directory.\n\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n  \"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n  \n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n  \n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "05_pytorch_going_modular.html#train-evaluate-and-save-the-model",
    "href": "05_pytorch_going_modular.html#train-evaluate-and-save-the-model",
    "title": "7Â  05 - PyTorch ëª¨ë“ˆí™”",
    "section": "7.10 6. Train, evaluate and save the model",
    "text": "7.10 6. Train, evaluate and save the model\nLetâ€™s leverage the functions weâ€™ve got above to train, test and save a model to file.\n\n# Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader,\n                        test_dataloader=test_dataloader,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS,\n                        device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n\n# Save the model\nsave_model(model=model_0,\n           target_dir=\"models\",\n           model_name=\"05_going_modular_cell_mode_tinyvgg_model.pth\")\n\n\n\n\nEpoch: 1 | train_loss: 1.0955 | train_acc: 0.3867 | test_loss: 1.0627 | test_acc: 0.4133\nEpoch: 2 | train_loss: 1.0102 | train_acc: 0.5200 | test_loss: 1.0222 | test_acc: 0.4400\nEpoch: 3 | train_loss: 0.9552 | train_acc: 0.5822 | test_loss: 1.0067 | test_acc: 0.4667\nEpoch: 4 | train_loss: 0.8913 | train_acc: 0.5867 | test_loss: 1.0090 | test_acc: 0.4400\nEpoch: 5 | train_loss: 0.8608 | train_acc: 0.6311 | test_loss: 1.0214 | test_acc: 0.4533\n[INFO] Total training time: 5.859 seconds\n[INFO] Saving model to: models/05_going_modular_cell_mode_tinyvgg_model.pth\n\n\nWe finish with a saved image classification model at models/05_going_modular_cell_mode_tinyvgg_model.pth.\nThe code continues in 05. Going Modular: Part 2 (script mode).",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>05 - PyTorch ëª¨ë“ˆí™”</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html",
    "href": "06_pytorch_transfer_learning.html",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "",
    "text": "8.1 What is transfer learning?\nWeâ€™ve built a few models by hand so far.\nBut their performance has been poor.\nYou might be thinking, is there a well-performing model that already exists for our problem?\nAnd in the world of deep learning, the answer is often yes.\nWeâ€™ll see how by using a powerful technique called transfer learning.\nTransfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\nFor example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our FoodVision Mini model.\nOr we could take the patterns from a language model (a model thatâ€™s been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\nThe premise remains: find a well-performing existing model and apply it to your own problem.\nExample of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#why-use-transfer-learning",
    "href": "06_pytorch_transfer_learning.html#why-use-transfer-learning",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.2 Why use transfer learning?",
    "text": "8.2 Why use transfer learning?\nThere are two main benefits to using transfer learning:\n\nCan leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\nCan leverage a working model which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.\n\n\nWeâ€™ll be putting these to the test for our FoodVision Mini problem, weâ€™ll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.\nBoth research and practice support the use of transfer learning too.\nA finding from a recent machine learning research paper recommended practionerâ€™s use transfer learning wherever possible.\n\nA study into the effects of whether training from scratch or using transfer learning was better from a practionerâ€™s point of view, found transfer learning to be far more beneficial in terms of cost and time. Source: How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers paper section 6 (conclusion).\nAnd Jeremy Howard (founder of fastai) is a big proponent of transfer learning.\n\nThe things that really make a difference (transfer learning), if we can do better at transfer learning, itâ€™s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. â€” Jeremy Howard on the Lex Fridman Podcast",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#where-to-find-pretrained-models",
    "href": "06_pytorch_transfer_learning.html#where-to-find-pretrained-models",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.3 Where to find pretrained models",
    "text": "8.3 Where to find pretrained models\nThe world of deep learning is an amazing place.\nSo amazing that many people around the world share their work.\nOften, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\nAnd there are several places you can find pretrained models to use for your own problems.\n\n\n\nLocation\nWhatâ€™s there?\nLink(s)\n\n\n\n\nPyTorch domain libraries\nEach of the PyTorch domain libraries (torchvision, torchtext) come with pretrained models of some form. The models there work right within PyTorch.\ntorchvision.models, torchtext.models, torchaudio.models, torchrec.models\n\n\nHuggingFace Hub\nA series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. Thereâ€™s plenty of different datasets too.\nhttps://huggingface.co/models, https://huggingface.co/datasets\n\n\ntimm (PyTorch Image Models) library\nAlmost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features.\nhttps://github.com/rwightman/pytorch-image-models\n\n\nPaperswithcode\nA collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks.\nhttps://paperswithcode.com/\n\n\n\n\nWith access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, â€œDoes a pretrained model exist for my problem?â€\n\nExercise: Spend 5-minutes going through torchvision.models as well as the HuggingFace Hub Models page, what do you find? (thereâ€™s no right answers here, itâ€™s just to practice exploring)",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "06_pytorch_transfer_learning.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.4 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "8.4 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nWeâ€™re going to take a pretrained model from torchvision.models and customise it to work on (and hopefully improve) our FoodVision Mini problem.\n\n\n\n\n\n\n\nTopic\nContents\n\n\n\n\n0. Getting setup\nWeâ€™ve written a fair bit of useful code over the past few sections, letâ€™s download it and make sure we can use it again.\n\n\n1. Get data\nLetâ€™s get the pizza, steak and sushi image classification dataset weâ€™ve been using to try and improve our modelâ€™s results.\n\n\n2. Create Datasets and DataLoaders\nWeâ€™ll use the data_setup.py script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders.\n\n\n3. Get and customise a pretrained model\nHere weâ€™ll download a pretrained model from torchvision.models and customise it to our own problem.\n\n\n4. Train model\nLetâ€™s see how the new pretrained model goes on our pizza, steak, sushi dataset. Weâ€™ll use the training functions we created in the previous chapter.\n\n\n5. Evaluate the model by plotting loss curves\nHow did our first transfer learning model go? Did it overfit or underfit?\n\n\n6. Make predictions on images from the test set\nItâ€™s one thing to check out a modelâ€™s evaluation metrics but itâ€™s another thing to view its predictions on test samples, letâ€™s visualize, visualize, visualize!",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#where-can-you-get-help",
    "href": "06_pytorch_transfer_learning.html#where-can-you-get-help",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.5 Where can you get help?",
    "text": "8.5 Where can you get help?\nAll of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#getting-setup",
    "href": "06_pytorch_transfer_learning.html#getting-setup",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.6 0. Getting setup",
    "text": "8.6 0. Getting setup\nLetâ€™s get started by importing/downloading the required modules for this section.\nTo save us writing extra code, weâ€™re going to be leveraging some of the Python scripts (such as data_setup.py and engine.py) we created in the previous section, 05. PyTorch Going Modular.\nSpecifically, weâ€™re going to download the going_modular directory from the pytorch-deep-learning repository (if we donâ€™t already have it).\nWeâ€™ll also get the torchinfo package if itâ€™s not available.\ntorchinfo will help later on to give us a visual representation of our model.\n\nì°¸ê³ : As of June 2022, this notebook uses the nightly versions of torch and torchvision as torchvision v0.13+ is required for using the updated multi-weights API. You can install these using the command below.\n\n\n# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\ntorch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n\n\n\n# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n\nNow letâ€™s setup device agnostic code.\n\nì°¸ê³ : If youâ€™re using Google Colab, and you donâ€™t have a GPU turned on yet, itâ€™s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.\n\n\n# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#get-data",
    "href": "06_pytorch_transfer_learning.html#get-data",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.7 1. Get data",
    "text": "8.7 1. Get data\nBefore we can start to use transfer learning, weâ€™ll need a dataset.\nTo see how transfer learning compares to our previous attempts at model building, weâ€™ll download the same dataset weâ€™ve been using for FoodVision Mini.\nLetâ€™s write some code to download the pizza_steak_sushi.zip dataset from the course GitHub and then unzip it.\nWe can also make sure if weâ€™ve already got the data, it doesnâ€™t redownload.\n\nimport os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n\ndata/pizza_steak_sushi directory exists.\n\n\nExcellent!\nNow weâ€™ve got the same dataset weâ€™ve been using previously, a series of images of pizza, steak and sushi in standard image classification format.\nLetâ€™s now create paths to our training and test directories.\n\n# Setup Dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#create-datasets-and-dataloaders",
    "href": "06_pytorch_transfer_learning.html#create-datasets-and-dataloaders",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.8 2. Create Datasets and DataLoaders",
    "text": "8.8 2. Create Datasets and DataLoaders\nSince weâ€™ve downloaded the going_modular directory, we can use the data_setup.py script we created in section 05. PyTorch Going Modular to prepare and setup our DataLoaders.\nBut since weâ€™ll be using a pretrained model from torchvision.models, thereâ€™s a specific transform we need to prepare our images first.\n\n8.8.1 2.1 Creating a transform for torchvision.models (manual creation)\n\nì°¸ê³ : As of torchvision v0.13+, thereâ€™s an update to how data transforms can be created using torchvision.models. Iâ€™ve called the previous method â€œmanual creationâ€ and the new method â€œauto creationâ€. This notebook showcases both.\n\nWhen using a pretrained model, itâ€™s important that your custom data going into the model is prepared in the same way as the original training data that went into the model.\nPrior to torchvision v0.13+, to create a transform for a pretrained model in torchvision.models, the documentation stated:\n\nAll pre-trained models expect input images normalized in the same way, i.e.Â mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\nThe images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\nYou can use the following transform to normalize:\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\nThe good news is, we can achieve the above transformations with a combination of:\n\n\n\n\n\n\n\n\nTransform number\nTransform required\nCode to perform transform\n\n\n\n\n1\nMini-batches of size [batch_size, 3, height, width] where height and width are at least 224x224^.\ntorchvision.transforms.Resize() to resize images into [3, 224, 224]^ and torch.utils.data.DataLoader() to create batches of images.\n\n\n2\nValues between 0 & 1.\ntorchvision.transforms.ToTensor()\n\n\n3\nA mean of [0.485, 0.456, 0.406] (values across each colour channel).\ntorchvision.transforms.Normalize(mean=...) to adjust the mean of our images.\n\n\n4\nA standard deviation of [0.229, 0.224, 0.225] (values across each colour channel).\ntorchvision.transforms.Normalize(std=...) to adjust the standard deviation of our images.\n\n\n\n\nì°¸ê³ : ^some pretrained models from torchvision.models in different sizes to [3, 224, 224], for example, some might take them in [3, 240, 240]. For specific input image sizes, see the documentation.\n\n\nQuestion: Where did the mean and standard deviation values come from? Why do we need to do this?\nThese were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.\nWe also donâ€™t need to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (theyâ€™ll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.\n\nLetâ€™s compose a series of torchvision.transforms to perform the above steps.\n\n# Create a transforms pipeline manually (required for torchvision &lt; 0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n\nWonderful!\nNow weâ€™ve got a manually created series of transforms ready to prepare our images, letâ€™s create training and testing DataLoaders.\nWe can create these using the create_dataloaders function from the data_setup.py script we created in 05. PyTorch Going Modular Part 2.\nWeâ€™ll set batch_size=32 so our model seeâ€™s mini-batches of 32 samples at a time.\nAnd we can transform our images using the transform pipeline we created above by setting transform=simple_transform.\n\nì°¸ê³ : Iâ€™ve included this manual creation of transforms in this notebook because you may come across resources that use this style. Itâ€™s also important to note that because these transforms are manually created, theyâ€™re also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could.\n\n\n# Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a3a60&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a37c0&gt;,\n ['pizza', 'steak', 'sushi'])\n\n\n\n\n8.8.2 2.2 Creating a transform for torchvision.models (auto creation)\nAs previously stated, when using a pretrained model, itâ€™s important that your custom data going into the model is prepared in the same way as the original training data that went into the model.\nAbove we saw how to manually create a transform for a pretrained model.\nBut as of torchvision v0.13+, an automatic transform creation feature has been added.\nWhen you setup a model from torchvision.models and select the pretrained model weights youâ€™d like to use, for example, say weâ€™d like to use:\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\nWhere, * EfficientNet_B0_Weights is the model architecture weights weâ€™d like to use (there are many different model architecture options in torchvision.models). * DEFAULT means the best available weights (the best performance in ImageNet). * ì°¸ê³ : Depending on the model architecture you choose, you may also see other options such as IMAGENET_V1 and IMAGENET_V2 where generally the higher version number the better. Though if you want the best available, DEFAULT is the easiest option. See the torchvision.models documentation for more.\nLetâ€™s try it out.\n\n# Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n\nEfficientNet_B0_Weights.IMAGENET1K_V1\n\n\nAnd now to access the transforms assosciated with our weights, we can use the transforms() method.\nThis is essentially saying â€œget the data transforms that were used to train the EfficientNet_B0_Weights on ImageNetâ€.\n\n# Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms\n\nImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n\n\nNotice how auto_transforms is very similar to manual_transforms, the only difference is that auto_transforms came with the model architecture we chose, where as we had to create manual_transforms by hand.\nThe benefit of automatically creating a transform through weights.transforms() is that you ensure youâ€™re using the same data transformation as the pretrained model used when it was trained.\nHowever, the tradeoff of using automatically created transforms is a lack of customization.\nWe can use auto_transforms to create DataLoaders with create_dataloaders() just as before.\n\n# Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951460&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951550&gt;,\n ['pizza', 'steak', 'sushi'])",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#getting-a-pretrained-model",
    "href": "06_pytorch_transfer_learning.html#getting-a-pretrained-model",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.9 3. Getting a pretrained model",
    "text": "8.9 3. Getting a pretrained model\nAlright, here comes the fun part!\nOver the past few notebooks weâ€™ve been building PyTorch neural networks from scratch.\nAnd while thatâ€™s a good skill to have, our models havenâ€™t been performing as well as weâ€™d like.\nThatâ€™s where transfer learning comes in.\nThe whole idea of transfer learning is to take an already well-performing model on a problem-space similar to yours and then customising it to your use case.\nSince weâ€™re working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in torchvision.models.\nExploring the documentation, youâ€™ll find plenty of common computer vision architecture backbones such as:\n\n\n\nArchitecuture backbone\nCode\n\n\n\n\nResNetâ€™s\ntorchvision.models.resnet18(), torchvision.models.resnet50()â€¦\n\n\nVGG (similar to what we used for TinyVGG)\ntorchvision.models.vgg16()\n\n\nEfficientNetâ€™s\ntorchvision.models.efficientnet_b0(), torchvision.models.efficientnet_b1()â€¦\n\n\nVisionTransformer (ViTâ€™s)\ntorchvision.models.vit_b_16(), torchvision.models.vit_b_32()â€¦\n\n\nConvNeXt\ntorchvision.models.convnext_tiny(), torchvision.models.convnext_small()â€¦\n\n\nMore available in torchvision.models\ntorchvision.models...\n\n\n\n\n8.9.1 3.1 Which pretrained model should you use?\nIt depends on your problem/the device youâ€™re working with.\nGenerally, the higher number in the model name (e.g.Â efficientnet_b0() -&gt; efficientnet_b1() -&gt; efficientnet_b7()) means better performance but a larger model.\nYou might think better performance is always better, right?\nThatâ€™s true but some better performing models are too big for some devices.\nFor example, say youâ€™d like to run your model on a mobile-device, youâ€™ll have to take into account the limited compute resources on the device, thus youâ€™d be looking for a smaller model.\nBut if youâ€™ve got unlimited compute power, as The Bitter Lesson states, youâ€™d likely take the biggest, most compute hungry model you can.\nUnderstanding this performance vs.Â speed vs.Â size tradeoff will come with time and practice.\nFor me, Iâ€™ve found a nice balance in the efficientnet_bX models.\nAs of May 2022, Nutrify (the machine learning powered app Iâ€™m working on) is powered by an efficientnet_b0.\nComma.ai (a company that makes open source self-driving car software) uses an efficientnet_b2 to learn a representation of the road.\n\nì°¸ê³ : Even though weâ€™re using efficientnet_bX, itâ€™s important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem.\n\n\n\n8.9.2 3.2 Setting up a pretrained model\nThe pretrained model weâ€™re going to be using is torchvision.models.efficientnet_b0().\nThe architecture is from the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n\nExample of what weâ€™re going to create, a pretrained EfficientNet_B0 model from torchvision.models with the output layer adjusted for our use case of classifying pizza, steak and sushi images.\nWe can setup the EfficientNet_B0 pretrained ImageNet weights using the same code as we used to create the transforms.\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights for ImageNet\nThis means the model has already been trained on millions of images and has a good base representation of image data.\nThe PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNetâ€™s 1000 classes.\nWeâ€™ll also send it to the target device.\n\n# OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n\n# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n#model # uncomment to output (it's very long)\n\n\nì°¸ê³ : In previous versions of torchvision, youâ€™d create a prertained model with code like:\nmodel = torchvision.models.efficientnet_b0(pretrained=True).to(device)\nHowever, running this using torchvision v0.13+ will result in errors such as the following:\nUserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\nAndâ€¦\nUserWarning: Arguments other than a weight enum or None for weights are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing weights=EfficientNet_B0_Weights.IMAGENET1K_V1. You can also use weights=EfficientNet_B0_Weights.DEFAULT to get the most up-to-date weights.\n\nIf we print the model, we get something similar to the following:\n\nLots and lots and lots of layers.\nThis is one of the benefits of transfer learning, taking an existing model, thatâ€™s been crafted by some of the best engineers in the world and applying to your own problem.\nOur efficientnet_b0 comes in three main parts: 1. features - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as features or feature extractor, â€œthe base layers of the model learn the different features of imagesâ€). 2. avgpool - Takes the average of the output of the features layer(s) and turns it into a feature vector. 3. classifier - Turns the feature vector into a vector with the same dimensionality as the number of required output classes (since efficientnet_b0 is pretrained on ImageNet and because ImageNet has 1000 classes, out_features=1000 is the default).\n\n\n8.9.3 3.3 Getting a summary of our model with torchinfo.summary()\nTo learn more about our model, letâ€™s use torchinfoâ€™s summary() method.\nTo do so, weâ€™ll pass in: * model - the model weâ€™d like to get a summary of. * input_size - the shape of the data weâ€™d like to pass to our model, for the case of efficientnet_b0, the input size is (batch_size, 3, 224, 224), though other variants of efficientnet_bX have different input sizes. * ì°¸ê³ : Many modern models can handle input images of varying sizes thanks to torch.nn.AdaptiveAvgPool2d(), this layer adaptively adjusts the output_size of a given input as required. You can try this out by passing different size input images to summary() or your models. * col_names - the various information columns weâ€™d like to see about our model. * col_width - how wide the columns should be for the summary. * row_settings - what features to show in a row.\n\n# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n) \n\n============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\nâ”œâ”€Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\nâ”‚    â””â”€Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\nâ”‚    â”‚    â””â”€Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\nâ”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\nâ”‚    â”‚    â””â”€SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\nâ”‚    â””â”€Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\nâ”‚    â””â”€Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\nâ”‚    â””â”€Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\nâ”‚    â””â”€Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\nâ”‚    â”‚    â””â”€MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\nâ”‚    â””â”€Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\nâ”‚    â”‚    â””â”€MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\nâ”‚    â””â”€Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\nâ”‚    â”‚    â””â”€MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\nâ”‚    â”‚    â””â”€MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\nâ”‚    â””â”€Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\nâ”‚    â””â”€Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\nâ”‚    â”‚    â””â”€Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\nâ”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\nâ”‚    â”‚    â””â”€SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\nâ”œâ”€AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\nâ”œâ”€Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\nâ”‚    â””â”€Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\nâ”‚    â””â”€Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n============================================================================================================================================\nTotal params: 5,288,548\nTrainable params: 5,288,548\nNon-trainable params: 0\nTotal mult-adds (G): 12.35\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.35\nParams size (MB): 21.15\nEstimated Total Size (MB): 3492.77\n============================================================================================================================================\n\n\n\nWoah!\nNow thatâ€™s a big model!\nFrom the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.\nAnd there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data.\nFor reference, our model from previous sections, TinyVGG had 8,083 parameters vs.Â 5,288,548 parameters for efficientnet_b0, an increase of ~654x!\nWhat do you think, will this mean better performance?\n\n\n8.9.4 3.4 Freezing the base model and changing the output layer to suit our needs\nThe process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the features section) and then adjust the output layers (also called head/classifier layers) to suit your needs.\n\nYou can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original torchvision.models.efficientnet_b0() comes with out_features=1000 because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need out_features=3.\nLetâ€™s freeze all of the layers/parameters in the features section of our efficientnet_b0 model.\n\nì°¸ê³ : To freeze layers means to keep them how they are during training. For example, if your model has pretrained layers, to freeze them would be to say, â€œdonâ€™t change any of the patterns in these layers during training, keep them how they are.â€ In essence, weâ€™d like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.\n\nWe can freeze all of the layers/parameters in the features section by setting the attribute requires_grad=False.\nFor parameters with requires_grad=False, PyTorch doesnâ€™t track gradient updates and in turn, these parameters wonâ€™t be changed by our optimizer during training.\nIn essence, a parameter with requires_grad=False is â€œuntrainableâ€ or â€œfrozenâ€ in place.\n\n# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n\nFeature extractor layers frozen!\nLetâ€™s now adjust the output layer or the classifier portion of our pretrained model to our needs.\nRight now our pretrained model has out_features=1000 because there are 1000 classes in ImageNet.\nHowever, we donâ€™t have 1000 classes, we only have three, pizza, steak and sushi.\nWe can change the classifier portion of our model by creating a new series of layers.\nThe current classifier consists of:\n(classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\nWeâ€™ll keep the Dropout layer the same using torch.nn.Dropout(p=0.2, inplace=True).\n\nì°¸ê³ : Dropout layers randomly remove connections between two neural network layers with a probability of p. For example, if p=0.2, 20% of connections between neural network layers will be removed at random each pass. This practice is meant to help regularize (prevent overfitting) a model by making sure the connections that remain learn features to compensate for the removal of the other connections (hopefully these remaining features are more general).\n\nAnd weâ€™ll keep in_features=1280 for our Linear output layer but weâ€™ll change the out_features value to the length of our class_names (len(['pizza', 'steak', 'sushi']) = 3).\nOur new classifier layer should be on the same device as our model.\n\n# Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n\nNice!\nOutput layer updated, letâ€™s get another summary of our model and see whatâ€™s changed.\n\n# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n\n============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\nâ”œâ”€Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\nâ”‚    â””â”€Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\nâ”‚    â”‚    â””â”€Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\nâ”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\nâ”‚    â”‚    â””â”€SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\nâ”‚    â””â”€Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\nâ”‚    â””â”€Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\nâ”‚    â””â”€Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\nâ”‚    â””â”€Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\nâ”‚    â”‚    â””â”€MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\nâ”‚    â””â”€Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\nâ”‚    â”‚    â””â”€MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\nâ”‚    â””â”€Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\nâ”‚    â”‚    â””â”€MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\nâ”‚    â”‚    â””â”€MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\nâ”‚    â”‚    â””â”€MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\nâ”‚    â””â”€Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\nâ”‚    â”‚    â””â”€MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\nâ”‚    â””â”€Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\nâ”‚    â”‚    â””â”€Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\nâ”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\nâ”‚    â”‚    â””â”€SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\nâ”œâ”€AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\nâ”œâ”€Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\nâ”‚    â””â”€Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\nâ”‚    â””â”€Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n============================================================================================================================================\nTotal params: 4,011,391\nTrainable params: 3,843\nNon-trainable params: 4,007,548\nTotal mult-adds (G): 12.31\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.09\nParams size (MB): 16.05\nEstimated Total Size (MB): 3487.41\n============================================================================================================================================\n\n\n\nHo, ho! Thereâ€™s a fair few changes here!\nLetâ€™s go through them: * Trainable column - Youâ€™ll see that many of the base layers (the ones in the features portion) have their Trainable value as False. This is because we set their attribute requires_grad=False. Unless we change this, these layers wonâ€™t be updated during furture training. * Output shape of classifier - The classifier portion of the model now has an Output Shape value of [32, 3] instead of [32, 1000]. Itâ€™s Trainable value is also True. This means its parameters will be updated during training. In essence, weâ€™re using the features portion to feed our classifier portion a base representation of an image and then our classifier layer is going to learn how to base representation aligns with our problem. * Less trainable parameters - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the classifier as trainable, thereâ€™s now only 3,843 trainable parameters (even less than our TinyVGG model). Though thereâ€™s also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our classifier layer.\n\nì°¸ê³ : The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#train-model",
    "href": "06_pytorch_transfer_learning.html#train-model",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.10 4. Train model",
    "text": "8.10 4. Train model\nNow weâ€™ve got a pretraiend model thatâ€™s semi-frozen and has a customised classifier, how about we see transfer learning in action?\nTo begin training, letâ€™s create a loss function and an optimizer.\nBecause weâ€™re still working with multi-class classification, weâ€™ll use nn.CrossEntropyLoss() for the loss function.\nAnd weâ€™ll stick with torch.optim.Adam() as our optimizer with lr=0.001.\n\n# Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nWonderful!\nTo train our model, we can use train() function we defined in the 05. PyTorch Going Modular section 04.\nThe train() function is in the engine.py script inside the going_modular directory.\nLetâ€™s see how long it takes to train our model for 5 epochs.\n\nì°¸ê³ : Weâ€™re only going to be training the parameters classifier here as all of the other parameters in our model have been frozen.\n\n\n# Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n\n\n\n\nEpoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8717 | train_acc: 0.7773 | test_loss: 0.7912 | test_acc: 0.8153\nEpoch: 3 | train_loss: 0.7648 | train_acc: 0.7930 | test_loss: 0.7463 | test_acc: 0.8561\nEpoch: 4 | train_loss: 0.7108 | train_acc: 0.7539 | test_loss: 0.6372 | test_acc: 0.8655\nEpoch: 5 | train_loss: 0.6254 | train_acc: 0.7852 | test_loss: 0.6260 | test_acc: 0.8561\n[INFO] Total training time: 8.977 seconds\n\n\nWow!\nOur model trained quite fast (~5 seconds on my local machine with a NVIDIA TITAN RTX GPU/about 15 seconds on Google Colab with a NVIDIA P100 GPU).\nAnd it looks like it smashed our previous model results out of the park!\nWith an efficientnet_b0 backbone, our model achieves almost 85%+ accuracy on the test dataset, almost double what we were able to achieve with TinyVGG.\nNot bad for a model we downloaded with a few lines of code.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#evaluate-model-by-plotting-loss-curves",
    "href": "06_pytorch_transfer_learning.html#evaluate-model-by-plotting-loss-curves",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.11 5. Evaluate model by plotting loss curves",
    "text": "8.11 5. Evaluate model by plotting loss curves\nOur model looks like itâ€™s performing pretty well.\nLetâ€™s plot itâ€™s loss curves to see what the training looks like over time.\nWe can plot the loss curves using the function plot_loss_curves() we created in 04. PyTorch Custom Datasets section 7.8.\nThe function is stored in the helper_functions.py script so weâ€™ll try to import it and download the script if we donâ€™t have it.\n\n# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Plot the loss curves of our model\nplot_loss_curves(results)\n\n\n\n\n\n\n\n\nThose are some excellent looking loss curves!\nIt looks like the loss for both datasets (train and test) is heading in the right direction.\nThe same with the accuracy values, trending upwards.\nThat goes to show the power of transfer learning. Using a pretrained model often leads to pretty good results with a small amount of data in less time.\nI wonder what would happen if you tried to train the model for longer? Or if we added more data?\n\nQuestion: Looking at the loss curves, does our model look like itâ€™s overfitting or underfitting? Or perhaps neither? Hint: Check out notebook 04. PyTorch Custom Datasets part 8. What should an ideal loss curve look like? for ideas.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#make-predictions-on-images-from-the-test-set",
    "href": "06_pytorch_transfer_learning.html#make-predictions-on-images-from-the-test-set",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.12 6. Make predictions on images from the test set",
    "text": "8.12 6. Make predictions on images from the test set\nIt looks like our model performs well quantitatively but how about qualitatively?\nLetâ€™s find out by making some predictions with our model on images from the test set (these arenâ€™t seen during training) and plotting them.\nVisualize, visualize, visualize!\nOne thing weâ€™ll have to remember is that for our model to make predictions on an image, the image has to be in same format as the images our model was trained on.\nThis means weâ€™ll need to make sure our images have: * Same shape - If our images are different shapes to what our model was trained on, weâ€™ll get shape errors. * Same datatype - If our images are a different datatype (e.g.Â torch.int8 vs.Â torch.float32) weâ€™ll get datatype errors. * Same device - If our images are on a different device to our model, weâ€™ll get device errors. * Same transformations - If our model is trained on images that have been transformed in certain way (e.g.Â normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.\n\nì°¸ê³ : These requirements go for all kinds of data if youâ€™re trying to make predictions with a trained model. Data youâ€™d like to predict on should be in the same format as your model was trained on.\n\nTo do all of this, weâ€™ll create a function pred_and_plot_image() to:\n\nTake in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.\nOpen an image with PIL.Image.open().\nCreate a transform for the image (this will default to the manual_transforms we created above or it could use a transform generated from weights.transforms()).\nMake sure the model is on the target device.\nTurn on model eval mode with model.eval() (this turns off layers like nn.Dropout(), so they arenâ€™t used for inference) and the inference mode context manager.\nTransform the target image with the transform made in step 3 and add an extra batch dimension with torch.unsqueeze(dim=0) so our input image has shape [batch_size, color_channels, height, width].\nMake a prediction on the image by passing it to the model ensuring itâ€™s on the target device.\nConvert the modelâ€™s output logits to prediction probabilities with torch.softmax().\nConvert modelâ€™s prediction probabilities to prediction labels with torch.argmax().\nPlot the image with matplotlib and set the title to the prediction label from step 9 and prediction probability from step 8.\n\n\nì°¸ê³ : This is a similar function to 04. PyTorch Custom Datasets section 11.3â€™s pred_and_plot_image() with a few tweaked steps.\n\n\nfrom typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Take in a trained model, class names, image path, image size, a transform and target device\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n\nWhat a good looking function!\nLetâ€™s test it out by making predictions on a few random images from the test set.\nWe can get a list of all the test image paths using list(Path(test_dir).glob(\"*/*.jpg\")), the stars in the glob() method say â€œany file matching this patternâ€, in other words, any file ending in .jpg (all of our images).\nAnd then we can randomly sample a number of these using Pythonâ€™s random.sample(populuation, k) where population is the sequence to sample and k is the number of samples to retrieve.\n\n# Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWoohoo!\nThose predictions look far better than the ones our TinyVGG model was previously making.\n\n8.12.1 6.1 Making predictions on a custom image\nIt looks like our model does well qualitatively on data from the test set.\nBut how about on our own custom image?\nThatâ€™s where the real fun of machine learning is!\nPredicting on your own custom data, outisde of any training or test set.\nTo test our model on a custom image, letâ€™s import the old faithful pizza-dad.jpeg image (an image of my dad eating pizza).\nWeâ€™ll then pass it to the pred_and_plot_image() function we created above and see what happens.\n\n# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n\ndata/04-pizza-dad.jpeg already exists, skipping download.\n\n\n\n\n\n\n\n\n\nTwo thumbs up!\nLooks like our model go it right again!\nBut this time the prediction probability is higher than the one from TinyVGG (0.373) in 04. PyTorch Custom Datasets section 11.3.\nThis indicates our efficientnet_b0 model is more confident in its prediction where as our TinyVGG model was par with just guessing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#main-takeaways",
    "href": "06_pytorch_transfer_learning.html#main-takeaways",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.13 Main takeaways",
    "text": "8.13 Main takeaways\n\nTransfer learning often allows to you get good results with a relatively small amount of custom data.\nKnowing the power of transfer learning, itâ€™s a good idea to ask at the start of every problem, â€œdoes an existing well-performing model exist for my problem?â€\nWhen using a pretrained model, itâ€™s important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.\nThe same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.\nThere are several different places to find pretrained models from the PyTorch domain libraries, HuggingFace Hub and libraries such as timm (PyTorch Image Models).",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "06_pytorch_transfer_learning.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.14 ì—°ìŠµ ë¬¸ì œ",
    "text": "8.14 ì—°ìŠµ ë¬¸ì œ\nAll of the exercises are focused on practicing the code above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 06 * Example solutions notebook for 06 (try the exercises before looking at this) * See a live video walkthrough of the solutions on YouTube (errors and all)\n\nMake predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels. Check out 03. PyTorch Computer Vision section 10 for ideas.\nGet the â€œmost wrongâ€ of the predictions on the test dataset and plot the 5 â€œmost wrongâ€ images. You can do this by:\n\nPredicting across all of the test dataset, storing the labels and predicted probabilities.\nSort the predictions by wrong prediction and then descending predicted probabilities, this will give you the wrong predictions with the highest prediction probabilities, in other words, the â€œmost wrongâ€.\nPlot the top 5 â€œmost wrongâ€ images, why do you think the model got these wrong?\n\nPredict on your own image of pizza/steak/sushi - how does the model go? What happens if you predict on an image that isnâ€™t pizza/steak/sushi?\nTrain the model from section 4 above for longer (10 epochs should do), what happens to the performance?\nTrain the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.\n\nYou can find the 20% Pizza, Steak, Sushi dataset on the course GitHub. It was created with the notebook extras/04_custom_data_creation.ipynb.\n\nTry a different model from torchvision.models on the Pizza, Steak, Sushi data, how does this model perform?\n\nYouâ€™ll have to change the size of the classifier layer to suit our problem.\nYou may want to try an EfficientNet with a higher number than our B0, perhaps torchvision.models.efficientnet_b2()?",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "06_pytorch_transfer_learning.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "06_pytorch_transfer_learning.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "8Â  06 - PyTorch ì „ì´ í•™ìŠµ",
    "section": "8.15 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "8.15 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nLook up what â€œmodel fine-tuningâ€ is and spend 30-minutes researching different methods to perform it with PyTorch. How would we change our code to fine-tine? Tip: fine-tuning usually works best if you have lots of custom data, where as, feature extraction is typically better if you have less custom data.\nCheck out the new/upcoming PyTorch multi-weights API (still in beta at time of writing, May 2022), itâ€™s a new way to perform transfer learning in PyTorch. What changes to our code would need to made to use the new API?\nTry to create your own classifier on two classes of images, for example, you could collect 10 photos of your dog and your friends dog and train a model to classify the two dogs. This would be a good way to practice creating a dataset as well as building a model on that dataset.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>06 - PyTorch ì „ì´ í•™ìŠµ</span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html",
    "href": "07_pytorch_experiment_tracking.html",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "",
    "text": "9.1 What is experiment tracking?\nWeâ€™ve trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).\nAnd so far weâ€™ve keep track of them via Python dictionaries.\nOr just comparing them by the metric print outs during training.\nWhat if you wanted to run a dozen (or more) different models at once?\nSurely thereâ€™s a better wayâ€¦\nThere is.\nExperiment tracking.\nAnd since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.\nSo welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.\nWeâ€™re going to answer the question: how do I track my machine learning experiments?\nMachine learning and deep learning are very experimental.\nYou have to put on your artistâ€™s beret/chefâ€™s hat to cook up lots of different models.\nAnd you have to put on your scientistâ€™s coat to track the results of various combinations of data, model architectures and training regimes.\nThatâ€™s where experiment tracking comes in.\nIf youâ€™re running lots of different experiments, experiment tracking helps you figure out what works and what doesnâ€™t.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#why-track-experiments",
    "href": "07_pytorch_experiment_tracking.html#why-track-experiments",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.2 Why track experiments?",
    "text": "9.2 Why track experiments?\nIf youâ€™re only running a handful of models (like weâ€™ve done so far), it might be okay just to track their results in print outs and a few dictionaries.\nHowever, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.\nSo if youâ€™re following the machine learning practitionerâ€™s motto of experiment, experiment, experiment!, youâ€™ll want a way to track them.\n\nAfter building a few models and tracking their results, youâ€™ll start to notice how quickly it can get out of hand.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#different-ways-to-track-machine-learning-experiments",
    "href": "07_pytorch_experiment_tracking.html#different-ways-to-track-machine-learning-experiments",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.3 Different ways to track machine learning experiments",
    "text": "9.3 Different ways to track machine learning experiments\nThere are as many different ways to track machine learning experiments as there is experiments to run.\nThis table covers a few.\n\n\n\nMethod\nSetup\nPros\nCons\nCost\n\n\n\n\nPython dictionaries, CSV files, print outs\nNone\nEasy to setup, runs in pure Python\nHard to keep track of large numbers of experiments\nFree\n\n\nTensorBoard\nMinimal, install tensorboard\nExtensions built into PyTorch, widely recognized and used, easily scales.\nUser-experience not as nice as other options.\nFree\n\n\nWeights & Biases Experiment Tracking\nMinimal, install wandb, make an account\nIncredible user experience, make experiments public, tracks almost anything.\nRequires external resource outside of PyTorch.\nFree for personal use\n\n\nMLFlow\nMinimal, install mlflow and starting tracking\nFully open-source MLOps lifecycle management, many integrations.\nLittle bit harder to setup a remote tracking server than other services.\nFree\n\n\n\n\nVarious places and techniques you can use to track your machine learning experiments. ì°¸ê³ : There are various other options similar to Weights & Biases and open-source options similar to MLflow but Iâ€™ve left them out for brevity. You can find more by searching â€œmachine learning experiment trackingâ€.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "07_pytorch_experiment_tracking.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.4 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "9.4 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nWeâ€™re going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.\nAnd due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.\nHowever, the principles weâ€™re going to cover are similar across all of the other tools for experiment tracking.\n\n\n\n\n\n\n\nTopic\nContents\n\n\n\n\n0. Getting setup\nWeâ€™ve written a fair bit of useful code over the past few sections, letâ€™s download it and make sure we can use it again.\n\n\n1. Get data\nLetâ€™s get the pizza, steak and sushi image classification dataset weâ€™ve been using to try and improve our FoodVision Mini modelâ€™s results.\n\n\n2. Create Datasets and DataLoaders\nWeâ€™ll use the data_setup.py script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders.\n\n\n3. Get and customise a pretrained model\nJust like the last section, 06. PyTorch Transfer Learning weâ€™ll download a pretrained model from torchvision.models and customise it to our own problem.\n\n\n4. Train model amd track results\nLetâ€™s see what itâ€™s like to train and track the training results of a single model using TensorBoard.\n\n\n5. View our modelâ€™s results in TensorBoard\nPreviously we visualized our modelâ€™s loss curves with a helper function, now letâ€™s see what they look like in TensorBoard.\n\n\n6. Creating a helper function to track experiments\nIf weâ€™re going to be adhering to the machine learner practitionerâ€™s motto of experiment, experiment, experiment!, we best create a function that will help us save our modelling experiment results.\n\n\n7. Setting up a series of modelling experiments\nInstead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times.\n\n\n8. View modelling experiments in TensorBoard\nBy this stage weâ€™ll have run eight modelling experiments in one go, a fair bit to keep track of, letâ€™s what their results look like in TensorBoard.\n\n\n9. Load in the best model and make predictions with it\nThe point of experiment tracking is to figure out which model performs the best, letâ€™s load in the best performing model and make some predictions with it to visualize, visualize, visualize!.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#where-can-you-get-help",
    "href": "07_pytorch_experiment_tracking.html#where-can-you-get-help",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.5 Where can you get help?",
    "text": "9.5 Where can you get help?\nAll of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#getting-setup",
    "href": "07_pytorch_experiment_tracking.html#getting-setup",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.6 0. Getting setup",
    "text": "9.6 0. Getting setup\nLetâ€™s start by downloading all of the modules weâ€™ll need for this section.\nTo save us writing extra code, weâ€™re going to be leveraging some of the Python scripts (such as data_setup.py and engine.py) we created in section, 05. PyTorch Going Modular.\nSpecifically, weâ€™re going to download the going_modular directory from the pytorch-deep-learning repository (if we donâ€™t already have it).\nWeâ€™ll also get the torchinfo package if itâ€™s not available.\ntorchinfo will help later on to give us visual summaries of our model(s).\nAnd since weâ€™re using a newer version of the torchvision package (v0.13 as of June 2022), weâ€™ll make sure weâ€™ve got the latest versions.\n\n# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\ntorch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n\n\n\nì°¸ê³ : If youâ€™re using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify youâ€™ve got the right versions of torch (0.12+) and torchvision (0.13+).\n\n\n# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n\nNow letâ€™s setup device agnostic code.\n\nì°¸ê³ : If youâ€™re using Google Colab, and you donâ€™t have a GPU turned on yet, itâ€™s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\n9.6.1 Create a helper function to set seeds\nSince weâ€™ve been setting random seeds a whole bunch throughout previous sections, how about we functionize it?\nLetâ€™s create a function to â€œset the seedsâ€ called set_seeds().\n\nì°¸ê³ : Recall a random seed is a way of flavouring the randomness generated by a computer. They arenâ€™t necessary to always set when running machine learning code, however, they help ensure thereâ€™s an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally arenâ€™t required.\n\n\n# Set seeds\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#get-data",
    "href": "07_pytorch_experiment_tracking.html#get-data",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.7 1. Get data",
    "text": "9.7 1. Get data\nAs always, before we can run machine learning experiments, weâ€™ll need a dataset.\nWeâ€™re going to continue trying to improve upon the results weâ€™ve been getting on FoodVision Mini.\nIn the previous section, 06. PyTorch Transfer Learning, we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.\nSo how about we run some experiments and try to further improve our results?\nTo do so, weâ€™ll use similar code to the previous section to download the pizza_steak_sushi.zip (if the data doesnâ€™t already exist) except this time its been functionised.\nThis will allow us to use it again later.\n\nimport os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    \n    Returns:\n        pathlib.Path to downloaded data.\n    \n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n\n[INFO] data/pizza_steak_sushi directory exists, skipping download.\n\n\nPosixPath('data/pizza_steak_sushi')\n\n\nExcellent! Looks like weâ€™ve got our pizza, steak and sushi images in standard image classification format ready to go.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#create-datasets-and-dataloaders",
    "href": "07_pytorch_experiment_tracking.html#create-datasets-and-dataloaders",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.8 2. Create Datasets and DataLoaders",
    "text": "9.8 2. Create Datasets and DataLoaders\nNow weâ€™ve got some data, letâ€™s turn it into PyTorch DataLoaders.\nWe can do so using the create_dataloaders() function we created in 05. PyTorch Going Modular part 2.\nAnd since weâ€™ll be using transfer learning and specifically pretrained models from torchvision.models, weâ€™ll create a transform to prepare our images correctly.\nTo transform our images in tensors, we can use: 1. Manually created transforms using torchvision.transforms. 2. Automatically created transforms using torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms(). * Where MODEL_NAME is a specific torchvision.models architecture, MODEL_WEIGHTS is a specific set of pretrained weights and DEFAULT means the â€œbest available weightsâ€.\nWe saw an example of each of these in 06. PyTorch Transfer Learning section 2.\nLetâ€™s see first an example of manually creating a torchvision.transforms pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms donâ€™t match the pretrained model).\nThe main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained torchvision.models are all pretrained on ImageNet).\nWe can do this with:\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n9.8.1 2.1 Create DataLoaders using manually created transforms\n\n# Setup directories\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n\nManually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,\n ['pizza', 'steak', 'sushi'])\n\n\n\n\n9.8.2 2.2 Create DataLoaders using automatically created transforms\nData transformed and DataLoaders created!\nLetâ€™s now see what the same transformation pipeline looks like but this time by using automatic transforms.\nWe can do this by first instantiating a set of pretrained weights (for example weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT) weâ€™d like to use and calling the transforms() method on it.\n\n# Setup dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup pretrained weights (plenty of these available in torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Get transforms from weights (these are the transforms that were used to obtain the weights)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n\nAutomatically created transforms: ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,\n ['pizza', 'steak', 'sushi'])",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head",
    "href": "07_pytorch_experiment_tracking.html#getting-a-pretrained-model-freezing-the-base-layers-and-changing-the-classifier-head",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.9 3. Getting a pretrained model, freezing the base layers and changing the classifier head",
    "text": "9.9 3. Getting a pretrained model, freezing the base layers and changing the classifier head\nBefore we run and track multiple modelling experiments, letâ€™s see what itâ€™s like to run and track a single one.\nAnd since our data is ready, the next thing weâ€™ll need is a model.\nLetâ€™s download the pretrained weights for a torchvision.models.efficientnet_b0() model and prepare it for use with our own data.\n\n# ì°¸ê³ : This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n\n# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Setup the model with the pretrained weights and send it to the target device\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# View the output of the model\n# model\n\nWonderful!\nNow weâ€™ve got a pretrained model letâ€™s turn into a feature extractor model.\nIn essence, weâ€™ll freeze the base layers of the model (weâ€™ll use these to extract features from our input images) and weâ€™ll change the classifier head (output layer) to suit the number of classes weâ€™re working with (weâ€™ve got 3 classes: pizza, steak, sushi).\n\nì°¸ê³ : The idea of creating a feature extractor model (what weâ€™re doing here) was covered in more depth in 06. PyTorch Transfer Learning section 3.2: Setting up a pretrained model.\n\n\n# Freeze all base layers by setting requires_grad attribute to False\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Since we're creating a new layer with random weights (torch.nn.Linear), \n# let's set the seeds\nset_seeds() \n\n# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n\nBase layers frozen, classifier head changed, letâ€™s get a summary of our model with torchinfo.summary().\n\nfrom torchinfo import summary\n\n# # Get a summary of the model (uncomment for full output)\n# summary(model, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n#         verbose=0,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n\n\nOutput of torchinfo.summary() with our feature extractor EffNetB0 model, notice how the base layers are frozen (not trainable) and the output layers are customized to our own problem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#train-model-and-track-results",
    "href": "07_pytorch_experiment_tracking.html#train-model-and-track-results",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.10 4. Train model and track results",
    "text": "9.10 4. Train model and track results\nModel ready to go!\nLetâ€™s get ready to train it by creating a loss function and an optimizer.\nSince weâ€™re working with multiple classes, weâ€™ll use torch.nn.CrossEntropyLoss() as the loss function.\nAnd weâ€™ll stick with torch.optim.Adam() with learning rate of 0.001 for the optimizer.\n\n# Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\n9.10.1 Adjust train() function to track results with SummaryWriter()\nBeautiful!\nAll of the pieces of our training code are starting to come together.\nLetâ€™s now add the final piece to track our experiments.\nPreviously, weâ€™ve tracked our modelling experiments using multiple Python dictionaries (one for each model).\nBut you can imagine this could get out of hand if we were running anything more than a few experiments.\nNot to worry, thereâ€™s a better option!\nWe can use PyTorchâ€™s torch.utils.tensorboard.SummaryWriter() class to save various parts of our modelâ€™s training progress to file.\nBy default, the SummaryWriter() class saves various information about our model to a file set by the log_dir parameter.\nThe default location for log_dir is under runs/CURRENT_DATETIME_HOSTNAME, where the HOSTNAME is the name of your computer.\nBut of course, you can change where your experiments are tracked (the filename is as customisable as youâ€™d like).\nThe outputs of the SummaryWriter() are saved in TensorBoard format.\nTensorBoard is a part of the TensorFlow deep learning library and is an excellent way to visualize different parts of your model.\nTo start tracking our modelling experiments, letâ€™s create a default SummaryWriter() instance.\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept:\n    print(\"[INFO] Couldn't find tensorboard... installing it.\")\n    !pip install -q tensorboard\n    from torch.utils.tensorboard import SummaryWriter\n\n\n# Create a writer with all default settings\nwriter = SummaryWriter()\n\nNow to use the writer, we could write a new training loop or we could adjust the existing train() function we created in 05. PyTorch Going Modular section 4.\nLetâ€™s take the latter option.\nWeâ€™ll get the train() function from engine.py and adjust it to use writer.\nSpecifically, weâ€™ll add the ability for our train() function to log our modelâ€™s training and test loss and accuracy values.\nWe can do this with writer.add_scalars(main_tag, tag_scalar_dict), where: * main_tag (string) - the name for the scalars being tracked (e.g.Â â€œAccuracyâ€) * tag_scalar_dict (dict) - a dictionary of the values being tracked (e.g.Â {\"train_loss\": 0.3454}) * &gt; ì°¸ê³ : The method is called add_scalars() because our loss and accuracy values are generally scalars (single values).\nOnce weâ€™ve finished tracking values, weâ€™ll call writer.close() to tell the writer to stop looking for values to track.\nTo start modifying train() weâ€™ll also import train_step() and test_step() from engine.py.\n\nì°¸ê³ : You can track information about your model almost anywhere in your code. But quite often experiments will be tracked while a model is training (inside a training/testing loop).\nThe torch.utils.tensorboard.SummaryWriter() class also has many different methods to track different things about your model/data, such as add_graph() which tracks the computation graph of your model. For more options, check the SummaryWriter() documentation.\n\n\nfrom typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Import train() function from: \n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      \n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n\nWoohoo!\nOur train() function is now updated to use a SummaryWriter() instance to track our modelâ€™s results.\nHow about we try it out for 5 epochs?\n\n# Train model\n# ì°¸ê³ : Not using engine.train() since the original script isn't updated to use writer\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n\n\n\n\nEpoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561\nEpoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864\nEpoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049\nEpoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737\n\n\n\nì°¸ê³ : You might notice the results here are slightly different to what our model got in 06. PyTorch Transfer Learning. The difference comes from using the engine.train() and our modified train() function. Can you guess why? The PyTorch documentation on randomness may help more.\n\nRunning the cell above we get similar outputs we got in 06. PyTorch Transfer Learning section 4: Train model but the difference is behind the scenes our writer instance has created a runs/ directory storing our modelâ€™s results.\nFor example, the save location might look like:\nruns/Jun21_00-46-03_daniels_macbook_pro\nWhere the default format is runs/CURRENT_DATETIME_HOSTNAME.\nWeâ€™ll check these out in a second but just as a reminder, we were previously tracking our modelâ€™s results in a dictionary.\n\n# Check out the model results\nresults\n\n{'train_loss': [1.0923754647374153,\n  0.8974628075957298,\n  0.803724929690361,\n  0.6769256368279457,\n  0.7064960040152073],\n 'train_acc': [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],\n 'test_loss': [0.9132757981618246,\n  0.7837507526079813,\n  0.6722926497459412,\n  0.6698453426361084,\n  0.6746167540550232],\n 'test_acc': [0.5397727272727273,\n  0.8560606060606061,\n  0.8863636363636364,\n  0.8049242424242425,\n  0.7736742424242425]}\n\n\nHmmm, we could format this to be a nice plot but could you image keeping track of a bunch of these dictionaries?\nThere has to be a better wayâ€¦",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#view-our-models-results-in-tensorboard",
    "href": "07_pytorch_experiment_tracking.html#view-our-models-results-in-tensorboard",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.11 5. View our modelâ€™s results in TensorBoard",
    "text": "9.11 5. View our modelâ€™s results in TensorBoard\nThe SummaryWriter() class stores our modelâ€™s results in a directory called runs/ in TensorBoard format by default.\nTensorBoard is a visualization program created by the TensorFlow team to view and inspect information about models and data.\nYou know what that means?\nItâ€™s time to follow the data visualizerâ€™s motto and visualize, visualize, visualize!\nYou can view TensorBoard in a number of ways:\n\n\n\nCode environment\nHow to view TensorBoard\nResource\n\n\n\n\nVS Code (notebooks or Python scripts)\nPress SHIFT + CMD + P to open the Command Palette and search for the command â€œPython: Launch TensorBoardâ€.\nVS Code Guide on TensorBoard and PyTorch\n\n\nJupyter and Colab Notebooks\nMake sure TensorBoard is installed, load it with %load_ext tensorboard and then view your results with %tensorboard --logdir DIR_WITH_LOGS.\ntorch.utils.tensorboard and Get started with TensorBoard\n\n\n\nYou can also upload your experiments to tensorboard.dev to share them publicly with others.\nRunning the following code in a Google Colab or Jupyter Notebook will start an interactive TensorBoard session to view TensorBoard files in the runs/ directory.\n%load_ext tensorboard # line magic to load TensorBoard\n%tensorboard --logdir runs # run TensorBoard session with the \"runs/\" directory\n\n# Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n\nIf all went correctly, you should see something like the following:\n\nViewing a single modelling experimentâ€™s results for accuracy and loss in TensorBoard.\n\nì°¸ê³ : For more information on running TensorBoard in notebooks or in other locations, see the following: * Using TensorBoard in Notebooks guide by TensorFlow * Get started with TensorBoard.dev (helpful for uploading your TensorBoard logs to a shareable link)",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#create-a-helper-function-to-build-summarywriter-instances",
    "href": "07_pytorch_experiment_tracking.html#create-a-helper-function-to-build-summarywriter-instances",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.12 6. Create a helper function to build SummaryWriter() instances",
    "text": "9.12 6. Create a helper function to build SummaryWriter() instances\nThe SummaryWriter() class logs various information to a directory specified by the log_dir parameter.\nHow about we make a helper function to create a custom directory per experiment?\nIn essence, each experiment gets its own logs directory.\nFor example, say weâ€™d like to track things like: * Experiment date/timestamp - when did the experiment take place? * Experiment name - is there something weâ€™d like to call the experiment? * Model name - what model was used? * Extra - should anything else be tracked?\nYou could track almost anything here and be as creative as you want but these should be enough to start.\nLetâ€™s create a helper function called create_writer() that produces a SummaryWriter() instance tracking to a custom log_dir.\nIdeally, weâ€™d like the log_dir to be something like:\nruns/YYYY-MM-DD/experiment_name/model_name/extra\nWhere YYYY-MM-DD is the date the experiment was run (you could add the time if you wanted to as well).\n\ndef create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n\nBeautiful!\nNow weâ€™ve got a create_writer() function, letâ€™s try it out.\n\n# Create an example writer\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n\n\nLooking good, now weâ€™ve got a way to log and trace back our various experiments.\n\n9.12.1 6.1 Update the train() function to include a writer parameter\nOur create_writer() function works fantastic.\nHow about we give our train() function the ability to take in a writer parameter so we actively update the SummaryWriter() instance weâ€™re using each time we call train().\nFor example, say weâ€™re running a series of experiments, calling train() multiple times for multiple different models, it would be good if each experiment used a different writer.\nOne writer per experiment = one logs directory per experiment.\nTo adjust the train() function weâ€™ll add a writer parameter to the function and then weâ€™ll add some code to see if thereâ€™s a writer and if so, weâ€™ll track our information there.\n\nfrom typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Add writer parameter to train()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#setting-up-a-series-of-modelling-experiments",
    "href": "07_pytorch_experiment_tracking.html#setting-up-a-series-of-modelling-experiments",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.13 7. Setting up a series of modelling experiments",
    "text": "9.13 7. Setting up a series of modelling experiments\nItâ€™s to step things up a notch.\nPreviously weâ€™ve been running various experiments and inspecting the results one by one.\nBut what if we could run multiple experiments and then inspect the results all together?\nYou in?\nCâ€™mon, letâ€™s go.\n\n9.13.1 7.1 What kind of experiments should you run?\nThatâ€™s the million dollar question in machine learning.\nBecause thereâ€™s really no limit to the experiments you can run.\nSuch a freedom is why machine learning is so exciting and terrifying at the same time.\nThis is where youâ€™ll have to put on your scientist coat and remember the machine learning practitionerâ€™s motto: experiment, experiment, experiment!\nEvery hyperparameter stands as a starting point for a different experiment: * Change the number of epochs. * Change the number of layers/hidden units. * Change the amount of data. * Change the learning rate. * Try different kinds of data augmentation. * Choose a different model architecture.\nWith practice and running many different experiments, youâ€™ll start to build an intuition of what might help your model.\nI say might on purpose because thereâ€™s no guarantees.\nBut generally, in light of The Bitter Lesson (Iâ€™ve mentioned this twice now because itâ€™s an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.\nHowever, when youâ€™re first approaching a machine learning problem: start small and if something works, scale it up.\nYour first batch of experiments should take no longer than a few seconds to a few minutes to run.\nThe quicker you can experiment, the faster you can work out what doesnâ€™t work, in turn, the faster you can work out what does work.\n\n\n9.13.2 7.2 What experiments are we going to run?\nOur goal is to improve the model powering FoodVision Mini without it getting too big.\nIn essence, our ideal model achieves a high level of test set accuracy (90%+) but doesnâ€™t take too long to train/perform inference (make predictions).\nWeâ€™ve got plenty of options but how about we keep things simple?\nLetâ€™s try a combination of: 1. A different amount of data (10% of Pizza, Steak, Sushi vs.Â 20%) 2. A different model (torchvision.models.efficientnet_b0 vs.Â torchvision.models.efficientnet_b2) 3. A different training time (5 epochs vs.Â 10 epochs)\nBreaking these down we get:\n\n\n\n\n\n\n\n\n\nExperiment number\nTraining Dataset\nModel (pretrained on ImageNet)\nNumber of epochs\n\n\n\n\n1\nPizza, Steak, Sushi 10% percent\nEfficientNetB0\n5\n\n\n2\nPizza, Steak, Sushi 10% percent\nEfficientNetB2\n5\n\n\n3\nPizza, Steak, Sushi 10% percent\nEfficientNetB0\n10\n\n\n4\nPizza, Steak, Sushi 10% percent\nEfficientNetB2\n10\n\n\n5\nPizza, Steak, Sushi 20% percent\nEfficientNetB0\n5\n\n\n6\nPizza, Steak, Sushi 20% percent\nEfficientNetB2\n5\n\n\n7\nPizza, Steak, Sushi 20% percent\nEfficientNetB0\n10\n\n\n8\nPizza, Steak, Sushi 20% percent\nEfficientNetB2\n10\n\n\n\nNotice how weâ€™re slowly scaling things up.\nWith each experiment we slowly increase the amount of data, the model size and the length of training.\nBy the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.\n\nì°¸ê³ : I want to be clear that there truly is no limit to amount of experiments you can run. What weâ€™ve designed here is only a very small subset of options. However, you canâ€™t test everything so best to try a few things to begin with and then follow the ones which work the best.\nAnd as a reminder, the datasets weâ€™re using are a subset of the Food101 dataset (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the 04_custom_data_creation.ipynb notebook.\n\n\n\n9.13.3 7.3 Download different datasets\nBefore we start running our series of experiments, we need to make sure our datasets are ready.\nWeâ€™ll need two forms of a training set: 1. A training set with 10% of the data of Food101 pizza, steak, sushi images (weâ€™ve already created this above but weâ€™ll do it again for completeness). 2. A training set with 20% of the data of Food101 pizza, steak, sushi images.\nFor consistency, all experiments will use the same testing dataset (the one from the 10% data split).\nWeâ€™ll start by downloading the various datasets we need using the download_data() function we created earlier.\nBoth datasets are available from the course GitHub: 1. Pizza, steak, sushi 10% training data. 2. Pizza, steak, sushi 20% training data.\n\n# Download 10 percent and 20 percent training data (if necessary)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\n[INFO] data/pizza_steak_sushi directory exists, skipping download.\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n\n\nData downloaded!\nNow letâ€™s setup the filepaths to data weâ€™ll be using for the different experiments.\nWeâ€™ll create different training directory paths but weâ€™ll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%).\n\n# Setup training directory paths\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Setup testing directory paths (note: use the same test dataset for both to compare the results)\ntest_dir = data_10_percent_path / \"test\"\n\n# Check the directories\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n\nTraining directory 10%: data/pizza_steak_sushi/train\nTraining directory 20%: data/pizza_steak_sushi_20_percent/train\nTesting directory: data/pizza_steak_sushi/test\n\n\n\n\n9.13.4 7.4 Transform Datasets and create DataLoaders\nNext weâ€™ll create a series of transforms to prepare our images for our model(s).\nTo keep things consistent, weâ€™ll manually create a transform (just like we did above) and use the same transform across all of the datasets.\nThe transform will: 1. Resize all the images (weâ€™ll start with 224, 224 but this could be changed). 2. Turn them into tensors with values between 0 & 1. 3. Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from torchvision.models have been pretrained on ImageNet).\n\nfrom torchvision import transforms\n\n# Create a transform to normalize data distribution to be inline with ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose transforms into a pipeline\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n\nTransform ready!\nNow letâ€™s create our DataLoaders using the create_dataloaders() function from data_setup.py we created in 05. PyTorch Going Modular section 2.\nWeâ€™ll create the DataLoaders with a batch size of 32.\nFor all of our experiments weâ€™ll be using the same test_dataloader (to keep comparisons consistent).\n\nBATCH_SIZE = 32\n\n# Create 10% training and test DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create 20% training and test data DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n\nNumber of batches of size 32 in 10 percent training data: 8\nNumber of batches of size 32 in 20 percent training data: 15\nNumber of batches of size 32 in testing data: 8 (all experiments will use the same test set)\nNumber of classes: 3, class names: ['pizza', 'steak', 'sushi']\n\n\n\n\n9.13.5 7.5 Create feature extractor models\nTime to start building our models.\nWeâ€™re going to create two feature extractor models:\n\ntorchvision.models.efficientnet_b0() pretrained backbone + custom classifier head (EffNetB0 for short).\ntorchvision.models.efficientnet_b2() pretrained backbone + custom classifier head (EffNetB2 for short).\n\nTo do this, weâ€™ll freeze the base layers (the feature layers) and update the modelâ€™s classifier heads (output layers) to suit our problem just like we did in 06. PyTorch Transfer Learning section 3.4.\nWe saw in the previous chapter the in_features parameter to the classifier head of EffNetB0 is 1280 (the backbone turns the input image into a feature vector of size 1280).\nSince EffNetB2 has a different number of layers and parameters, weâ€™ll need to adapt it accordingly.\n\nì°¸ê³ : Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way youâ€™ll know how youâ€™ll have to prepare your input data/update the model to have the correct output shape.\n\nWe can find the input and output shapes of EffNetB2 using torchinfo.summary() and passing in the input_size=(32, 3, 224, 224) parameter ((32, 3, 224, 224) is equivalent to (batch_size, color_channels, height, width), i.e we pass in an example of what a single batch of data would be to our model).\n\nì°¸ê³ : Many modern models can handle input images of varying sizes thanks to torch.nn.AdaptiveAvgPool2d() layer, this layer adaptively adjusts the output_size of a given input as required. You can try this out by passing different size input images to torchinfo.summary() or to your own models using the layer.\n\nTo find the required input shape to the final layer of EffNetB2, letâ€™s: 1. Create an instance of torchvision.models.efficientnet_b2(pretrained=True). 2. See the various input and output shapes by running torchinfo.summary(). 3. Print out the number of in_features by inspecting state_dict() of the classifier portion of EffNetB2 and printing the length of the weight matrix. * ì°¸ê³ : You could also just inspect the output of effnetb2.classifier.\n\nimport torchvision\nfrom torchinfo import summary\n\n# 1. Create an instance of EffNetB2 with pretrained weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n# 3. Get the number of in_features of the EfficientNetB2 classifier layer\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n\nNumber of in_features to final layer of EfficientNetB2: 1408\n\n\n\nModel summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.\nNow we know the required number of in_features for the EffNetB2 model, letâ€™s create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.\nWe want these functions to: 1. Get the base model from torchvision.models 2. Freeze the base layers in the model (set requires_grad=False) 3. Set the random seeds (we donâ€™t need to do this but since weâ€™re running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment) 4. Change the classifier head (to suit our problem) 5. Give the model a name (e.g.Â â€œeffnetb0â€ for EffNetB0)\n\nimport torchvision\nfrom torch import nn\n\n# Get num out features (one for each class pizza, steak, sushi)\nOUT_FEATURES = len(class_names)\n\n# Create an EffNetB0 feature extractor\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Create an EffNetB2 feature extractor\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\nThose are some nice looking functions!\nLetâ€™s test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their summary().\n\neffnetb0 = create_effnetb0() \n\n# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n# summary(model=effnetb0, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n[INFO] Created new effnetb0 model.\n\n\n\nModel summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).\n\neffnetb2 = create_effnetb2()\n\n# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n[INFO] Created new effnetb2 model.\n\n\n\nModel summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).\nLooking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.\n\n\n\n\n\n\n\n\n\nModel\nTotal parameters (before freezing/changing head)\nTotal parameters (after freezing/changing head)\nTotal trainable parameters (after freezing/changing head)\n\n\n\n\nEfficientNetB0\n5,288,548\n4,011,391\n3,843\n\n\nEfficientNetB2\n9,109,994\n7,705,221\n4,227\n\n\n\nThis gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.\nHowever, the trainable parameters for each model (the classifier heads) arenâ€™t very different.\nWill these extra parameters lead to better results?\nWeâ€™ll have to wait and seeâ€¦\n\nì°¸ê³ : In the spirit of experimenting, you really could try almost any model from torchvision.models in a similar fashion to what weâ€™re doing here. Iâ€™ve only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like torchvision.models.convnext_tiny() or torchvision.models.convnext_small() into the mix.\n\n\n\n9.13.6 7.6 Create experiments and set up training code\nWeâ€™ve prepared our data and prepared our models, the time has come to setup some experiments!\nWeâ€™ll start by creating two lists and a dictionary: 1. A list of the number of epochs weâ€™d like to test ([5, 10]) 2. A list of the models weâ€™d like to test ([\"effnetb0\", \"effnetb2\"]) 3. A dictionary of the different training DataLoaders\n\n# 1. Create epochs list\nnum_epochs = [5, 10]\n\n# 2. Create models list (need to create a new model for each experiment)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Create dataloaders dictionary for various dataloaders\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n\nLists and dictionary created!\nNow we can write code to iterate through each of the different options and try out each of the different combinations.\nWeâ€™ll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.\nSpecifically, letâ€™s go through the following steps: 1. Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results). 2. Keep track of different experiment numbers (this is mostly for pretty print outs). 3. Loop through the train_dataloaders dictionary items for each of the different training DataLoaders. 4. Loop through the list of epoch numbers. 5. Loop through the list of different model names. 6. Create information print outs for the current running experiment (so we know whatâ€™s happening). 7. Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint). 8. Create a new loss function (torch.nn.CrossEntropyLoss()) and optimizer (torch.optim.Adam(params=model.parameters(), lr=0.001)) for each new experiment. 9. Train the model with the modified train() function passing the appropriate details to the writer parameter. 10. Save the trained model with an appropriate file name to file with save_model() from utils.py.\nWe can also use the %%time magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.\nLetâ€™s do it!\n\n%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Set the random seeds\nset_seeds(seed=42)\n\n# 2. Keep track of experiment numbers\nexperiment_number = 0\n\n# 3. Loop through each DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n\n[INFO] Experiment number: 1\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 2\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 3\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\nEpoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864\nEpoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864\nEpoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759\nEpoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873\nEpoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 4\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\nEpoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769\nEpoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977\nEpoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466\nEpoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 5\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 6\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 7\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\nEpoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072\nEpoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072\nEpoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873\nEpoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280\nEpoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 8\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...\n\n\n\n\n\nEpoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\nEpoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977\nEpoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384\nEpoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280\nEpoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\nCPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s\nWall time: 2min 33s",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#view-experiments-in-tensorboard",
    "href": "07_pytorch_experiment_tracking.html#view-experiments-in-tensorboard",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.14 8. View experiments in TensorBoard",
    "text": "9.14 8. View experiments in TensorBoard\nHo, ho!\nLook at us go!\nTraining eight models in one go?\nNow thatâ€™s living up to the motto!\nExperiment, experiment, experiment!\nHow about we check out the results in TensorBoard?\n\n# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n\nRunning the cell above we should get an output similar to the following.\n\nì°¸ê³ : Depending on the random seeds you used/hardware you used thereâ€™s a chance your numbers arenâ€™t exactly the same as whatâ€™s here. This is okay. Itâ€™s due to the inherent randomness of deep learning. What matters most is the trend. Where your numbers are heading. If theyâ€™re off by a large amount, perhaps thereâ€™s something wrong and best to go back and check the code. But if theyâ€™re off by a small amount (say a couple of decimal places or so), thatâ€™s okay.\n\n\nVisualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB2 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.\nYou can also upload your TensorBoard experiment results to tensorboard.dev to host them publically for free.\nFor example, running code similiar to the following:\n\n# # Upload the results to TensorBoard.dev (uncomment to try it out)\n# !tensorboard dev upload --logdir runs \\\n#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n#     --description \"Comparing results of different model size, training data amount and training time.\"\n\nRunning the cell above results in the experiments from this notebook being publically viewable at: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/\n\nì°¸ê³ : Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they donâ€™t contain sensitive information.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#load-in-the-best-model-and-make-predictions-with-it",
    "href": "07_pytorch_experiment_tracking.html#load-in-the-best-model-and-make-predictions-with-it",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.15 9. Load in the best model and make predictions with it",
    "text": "9.15 9. Load in the best model and make predictions with it\nLooking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).\nThis is the experiment that used: * EffNetB2 (double the parameters of EffNetB0) * 20% pizza, steak, sushi training data (double the original training data) * 10 epochs (double the original training time)\nIn essence, our biggest model achieved the best results.\nThough it wasnâ€™t as if these results were far better than the other models.\nThe same model on the same data achieved similar results in half the training time (experiment number 6).\nThis suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.\nInspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).\nMore experiments could be done to further test this but for now, letâ€™s import our best performing model from experiment eight (saved to: models/07_effnetb2_data_20_percent_10_epochs.pth, you can download this model from the course GitHub) and perform some qualitative evaluations.\nIn other words, letâ€™s visualize, visualize, visualize!\nWe can import the best saved model by creating a new instance of EffNetB2 using the create_effnetb2() function and then load in the saved state_dict() with torch.load().\n\n# Setup the best model filepath\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\nbest_model = create_effnetb2()\n\n# Load the saved best model state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n\n[INFO] Created new effnetb2 model.\n\n\n&lt;All keys matched successfully&gt;\n\n\nBest model loaded!\nWhile weâ€™re here, letâ€™s check its filesize.\nThis is an important consideration later on when deploying the model (incorporating it in an app).\nIf the model is too large, it can be hard to deploy.\n\n# Check the model file size\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n\nEfficientNetB2 feature extractor model size: 29 MB\n\n\nLooks like our best model so far is 29 MB in size. Weâ€™ll keep this in mind if we wanted to deploy it later on.\nTime to make and visualize some predictions.\nWe created a pred_and_plot_image() function use a trained model to make predictions on an image in 06. PyTorch Transfer Learning section 6.\nAnd we can reuse this function by importing it from going_modular.going_modular.predictions.py (I put the pred_and_plot_image() function in a script so we could reuse it).\nSo to make predictions on various images the model hasnâ€™t seen before, weâ€™ll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then weâ€™ll randomly select a subset of these filepaths to pass to our pred_and_plot_image() function.\n\n# Import function to make predictions on images and plot them \n# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Get a random list of 3 images from 20% test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterate through random test image paths, make predictions on them and plot them\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNice!\nRunning the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models weâ€™ve built.\nThis suggests the model is more confident in the decisions itâ€™s making.\n\n9.15.1 9.1 Predict on a custom image with the best model\nMaking predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.\nSo letâ€™s import the trusty pizza dad image (a photo of my dad in front of a pizza) weâ€™ve been using for the past couple of sections and see how our model performs on it.\n\n# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n\ndata/04-pizza-dad.jpeg already exists, skipping download.\n\n\n\n\n\n\n\n\n\nWoah!\nTwo thumbs again!\nOur best model predicts â€œpizzaâ€ correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in 06. PyTorch Transfer Learning section 6.1.\nThis again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.\nI wonder what could improve our modelâ€™s performance even further?\nIâ€™ll leave that as a challenge for you to investigate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#main-takeaways",
    "href": "07_pytorch_experiment_tracking.html#main-takeaways",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.16 Main takeaways",
    "text": "9.16 Main takeaways\nWeâ€™ve now gone full circle on the PyTorch workflow introduced in 01. PyTorch Workflow Fundamentals, weâ€™ve gotten data ready, weâ€™ve built and picked a pretrained model, weâ€™ve used our various helper functions to train and evaluate the model and in this notebook weâ€™ve improved our FoodVision Mini model by running and tracking a series of experiments.\n\nYou should be proud of yourself, this is no small feat!\nThe main ideas you should take away from this Milestone Project 1 are:\n\nThe machine learning practionerâ€™s motto: experiment, experiment, experiment! (though weâ€™ve been doing plenty of this already).\nIn the beginning, keep your experiments small so you can work fast, your first few experiments shouldnâ€™t take more than a few seconds to a few minutes to run.\nThe more experiments you do, the quicker you can figure out what doesnâ€™t work.\nScale up when you find something that works. For example, since weâ€™ve found a pretty good performing model with EffNetB2 as a feature extractor, perhaps youâ€™d now like to see what happens when you scale it up to the whole Food101 dataset from torchvision.datasets.\nProgrammatically tracking your experiments takes a few steps to set up but itâ€™s worth it in the long run so you can figure out what works and what doesnâ€™t.\n\nThere are many different machine learning experiment trackers out there so explore a few and try them out.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "07_pytorch_experiment_tracking.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.17 ì—°ìŠµ ë¬¸ì œ",
    "text": "9.17 ì—°ìŠµ ë¬¸ì œ\n\nì°¸ê³ : These exercises expect the use of torchvision v0.13+ (currently in beta as of June 2022). You can install the nightly version via the PyTorch getting started page.\n\nAll of the exercises are focused on practicing the code above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nAll exercises should be completed using device-agnostic code.\nResources: * Exercise template notebook for 07 * Example solutions notebook for 07 (try the exercises before looking at this) * See a live video walkthrough of the solutions on YouTube (errors and all)\n\nPick a larger model from torchvision.models to add to the list of experiments (for example, EffNetB3 or higher).\n\nHow does it perform compared to our existing models?\n\nIntroduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n\nFor example, you could have one training DataLoader that uses data augmentation (e.g.Â train_dataloader_20_percent_aug and train_dataloader_20_percent_no_aug) and then compare the results of two of the same model types training on these two DataLoaders.\nì°¸ê³ : You may need to alter the create_dataloaders() function to be able to take a transform for the training data and the testing data (because you donâ€™t need to perform data augmentation on the test data). See 04. PyTorch Custom Datasets section 6 for examples of using data augmentation or the script below for an example:\n\n\n# ì°¸ê³ : Data augmentation transform like this should only be performed on training data\ntrain_transform_data_aug = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    normalize\n])\n\n# Helper function to view images in a DataLoader (works with data augmentation transforms or not) \ndef view_dataloader_images(dataloader, n=10):\n    if n &gt; 10:\n        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n        n = 10\n    imgs, labels = next(iter(dataloader))\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        # Min max scale the image for display purposes\n        targ_image = imgs[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # Plot images with appropriate axes information\n        plt.subplot(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n        plt.title(class_names[labels[i]])\n        plt.axis(False)\n\n# Have to update `create_dataloaders()` to handle different augmentations\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\nNUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n\n# ì°¸ê³ : this is an update version of data_setup.create_dataloaders to handle\n# differnt train and test transforms.\ndef create_dataloaders(\n    train_dir, \n    test_dir, \n    train_transform, # add parameter for train transform (transforms on train dataset)\n    test_transform,  # add parameter for test transform (transforms on test dataset)\n    batch_size=32, num_workers=NUM_WORKERS\n):\n    # Use ImageFolder to create dataset(s)\n    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    # Get class names\n    class_names = train_data.classes\n\n    # Turn images into data loaders\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names\n\nScale up the dataset to turn FoodVision Mini into FoodVision Big using the entire Food101 dataset from torchvision.models\n\nYou could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\nIf you try more than one model, it would be good to have the modelâ€™s results tracked.\nIf you load the Food101 dataset from torchvision.models, youâ€™ll have to create PyTorch DataLoaders to use it in training.\nì°¸ê³ : Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "07_pytorch_experiment_tracking.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "07_pytorch_experiment_tracking.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "9Â  07 - PyTorch ì‹¤í—˜ ì¶”ì ",
    "section": "9.18 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "9.18 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nRead The Bitter Lesson blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.\nGo through the PyTorch YouTube/code tutorial for TensorBoard for 20-minutes and see how it compares to the code weâ€™ve written in this notebook.\nPerhaps you may want to view and rearrange your modelâ€™s TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), thereâ€™s a guide for this in the TensorBoard documentation.\nIf you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the PyTorch Development in VSCode guide.\nTo go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the PyTorch documentation for the PyTorch profiler.\nMade With ML is an outstanding resource for all things machine learning by Goku Mohandas and their guide on experiment tracking contains a fantastic introduction to tracking machine learning experiments with MLflow.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>07 - PyTorch ì‹¤í—˜ ì¶”ì </span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html",
    "href": "08_pytorch_paper_replicating.html",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "",
    "text": "10.1 TK - What is paper replicating?\nTK intro\nWant to recreate ViT paper: â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleâ€ - https://arxiv.org/abs/2010.11929 - TK will refer to this as â€œViT paperâ€ throughout.\nItâ€™s no secret machine learning is advancing fast.\nMany of these advances get published in machine learning research papers.\nAnd the goal of paper replicating is to take replicate these advances with code so you can use the techniques for your own problem.\nFor example, letâ€™s say a new model architecture gets released that performs better than any other architecture before on various benchmarks, wouldnâ€™t it be nice to try that architecture on your own problems?",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---what-is-paper-replicating",
    "href": "08_pytorch_paper_replicating.html#tk---what-is-paper-replicating",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "",
    "text": "TK image: paper replicating = research paper (language + diagrams + math) -&gt; code (turn language, diagrams and math into usable code) / (translate a research paper into usable code)",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---what-is-a-machine-learning-research-paper",
    "href": "08_pytorch_paper_replicating.html#tk---what-is-a-machine-learning-research-paper",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.2 TK - What is a machine learning research paper?",
    "text": "10.2 TK - What is a machine learning research paper?\nA machine learning research paper is a scientific paper that details findings of a research group on a specific area.\nThe contents of a machine learning research paper can vary from paper to paper but they generally follow the structure:\n\n\n\n\n\n\n\nSection\nContents\n\n\n\n\nAbstract\nAn overview/summary of the paperâ€™s main findings/contributions.\n\n\nIntroduction\nWhatâ€™s the paperâ€™s main problem and what are previous methods used to try and solve it?\n\n\nMethod\nHow did the researchers go about conducting their research? For example, what model(s) were used, data sources, training setups, etc.\n\n\nResults\nWhat are the outcomes of the paper? If a new type of model or training setup was used, how did the results of findings compare to previous works (this is where experiment tracking comes in handy)?\n\n\nConclusion\nWhat are the limitations of the suggested methods? What are some next steps for the research community?\n\n\nReferences\nWhat resources/other papers did the researchers look at to build their own body of work?\n\n\nAppendix\nAre there any extra resources/findings to look at that werenâ€™t included in any of the above sections?",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---why-replicate-a-machine-learning-research-paper",
    "href": "08_pytorch_paper_replicating.html#tk---why-replicate-a-machine-learning-research-paper",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.3 TK - Why replicate a machine learning research paper?",
    "text": "10.3 TK - Why replicate a machine learning research paper?\nA machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.\nAnd if these experiments lead to better results in an area related to the problem youâ€™re working on, itâ€™d be nice to try them out.\nAlso, replicating the work of others is a fantastic way to practice your skills.\n\nGeorge Hotz is founder of comma.ai, a self-driving car company and livestreams machine learning coding on Twitch and those videos get posted in full to YouTube. I pulled this quote from one of his livestreams. The â€œÙ­â€ is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).\nWhen you first start trying to replicate research papers, youâ€™ll likely be overwhelmed.\nThatâ€™s normal.\nResearch teams spend weeks, months and sometimes years creating these works so it makes sense if it takes you sometime to even read let alone reproduce the works.\nReplicating research is such a tough problem, phenomenal machine learning libraries and tools such as, HuggingFace, PyTorch Image Models (timm library) and fast.ai have been born out of making machine learning research more accessible.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---where-can-you-find-code-examples-for-machine-learning-research-papers",
    "href": "08_pytorch_paper_replicating.html#tk---where-can-you-find-code-examples-for-machine-learning-research-papers",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.4 TK - Where can you find code examples for machine learning research papers?",
    "text": "10.4 TK - Where can you find code examples for machine learning research papers?\nOne of the first things youâ€™ll notice when it comes to machine learning research is: thereâ€™s a lot of it.\nSo beware, trying to stay on top of it is like trying to outrun a hamster wheel.\nFollow your interest, pick a few things that stand out to you.\nIn saying this, there are several places to find and read machine learning research papers: * arXiv - Pronounced â€œarchiveâ€, arXiv is a free and open resource for reading technical articles on everything from physics to computer science (inlcuding machine learning). * Papers with Code - A curated collection of trending, active and greatest machine learning papers, many of which include code resources attached. Also includes a collection of common machine learning datasets, benchmarks and current state-of-the-art models. * AK Twitter - The AK Twitter account publishes machine learning research highlights, often with live demos almost every day. I donâ€™t understand 9/10 posts but I find it fun to explore every so often. * lucidrainsâ€™ vit-pytorch GitHub repository - Less of a place to find research papers and more of an example of what paper replicating with code on a larger-scale looks like. The vit-pytorch repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code (much of the inspiration for this notebook was gathered from this repository).\nTK image: showcase the above",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "08_pytorch_paper_replicating.html#tk---ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.5 TK - ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "10.5 TK - ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nTODO\n\nViT -&gt; FoodVision Mini\nLayers = collections of functions to manipulate data -&gt; Architectures = collections of layers (blocks) -&gt; All layers (and blocks) have inputs and outputs\n\nReplicating research papers starts by figuring out the inputs and outputs of your layers -&gt; blocks -&gt; model",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---where-can-you-get-help",
    "href": "08_pytorch_paper_replicating.html#tk---where-can-you-get-help",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.6 TK - Where can you get help?",
    "text": "10.6 TK - Where can you get help?\nAll of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-0.-getting-setup",
    "href": "08_pytorch_paper_replicating.html#tk-0.-getting-setup",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.7 TK 0. Getting setup",
    "text": "10.7 TK 0. Getting setup\nAs weâ€™ve done previously, letâ€™s make sure weâ€™ve got all of the modules weâ€™ll need for this section.\nWeâ€™ll import the Python scripts (such as data_setup.py and engine.py) we created in 05. PyTorch Going Modular.\nTo do so, weâ€™ll download going_modular directory from the pytorch-deep-learning repository (if we donâ€™t already have it).\nWeâ€™ll also get the torchinfo package if itâ€™s not available.\ntorchinfo will help later on to give us a visual representation of our model.\nAnd since later on weâ€™ll be using a newer version of the torchvision package (as of June 2022), weâ€™ll make sure weâ€™ve got the latest versions.\n\n# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\ntorch version: 1.12.0+cu102\ntorchvision version: 0.13.0+cu102\n\n\n\nì°¸ê³ : If youâ€™re using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify youâ€™ve got the right versions of torch and torchvision.\n\nNow weâ€™ll continue with the regular imports, setting up device agnostic code and this time weâ€™ll also get the helper_functions.py script from GitHub.\nThe helper_functions.py script contains several functions we created in previous sections: * set_seeds() to set the random seeds (created in 07. PyTorch Experiment Tracking section 0). * download_data() to download a data source given a link (created in 07. PyTorch Experiment Tracking section 1). * plot_loss_curves() to inspect our modelâ€™s training results (created in 04. PyTorch Custom Datasets section 7.8)\n\nì°¸ê³ : It may be a better idea for many of the functions in the helper_functions.py script to be merged into going_modular/going_modular/utils.py, perhaps thatâ€™s an extension youâ€™d like to try.\n\n\n# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n\n\nì°¸ê³ : If youâ€™re using Google Colab, and you donâ€™t have a GPU turned on yet, itâ€™s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-1.-get-data",
    "href": "08_pytorch_paper_replicating.html#tk-1.-get-data",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.8 TK 1. Get Data",
    "text": "10.8 TK 1. Get Data\nSince weâ€™re continuing on with FoodVision Mini, letâ€™s download the pizza, steak and sushi image dataset weâ€™ve been using.\nTo do so we can use the download_data() function from helper_functions.py that we created in 07. PyTorch Experiment Tracking section 1.\nWeâ€™ll source to the raw GitHub link of the pizza_steak_sushi.zip data and the destination to pizza_steak_sushi.\n\n# Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n\n[INFO] data/pizza_steak_sushi directory exists, skipping download.\n\n\nPosixPath('data/pizza_steak_sushi')\n\n\nBeautiful! Data downloaded, letâ€™s setup the training and test directories.\n\n# Setup directory paths to train and test images\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-2.-create-datasets-and-dataloaders",
    "href": "08_pytorch_paper_replicating.html#tk-2.-create-datasets-and-dataloaders",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.9 TK 2. Create Datasets and DataLoaders",
    "text": "10.9 TK 2. Create Datasets and DataLoaders\nSince weâ€™ve got some data, letâ€™s now turn it into DataLoaderâ€™s.\nTo do so we can use the create_dataloaders() function in data_setup.py.\nFirst, weâ€™ll create a transform to prepare our images.\nThis where one of the first references to the ViT paper will come in.\nIn Table 3, the training resolution is mentioned as being 224 (height=224, width=224).\n\nYou can often find various hyperparameter settings listed in a table. In this case weâ€™re still preparing our data, so weâ€™re mainly concerned with things like image size and batch size. Source: Table 3 in ViT paper.\nSo weâ€™ll make sure our transform resizes our images appropriately.\nAnd since weâ€™ll be training our model from scratch (no transfer learning to begin with), we wonâ€™t provide a normalize transform like we did in 06. PyTorch Transfer Learning section 2.1.\n\n10.9.1 2.1 Prepare transforms for images\n\n# Create image size (from Table 3 in the ViT paper) \nIMG_SIZE = 224\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\nManually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n)\n\n\n\n\n10.9.2 2.2 Turn images into DataLoaderâ€™s\nTransforms created!\nLetâ€™s now create our DataLoaderâ€™s.\nThe ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size weâ€™ve been using (32).\nWeâ€™re going to stick with a batch size of 32.\nWhy?\nBecause some hardware (including the free tier of Google Colab) may not be able to handle a batch size of 4096.\nHaving a batch size of 4096 means that 4096 images need to fit into the GPU memory at a time.\nThis works when youâ€™ve got the hardware to handle it like a research team from Google often does but when youâ€™re running on a single GPU (such as using Google Colab), making sure things work with smaller batch size first is a good idea.\nAn extension of this project could be to try a higher batch size value and see what happens.\n\nì°¸ê³ : Weâ€™re using the pin_memory=True parameter in the create_dataloaders() function to speed up computation. pin_memory=True avoids unnecessary copying of memory between the CPU and GPU memory by â€œpinningâ€ examples that have been seen before. For more on this concept. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). See the PyTorch torch.utils.data.DataLoader documentation or Making Deep Learning Go Brrrr from First Principles by Horace He for more.\n\n\n# Set the batch size\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n\n(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa8cc731550&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa8cc731520&gt;,\n ['pizza', 'steak', 'sushi'])\n\n\n\n\n10.9.3 TK 2.3 Visualize a single image\nNow weâ€™ve loaded our data, letâ€™s visualize, visualize, visualize!\nAn important step in the ViT paper is preparing the images into patches.\nWeâ€™ll get to what this means in a second but for now, letâ€™s view a single image and its label.\nTo do so, letâ€™s get a single image and label from a batch of data and inspect their shapes.\n\n# Get a batch of images\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Get a single image from the batch\nimage, label = image_batch[0], label_batch[0]\n\n# View the batch shapes\nimage.shape, label\n\n(torch.Size([3, 224, 224]), tensor(0))\n\n\nWonderful!\nNow letâ€™s plot the image and its label with matplotlib.\n\n# Plot image with matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);\n\n\n\n\n\n\n\n\nNice!\nLooks like our images are importing correctly, letâ€™s continue with the paper replication.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-3.-replicating-the-vit-paper-an-overview",
    "href": "08_pytorch_paper_replicating.html#tk-3.-replicating-the-vit-paper-an-overview",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.10 TK 3. Replicating the ViT paper: an overview",
    "text": "10.10 TK 3. Replicating the ViT paper: an overview\nBefore we write anymore code, letâ€™s discuss what weâ€™re doing.\nWeâ€™d like to replicate the ViT paper for our own problem, FoodVision Mini.\nSo our inputs are: images of pizza, steak and sushi.\nAnd our ideal model outputs are: predicted labels of pizza, steak or sushi.\nNo different to what weâ€™ve been doing throughout the previous sections.\nThe question is: how do we go from our inputs to the desired outputs?\n\n10.10.1 3.1 Inputs and outputs, layers and blocks\nViT is a deep learning neural network architecture.\nAnd any neural network architecture is generally comprised of layers.\nAnd a collection of layers is often referred to as a block.\nAnd stacking many blocks together is what gives us the whole architecture.\nA layer takes an input (say an image tensor), performs some kind of function on it (for example whatâ€™s in the layerâ€™s forward() method) and then returns an output.\nSo if a single layer takes an input and gives an output, then a collection of layers or a block also takes an input and gives an output.\nLetâ€™s make this concrete: * Layer - takes an input, performs a function on it, returns an output. * Block - a collection of layers, takes an input, performs a series of functions on it, returns an output. * Architecture (or model) - a collection of blocks, takes an input, performs a series of functions on it, returns an output.\nThis ideology is what weâ€™re going to be using to replicate the ViT paper.\nWeâ€™re going to take it layer by layer, block by block, function by function putting the pieces of the puzzle together like Lego to get our desired overall architecture.\nThe reason we do this is because looking at a whole research paper can be intimidating.\nSo for a better understanding, weâ€™ll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.\nTK image: stacking the network together like lego (functions + layers + blocks = model).\n\n\n10.10.2 3.2 Getting specific: Whatâ€™s ViT made of?\nThere are many little details about the ViT model sprinkled throughout the paper.\nFinding them all is like one big treasure hunt!\nRemember, a research paper is often months of work compressed into a few pages so itâ€™s understandable for it to take of practice to replicate.\nHowever, the main three resources weâ€™ll be looking at for the architecture design are: 1. Figure 1 - This gives an overview of the model in a graphical sense, you could almost recreate the architecture with this figure alone. 2. Four equations in section 3.1 - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1. 3. Table 1 - This table shows the various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. Weâ€™ll be focused on the smallest version, ViT-Base.\n\n10.10.2.1 TK 3.2.1 Exploring Figure 1\nLetâ€™s start by going through Figure 1 of the ViT Paper.\nThe main things weâ€™ll be paying attention to are: 1. Layers - takes an input, performs an operation or function, produces an output. 2. Blocks - a collection of layers, which in turn also takes an input and produces an output.\n\nFigure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.\nThe ViT architecture is comprised of several stages: * Patch + Position Embedding (inputs) - Turns the input image into a sequence of image patches and add a position number what order the patch comes in. * Linear projection of flattened patches (Embedded Patches) - The image patches get turned into an embedding, the benefit of using an embedding rather than just the image values is that an embedding is a learnable representation (typically in the form of a vector) of the image that can improve with training. * Norm - This is short for â€œLayer Normalizationâ€ or â€œLayerNormâ€, a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer torch.nn.LayerNorm(). * Multi-Head Attention - This is a Multi-Headed Self-Attention layer or â€œMSAâ€ for short. You can create an MSA layer via the PyTorch layer torch.nn.MultiheadAttention(). * MLP (or Multilayer perceptron) - A MLP can often refer to any collection of feedforward layers (or in PyTorchâ€™s case, a collection of layers with a forward() method). In the ViT Paper, the authors refer to the MLP as â€œMLP blockâ€ and it contains two torch.nn.Linear() layers with a torch.nn.GELU() non-linearity activation in between them (section 3.1) and a torch.nn.Dropout() layer after each (Appendex B.1). * Transformer Encoder - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the â€œ+â€ symbols) meaning the layerâ€™s inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother. * MLP Head - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since weâ€™re working on image classification, you could also call this the â€œclassifier headâ€. The structure of the MLP Head is similar to the MLP block.\nYou might notice that many of the pieces of the ViT architecture can be created with existing PyTorch layers.\nThis is because of how PyTorch is designed, itâ€™s one of the main purposes of PyTorch to create reusable neural network layers for both researchers and machine learning practitioners.\n\nQuestion: Why not code everything from scratch?\nYou could definitely do that by reproducing all of the math equations from the paper with custom PyTorch layers and that would certainly be an educative exercise, however, using pre-existing PyTorch layers is usually favoured as pre-existing layers have often been extensively tested and performance checked to make sure they run correctly and fast.\n\n\nì°¸ê³ : Weâ€™re going to focused on write PyTorch code to create these layers, for the background on what each of these layers does, Iâ€™d suggest reading the ViT Paper in full or reading the linked resources for each layer.\n\nLetâ€™s take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.\n\nFigure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class â€œpizzaâ€ is returned.\n\n\n10.10.2.2 TK - 3.2.2 Exploring the Four Equations\nThe next main part(s) of the ViT paper weâ€™re going to look at are the four equations in section 3.1.\n\nThese four equations represent the math behind the four major parts of the ViT architecture.\nSection 3.1 describes each of these (some of the text has been omitted for brevity, bolded text is mine):\n\n\n\n\n\n\n\nEquation number\nDescription from ViT paper section 3.1\n\n\n\n\n1\nâ€¦The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n\n\n2\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n\n\n3\nSee above.\n\n\n4\nSimilar to BERTâ€™s [ class ] token, we prepend a learnable embedding to the sequence of embedded patches \\(\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)\\), whose state at the output of the Transformer encoder \\(\\left(\\mathbf{z}_{L}^{0}\\right)\\) serves as the image representation \\(\\mathbf{y}\\) (Eq. 4)â€¦\n\n\n\nLetâ€™s map these descriptions to the ViT architecture in Figure 1.\n\nConnecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks. Some details such as â€œresidual connections after every blockâ€ are referred to in Figure 1 and in the text but not in the equations.\nThereâ€™s a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.\nHow about we break down each equation further (it will be our goal to recreate these with code)?\nIn all equations (except equation 4), â€œ\\(\\mathbf{z}\\)â€ is the raw output of a particular layer:\n\n\\(\\mathbf{z}_{0}\\) is â€œz zeroâ€ (this is the output of the initial patch embedding layer)\n\\(\\mathbf{z}_{\\ell}^{\\prime}\\) is â€œz of a particular layer primeâ€ (or an intermediary value of z)\n\\(\\mathbf{z}_{\\ell}\\) is â€œz of a particular layerâ€\n\nAnd \\(\\mathbf{y}\\) is the overall output of the architecture.\nEquation 1\n\\[\n\\begin{aligned}\n\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n\\end{aligned}\n\\]\nThis equation deals with the class token, patch embedding and position embedding (\\(\\mathbf{E}\\) is for embedding) of the input image.\nIn vector form, the embedding might look something like:\nTK - update the vector form to reflect a real exmaple\nx_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\nWhere each of the elements in the vector is learnable (their requires_grad=True).\nEquation 2\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n\\end{aligned}\n\\]\nThis says that for every layer from \\(1\\) through to \\(L\\) (the total number of layers), thereâ€™s a Multi-Head Attention layer (MSA) wrapping a LayerNorm layer (LN).\nThe addition on the end is the equivalent of adding the input to the output and forming a skip/residual connection.\nWeâ€™ll call this layer the â€œMSA blockâ€.\nIn pseudocode, this might look like:\nx_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\nNotice the skip connection on the end (adding the input of the layers to the output of the layers).\nEquation 3\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\\\\n\\end{aligned}\n\\]\nThis says that for every layer from \\(1\\) through to \\(L\\) (the total number of layers), thereâ€™s also a Multilayer Perceptron layer (MLP) wrapping a LayerNorm layer (LN).\nThe addition on the end is showing the presence of a skip/residual connection.\nWeâ€™ll call this layer the â€œMLP blockâ€.\nIn pseudocode, this might look like:\nx_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\nNotice the skip connection on the end (adding the input of the layers to the output of the layers).\nEquation 4\n\\[\n\\begin{aligned}\n\\mathbf{y} &=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & &\n\\end{aligned}\n\\]\nThis says for the last layer \\(L\\), the output \\(y\\) is the 0 index token of \\(z\\) wrapped in a LayerNorm layer (LN).\nOr in our case, the 0 index of x_output_MLP_block:\ny = LN_layer(Linear_layer(x_output_MLP_block[0]))\nOf course there are some simplifications above but weâ€™ll take care of those when we start to write PyTorch code for each section.\n\nì°¸ê³ : The above section covers alot of information. But donâ€™t forget if something doesnâ€™t make sense, you can always research it further. By asking questions like â€œwhat is a residual connection?â€.\n\n\n\n10.10.2.3 TK - 3.2.3 Exploring Table 1\nThe final piece of the ViT architecture puzzle weâ€™ll focus on (for now) is Table 1.\n\n\n\nModel\nLayers\nHidden size \\(D\\)\nMLP size\nHeads\nParams\n\n\n\n\nViT-Base\n12\n768\n3072\n12\n\\(86M\\)\n\n\nViT-Large\n24\n1024\n4096\n16\n\\(307M\\)\n\n\nViT-Huge\n32\n1280\n5120\n16\n\\(632M\\)\n\n\n\n\n&lt;i&gt;Table 1: Details of Vision Transformer model variants. Source: &lt;a href=\"https://arxiv.org/abs/2010.11929\"&gt;ViT paper&lt;/a&gt;.&lt;/i&gt;\n\n\nThis table showcasing the various hyperparameters of each of the ViT architectures.\nYou can see the numbers gradually increase from ViT-Base to ViT-Huge.\nWeâ€™re going to focus on replicating ViT-Base (start small and scale up when necessary) but weâ€™ll be writing code that could easily scale up to the larger variants.\nBreaking the hyperparameters down: * Layers - How many Transformer encoder blocks are there? (each of these will contain a MSA block and MLP block) * Hidden size \\(D\\) - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more compute. * MLP size - What are the number of hidden units in the MLP layers? * Heads - How many heads are there in the Multi-Head Attention layers? * Params - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more compute. Youâ€™ll notice even ViT-Base has far more parameters than any other model weâ€™ve used so far.\nWeâ€™ll use these values as the hyperparameter settings for our ViT architecture.\n\n\n\n10.10.3 TK - 3.3 My workflow for replicating papers\nWhen I start working on replicating a paper, I go through the following steps:\n\nRead the whole paper end-to-end once (to get an idea of the main concepts).\nGo back through each section and see how they line up with each other and start thinking about how they might be turned into code (just like above).\nRepeat step 2 until Iâ€™ve got a fairly good outline.\nUse mathpix.com (a very handy tool) to turn any sections of the paper into markdown/LaTeX to put into notebooks.\nReplicate the simplest version of the model possible.\nIf I get stuck, look up other examples.\n\nTK - gif of mathpix\nWeâ€™ve already gone through the first few steps above (and if you havenâ€™t read the full paper yet, Iâ€™d encourage you to give it a go) but what weâ€™ll be focusing on next is step 5: replicating the simplest version fo the model possible.\nThis is why weâ€™re starting with ViT-Base.\nReplicating the smallest version of the architecture possible, get it working and then we can scale up if we wanted to.\n\nì°¸ê³ : If youâ€™ve never read a research paper before, many of the above steps can be intimidating. But donâ€™t worry, like anything, your skills at reading and replicating papers will improve with practice. Donâ€™t forget, a research paper is often months of work by many people compressed into a few pages. So trying to replicate it on your own is no small feat.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding",
    "href": "08_pytorch_paper_replicating.html#tk-4.-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.11 TK 4. Equation 1: Split data into patches and creating the class, position and patch embedding",
    "text": "10.11 TK 4. Equation 1: Split data into patches and creating the class, position and patch embedding\nI remember one of my machine learning engineer friends used to say â€œitâ€™s all about the embedding.â€\nAs in, if you can represent your data in a good, learnable way (as embeddings are learnable representations), chances are a learning algorithm will be able to perform well on them.\nSo with that being said, letâ€™s start by creating the class, position and patch embeddings for the ViT architecture.\nWeâ€™ll start with the patch embedding.\nThis means weâ€™ll be turning our input images in a sequence of patches and then embedding those patches.\nRecall that an embedding is a learnable representation of some form and is often a vector. The term learnable is important because this means the representation of an input image can be improved and learned over time.\nWeâ€™ll begin by following the opening paragraph of section 3.1 of the ViT paper (bold mine):\n\ní‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í† í° ì„ë² ë”©ì˜ 1D ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. To handle 2D images, we reshape the image \\(\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}\\), where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N=H W / P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size \\(D\\) through all of its layers, so we flatten the patches and map to \\(D\\) dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n\nAnd size weâ€™re dealing with image shapes, letâ€™s keep in mind the line from Table 3 of the ViT paper:\n\nTraining resolution is 224.\n\nLetâ€™s break down the text above.\n\n\\(D\\) is the size of the patch embeddings, different values for \\(D\\) can be found in Table 1.\nThe image starts as 2D with size \\({H \\times W \\times C}\\).\nThe image gets converted to a sequence of flattened 2D patches with size \\({N \\times\\left(P^{2} \\cdot C\\right)}\\).\n\n\\((H, W)\\) is the resolution of the original image.\n\\(C\\) is the number of channels.\n\\((P, P)\\) is the resolution of each image patch (patch size).\n\\(N=H W / P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.\n\n\n\nMapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.\n\n10.11.1 TK - 4.1 Calculating patch embedding input and output shapes by hand\nHow about we start by calculating these input and output shape values by hand?\nTo do so, letâ€™s create some variables to mimic each of the terms (such as \\(H\\), \\(W\\) etc) above.\nWeâ€™ll use a patch size (\\(P\\)) of 16 since itâ€™s the best performing version of ViT-Base uses (see column â€œViT-B/16â€ of Table 5 in the ViT paper for more).\n\n# Create example values\nheight = 224 # H (\"The training resolution is 224.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n\nNumber of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196\n\n\nWeâ€™ve got the number of patches, how about we create the image output size as well?\nBetter yet, letâ€™s replicate the input and output shapes of the patch embedding layer.\nRecall:\n\nInput: The image starts as 2D with size \\({H \\times W \\times C}\\).\nOutput: The image gets converted to a sequence of flattened 2D patches with size \\({N \\times\\left(P^{2} \\cdot C\\right)}\\).\n\n\n# Input shape\ninput_shape = (height, width, color_channels)\n\n# Output shape\noutput_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"Input shape (2D image): {input_shape}\")\nprint(f\"Output shape (flattened 2D patches): {output_shape}\")\n\nInput shape (2D image): (224, 224, 3)\nOutput shape (flattened 2D patches): (196, 768)\n\n\nInput and output shapes acquired!\n\n\n10.11.2 TK - 4.2 Turning a single image into patches\nNow we know the ideal input and output shapes for our patch embedding layer.\nWhat weâ€™re doing here is breaking the overall architecture down into smaller pieces, focusing on the inputs and outputs of individual layers.\nSo how do we create the patch embedding layer?\nWeâ€™ll get to that shortly, first, letâ€™s visualize, visualize, visualize! what it looks like to turn an image into patches.\nLetâ€™s start with our single image.\n\n# View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n\n\n\n\n\n\n\n\nWe want to turn this image into patches of itself inline with Figure 1 of the ViT paper.\nHow about we start by just visualizing the top row of patched pixels?\nWe can do this by indexing on the different image dimensions.\n\n# Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) \nimage_permuted = image.permute(1, 2, 0)\n\n# Index to plot the top row of patched pixels\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n\n\n\n\n\n\n\n\nNow weâ€™ve got the top row, letâ€™s turn it into patches.\nWe can do this by iterating through the number of patches thereâ€™d be in the top row.\n\n# Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \nprint(f\"Number of patches per row: {num_patches}\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=1, \n                        ncols=img_size // patch_size, # one column for each patch\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Iterate through number of patches in the top row\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n    axs[i].set_xlabel(i+1) # set the label\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n\nNumber of patches per row: 14.0\n\n\n\n\n\n\n\n\n\nThose are some nice looking patches!\nHow about we do it for the whole image?\nThis time weâ€™ll iterate through the indexs for height and width and plot each patch as itâ€™s own subplot.\n\n# Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size \nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \nprint(f\"Number of patches per row: {num_patches}\\nNumber of patches per column: {num_patches}\\nTotal patches: {num_patches*num_patches}\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n                        ncols=img_size // patch_size, \n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Loop through height and width of image\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n        \n        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height \n                                        patch_width:patch_width+patch_size, # iterate through width\n                                        :]) # get all color channels\n        \n        # Set up label information, remove the ticks for clarity and set labels to outside\n        axs[i, j].set_ylabel(i+1, \n                             rotation=\"horizontal\", \n                             horizontalalignment=\"right\", \n                             verticalalignment=\"center\") \n        axs[i, j].set_xlabel(j+1) \n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# Set a super title\nfig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16)\nplt.show()\n\nNumber of patches per row: 14.0\nNumber of patches per column: 14.0\nTotal patches: 196.0\n\n\n\n\n\n\n\n\n\nImage patchified!\nWoah, that looks cool.\nNow how do we turn each of these patches into an embedding and convert them into a sequence?\nHint: we can use PyTorch layers. Can you guess which?\n\n\n10.11.3 TK - 4.3 Creating image patches with torch.nn.Conv2d()\nItâ€™s time to start moving towards replicating the patch embedding layers with PyTorch.\nTo visualize our single image we wrote code to loop through the different height and width dimensions of a single image and plot individual patches.\nThis operation is very similar to the convolutional operation we saw in 03. PyTorch Computer Vision section 7.1: Stepping through nn.Conv2d().\nIn fact, the authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):\n\nHybrid Architecture. As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection \\(\\mathbf{E}\\) (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size \\(1 \\times 1\\), which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.\n\nThe â€œfeature mapâ€ theyâ€™re refering to are the weights/activations produced by a convolutional layer passing over a given image.\n\nBy setting the kernel_size and stride parameters of a torch.nn.Conv2d() layer equal to the patch_size, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a â€œLinear Projectionâ€ in the ViT paper) of each patch.\nRemember our ideal input and output shapes for the patch embedding layer?\n\nInput: The image starts as 2D with size \\({H \\times W \\times C}\\).\nOutput: The image gets converted to a sequence of flattened 2D patches with size \\({N \\times\\left(P^{2} \\cdot C\\right)}\\).\n\nOr for an image size of 224 and patch size of 16:\n\nInput (2D image): (224, 224, 3)\nOutput (flattened 2D patches): (196, 768)\n\nWe can recreate these with: * torch.nn.Conv2d() for turning our image into patches of CNN feature maps. * torch.nn.Flatten() for flattening the spatial dimensions of the feature map.\nLetâ€™s start with the torch.nn.Conv2d() layer.\nWe can replicate the creation of patches by setting the kernel_size and stride equal to patch_size.\nThis means each convolutional kernel will be of size (patch_size x patch_size) or if patch_size=16, (16 x 16) (the equivalent of one whole patch)\nAnd each step or stride of the convolutional kernel will be patch_size pixels long or 16 pixels long (equivalent of stepping to the next patch).\nWeâ€™ll set in_channels=3 for the number of color channels in our image and weâ€™ll set out_channels=768, the same as the \\(D\\) value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a vector of size 768).\n\nfrom torch import nn\n\n# Set the patch size\npatch_size=16\n\n# Create the Conv2d layer with hyperparameters from the ViT paper\nconv2d = nn.Conv2d(in_channels=3, # number of color channels\n                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n                   stride=patch_size,\n                   padding=0)\n\nNow weâ€™ve got a convoluational layer, letâ€™s see what happens when we pass a single image through it.\n\n# View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n\n\n\n\n\n\n\n\n\n# Pass the image through the convolutional layer \nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)\nprint(image_out_of_conv.shape)\n\ntorch.Size([1, 768, 14, 14])\n\n\nPassing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or \\(D\\)) feature/activation maps.\nSo its output shape can be read as:\ntorch.Size([1, 768, 14, 14]) -&gt; [batch_size, embedding_dim, feature_map_height, feature_map_width]\nLetâ€™s visualize five random feature maps and see what they look like.\n\n# Plot random 5 convolutional feature maps\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\nprint(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n\n# Create plot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# Plot random image feature maps\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n\nShowing random convolutional feature maps from indexes: [180, 39, 286, 72, 105]\n\n\n\n\n\n\n\n\n\nNotice how the feature maps all kind of represent the original image, visualizing a few you can see the different major outlines and some major features.\nThe important thing to note is that these features may change over time as the neural network learns.\nAnd because of these, these feature maps can be considered a learnable embedding of our image.\nLetâ€™s check one out in numerical form.\n\n# Get a single feature map in tensor form\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n\n(tensor([[[0.2642, 1.1367, 1.0221, 0.9712, 1.0950, 1.2478, 1.2884, 1.1481,\n           0.9640, 0.6204, 0.5996, 0.5855, 0.5560, 0.4992],\n          [1.1578, 1.0633, 0.9593, 1.1931, 1.3194, 1.1306, 0.7317, 0.4322,\n           0.6025, 0.7246, 0.7891, 0.5383, 0.4786, 0.7098],\n          [1.0163, 0.9368, 1.1675, 1.3373, 1.0429, 0.7497, 0.7445, 0.4270,\n           0.7430, 0.8750, 0.6105, 0.5147, 1.0207, 0.7852],\n          [1.0669, 1.0157, 1.3291, 1.0117, 0.4848, 0.6802, 0.8365, 0.7736,\n           0.8618, 0.9144, 0.8926, 0.9795, 0.7475, 0.7585],\n          [0.9371, 1.1937, 1.0068, 0.6377, 0.7283, 0.9625, 1.0372, 0.8920,\n           0.9372, 0.9034, 0.9683, 0.9405, 0.5958, 0.8740],\n          [0.9419, 0.8599, 0.5429, 0.6954, 1.0202, 0.9093, 1.0003, 0.7619,\n           0.8472, 0.8062, 0.6418, 0.7741, 0.5791, 0.9816],\n          [0.7965, 0.7202, 0.6424, 0.9137, 0.8264, 1.0243, 1.0920, 0.9548,\n           0.9166, 0.7937, 0.4675, 0.5346, 0.7774, 1.1001],\n          [0.3298, 0.4832, 0.5324, 0.7486, 0.8303, 0.8101, 0.9969, 0.9931,\n           1.0058, 0.6002, 0.6643, 0.7254, 0.8453, 1.1323],\n          [0.5384, 0.4798, 0.6725, 0.8014, 0.7044, 0.7988, 0.8185, 0.8911,\n           0.9720, 0.8939, 0.6234, 0.5674, 0.5775, 1.0011],\n          [0.6199, 0.6465, 0.6503, 0.6215, 0.8154, 0.7950, 0.8647, 0.9872,\n           0.8513, 0.8833, 0.5799, 0.5914, 0.6936, 1.0554],\n          [0.5140, 0.6462, 0.6982, 0.7445, 0.7394, 0.8124, 0.7462, 0.9183,\n           0.7471, 0.9436, 0.7147, 0.6396, 0.5795, 1.0201],\n          [0.5467, 0.7408, 0.6854, 0.6624, 0.7465, 0.5077, 0.7633, 0.8709,\n           1.0026, 0.7276, 0.7847, 0.5811, 0.5521, 1.0318],\n          [0.8041, 0.8868, 0.5559, 0.5889, 0.7236, 0.6976, 0.7940, 0.9365,\n           0.9110, 0.8182, 0.7013, 0.4890, 0.8364, 1.0031],\n          [0.1976, 1.0262, 1.1979, 0.9982, 0.9644, 0.8868, 0.9556, 1.0204,\n           1.0060, 0.9586, 0.9351, 0.8819, 0.9290, 0.9289]]],\n        grad_fn=&lt;SliceBackward0&gt;),\n True)\n\n\nThe grad_fn output of the single_feature_map and the required_grad=True attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training.\n\n\n10.11.4 TK - 4.4 Flattening the patch embedding with torch.nn.Flatten()\nWeâ€™ve turned our image into patch embeddings but theyâ€™re still in 2D format.\nHow do we get them into the desired output shape of the patch embedding layer of the ViT model?\n\nDesried output (flattened 2D patches): (196, 768) -&gt; \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)\n\nLetâ€™s check the current shape.\n\n# Current tensor shape\nprint(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\")\n\nCurrent tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\n\n\nWell weâ€™ve got the 768 part ( \\((P^{2} \\cdot C)\\) ) but we still need the number of patches (\\(N\\)).\nReading back through section 3.1 of the ViT paper it says (bold mine):\n\nAs a special case, the patches can have spatial size \\(1 \\times 1\\), which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\n\nFlattening the spatial dimensions of the feature map hey?\nWhat layer do we have in PyTorch that can flatten?\nHow about torch.nn.Flatten()?\nBut we donâ€™t want to flatten the whole tensor, we only want to flatten the â€œspatial dimensions of the feature mapâ€.\nWhich in our case is the feature_map_height and feature_map_width dimensions of image_out_of_conv.\nSo how about we create a torch.nn.Flatten() layer to only flatten those dimensions, we can use the start_dim and end_dim parameters to set that up?\n\n# Create flatten layer\nflatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n                     end_dim=3) # flatten feature_map_width (dimension 3)\n\nNice! Now letâ€™s put it all together!\nWeâ€™ll: 1. Take a single image. 2. Put in through the convolutional layer (conv2d) to turn the image into 2D feature maps (patch embeddings). 3. Flatten the 2D feature map into a single sequence.\n\n# 1. View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"Original image shape: {image.shape}\")\n\n# 2. Turn image into feature maps\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\nprint(f\"Image feature map shape: {image_out_of_conv.shape}\")\n\n# 3. Flatten the feature maps\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")\n\nOriginal image shape: torch.Size([3, 224, 224])\nImage feature map shape: torch.Size([1, 768, 14, 14])\nFlattened image feature map shape: torch.Size([1, 768, 196])\n\n\n\n\n\n\n\n\n\nWoohoo! It looks like our image_out_of_conv_flattened shape is very close to our desired output shape:\n\nDesried output (flattened 2D patches): (196, 768) -&gt; \\({N \\times\\left(P^{2} \\cdot C\\right)}\\)\nCurrent shape: (1, 768, 196)\n\nThe only difference is our current shape has a batch size and the dimensions are in a different order to the desired output.\nHow could we fix this?\nWell, how about we rearrange the dimensions?\nWe can do so with torch.Tensor.permute() just like we do when rearranging image tensors to plot them with matplotlib.\nLetâ€™s try.\n\n# Get flattened image patch embeddings in right shape \nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2â€¢C, N] -&gt; [batch_size, N, P^2â€¢C]\nprint(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\")\n\nPatch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]\n\n\nYes!!!\nWeâ€™ve now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers.\nHow about we visualize one of the flattened feature maps?\n\n# Get a single flattened feature map\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0]\n\n# Plot the flattened feature map visually\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n\n\n\n\n\n\n\n\nHmm, the flattened feature map doesnâ€™t look like much visually, but thatâ€™s not what weâ€™re concerned about, this is what will be the output of the patching embedding layer and the input to the rest of the ViT architecture.\nTK image - single image -&gt; conv2d -&gt; flatten -&gt; get the output above (show the workflow and transformation, this could be the gif weâ€™ve but using but extended to work with the flatten section)\n\nì°¸ê³ : The original Transformer architecture was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why the input to the ViT architecture is processed in the way it is. Weâ€™re essentially taking a 2D image and formatting it so it appears as a 1D sequence of text.\n\nHow about we view the flattened feature map in tensor form?\n\n# See the flattened feature map as a tensor\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n\n(tensor([[0.2642, 1.1367, 1.0221, 0.9712, 1.0950, 1.2478, 1.2884, 1.1481, 0.9640,\n          0.6204, 0.5996, 0.5855, 0.5560, 0.4992, 1.1578, 1.0633, 0.9593, 1.1931,\n          1.3194, 1.1306, 0.7317, 0.4322, 0.6025, 0.7246, 0.7891, 0.5383, 0.4786,\n          0.7098, 1.0163, 0.9368, 1.1675, 1.3373, 1.0429, 0.7497, 0.7445, 0.4270,\n          0.7430, 0.8750, 0.6105, 0.5147, 1.0207, 0.7852, 1.0669, 1.0157, 1.3291,\n          1.0117, 0.4848, 0.6802, 0.8365, 0.7736, 0.8618, 0.9144, 0.8926, 0.9795,\n          0.7475, 0.7585, 0.9371, 1.1937, 1.0068, 0.6377, 0.7283, 0.9625, 1.0372,\n          0.8920, 0.9372, 0.9034, 0.9683, 0.9405, 0.5958, 0.8740, 0.9419, 0.8599,\n          0.5429, 0.6954, 1.0202, 0.9093, 1.0003, 0.7619, 0.8472, 0.8062, 0.6418,\n          0.7741, 0.5791, 0.9816, 0.7965, 0.7202, 0.6424, 0.9137, 0.8264, 1.0243,\n          1.0920, 0.9548, 0.9166, 0.7937, 0.4675, 0.5346, 0.7774, 1.1001, 0.3298,\n          0.4832, 0.5324, 0.7486, 0.8303, 0.8101, 0.9969, 0.9931, 1.0058, 0.6002,\n          0.6643, 0.7254, 0.8453, 1.1323, 0.5384, 0.4798, 0.6725, 0.8014, 0.7044,\n          0.7988, 0.8185, 0.8911, 0.9720, 0.8939, 0.6234, 0.5674, 0.5775, 1.0011,\n          0.6199, 0.6465, 0.6503, 0.6215, 0.8154, 0.7950, 0.8647, 0.9872, 0.8513,\n          0.8833, 0.5799, 0.5914, 0.6936, 1.0554, 0.5140, 0.6462, 0.6982, 0.7445,\n          0.7394, 0.8124, 0.7462, 0.9183, 0.7471, 0.9436, 0.7147, 0.6396, 0.5795,\n          1.0201, 0.5467, 0.7408, 0.6854, 0.6624, 0.7465, 0.5077, 0.7633, 0.8709,\n          1.0026, 0.7276, 0.7847, 0.5811, 0.5521, 1.0318, 0.8041, 0.8868, 0.5559,\n          0.5889, 0.7236, 0.6976, 0.7940, 0.9365, 0.9110, 0.8182, 0.7013, 0.4890,\n          0.8364, 1.0031, 0.1976, 1.0262, 1.1979, 0.9982, 0.9644, 0.8868, 0.9556,\n          1.0204, 1.0060, 0.9586, 0.9351, 0.8819, 0.9290, 0.9289]],\n        grad_fn=&lt;SelectBackward0&gt;),\n True,\n torch.Size([1, 196]))\n\n\nBeautiful!\nWeâ€™ve turned our single 2D image into a single 1D learnable embedding vector (or â€œLinear Projection of Flattned Patchesâ€ in Figure 1 of the ViT paper).\n\n\n10.11.5 TK - 4.5 Turning the ViT patch embedding layer into a PyTorch module\nTime to put everything weâ€™ve done for creating the patch embedding into a single PyTorch layer.\nWe can do so by subclassing nn.Module and creating a small PyTorch â€œmodelâ€ to do all of the steps above.\nSpecifically weâ€™ll: 1. Create a class called PatchEmbedding which subclasses nn.Module (so it can be used a PyTorch layer). 2. Initialize the class with the parameters in_channels=3, patch_size=16 (for ViT-Base) and embedding_dim=768 (this is \\(D\\) for ViT-Base from Table 1). 3. Create a layer to turn an image into patches using nn.Conv2d() (just like in 4.3 above). 4. Create a layer to flatten the patch feature maps into a single dimension (just like in 4.4 above). 5. Define a forward() method to take an input and pass it through the layers created in 3 and 4. 6. Make sure the output shape reflects the required output shape of the ViT architecture (\\({N \\times\\left(P^{2} \\cdot C\\right)}\\)).\nLetâ€™s do it!\n\n# 1. Create a class which subclasses nn.Module\nclass PatchEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n    \n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\" \n    # 2. Initialize the class with appropriate variables\n    def __init__(self, \n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n        \n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method \n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n        \n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched) \n        # 6. Make sure the output shape has the right order \n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2â€¢C, N] -&gt; [batch_size, N, P^2â€¢C]\n\nPatchEmbedding layer created!\nLetâ€™s try it out on a single image.\n\nset_seeds()\n\n# Create an instance of patch embedding layer\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# Pass a single image through\nprint(f\"Input image shape: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\nprint(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n\nInput image shape: torch.Size([1, 3, 224, 224])\nOutput patch embedding shape: torch.Size([1, 196, 768])\n\n\nBeautiful!\nThe output shape matches the ideal input and output shapes weâ€™d like to see from the patch embedding layer:\n\nInput: The image starts as 2D with size \\({H \\times W \\times C}\\).\nOutput: The image gets converted to a sequence of flattened 2D patches with size \\({N \\times\\left(P^{2} \\cdot C\\right)}\\).\n\nWhere: * \\((H, W)\\) is the resolution of the original image. * \\(C\\) is the number of channels. * \\((P, P)\\) is the resolution of each image patch (patch size). * \\(N=H W / P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.\nWeâ€™ve now replicated the patch embedding for equation 1 but not the class token/position embedding.\nWeâ€™ll get to these later on.\n\nOur PatchEmbedding class (right) replicates the patch embedding of the ViT architecture from Figure 1 and Equation 1 from the ViT paper (left). However, the learnable class embedding and position embeddings havenâ€™t been created yet. These will come soon.\nLetâ€™s now get a summary of our PatchEmbedding layer.\n\n# Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# Get a summary of the input and outputs of PatchEmbedding\nsummary(PatchEmbedding(), \n        input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])\n\n========================================================================================================================\nLayer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n========================================================================================================================\nPatchEmbedding (PatchEmbedding)          [1, 3, 224, 224]     [1, 196, 768]        --                   True\nâ”œâ”€Conv2d (patcher)                       [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\nâ”œâ”€Flatten (flatten)                      [1, 768, 14, 14]     [1, 768, 196]        --                   --\n========================================================================================================================\nTotal params: 590,592\nTrainable params: 590,592\nNon-trainable params: 0\nTotal mult-adds (M): 115.76\n========================================================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 1.20\nParams size (MB): 2.36\nEstimated Total Size (MB): 4.17\n========================================================================================================================\n\n\n\n\n10.11.6 TK 4.6 Creating the class token embedding\nOkay weâ€™ve made the image patch embedding, time to get to work on the class token embedding.\nOr \\(\\mathbf{x}_\\text {class }\\) from equation 1.\n\nLeft: Figure 1 from the ViT paper with the â€œclassification tokenâ€ or [class] embedding token weâ€™re going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.\nReading the second paragraph of section 3.1 from the ViT paper, we see the following description:\n\nSimilar to BERTâ€™s [ class ] token, we prepend a learnable embedding to the sequence of embedded patches \\(\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)\\), whose state at the output of the Transformer encoder \\(\\left(\\mathbf{z}_{L}^{0}\\right)\\) serves as the image representation \\(\\mathbf{y}\\) (Eq. 4).\n\n\nì°¸ê³ : BERT (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a [ class ] token at the start of a sequence originated, class being a description for the â€œclassificationâ€ class the sequence belonged to.\n\nSo we need to â€œpreprend a learnable embedding to the sequence of embedded patchesâ€.\nLetâ€™s start by viewing our sequence of embedded patches tensor (created in 4.5) and its shape.\n\n# View the patch embedding and patch embedding shape\nprint(patch_embedded_image) \nprint(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n\ntensor([[[-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],\n         [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],\n         [-0.6015,  0.1235, -0.2506,  ...,  0.6307, -0.4673,  0.2756],\n         ...,\n         [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],\n         [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],\n         [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],\n       grad_fn=&lt;PermuteBackward0&gt;)\nPatch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n\n\nTo â€œprepend a learnable embedding to the sequence of embedded patchesâ€ we need to create a learnable embedding in the shape of the embedding_dimension (\\(D\\)) and then add it to the number_of_patches dimension.\nOr in pseudocode:\npatch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = learnable_embedding\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\nNotice the concatenation (torch.cat()) happens on dim=1 (the number_of_patches dimension).\nLetâ€™s create a learnable embedding for the class token.\nTo do so, weâ€™ll get the batch size and embedding dimension shape and then weâ€™ll create a torch.ones() tensor in the shape [batch_size, 1, embedding_dimension].\nAnd weâ€™ll make the tensor learnable by passing it to nn.Parameter() with requires_grad=True.\n\n# Get the batch size and embedding dimension\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_patches, embedding_dimension]\n                           requires_grad=True) # make sure the embedding is learnable\n\n# Show the first 10 examples of the class_token\nprint(class_token[:, :, :10])\n\n# Print the class_token shape\nprint(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\")\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nClass token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]\n\n\n\nì°¸ê³ : Here weâ€™re only creating the class token embedding as torch.ones() for demonstration purposes, in reality, youâ€™d likely create the class token embedding with torch.randn() (start with a random number).\n\nSee how the number_of_patches dimension of class_token is 1 since we only want to prepend one class token value to the start of the patch embedding sequence.\nNow weâ€™ve got the class token embedding, letâ€™s prepend it to our sequence of image patches, patch_embedded_image.\nWe can so using torch.cat() and set dim=1 (so class_tokenâ€™s number_of_patches dimension is preprended to patch_embedded_imageâ€™s number_of_patches dimension).\n\n# Add the class token embedding to the front of the patch embedding\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), \n                                                      dim=1) # concat on first dimension\n\n# Print the sequence of patch embeddings with the prepended class token embedding\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n\ntensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n         [-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],\n         [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],\n         ...,\n         [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],\n         [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],\n         [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],\n       grad_fn=&lt;CatBackward0&gt;)\nSequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n\n\nNice! Learnable class token prepended!\n\nReviewing what weâ€™ve done to create the learnable class token, we start with a sequence of image patch embeddings created by PatchEmbedding() on single image, we then created a learnable class token with one value for each of the embedding dimensions and then prepended it to the original sequence of patch embeddings. ì°¸ê³ : Using torch.ones() to create the learnable class token is mostly for demonstration purposes only, in practice, youâ€™d like create it with torch.randn().\n\n\n10.11.7 TK 4.7 Creating the position embedding\nWell, weâ€™ve got the class token embedding and the patch embedding, now how might we create the position embedding?\nOr \\(\\mathbf{E}_{\\text {pos }}\\) from equation 1 where \\(E\\) stands for â€œembeddingâ€.\n\nLeft: Figure 1 from the ViT paper with the position embedding weâ€™re going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.\nLetâ€™s find out more by reading section 3.1 of the ViT paper (bold mine):\n\nPosition embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n\nTo start creating the position embeddings, letâ€™s view our current embeddings.\n\n# View the sequence of patch embeddings with the prepended class embedding\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n\n(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n          [-0.4923,  0.0265, -0.0909,  ...,  0.1478, -0.0986,  0.2243],\n          [-0.9849,  0.3805, -0.3638,  ...,  0.6115, -0.0805,  0.2097],\n          ...,\n          [-0.6668,  0.1713, -0.1711,  ...,  0.4699, -0.2881,  0.2599],\n          [-0.6983,  0.1949, -0.1884,  ...,  0.5152, -0.3126,  0.2151],\n          [-0.6889,  0.1862, -0.1444,  ...,  0.5019, -0.3564,  0.2378]]],\n        grad_fn=&lt;CatBackward0&gt;),\n torch.Size([1, 197, 768]))\n\n\nEquation 1 states that the position embeddings should have the shape \\((N + 1) \\times D\\) where: * \\(N=H W / P^{2}\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. * \\(D\\) is the size of the patch embeddings, different values for \\(D\\) can be found in Table 1.\nLuckily weâ€™ve got both of these values already.\nSo letâ€™s make a learnable 1D embedding with torch.ones() to create \\(\\mathbf{E}_{\\text {pos }}\\).\n\n# Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# Get embedding dimension\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# Create the learnable 1D position embedding\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1, \n                                             embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\nprint(position_embedding[:, :10, :10])\nprint(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nPosition embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n\n\n\nì°¸ê³ : Only creating the position embedding as torch.ones() for demonstration purposes, in reality, youâ€™d likely create the position embedding with torch.randn() (start with a random number and improve via gradient descent).\n\nPosition embeddings created!\nLetâ€™s add them to our sequence of patch embeddings with a prepended class token.\n\n# Add the position embedding to the patch and class token embedding\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n\ntensor([[[2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0000, 2.0000],\n         [0.5077, 1.0265, 0.9091,  ..., 1.1478, 0.9014, 1.2243],\n         [0.0151, 1.3805, 0.6362,  ..., 1.6115, 0.9195, 1.2097],\n         ...,\n         [0.3332, 1.1713, 0.8289,  ..., 1.4699, 0.7119, 1.2599],\n         [0.3017, 1.1949, 0.8116,  ..., 1.5152, 0.6874, 1.2151],\n         [0.3111, 1.1862, 0.8556,  ..., 1.5019, 0.6436, 1.2378]]],\n       grad_fn=&lt;AddBackward0&gt;)\nPatch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n\n\nNotice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with torch.ones()).\n\nì°¸ê³ : We could put both the class token embedding and position embedding into their own layer if we wanted to. But weâ€™ll see later on how they can be incorporated into the overall ViT architectureâ€™s forward() method.\n\n\nThe workflow weâ€™ve used for adding the position embeddings to the sequence of patch embeddings and class token. ì°¸ê³ : torch.ones() only used to create embeddings for illustration purposes, in practice, youâ€™d likely use torch.randn() to start with a random number.\n\n\n10.11.8 TK 4.8 ì „ì²´ ê³¼ì • í•©ì¹˜ê¸°: from image to embedding\nAlright, weâ€™ve come a long way in terms of turning our input images into an embedding and replicating equation 1 from section 3.1 of the ViT paper:\n\\[\n\\begin{aligned}\n\\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}\n\\end{aligned}\n\\]\nLetâ€™s now put everything together in a single code cell and go from input image (\\(x\\)) to output embedding \\({z}_0\\).\nWe can do so by: 1. Setting the patch size (weâ€™ll use 16 as itâ€™s widely used throughout the paper and for ViT-Base). 2. Getting a single image, printing itâ€™s shape and storing its height and width. 3. Adding a batch dimension to the single image so itâ€™s compatible with our PatchEmbedding layer. 4. Creating a PatchEmbedding layer with a patch_size=16 and embedding_dim=768 (from Table 1 for ViT-Base). 5. Passing the single image through the PatchEmbedding layer in 4 to create a sequence of patch embeddings. 6. Creating a class token embedding like in section 4.6. 7. Prepending the class token emebdding to the patch embeddings created in step 5. 8. Creating a position embedding like in section 4.7. 9. Adding the position embedding to the class token and patch embeddings created in step 7.\nWeâ€™ll also make sure to set the random seeds with set_seeds() and print out the shapes of different tensors along the way.\n\nset_seeds()\n\n# 1. Set patch size\npatch_size = 16\n\n# 2. Print shape of original image tensor and get the image dimensions\nprint(f\"Image tensor shape: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. Get image tensor and add batch dimension\nx = image.unsqueeze(0)\nprint(f\"Input image with batch dimension shape: {x.shape}\")\n\n# 4. Create patch embedding layer\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. Pass image through patch embedding layer\npatch_embedding = patch_embedding_layer(x)\nprint(f\"Patching embedding shape: {patch_embedding.shape}\")\n\n# 6. Create class token embedding\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # make sure it's learnable\nprint(f\"Class token embedding shape: {class_token.shape}\")\n\n# 7. Prepend class token embedding to patch embedding\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n\n# 8. Create position embedding\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# 9. Add position embedding to patch embedding with class token\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n\nImage tensor shape: torch.Size([3, 224, 224])\nInput image with batch dimension shape: torch.Size([1, 3, 224, 224])\nPatching embedding shape: torch.Size([1, 196, 768])\nClass token embedding shape: torch.Size([1, 1, 768])\nPatch embedding with class token shape: torch.Size([1, 197, 768])\nPatch and position embedding shape: torch.Size([1, 197, 768])\n\n\nWoohoo!\nFrom a single image to patch and position embeddings in a single cell of code.\n\nMapping equation 1 from the ViT paper to our PyTorch code. This is the essence of paper replicating, taking a research paper and turning it into usable code.\nNow weâ€™ve got a way to encode our images and pass them to the Transformer Encoder in Figure 1 of the ViT paper.\n\nAnimating the entire ViT workflow: from patch embeddings to transformer encoder to MLP head.\nFrom a code perspective, creating the patch embedding is probably the largest section of replicating the ViT paper.\nMany of the other parts of the ViT paper such as the Multi-Head Attention and Norm layers can be created using existing PyTorch layers.\nOnwards!",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk.-5.-equation-2-multi-head-attention-msa",
    "href": "08_pytorch_paper_replicating.html#tk.-5.-equation-2-multi-head-attention-msa",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.12 TK. 5. Equation 2: Multi-Head Attention (MSA)",
    "text": "10.12 TK. 5. Equation 2: Multi-Head Attention (MSA)\nWeâ€™ve got our input data patchified and embedded, now letâ€™s move onto the next part of the ViT architecture.\nTo start, weâ€™ll break down the Transformer Encoder section into two parts (start small and increase when necessary).\nThe first being equation 2 and the second being equation 3.\nRecall equation 2 states:\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n\\end{aligned}\n\\]\nThis indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output).\n\nLeft: Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. Right: Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.\nMany layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch.\nIn saying this, to replicate these layers and residual connection with PyTorch code we can use: * Multi-Head Self Attention (MSA) - torch.nn.MultiheadAttention(). * Norm (LN or LayerNorm) - torch.nn.LayerNorm(). * Residual connection - add the input to output (weâ€™ll see this later on when we create the full Transformer Encoder block).\n\n10.12.1 5.1 The LayerNorm (LN) layer\nLayer Normalization (torch.nn.LayerNorm() or Norm or LayerNorm or LN) normalizes an input over the last dimension.\nYou can set normalized_shape to be equal to the dimension size youâ€™d like to noramlize over (in our case itâ€™ll be \\(D\\) or 768 for ViT-Base).\nYou can find the formal definition of torch.nn.LayerNorm() in the PyTorch documentation.\nWhat does it do?\nLayer Normalization helps improve training time and model generalization (ability to adapt to unseen data).\nI like to think of any kind of normalization as â€œgetting the data into a similar formatâ€ or â€œgetting data samples into a similar distributionâ€.\nImagine trying to walk up (or down) a set of stairs all with differing heights and lengths.\nItâ€™d take some adjustment each step right?\nAnd what you learn for each step wouldnâ€™t necessary help with the next one since they all differ.\nNormalization (including Layer Normalization) is the equivalent of making all the stairs the same height and length except the stairs are your data samples.\nSo just like you can walk up (or down) stairs with similar heights and lengths much easier than those with unequal heights and widths, neural networks can optimize over data samples with similar distributions (similar mean and standard-deviations) easier than those with varying distributions.\n\n\n10.12.2 5.2 The Multi-Head Self Attention (MSA) layer\nThe power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the Attention is all you need research paper.\nThere are many resources online to learn more about the Transformer architeture and attention mechanism online such as Jay Alammarâ€™s wonderful Illustrated Transformer post and Illustrated Attention post.\nBut weâ€™re going to focus more on coding an existing PyTorch MSA implementation than creating our own.\nHowever, you can find the formal defintion of the ViT paperâ€™s MSA implementation is defined in Appendix A:\n\nLeft: Vision Transformer architecture overview from Figure 1 of the ViT paper. Right: Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.\nThe image above highlights the triple input to the MSA layer.\nThis is known as query, key, value input or qkv for short which is fundamental to the self-attention mechanism.\nIn our case, the triple input will be three versions of the output of the Norm layer.\nOr three versions of our layer-normalized image patch and position embeddings created in section 4.8.\nWe can implement the MSA layer in PyTorch with torch.nn.MultiheadAttention() with the parameters: * embed_dim - the embedding dimension from Table 1 (Hidden size \\(D\\)). * num_heads - how many attention heads to use (this is where the term â€œmultiheadâ€ comes from), this value is also in Table 1 (Heads). * dropout - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isnâ€™t used after the qkv-projections).\n\n\n10.12.3 5.3 Replicating Equation 2 with PyTorch layers\nLetâ€™s put everything weâ€™ve discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.\nTo do so, weâ€™ll:\n\nCreate a class called MultiheadSelfAttentionBlock() that inherits from torch.nn.Module.\nInitialize the class with hyperparameters from Table 1 of the ViT paper for the ViT-Base model.\nCreate a layer normalization (LN) layer with torch.nn.LayerNorm() with the normalized_shape parameter the same as our embedding dimension (\\(D\\) from Table 1).\nCreate a multi-head attention (MSA) layer with the appropriate embed_dim, num_heads, dropout and batch_first parameters.\nCreate a forward() method for our class passing the in the inputs through the LN layer and MSA layer.\n\n\n# 1. Create a class that inherits from nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # from Table 1 for ViT-Base\n                 num_heads:int=12, # from Table 1 for ViT-Base\n                 attn_dropout:int=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n        \n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n        \n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=False) # do we need the weights or just the layer outputs?\n        return attn_output\n\n\nì°¸ê³ : Unlike Figure 1, our MultiheadSelfAttentionBlock() doesnâ€™t include a skip or residual connection (â€œ\\(+\\mathbf{z}_{\\ell-1}\\)â€ in equation 2), weâ€™ll include this when we create the entire Transformer encoder later on.\n\nMSABlock created!\nLetâ€™s try it out by create an instance of our MultiheadSelfAttentionBlock and passing through the patch_and_position_embedding variable we created in section 4.8.\n\n# Create an instance of MSABlock\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1 \n                                                             num_heads=12) # from Table 1\n\n# Pass patch and position image embedding through MSABlock\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\nprint(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")\n\nInput shape of MSA block: torch.Size([1, 197, 768])\nOutput shape MSA block: torch.Size([1, 197, 768])\n\n\nNotice how the input and output shape of our data stays the same when it goes through the MSA block.\nThis doesnâ€™t mean the data doesnâ€™t change as it goes through.\nYou could try printing the input and output tensor to see how it changes (though this change will be across 1 * 197 * 768 values).\n\nLeft: Vision Transformer architecture from Figure 1 with Multi-Head Attention and LayerNorm layers highlighted, these layers make up equation 2 from section 3.1 of the paper. Right: Replicating equation 2 (without the skip connection on the end) using PyTorch layers.\nWeâ€™ve now officially replicated equation 2 (except for the residual connection on the end but weâ€™ll get to this in section 7)!\nOnto the next!",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-6.-equation-3-multilayer-perceptron-mlp",
    "href": "08_pytorch_paper_replicating.html#tk-6.-equation-3-multilayer-perceptron-mlp",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.13 TK 6. Equation 3: Multilayer Perceptron (MLP)",
    "text": "10.13 TK 6. Equation 3: Multilayer Perceptron (MLP)\nUPTOHERE: * Replicate equation 3 like replicating equation 2\n\nTK also called â€œfeedforwardâ€\n\n\nDropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.\n\n\nThe MLP contains two layers with a GELU non-linearity\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L\n\\end{aligned}\n\\]\n\nTK - GELU in PyTorch â€“ https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\n\n\n# Could also call this \"FeedForward\"\nclass MLPBlock(nn.Module):\n    \"\"\"Creates an MLPBlock of the Vision Transformer architecture.\"\"\"\n    def __init__(self,\n                 embedding_dim, # embedding dimension (Hidden Size D in Table 1)\n                 mlp_size, # MLP size in Table 1\n                 dropout=0): # \"Dropout... is applied to every dense layer... (Appendix B.1)\"\n        super().__init__()\n        \n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout)\n        )\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n\n\nmlp_block = MLPBlock(embedding_dim=768, # Table 1 \n                     mlp_size=3072) # Table 1\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\npatched_image_through_mlp_block.shape\n\ntorch.Size([1, 197, 768])",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-7.-create-the-transformer-encoder",
    "href": "08_pytorch_paper_replicating.html#tk-7.-create-the-transformer-encoder",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.14 TK 7. Create the Transformer Encoder",
    "text": "10.14 TK 7. Create the Transformer Encoder\n\nTk - what is an â€œencoderâ€?\nTk - â€œtransformer blockâ€ or â€œtransformer encoderâ€? - line this up with the paper\n\nSee here for pre-built transformer blocks/layers: https://pytorch.org/docs/stable/nn.html#transformer-layers\n\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"Creates a Transformer Encoder block.\"\"\"\n    def __init__(self,\n                 embedding_dim=768, # From Table 1\n                 num_heads=12, # From Table 1\n                 mlp_size=3072, # From Table 1\n                 mlp_dropout=0.1,\n                 attn_dropout=0):\n        super().__init__()\n\n        # Create MSA Block (for equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n        # Create MLP Block (for equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n        \n    def forward(self, x):\n        x = self.msa_block(x) + x # Create skip connection\n        x = self.mlp_block(x) + x # Create skip connection\n        return x",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-8.-ì „ì²´-ê³¼ì •-í•©ì¹˜ê¸°-to-create-vit",
    "href": "08_pytorch_paper_replicating.html#tk-8.-ì „ì²´-ê³¼ì •-í•©ì¹˜ê¸°-to-create-vit",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.15 TK 8. ì „ì²´ ê³¼ì • í•©ì¹˜ê¸° to create ViT",
    "text": "10.15 TK 8. ì „ì²´ ê³¼ì • í•©ì¹˜ê¸° to create ViT\nTK - replicate this with the TransformerEncoderLayer - https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/\nCombine the transformer blocks and patched embedding into a ViT architecture.\n\nclass ViT(nn.Module):\n    \"\"\"Creates a Vision Transformer architecture.\"\"\"\n    def __init__(self,\n                 img_size=224, # From Table 3 in ViT paper\n                 in_channels=3,\n                 patch_size=16,\n                 num_transformer_layers=12, # From Table 1 in ViT paper\n                 embedding_dim=768,\n                 mlp_size=3072,\n                 num_heads=12,\n                 attn_dropout=0,\n                 mlp_dropout=0.1,\n                 embedding_dropout=0.1,\n                 num_classes=1000): # default for ImageNet\n        super().__init__() # don't forget the super().__init__()!\n    \n        # Get image size\n        self.img_height, self.img_width = img_size, img_size\n        \n        # Calculate number of patches (height * width/patch^2)\n        self.num_patches = (self.img_height * self.img_width) // patch_size**2\n        \n                 \n        # Create class embedding (needs to go at front of sequence embedding)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n        \n         # Create position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n                \n        # Create embedding dropout\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n        \n        # Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n        \n        # Create transformer encoder blocks\n        self.transformer_enedoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n       \n        # Create classifier head (equation 4)\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim, \n                      out_features=num_classes)\n        )\n    \n    def forward(self, x):\n        # Get batch size\n        batch_size = x.shape[0]\n        # Create class token embedding\n        class_token = self.class_embedding.expand(batch_size, -1, -1)\n\n        # Create patch embedding\n        x = self.patch_embedding(x)\n\n        # Concat class embedding and patch embedding (equation 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # Add position embedding to patch embedding (equation 1) for every batch\n        x = self.position_embedding + x\n\n        # Run embedding dropout\n        x = self.embedding_dropout(x)\n\n        # Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n        x = self.transformer_enedoder(x)\n\n        # Put 0 index logit through classifier (equation 4)\n        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n\n        return x\n        \n\n\nbatch_size = 32\nclass_tokens = nn.Parameter(data=torch.randn(1, 1, 768))\nclass_tokens.expand(batch_size, -1, -1).shape\n\ntorch.Size([32, 1, 768])\n\n\n\nset_seeds()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nrand_image = torch.randn(1, 3, 224, 224)\n# vit = ViT(num_classes=len(class_names)) \nvit = ViT(num_classes=3)\nvit(rand_image)\n\ntensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-9.-inspect-the-model",
    "href": "08_pytorch_paper_replicating.html#tk-9.-inspect-the-model",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.16 TK 9. Inspect the model",
    "text": "10.16 TK 9. Inspect the model\n\nì°¸ê³ : If you go too big, your hardware might not be able to handle itâ€¦ (e.g.Â too high of a batch sizeâ€¦)\n\nTK - Number of parameters should be equivalent to: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 (num_params=86,567,656)\n\nfrom torchinfo import summary\n\n# TK - clean up the summary so it looks nice when it prints out \n# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=vit, \n        input_size=(128, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n\n======================================================================================================================================================\nLayer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n======================================================================================================================================================\nViT (ViT)                                                              [128, 3, 224, 224]   [128, 3]             152,064              True\nâ”œâ”€Dropout (embedding_dropout)                                          [128, 197, 768]      [128, 197, 768]      --                   --\nâ”œâ”€PatchEmbedding (patch_embedding)                                     [128, 3, 224, 224]   [128, 196, 768]      --                   True\nâ”‚    â””â”€Conv2d (patcher)                                                [128, 3, 224, 224]   [128, 768, 14, 14]   590,592              True\nâ”‚    â””â”€Flatten (flatten)                                               [128, 768, 14, 14]   [128, 768, 196]      --                   --\nâ”œâ”€Dropout (embedding_dropout)                                          [128, 197, 768]      [128, 197, 768]      --                   --\nâ”œâ”€Sequential (transformer_enedoder)                                    [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â””â”€TransformerEncoderBlock (0)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (1)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (2)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (3)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (4)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (5)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (6)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (7)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (8)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (9)                                     [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (10)                                    [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”‚    â””â”€TransformerEncoderBlock (11)                                    [128, 197, 768]      [128, 197, 768]      --                   True\nâ”‚    â”‚    â””â”€MultiheadSelfAttentionBlock (msa_block)                    [128, 197, 768]      [128, 197, 768]      2,363,904            True\nâ”‚    â”‚    â””â”€MLPBlock (mlp_block)                                       [128, 197, 768]      [128, 197, 768]      4,723,968            True\nâ”œâ”€Sequential (classifier)                                              [128, 768]           [128, 3]             --                   True\nâ”‚    â””â”€LayerNorm (0)                                                   [128, 768]           [128, 768]           1,536                True\nâ”‚    â””â”€Linear (1)                                                      [128, 768]           [128, 3]             2,307                True\n======================================================================================================================================================\nTotal params: 85,800,963\nTrainable params: 85,800,963\nNon-trainable params: 0\nTotal mult-adds (G): 22.08\n======================================================================================================================================================\nInput size (MB): 77.07\nForward/backward pass size (MB): 13168.81\nParams size (MB): 257.55\nEstimated Total Size (MB): 13503.43\n======================================================================================================================================================\n\n\n\nTK - same number of parameters as: https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16 -&gt; 86567656\n\n\nbatch_size = 32\ncls_embedding = nn.Parameter(torch.randn(1, 1, 768))\n# See here: https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html\ncls_embedding.shape, cls_embedding.expand(batch_size, -1, -1).shape\n\n(torch.Size([1, 1, 768]), torch.Size([32, 1, 768]))",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-10.-train-model",
    "href": "08_pytorch_paper_replicating.html#tk-10.-train-model",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.17 TK 10. Train model",
    "text": "10.17 TK 10. Train model\n\nfrom going_modular.going_modular import engine\n\noptimizer = torch.optim.Adam(params=vit.parameters(), \n                             lr=1e-3,\n                             betas=(0.9, 0.999), # default\n                             weight_decay=0.1) # from the ViT paper section 4.1\nloss_fn = torch.nn.CrossEntropyLoss()\n\nset_seeds()\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)\n\n\n\n\nEpoch: 1 | train_loss: 4.8759 | train_acc: 0.2891 | test_loss: 1.0465 | test_acc: 0.5417\nEpoch: 2 | train_loss: 1.5900 | train_acc: 0.2617 | test_loss: 1.5876 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.4644 | train_acc: 0.2617 | test_loss: 1.2738 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.3159 | train_acc: 0.2773 | test_loss: 1.7498 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.3114 | train_acc: 0.3008 | test_loss: 1.7444 | test_acc: 0.2604\nEpoch: 6 | train_loss: 1.2445 | train_acc: 0.3008 | test_loss: 1.9704 | test_acc: 0.1979\nEpoch: 7 | train_loss: 1.2050 | train_acc: 0.3984 | test_loss: 3.5480 | test_acc: 0.1979\nEpoch: 8 | train_loss: 1.4368 | train_acc: 0.4258 | test_loss: 1.8324 | test_acc: 0.2604\nEpoch: 9 | train_loss: 1.5757 | train_acc: 0.2344 | test_loss: 1.2848 | test_acc: 0.5417\nEpoch: 10 | train_loss: 1.4658 | train_acc: 0.4023 | test_loss: 1.2389 | test_acc: 0.2604",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-11.-evaluate-model",
    "href": "08_pytorch_paper_replicating.html#tk-11.-evaluate-model",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.18 TK 11. Evaluate model",
    "text": "10.18 TK 11. Evaluate model\nTK - plot the loss curves\n\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(results)\n\n\n\n\n\n\n\n\nTK - why do the loss curves look the way they do? (too big of a model, not enough data)",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset",
    "href": "08_pytorch_paper_replicating.html#tk-12.-bring-in-pretrained-vit-from-torchvision.models-on-same-dataset",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.19 TK 12. Bring in pretrained ViT from torchvision.models on same dataset",
    "text": "10.19 TK 12. Bring in pretrained ViT from torchvision.models on same dataset\n\nGet a similar model from here - https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16\n\n\n# The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__) \nprint(torchvision.__version__)\n\n1.12.0+cu102\n0.13.0+cu102\n\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\n# Set seeds\ndef set_seeds(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\n# Requires torchvision &gt;= 0.13\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# Freeze the base parameters\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n    \n# Change the classifier head\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n\n\n# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=pretrained_vit, \n        input_size=(128, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n\n======================================================================================================================================================\nLayer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n======================================================================================================================================================\nVisionTransformer (VisionTransformer)                                  [128, 3, 224, 224]   [128, 3]             768                  Partial\nâ”œâ”€Conv2d (conv_proj)                                                   [128, 3, 224, 224]   [128, 768, 14, 14]   (590,592)            False\nâ”œâ”€Encoder (encoder)                                                    [128, 197, 768]      [128, 197, 768]      151,296              False\nâ”‚    â””â”€Dropout (dropout)                                               [128, 197, 768]      [128, 197, 768]      --                   --\nâ”‚    â””â”€Sequential (layers)                                             [128, 197, 768]      [128, 197, 768]      --                   False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_0)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_1)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_2)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_3)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_4)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_5)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_6)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_7)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_8)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_9)                             [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_10)                            [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â”‚    â””â”€EncoderBlock (encoder_layer_11)                            [128, 197, 768]      [128, 197, 768]      (7,087,872)          False\nâ”‚    â””â”€LayerNorm (ln)                                                  [128, 197, 768]      [128, 197, 768]      (1,536)              False\nâ”œâ”€Linear (heads)                                                       [128, 768]           [128, 3]             2,307                True\n======================================================================================================================================================\nTotal params: 85,800,963\nTrainable params: 2,307\nNon-trainable params: 85,798,656\nTotal mult-adds (G): 22.08\n======================================================================================================================================================\nInput size (MB): 77.07\nForward/backward pass size (MB): 13322.95\nParams size (MB): 257.55\nEstimated Total Size (MB): 13657.57\n======================================================================================================================================================\n\n\n\n# TK - the above output has the same number of parameters as our own created model\n\n\n# Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n\n[INFO] data/pizza_steak_sushi directory exists, skipping download.\n\n\nPosixPath('data/pizza_steak_sushi')\n\n\n\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\" \ntrain_dir, test_dir\n\n(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))\n\n\n\n# Create dataset for pretrained ViT\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=1024) # From here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n\nImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n\n\n\n# Train pretrained feature extractor ViT for 5 epochs on Pizza, Steak, Sushi\n# TK - can probably increase the batch_size here because we're using feature extraction and not \n# training the whole model\nfrom going_modular.going_modular import engine\n\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n\n\n\n\nEpoch: 1 | train_loss: 1.1490 | train_acc: 0.2356 | test_loss: 1.0584 | test_acc: 0.4667\nEpoch: 2 | train_loss: 1.0017 | train_acc: 0.5289 | test_loss: 0.9194 | test_acc: 0.6400\nEpoch: 3 | train_loss: 0.8716 | train_acc: 0.7244 | test_loss: 0.7983 | test_acc: 0.6667\nEpoch: 4 | train_loss: 0.7583 | train_acc: 0.8089 | test_loss: 0.6942 | test_acc: 0.7733\nEpoch: 5 | train_loss: 0.6608 | train_acc: 0.8622 | test_loss: 0.6060 | test_acc: 0.8800\nEpoch: 6 | train_loss: 0.5777 | train_acc: 0.8889 | test_loss: 0.5318 | test_acc: 0.8933\nEpoch: 7 | train_loss: 0.5076 | train_acc: 0.9156 | test_loss: 0.4700 | test_acc: 0.9067\nEpoch: 8 | train_loss: 0.4487 | train_acc: 0.9244 | test_loss: 0.4188 | test_acc: 0.9333\nEpoch: 9 | train_loss: 0.3993 | train_acc: 0.9378 | test_loss: 0.3765 | test_acc: 0.9333\nEpoch: 10 | train_loss: 0.3580 | train_acc: 0.9378 | test_loss: 0.3417 | test_acc: 0.9467\n\n\n\n# Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results) \n\n\n\n\n\n\n\n\n\n# Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n\n[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\n\n\n\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n\nPretrained ViT feature extractor model size: 327 MB",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---things-this-replication-misses-out-on",
    "href": "08_pytorch_paper_replicating.html#tk---things-this-replication-misses-out-on",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.20 TK - Things this replication misses out on",
    "text": "10.20 TK - Things this replication misses out on\nTK Put down the difference in the paper vs this replication * Many of these things are in Table 3: * training data (ImageNet from scratch vs FoodVision Mini data) * LR warmup * LR decay * Weight decay * Number of epochs",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---ì—°ìŠµ-ë¬¸ì œ",
    "href": "08_pytorch_paper_replicating.html#tk---ì—°ìŠµ-ë¬¸ì œ",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.21 TK - ì—°ìŠµ ë¬¸ì œ",
    "text": "10.21 TK - ì—°ìŠµ ë¬¸ì œ",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "08_pytorch_paper_replicating.html#tk---ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "08_pytorch_paper_replicating.html#tk---ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "10Â  08 - PyTorch ë…¼ë¬¸ ë³µì œ",
    "section": "10.22 TK - ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "10.22 TK - ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nlayernorm\nSee the illustrated transformer for an overview of the Transformer model: https://jalammar.github.io/illustrated-transformer/ + https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\nAttention is all you need paper - Yannic video\nVision transformer - yannic video",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>08 - PyTorch ë…¼ë¬¸ ë³µì œ</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html",
    "href": "09_pytorch_model_deployment.html",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "",
    "text": "11.1 What is machine learning model deployment?\nView Source Code | View Slides\nWelcome to Milestone Project 3: PyTorch Model Deployment!\nWeâ€™ve come a long way with our FoodVision Mini project.\nBut so far our PyTorch models have only been accessible to us.\nHow about we bring FoodVision Mini to life and make it publically accessible?\nIn other words, weâ€™re going to deploy our FoodVision Mini model to the internet as a usable app!\nTrying out the deployed version of FoodVision Mini (what weâ€™re going to build) on my lunch. The model got it right too ğŸ£!\nMachine learning model deployment is the process of making your machine learning model accessible to someone or something else.\nSomeone else being a person who can interact with your model in some way.\nFor example, someone taking a photo on their smartphone of food and then having our FoodVision Mini model classify it into pizza, steak or sushi.\nSomething else might be another program, app or even another model that interacts with your machine learning model(s).\nFor example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.\nOr an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.\nThese use cases can be mixed and matched as well.\nFor example, a Tesla carâ€™s computer vision system will interact with the carâ€™s route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).\nMachine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or Nutrify). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#why-deploy-a-machine-learning-model",
    "href": "09_pytorch_model_deployment.html#why-deploy-a-machine-learning-model",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.2 Why deploy a machine learning model?",
    "text": "11.2 Why deploy a machine learning model?\nOne of the most important philosophical questions in machine learning is:\n\n\n\nDeploying a model is as important as training one.\nBecause although you can get a pretty good idea of how your modelâ€™s going to function by evaluting it on a well crafted test set or visualizing its results, you never really know how itâ€™s going to perform until you release it to the wild.\nHaving people whoâ€™ve never used your model interact with it will often reveal edge cases you never thought of during training.\nFor example, what happens if someone was to upload a photo that wasnâ€™t of food to our FoodVision Mini model?\nOne solution would be to create another model that first classifies images as â€œfoodâ€ or â€œnot foodâ€ and passing the target image through that model first (this is what Nutrify does).\nThen if the image is of â€œfoodâ€ it goes to our FoodVision Mini model and gets classified into pizza, steak or sushi.\nAnd if itâ€™s â€œnot foodâ€, a message is displayed.\nBut what if these predictions were wrong?\nWhat happens then?\nYou can see how these questions could keep going.\nThus this highlights the importance of model deployment: it helps you figure out errors in your model that arenâ€™t obvious during training/testing.\n\nWe covered a PyTorch workflow back in 01. PyTorch Workflow. But once youâ€™ve got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world. For more resources on deployment and monitoring see PyTorch Extra Resources.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#different-types-of-machine-learning-model-deployment",
    "href": "09_pytorch_model_deployment.html#different-types-of-machine-learning-model-deployment",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.3 Different types of machine learning model deployment",
    "text": "11.3 Different types of machine learning model deployment\nWhole books could be written on the different types of machine learning model deployment (and many good ones are listed in PyTorch Extra Resources).\nAnd the field is still developing in terms of best practices.\nBut I like to start with the question:\n\nâ€œWhat is the most ideal scenario for my machine learning model to be used?â€\n\nAnd then work backwards from there.\nOf course, you may not know this ahead of time. But youâ€™re smart enough to imagine such things.\nIn the case of FoodVision Mini, our ideal scenario might be:\n\nSomeone takes a photo on a mobile device (through an app or web broswer).\nThe prediction comes back fast.\n\nEasy.\nSo weâ€™ve got two main criteria:\n\nThe model should work on a mobile device (this means there will be some compute constraints).\nThe model should make predictions fast (because a slow app is a boring app).\n\nAnd of course, depending on your use case, your requirements may vary.\nYou may notice the above two points break down into another two questions:\n\nWhereâ€™s it going to go? - As in, where is it going to be stored?\nHowâ€™s it going to function? - As in, does it return predictions immediately? Or do they come later?\n\n\nWhen starting to deploy machine learning models, itâ€™s helpful to start by asking whatâ€™s the most ideal use case and then work backwards from there, asking where the modelâ€™s going to go and then how itâ€™s going to function.\n\n11.3.1 Whereâ€™s it going to go?\nWhen you deploy your machine learning model, where does it live?\nThe main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isnâ€™t the actual device someone/something calls the model from).\nBoth have their pros and cons.\n\n\n\n\n\n\n\n\nDeployment location\nPros\nCons\n\n\n\n\nOn-device (edge/in the browser)\nCan be very fast (since no data leaves the device)\nLimited compute power (larger models take longer to run)\n\n\n\nPrivacy preserving (again no data has to leave the device)\nLimited storage space (smaller model size required)\n\n\n\nNo internet connection required (sometimes)\nDevice-specific skills often required\n\n\n\n\n\n\n\nOn cloud\nNear unlimited compute power (can scale up when needed)\nCosts can get out of hand (if proper scaling limits arenâ€™t enforced)\n\n\n\nCan deploy one model and use everywhere (via API)\nPredictions can be slower due to data having to leave device and predictions having to come back (network latency)\n\n\n\nLinks into existing cloud ecosystem\nData has to leave device (this may cause privacy concerns)\n\n\n\nThere are more details to these but Iâ€™ve left resources in the extra-curriculum to learn more.\nLetâ€™s give an example.\nIf weâ€™re deploying FoodVision Mini as an app, we want it to perform well and fast.\nSo which model would we prefer?\n\nA model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\nA model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per per prediction (bigger, better model but takes longer to compute).\n\nIâ€™ve made these numbers up but they showcase a potential difference between on-device and on the cloud.\nOption 1 could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.\nOption 2 could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).\nFor FoodVision Mini, weâ€™d likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.\n\nIn the case of a Tesla carâ€™s computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better thatâ€™s on the cloud? In this case, youâ€™d much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldnâ€™t be worth it (or potentially even impossible with poor signal areas).\n\nì°¸ê³ : For a full example of seeing what itâ€™s like to deploy a PyTorch model to an edge device, see the PyTorch tutorial on achieving real-time inference (30fps+) with a computer vision model on a Raspberry Pi.\n\n\n\n11.3.2 Howâ€™s it going to function?\nBack to the ideal use case, when you deploy your machine learning model, how should it work?\nAs in, would you like predictions returned immediately?\nOr is it okay for them to happen later?\nThese two scenarios are generally referred to as:\n\nOnline (real-time) - Predictions/inference happen immediately. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\nOffline (batch) - Predictions/inference happen periodically. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.\n\n\nì°¸ê³ : â€œBatchâ€ refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).\n\nThe main difference between each being: predictions being made immediately or periodically.\nPeriodically can have a varying timescale too, from every few seconds to every few hours or days.\nAnd you can mix and match the two.\nIn the case of FoodVision Mini, weâ€™d want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\nBut for our training pipeline, itâ€™s okay for it to happen in a batch (offline) fashion, which is what weâ€™ve been doing throughout the previous chapters.\n\n\n11.3.3 Ways to deploy a machine learning model\nWeâ€™ve discussed a couple of options for deploying machine learning models (on-device and cloud).\nAnd each of these will have their specific requirements:\n\n\n\nTool/resource\nDeployment type\n\n\n\n\nGoogleâ€™s ML Kit\nOn-device (Android and iOS)\n\n\nAppleâ€™s Core ML and coremltools Python package\nOn-device (all Apple devices)\n\n\nAmazon Web Serviceâ€™s (AWS) Sagemaker\nCloud\n\n\nGoogle Cloudâ€™s Vertex AI\nCloud\n\n\nMicrosoftâ€™s Azure Machine Learning\nCloud\n\n\nHugging Face Spaces\nCloud\n\n\nAPI with FastAPI\nCloud/self-hosted server\n\n\nAPI with TorchServe\nCloud/self-hosted server\n\n\nONNX (Open Neural Network Exchange)\nMany/general\n\n\nMany moreâ€¦\n\n\n\n\n\nì°¸ê³ : An application programming interface (API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.\n\nWhich option you choose will be highly dependent on what youâ€™re building/who youâ€™re working with.\nBut with so many options, it can be very intimidating.\nSo best to start small and keep it simple.\nAnd one of the best ways to do so is by turning your machine learning model into a demo app with Gradio and then deploying it on Hugging Face Spaces.\nWeâ€™ll be doing just that with FoodVision Mini later on.\n\nA handful of places and tools to host and deploy machine learning models. There are plenty Iâ€™ve missed so if youâ€™d like to add more, please leave a discussion on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "href": "09_pytorch_model_deployment.html#ì´ë²ˆ-ì¥ì—ì„œ-ë‹¤ë£°-ë‚´ìš©",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.4 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©",
    "text": "11.4 ì´ë²ˆ ì¥ì—ì„œ ë‹¤ë£° ë‚´ìš©\nEnough talking about deploying a machine learning model.\nLetâ€™s become machine learning engineers and actually deploy one.\nOur goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics: 1. Performance: 95%+ accuracy. 2. Speed: real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\nWeâ€™ll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\nThen weâ€™ll deploy the one which performs closest to our goal metrics.\nFinally, weâ€™ll finish with a (BIG) surprise bonus.\n\n\n\nTopic\nContents\n\n\n\n\n0. Getting setup\nWeâ€™ve written a fair bit of useful code over the past few sections, letâ€™s download it and make sure we can use it again.\n\n\n1. Get data\nLetâ€™s download the pizza_steak_sushi_20_percent.zip dataset so we can train our previously best performing models on the same dataset.\n\n\n2. FoodVision Mini model deployment experiment outline\nEven on the third milestone project, weâ€™re still going to be running multiple experiments to see which model (EffNetB2 or ViT) achieves closest to our goal metrics.\n\n\n3. Creating an EffNetB2 feature extractor\nAn EfficientNetB2 feature extractor performed the best on our pizza, steak, sushi dataset in 07. PyTorch Experiment Tracking, letâ€™s recreate it as a candidate for deployment.\n\n\n4. Creating a ViT feature extractor\nA ViT feature extractor has been the best performing model yet on our pizza, steak, sushi dataset in 08. PyTorch Paper Replicating, letâ€™s recreate it as a candidate for deployment alongside EffNetB2.\n\n\n5. Making predictions with our trained models and timing them\nWeâ€™ve built two of the best performing models yet, letâ€™s make predictions with them and track their results.\n\n\n6. Comparing model results, prediction times and size\nLetâ€™s compare our models to see which performs best with our goals.\n\n\n7. Bringing FoodVision Mini to life by creating a Gradio demo\nOne of our models performs better than the other (in terms of our goals), so letâ€™s turn it into a working app demo!\n\n\n8. Turning our FoodVision Mini Gradio demo into a deployable app\nOur Gradio app demo works locally, letâ€™s prepare it for deployment!\n\n\n9. Deploying our Gradio demo to HuggingFace Spaces\nLetâ€™s take FoodVision Mini to the web and make it pubically accessible for all!\n\n\n10. Creating a BIG surprise\nWeâ€™ve built FoodVision Mini, time to step things up a notch.\n\n\n11. Deploying our BIG surprise\nDeploying one app was fun, how about we make it two?",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#where-can-you-get-help",
    "href": "09_pytorch_model_deployment.html#where-can-you-get-help",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.5 Where can you get help?",
    "text": "11.5 Where can you get help?\nAll of the materials for this course are available on GitHub.\nIf you run into trouble, you can ask a question on the course GitHub Discussions page.\nAnd of course, thereâ€™s the PyTorch documentation and PyTorch developer forums, a very helpful place for all things PyTorch.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#getting-setup",
    "href": "09_pytorch_model_deployment.html#getting-setup",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.6 0. Getting setup",
    "text": "11.6 0. Getting setup\nAs weâ€™ve done previously, letâ€™s make sure weâ€™ve got all of the modules weâ€™ll need for this section.\nWeâ€™ll import the Python scripts (such as data_setup.py and engine.py) we created in 05. PyTorch Going Modular.\nTo do so, weâ€™ll download going_modular directory from the pytorch-deep-learning repository (if we donâ€™t already have it).\nWeâ€™ll also get the torchinfo package if itâ€™s not available.\ntorchinfo will help later on to give us a visual representation of our model.\nAnd since later on weâ€™ll be using torchvision v0.13 package (available as of July 2022), weâ€™ll make sure weâ€™ve got the latest versions.\n\nì°¸ê³ : If youâ€™re using Google Colab, and you donâ€™t have a GPU turned on yet, itâ€™s now time to turn one on via Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.\n\n\n# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n\ntorch version: 1.13.0.dev20220824+cu113\ntorchvision version: 0.14.0.dev20220824+cu113\n\n\n\nì°¸ê³ : If youâ€™re using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify youâ€™ve got the right versions of torch and torchvision.\n\nNow weâ€™ll continue with the regular imports, setting up device agnostic code and this time weâ€™ll also get the helper_functions.py script from GitHub.\nThe helper_functions.py script contains several functions we created in previous sections: * set_seeds() to set the random seeds (created in 07. PyTorch Experiment Tracking section 0). * download_data() to download a data source given a link (created in 07. PyTorch Experiment Tracking section 1). * plot_loss_curves() to inspect our modelâ€™s training results (created in 04. PyTorch Custom Datasets section 7.8)\n\nì°¸ê³ : It may be a better idea for many of the functions in the helper_functions.py script to be merged into going_modular/going_modular/utils.py, perhaps thatâ€™s an extension youâ€™d like to try.\n\n\n# Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n\nFinally, weâ€™ll setup device-agnostic code to make sure our models run on the GPU.\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#getting-data",
    "href": "09_pytorch_model_deployment.html#getting-data",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.7 1. Getting data",
    "text": "11.7 1. Getting data\nWe left off in 08. PyTorch Paper Replicating comparing our own Vision Transformer (ViT) feature extractor model to the EfficientNetB2 (EffNetB2) feature extractor model we created in 07. PyTorch Experiment Tracking.\nAnd we found that there was a slight difference in the comparison.\nThe EffNetB2 model was trained on 20% of the pizza, steak and sushi data from Food101 where as the ViT model was trained on 10%.\nSince our goal is to deploy the best model for our FoodVision Mini problem, letâ€™s start by downloading the 20% pizza, steak and sushi dataset and train an EffNetB2 feature extractor and ViT feature extractor on it and then compare the two models.\nThis way weâ€™ll be comparing apples to apples (one model trained on a dataset to another model trained on the same dataset).\n\nì°¸ê³ : The dataset weâ€™re downloading is a sample of the entire Food101 dataset (101 food classes with 1,000 images each). More specifically, 20% refers to 20% of images from the pizza, steak and sushi classes selected at random. You can see how this dataset was created in extras/04_custom_data_creation.ipynb and more details in 04. PyTorch Custom Datasets section 1.\n\nWe can download the data using the download_data() function we created in 07. PyTorch Experiment Tracking section 1 from helper_functions.py.\n\n# Download pizza, steak, sushi images from GitHub\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\ndata_20_percent_path\n\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n\n\nPosixPath('data/pizza_steak_sushi_20_percent')\n\n\nWonderful!\nNow weâ€™ve got a dataset, letâ€™s create training and test paths.\n\n# Setup directory paths to train and test images\ntrain_dir = data_20_percent_path / \"train\"\ntest_dir = data_20_percent_path / \"test\"",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#foodvision-mini-model-deployment-experiment-outline",
    "href": "09_pytorch_model_deployment.html#foodvision-mini-model-deployment-experiment-outline",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.8 2. FoodVision Mini model deployment experiment outline",
    "text": "11.8 2. FoodVision Mini model deployment experiment outline\nThe ideal deployed model FoodVision Mini performs well and fast.\nWeâ€™d like our model to perform as close to real-time as possible.\nReal-time in this case being ~30FPS (frames per second) because thatâ€™s about how fast the human eye can see (there is debate on this but letâ€™s just use ~30FPS as our benchmark).\nAnd for classifying three different classes (pizza, steak and sushi), weâ€™d like a model that performs at 95%+ accuracy.\nOf course, higher accuracy would be nice but this might sacrifice speed.\nSo our goals are:\n\nPerformance - A model that performs at 95%+ accuracy.\nSpeed - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).\n\n\nFoodVision Mini deployment goals. Weâ€™d like a fast predicting well-performing model (because a slow app is boring).\nWeâ€™ll put an emphasis on speed, meaning, weâ€™d prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.\nTo try and achieve these results, letâ€™s bring in our best performing models from the previous sections:\n\nEffNetB2 feature extractor (EffNetB2 for short) - originally created in 07. PyTorch Experiment Tracking section 7.5 using torchvision.models.efficientnet_b2() with adjusted classifier layers.\nViT-B/16 feature extractor (ViT for short) - originally created in 08. PyTorch Paper Replicating section 10 using torchvision.models.vit_b_16() with adjusted head layers.\n\nNote ViT-B/16 stands for â€œVision Transformer Base, patch size 16â€.\n\n\n\n\nì°¸ê³ : A â€œfeature extractor modelâ€ often starts with a model that has been pretrained on a dataset similar to your own problem. The pretrained modelâ€™s base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your own problem by training on your own data. We covered the concept of a feature extractor model in 06. PyTorch Transfer Learning section 3.4.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#creating-an-effnetb2-feature-extractor",
    "href": "09_pytorch_model_deployment.html#creating-an-effnetb2-feature-extractor",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.9 3. Creating an EffNetB2 feature extractor",
    "text": "11.9 3. Creating an EffNetB2 feature extractor\nWe first created an EffNetB2 feature extractor model in 07. PyTorch Experiment Tracking section 7.5.\nAnd by the end of that section we saw it performed very well.\nSo letâ€™s now recreate it here so we can compare its results to a ViT feature extractor trained on the same data.\nTo do so we can: 1. Setup the pretrained weights as weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT, where â€œDEFAULTâ€ means â€œbest currently availableâ€ (or could use weights=\"DEFAULT\"). 2. Get the pretrained model image transforms from the weights with the transforms() method (we need these so we can convert our images into the same format as the pretrained EffNetB2 was trained on). 3. Create a pretrained model instance by passing the weights to an instance of torchvision.models.efficientnet_b2. 4. Freeze the base layers in the model. 5. Update the classifier head to suit our own data.\n\n# 1. Setup pretrained EffNetB2 weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n\n# 2. Get EffNetB2 transforms\neffnetb2_transforms = effnetb2_weights.transforms()\n\n# 3. Setup pretrained model\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n\n# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\nfor param in effnetb2.parameters():\n    param.requires_grad = False\n\nNow to change the classifier head, letâ€™s first inspect it using the classifier attribute of our model.\n\n# Check out EffNetB2 classifier head\neffnetb2.classifier\n\nSequential(\n  (0): Dropout(p=0.3, inplace=True)\n  (1): Linear(in_features=1408, out_features=1000, bias=True)\n)\n\n\nExcellent! To change the classifier head to suit our own problem, letâ€™s replace the out_features variable with the same number of classes we have (in our case, out_features=3, one for pizza, steak, sushi).\n\nì°¸ê³ : This process of changing the output layers/classifier head will be dependent on the problem youâ€™re working on. For example, if you wanted a different number of outputs or a different kind of output, you would have to change the output layers accordingly.\n\n\n# 5. Update the classifier head\neffnetb2.classifier = nn.Sequential(\n    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n    nn.Linear(in_features=1408, # keep in_features same \n              out_features=3)) # change out_features to suit our number of classes\n\nBeautiful!\n\n11.9.1 3.1 Creating a function to make an EffNetB2 feature extractor\nLooks like our EffNetB2 feature extractor is ready to go, however, since thereâ€™s quite a few steps involved here, how about we turn the code above into a function we can re-use later?\nWeâ€™ll call it create_effnetb2_model() and itâ€™ll take a customizable number of classes and a random seed parameter for reproducibility.\nIdeally, it will return an EffNetB2 feature extractor along with its associated transforms.\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 4. Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 5. Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n\nWoohoo! Thatâ€™s a nice looking function, letâ€™s try it out.\n\neffnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n                                                      seed=42)\n\nNo errors, nice, now to really try it out, letâ€™s get a summary with torchinfo.summary().\n\nfrom torchinfo import summary\n\n# # Print EffNetB2 model summary (uncomment for full output) \n# summary(effnetb2, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n\n\nBase layers frozen, top layers trainable and customized!\n\n\n11.9.2 3.2 Creating DataLoaders for EffNetB2\nOur EffNetB2 feature extractor is ready, time to create some DataLoaders.\nWe can do this by using the data_setup.create_dataloaders() function we created in 05. PyTorch Going Modular section 2.\nWeâ€™ll use a batch_size of 32 and transform our images using the effnetb2_transforms so theyâ€™re in the same format that our effnetb2 model was trained on.\n\n# Setup DataLoaders\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                 test_dir=test_dir,\n                                                                                                 transform=effnetb2_transforms,\n                                                                                                 batch_size=32)\n\n\n\n11.9.3 3.3 Training EffNetB2 feature extractor\nModel ready, DataLoaders ready, letâ€™s train!\nJust like in 07. PyTorch Experiment Tracking section 7.6, ten epochs should be enough to get good results.\nWe can do so by creating an optimizer (weâ€™ll use torch.optim.Adam() with a learning rate of 1e-3), a loss function (weâ€™ll use torch.nn.CrossEntropyLoss() for multi-class classification) and then passing these as well as our DataLoaders to the engine.train() function we created in 05. PyTorch Going Modular section 4.\n\nfrom going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=effnetb2.parameters(),\n                             lr=1e-3)\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set seeds for reproducibility and train the model\nset_seeds()\neffnetb2_results = engine.train(model=effnetb2,\n                                train_dataloader=train_dataloader_effnetb2,\n                                test_dataloader=test_dataloader_effnetb2,\n                                epochs=10,\n                                optimizer=optimizer,\n                                loss_fn=loss_fn,\n                                device=device)\n\n\n\n\nEpoch: 1 | train_loss: 0.9856 | train_acc: 0.5604 | test_loss: 0.7408 | test_acc: 0.9347\nEpoch: 2 | train_loss: 0.7175 | train_acc: 0.8438 | test_loss: 0.5869 | test_acc: 0.9409\nEpoch: 3 | train_loss: 0.5876 | train_acc: 0.8917 | test_loss: 0.4909 | test_acc: 0.9500\nEpoch: 4 | train_loss: 0.4474 | train_acc: 0.9062 | test_loss: 0.4355 | test_acc: 0.9409\nEpoch: 5 | train_loss: 0.4290 | train_acc: 0.9104 | test_loss: 0.3915 | test_acc: 0.9443\nEpoch: 6 | train_loss: 0.4381 | train_acc: 0.8896 | test_loss: 0.3512 | test_acc: 0.9688\nEpoch: 7 | train_loss: 0.4245 | train_acc: 0.8771 | test_loss: 0.3268 | test_acc: 0.9563\nEpoch: 8 | train_loss: 0.3897 | train_acc: 0.8958 | test_loss: 0.3457 | test_acc: 0.9381\nEpoch: 9 | train_loss: 0.3749 | train_acc: 0.8812 | test_loss: 0.3129 | test_acc: 0.9131\nEpoch: 10 | train_loss: 0.3757 | train_acc: 0.8604 | test_loss: 0.2813 | test_acc: 0.9688\n\n\n\n\n11.9.4 3.4 Inspecting EffNetB2 loss curves\nNice!\nAs we saw in 07. PyTorch Experiment Tracking, the EffNetB2 feature extractor model works quite well on our data.\nLetâ€™s turn its results into loss curves to inspect them further.\n\nì°¸ê³ : Loss curves are one of the best ways to visualize how your modelâ€™s performing. For more on loss curves, check out 04. PyTorch Custom Datasets section 8: What should an ideal loss curve look like?\n\n\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)\n\n\n\n\n\n\n\n\nWoah!\nThose are some nice looking loss curves.\nIt looks like our model is performing quite well and perhaps would benefit from a little longer training and potentially some data augmentation (to help prevent potential overfitting occurring from longer training).\n\n\n11.9.5 3.5 Saving EffNetB2 feature extractor\nNow weâ€™ve got a well-performing trained model, letâ€™s save it to file so we can import and use it later.\nTo save our model we can use the utils.save_model() function we created in 05. PyTorch Going Modular section 5.\nWeâ€™ll set the target_dir to \"models\" and the model_name to \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\" (a little comprehensive but at least we know whatâ€™s going on).\n\nfrom going_modular.going_modular import utils\n\n# Save the model\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n\n[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n\n\n\n\n11.9.6 3.6 Checking the size of EffNetB2 feature extractor\nSince one of our criteria for deploying a model to power FoodVision Mini is speed (~30FPS or better), letâ€™s check the size of our model.\nWhy check the size?\nWell, while not always the case, the size of a model can influence its inference speed.\nAs in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.\nAnd because weâ€™d like our model to work on devices with limited computing power (e.g.Â on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).\nTo check our modelâ€™s size in bytes, we can use Pythonâ€™s pathlib.Path.stat(\"path_to_model\").st_size and then we can convert it (roughly) to megabytes by dividing it by (1024*1024).\n\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")\n\nPretrained EffNetB2 feature extractor model size: 29 MB\n\n\n\n\n11.9.7 3.7 Collecting EffNetB2 feature extractor stats\nWeâ€™ve got a few statistics about our EffNetB2 feature extractor model such as test loss, test accuracy and model size, how about we collect them all in a dictionary so we can compare them to the upcoming ViT feature extractor.\nAnd weâ€™ll calculate an extra one for fun, total number of parameters.\nWe can do so by counting the number of elements (or patterns/weights) in effnetb2.parameters(). Weâ€™ll access the number of elements in each parameter using the torch.numel() (short for â€œnumber of elementsâ€) method.\n\n# Count number of parameters in EffNetB2\neffnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\neffnetb2_total_params\n\n7705221\n\n\nExcellent!\nNow letâ€™s put everything in a dictionary so we can make comparisons later on.\n\n# Create a dictionary with EffNetB2 statistics\neffnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n                  \"number_of_parameters\": effnetb2_total_params,\n                  \"model_size (MB)\": pretrained_effnetb2_model_size}\neffnetb2_stats\n\n{'test_loss': 0.28128674924373626,\n 'test_acc': 0.96875,\n 'number_of_parameters': 7705221,\n 'model_size (MB)': 29}\n\n\nEpic!\nLooks like our EffNetB2 model is performing at over 95% accuracy!\nCriteria number 1: perform at 95%+ accuracy, tick!",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#creating-a-vit-feature-extractor",
    "href": "09_pytorch_model_deployment.html#creating-a-vit-feature-extractor",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.10 4. Creating a ViT feature extractor",
    "text": "11.10 4. Creating a ViT feature extractor\nTime to continue with our FoodVision Mini modelling experiments.\nThis time weâ€™re going to create a ViT feature extractor.\nAnd weâ€™ll do it in much the same way as the EffNetB2 feature extractor except this time with torchvision.models.vit_b_16() instead of torchvision.models.efficientnet_b2().\nWeâ€™ll start by creating a function called create_vit_model() which will be very similar to create_effnetb2_model() except of course returning a ViT feature extractor model and transforms rather than EffNetB2.\nAnother slight difference is that torchvision.models.vit_b_16()â€™s output layer is called heads rather than classifier.\n\n# Check out ViT heads layer\nvit = torchvision.models.vit_b_16()\nvit.heads\n\nSequential(\n  (head): Linear(in_features=768, out_features=1000, bias=True)\n)\n\n\nKnowing this, weâ€™ve got all the pieces of the puzzle we need.\n\ndef create_vit_model(num_classes:int=3, \n                     seed:int=42):\n    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of target classes. Defaults to 3.\n        seed (int, optional): random seed value for output layer. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): ViT-B/16 feature extractor model. \n        transforms (torchvision.transforms): ViT-B/16 image transforms.\n    \"\"\"\n    # Create ViT_B_16 pretrained weights, transforms and model\n    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.vit_b_16(weights=weights)\n\n    # Freeze all layers in model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head to suit our needs (this will be trainable)\n    torch.manual_seed(seed)\n    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n                                          out_features=num_classes)) # update to reflect target number of classes\n    \n    return model, transforms\n\nViT feature extraction model creation function ready!\nLetâ€™s test it out.\n\n# Create ViT model and transforms\nvit, vit_transforms = create_vit_model(num_classes=3,\n                                       seed=42)\n\nNo errors, lovely to see!\nNow letâ€™s get a nice-looking summary of our ViT model using torchinfo.summary().\n\nfrom torchinfo import summary\n\n# # Print ViT feature extractor model summary (uncomment for full output)\n# summary(vit, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n\n\nJust like our EffNetB2 feature extractor model, our ViT modelâ€™s base layers are frozen and the output layer is customized to our needs!\nDo you notice the big difference though?\nOur ViT model has far more parameters than our EffNetB2 model. Perhaps this will come into play when we compare our models across speed and performance later on.\n\n11.10.1 4.1 Create DataLoaders for ViT\nWeâ€™ve got our ViT model ready, now letâ€™s create some DataLoaders for it.\nWeâ€™ll do this in the same way we did for EffNetB2 except weâ€™ll use vit_transforms to transform our images into the same format the ViT model was trained on.\n\n# Setup ViT DataLoaders\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                       test_dir=test_dir,\n                                                                                       transform=vit_transforms,\n                                                                                       batch_size=32)\n\n\n\n11.10.2 4.2 Training ViT feature extractor\nYou know what time it isâ€¦\nâ€¦itâ€™s traininggggggg time (sung in the same tune as the song Closing Time).\nLetâ€™s train our ViT feature extractor model for 10 epochs using our engine.train() function with torch.optim.Adam() and a learning rate of 1e-3 as our optimizer and torch.nn.CrossEntropyLoss() as our loss function.\nWeâ€™ll use our set_seeds() function before training to try and make our results as reproducible as possible.\n\nfrom going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=1e-3)\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train ViT model with seeds set for reproducibility\nset_seeds()\nvit_results = engine.train(model=vit,\n                           train_dataloader=train_dataloader_vit,\n                           test_dataloader=test_dataloader_vit,\n                           epochs=10,\n                           optimizer=optimizer,\n                           loss_fn=loss_fn,\n                           device=device)\n\n\n\n\nEpoch: 1 | train_loss: 0.7023 | train_acc: 0.7500 | test_loss: 0.2714 | test_acc: 0.9290\nEpoch: 2 | train_loss: 0.2531 | train_acc: 0.9104 | test_loss: 0.1669 | test_acc: 0.9602\nEpoch: 3 | train_loss: 0.1766 | train_acc: 0.9542 | test_loss: 0.1270 | test_acc: 0.9693\nEpoch: 4 | train_loss: 0.1277 | train_acc: 0.9625 | test_loss: 0.1072 | test_acc: 0.9722\nEpoch: 5 | train_loss: 0.1163 | train_acc: 0.9646 | test_loss: 0.0950 | test_acc: 0.9784\nEpoch: 6 | train_loss: 0.1270 | train_acc: 0.9375 | test_loss: 0.0830 | test_acc: 0.9722\nEpoch: 7 | train_loss: 0.0899 | train_acc: 0.9771 | test_loss: 0.0844 | test_acc: 0.9784\nEpoch: 8 | train_loss: 0.0928 | train_acc: 0.9812 | test_loss: 0.0759 | test_acc: 0.9722\nEpoch: 9 | train_loss: 0.0933 | train_acc: 0.9792 | test_loss: 0.0729 | test_acc: 0.9784\nEpoch: 10 | train_loss: 0.0662 | train_acc: 0.9833 | test_loss: 0.0642 | test_acc: 0.9847\n\n\n\n\n11.10.3 4.3 Inspecting ViT loss curves\nAlright, alright, alright, ViT model trained, letâ€™s get visual and see some loss curves.\n\nì°¸ê³ : Donâ€™t forget you can see what an ideal set of loss curves should look like in 04. PyTorch Custom Datasets section 8.\n\n\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(vit_results)\n\n\n\n\n\n\n\n\nOhh yeah!\nThose are some nice looking loss curves. Just like our EffNetB2 feature extractor model, it looks our ViT model might benefit from a little longer training time and perhaps some data augmentation (to help prevent overfitting).\n\n\n11.10.4 4.4 Saving ViT feature extractor\nOur ViT model is performing outstanding!\nSo letâ€™s save it to file so we can import it and use it later if we wish.\nWe can do so using the utils.save_model() function we created in 05. PyTorch Going Modular section 5.\n\n# Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=vit,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n\n[INFO] Saving model to: models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n\n\n\n\n11.10.5 4.5 Checking the size of ViT feature extractor\nAnd since we want to compare our EffNetB2 model to our ViT model across a number of characteristics, letâ€™s find out its size.\nTo check our modelâ€™s size in bytes, we can use Pythonâ€™s pathlib.Path.stat(\"path_to_model\").st_size and then we can convert it (roughly) to megabytes by dividing it by (1024*1024).\n\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n\nPretrained ViT feature extractor model size: 327 MB\n\n\nHmm, how does the ViT feature extractor model size compare to our EffNetB2 model size?\nWeâ€™ll find this out shortly when we compare all of our modelâ€™s characteristics.\n\n\n11.10.6 4.6 Collecting ViT feature extractor stats\nLetâ€™s put together all of our ViT feature extractor model statistics.\nWe saw it in the summary output above but weâ€™ll calculate its total number of parameters.\n\n# Count number of parameters in ViT\nvit_total_params = sum(torch.numel(param) for param in vit.parameters())\nvit_total_params\n\n85800963\n\n\nWoah, that looks like a fair bit more than our EffNetB2!\n\nì°¸ê³ : A larger number of parameters (or weights/patterns) generally means a model has a higher capacity to learn, whether it actually uses this extra capacity is another story. In light of this, our EffNetB2 model has 7,705,221 parameters where as our ViT model has 85,800,963 (11.1x more) so we could assume that our ViT model has more of a capacity to learn, if given more data (more opportunities to learn). However, this larger capacity to learn ofen comes with an increased model filesize and a longer time to perform inference.\n\nNow letâ€™s create a dictionary with some important characteristics of our ViT model.\n\n# Create ViT statistics dictionary\nvit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n             \"test_acc\": vit_results[\"test_acc\"][-1],\n             \"number_of_parameters\": vit_total_params,\n             \"model_size (MB)\": pretrained_vit_model_size}\n\nvit_stats\n\n{'test_loss': 0.06418210905976593,\n 'test_acc': 0.984659090909091,\n 'number_of_parameters': 85800963,\n 'model_size (MB)': 327}\n\n\nNice! Looks like our ViT model achieves over 95% accuracy too.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#making-predictions-with-our-trained-models-and-timing-them",
    "href": "09_pytorch_model_deployment.html#making-predictions-with-our-trained-models-and-timing-them",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.11 5. Making predictions with our trained models and timing them",
    "text": "11.11 5. Making predictions with our trained models and timing them\nWeâ€™ve got a couple of trained models, both performing pretty well.\nNow how about we test them out doing what weâ€™d like them to do?\nAs in, letâ€™s see how they go making predictions (performing inference).\nWe know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?\nIdeally, if weâ€™re deploying our FoodVision Mini model to a mobile device so people can take photos of their food and identify it, weâ€™d like the predictions to happen at real-time (~30 frames per second).\nThatâ€™s why our second criteria is: a fast model.\nTo find out how long each of our models take to performance inference, letâ€™s create a function called pred_and_store() to iterate over each of the test dataset images one by one and perform a prediction.\nWeâ€™ll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary).\n\nì°¸ê³ : We time the predictions one by one rather than by batch because when our model is deployed, it will likely only be making a prediction on one image at a time. As in, someone takes a photo and our model predicts on that single image.\n\nSince weâ€™d like to make predictions across all the images in the test set, letâ€™s first get a list of all of the test image paths so we can iterate over them.\nTo do so, weâ€™ll use Pythonâ€™s pathlib.Path(\"target_dir\").glob(\"*/*.jpg\")) to find all of the filepaths in a target directory with the extension .jpg (all of our test images).\n\nfrom pathlib import Path\n\n# Get all test data paths\nprint(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\ntest_data_paths[:5]\n\n[INFO] Finding all filepaths ending with '.jpg' in directory: data/pizza_steak_sushi_20_percent/test\n\n\n[PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/2752603.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/39461.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/730464.jpg')]\n\n\n\n11.11.1 5.1 Creating a function to make predictions across the test dataset\nNow weâ€™ve got a list of our test image paths, letâ€™s get to work on our pred_and_store() function:\n\nCreate a function that takes a list of paths, a trained PyTorch model, a series of transforms (to prepare images), a list of target class names and a target device.\nCreate an empty list to store prediction dictionaries (we want the function to return a list of dictionaries, one for each prediction).\nLoop through the target input paths (steps 4-14 will happen inside the loop).\nCreate an empty dictionary for each iteration in the loop to store prediction values per sample.\nGet the sample path and ground truth class name (we can do this by inferring the class from the path).\nStart the prediction timer using Pythonâ€™s timeit.default_timer().\nOpen the image using PIL.Image.open(path).\nTransform the image so itâ€™s capable of being used with the target model as well as add a batch dimension and send the image to the target device.\nPrepare the model for inference by sending it to the target device and turning on eval() mode.\nTurn on torch.inference_mode() and pass the target transformed image to the model and calculate the prediction probability using torch.softmax() and the target label using torch.argmax().\nAdd the prediction probability and prediction class to the prediction dictionary created in step 4. Also make sure the prediction probability is on the CPU so it can be used with non-GPU libraries such as NumPy and pandas for later inspection.\nEnd the prediction timer started in step 6 and add the time to the prediction dictionary created in step 4.\nSee if the predicted class matches the ground truth class from step 5 and add the result to the prediction dictionary created in step 4.\nAppend the updated prediction dictionary to the empty list of predictions created in step 2.\nReturn the list of prediction dictionaries.\n\nA bunch of steps, but nothing we canâ€™t handle!\nLetâ€™s do it.\n\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom timeit import default_timer as timer \nfrom tqdm.auto import tqdm\nfrom typing import List, Dict\n\n# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\ndef pred_and_store(paths: List[pathlib.Path], \n                   model: torch.nn.Module,\n                   transform: torchvision.transforms, \n                   class_names: List[str], \n                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:\n    \n    # 2. Create an empty list to store prediction dictionaries\n    pred_list = []\n    \n    # 3. Loop through target paths\n    for path in tqdm(paths):\n        \n        # 4. Create empty dictionary to store prediction information for each sample\n        pred_dict = {}\n\n        # 5. Get the sample path and ground truth class name\n        pred_dict[\"image_path\"] = path\n        class_name = path.parent.stem\n        pred_dict[\"class_name\"] = class_name\n        \n        # 6. Start the prediction timer\n        start_time = timer()\n        \n        # 7. Open image path\n        img = Image.open(path)\n        \n        # 8. Transform the image, add batch dimension and put image on target device\n        transformed_image = transform(img).unsqueeze(0).to(device) \n        \n        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n        model.to(device)\n        model.eval()\n        \n        # 10. Get prediction probability, predicition label and prediction class\n        with torch.inference_mode():\n            pred_logit = model(transformed_image) # perform inference on target sample \n            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n\n            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n            pred_dict[\"pred_class\"] = pred_class\n            \n            # 12. End the timer and calculate time per pred\n            end_time = timer()\n            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n\n        # 13. Does the pred match the true label?\n        pred_dict[\"correct\"] = class_name == pred_class\n\n        # 14. Add the dictionary to the list of preds\n        pred_list.append(pred_dict)\n    \n    # 15. Return list of prediction dictionaries\n    return pred_list\n\nHo, ho!\nWhat a good looking function!\nAnd you know what, since our pred_and_store() is a pretty good utility function for making and storing predictions, it could be stored to going_modular.going_modular.predictions.py for later use. That might be an extension youâ€™d like to try, check out 05. PyTorch Going Modular for ideas.\n\n\n11.11.2 5.2 Making and timing predictions with EffNetB2\nTime to test out our pred_and_store() function!\nLetâ€™s start by using it to make predictions across the test dataset with our EffNetB2 model, paying attention to two details:\n\nDevice - Weâ€™ll hard code the device parameter to use \"cpu\" because when we deploy our model, we wonâ€™t always have access to a \"cuda\" (GPU) device.\n\nMaking the predictions on CPU will be a good indicator of speed of inference too because generally predictions on CPU devices are slower than GPU devices.\n\nTransforms - Weâ€™ll also be sure to set the transform parameter to effnetb2_transforms to make sure the images are opened and transformed in the same way our effnetb2 model has been trained on.\n\n\n# Make predictions across test dataset with EffNetB2\neffnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                          model=effnetb2,\n                                          transform=effnetb2_transforms,\n                                          class_names=class_names,\n                                          device=\"cpu\") # make predictions on CPU \n\n\n\n\nNice! Look at those predictions fly!\nLetâ€™s inspect the first couple and see what they look like.\n\n# Inspect the first 2 prediction dictionaries\neffnetb2_test_pred_dicts[:2]\n\n[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9293,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0494,\n  'correct': True},\n {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9534,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0264,\n  'correct': True}]\n\n\nWoohoo!\nIt looks like our pred_and_store() function worked nicely.\nThanks to our list of dictionaries data structure, weâ€™ve got plenty of useful information we can further inspect.\nTo do so, letâ€™s turn our list of dictionaries into a pandas DataFrame.\n\n# Turn the test_pred_dicts into a DataFrame\nimport pandas as pd\neffnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\neffnetb2_test_pred_df.head()\n\n\n\n\n\n\n\n\nimage_path\nclass_name\npred_prob\npred_class\ntime_for_pred\ncorrect\n\n\n\n\n0\ndata/pizza_steak_sushi_20_percent/test/steak/8...\nsteak\n0.9293\nsteak\n0.0494\nTrue\n\n\n1\ndata/pizza_steak_sushi_20_percent/test/steak/3...\nsteak\n0.9534\nsteak\n0.0264\nTrue\n\n\n2\ndata/pizza_steak_sushi_20_percent/test/steak/2...\nsteak\n0.7532\nsteak\n0.0256\nTrue\n\n\n3\ndata/pizza_steak_sushi_20_percent/test/steak/3...\nsteak\n0.5935\nsteak\n0.0263\nTrue\n\n\n4\ndata/pizza_steak_sushi_20_percent/test/steak/7...\nsteak\n0.8959\nsteak\n0.0269\nTrue\n\n\n\n\n\n\n\nBeautiful!\nLook how easily those prediction dictionaries turn into a structured format we can perform analysis on.\nSuch as finding how many predictions our EffNetB2 model got wrongâ€¦\n\n# Check number of correct predictions\neffnetb2_test_pred_df.correct.value_counts()\n\nTrue     145\nFalse      5\nName: correct, dtype: int64\n\n\nFive wrong predictions out of 150 total, not bad!\nAnd how about the average prediction time?\n\n# Find the average time per prediction \neffnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")\n\nEffNetB2 average time per prediction: 0.0269 seconds\n\n\nHmm, how does that average prediction time live up to our criteria of our model performing at real-time (~30FPS or 0.03 seconds per prediction)?\n\nì°¸ê³ : Prediction times will be different across different hardware types (e.g.Â a local Intel i9 vs Google Colab CPU). The better and faster the hardware, generally, the faster the prediction. For example, on my local deep learning PC with an Intel i9 chip, my average prediction time with EffNetB2 is around 0.031 seconds (just under real-time). However, on Google Colab (Iâ€™m not sure what CPU hardware Colab uses but it looks like it might be an Intel(R) Xeon(R)), my average prediction time with EffNetB2 is about 0.1396 seconds (3-4x slower).\n\nLetâ€™s add our EffNetB2 average time per prediction to our effnetb2_stats dictionary.\n\n# Add EffNetB2 average prediction time to stats dictionary \neffnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\neffnetb2_stats\n\n{'test_loss': 0.28128674924373626,\n 'test_acc': 0.96875,\n 'number_of_parameters': 7705221,\n 'model_size (MB)': 29,\n 'time_per_pred_cpu': 0.0269}\n\n\n\n\n11.11.3 5.3 Making and timing predictions with ViT\nWeâ€™ve made predictions with our EffNetB2 model, now letâ€™s do the same for our ViT model.\nTo do so, we can use the pred_and_store() function we created above except this time weâ€™ll pass in our vit model as well as the vit_transforms.\nAnd weâ€™ll keep the predictions on the CPU via device=\"cpu\" (a natural extension here would be to test the prediction times on CPU and on GPU).\n\n# Make list of prediction dictionaries with ViT feature extractor model on test images\nvit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                     model=vit,\n                                     transform=vit_transforms,\n                                     class_names=class_names,\n                                     device=\"cpu\")\n\n\n\n\nPredictions made!\nNow letâ€™s check out the first couple.\n\n# Check the first couple of ViT predictions on the test dataset\nvit_test_pred_dicts[:2]\n\n[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9933,\n  'pred_class': 'steak',\n  'time_for_pred': 0.1313,\n  'correct': True},\n {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9893,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0638,\n  'correct': True}]\n\n\nWonderful!\nAnd just like before, since our ViT modelâ€™s predictions are in the form of a list of dictionaries, we can easily turn them into a pandas DataFrame for further inspection.\n\n# Turn vit_test_pred_dicts into a DataFrame\nimport pandas as pd\nvit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\nvit_test_pred_df.head()\n\n\n\n\n\n\n\n\nimage_path\nclass_name\npred_prob\npred_class\ntime_for_pred\ncorrect\n\n\n\n\n0\ndata/pizza_steak_sushi_20_percent/test/steak/8...\nsteak\n0.9933\nsteak\n0.1313\nTrue\n\n\n1\ndata/pizza_steak_sushi_20_percent/test/steak/3...\nsteak\n0.9893\nsteak\n0.0638\nTrue\n\n\n2\ndata/pizza_steak_sushi_20_percent/test/steak/2...\nsteak\n0.9971\nsteak\n0.0627\nTrue\n\n\n3\ndata/pizza_steak_sushi_20_percent/test/steak/3...\nsteak\n0.7685\nsteak\n0.0632\nTrue\n\n\n4\ndata/pizza_steak_sushi_20_percent/test/steak/7...\nsteak\n0.9499\nsteak\n0.0641\nTrue\n\n\n\n\n\n\n\nHow many predictions did our ViT model get correct?\n\n# Count the number of correct predictions\nvit_test_pred_df.correct.value_counts()\n\nTrue     148\nFalse      2\nName: correct, dtype: int64\n\n\nWoah!\nOur ViT model did a little better than our EffNetB2 model in terms of correct predictions, only two samples wrong across the whole test dataset.\nAs an extension you might want to visualize the ViT modelâ€™s wrong predictions and see if thereâ€™s any reason why it mightâ€™ve got them wrong.\nHow about we calculate how long the ViT model took per prediction?\n\n# Calculate average time per prediction for ViT model\nvit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")\n\nViT average time per prediction: 0.0641 seconds\n\n\nWell, that looks a little slower than our EffNetB2 modelâ€™s average time per prediction but how does it look in terms of our second criteria: speed?\nFor now, letâ€™s add the value to our vit_stats dictionary so we can compare it to our EffNetB2 modelâ€™s stats.\n\nì°¸ê³ : The average time per prediction values will be highly dependent on the hardware you make them on. For example, for the ViT model, my average time per prediction (on the CPU) was 0.0693-0.0777 seconds on my local deep learning PC with an Intel i9 CPU. Where as on Google Colab, my average time per prediction with the ViT model was 0.6766-0.7113 seconds.\n\n\n# Add average prediction time for ViT model on CPU\nvit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\nvit_stats\n\n{'test_loss': 0.06418210905976593,\n 'test_acc': 0.984659090909091,\n 'number_of_parameters': 85800963,\n 'model_size (MB)': 327,\n 'time_per_pred_cpu': 0.0641}",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#comparing-model-results-prediction-times-and-size",
    "href": "09_pytorch_model_deployment.html#comparing-model-results-prediction-times-and-size",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.12 6. Comparing model results, prediction times and size",
    "text": "11.12 6. Comparing model results, prediction times and size\nOur two best model contenders have been trained and evaluated.\nNow letâ€™s put them head to head and compare across their different statistics.\nTo do so, letâ€™s turn our effnetb2_stats and vit_stats dictionaries into a pandas DataFrame.\nWeâ€™ll add a column to view the model names as well as convert the test accuracy to a whole percentage rather than decimal.\n\n# Turn stat dictionaries into DataFrame\ndf = pd.DataFrame([effnetb2_stats, vit_stats])\n\n# Add column for model names\ndf[\"model\"] = [\"EffNetB2\", \"ViT\"]\n\n# Convert accuracy to percentages\ndf[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n\ndf\n\n\n\n\n\n\n\n\ntest_loss\ntest_acc\nnumber_of_parameters\nmodel_size (MB)\ntime_per_pred_cpu\nmodel\n\n\n\n\n0\n0.281287\n96.88\n7705221\n29\n0.0269\nEffNetB2\n\n\n1\n0.064182\n98.47\n85800963\n327\n0.0641\nViT\n\n\n\n\n\n\n\nWonderful!\nIt seems our models are quite close in terms of overall test accuracy but how do they look across the other fields?\nOne way to find out would be to divide the ViT model statistics by the EffNetB2 model statistics to find out the different ratios between the models.\nLetâ€™s create another DataFrame to do so.\n\n# Compare ViT to EffNetB2 across different characteristics\npd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n             columns=[\"ViT to EffNetB2 ratios\"]).T\n\n\n\n\n\n\n\n\ntest_loss\ntest_acc\nnumber_of_parameters\nmodel_size (MB)\ntime_per_pred_cpu\n\n\n\n\nViT to EffNetB2 ratios\n0.228173\n1.016412\n11.135432\n11.275862\n2.3829\n\n\n\n\n\n\n\nIt seems our ViT model outperforms the EffNetB2 model across the performance metrics (test loss, where lower is better and test accuracy, where higher is better) but at the expense of having: * 11x+ the number of parameters. * 11x+ the model size. * 2.5x+ the prediction time per image.\nAre these tradeoffs worth it?\nPerhaps if we had unlimited compute power but for our use case of deploying the FoodVision Mini model to a smaller device (e.g.Â a mobile phone), weâ€™d likely start out with the EffNetB2 model for faster predictions at a slightly reduced performance but dramatically smaller size.\n\n11.12.1 6.1 Visualizing the speed vs.Â performance tradeoff\nWeâ€™ve seen that our ViT model outperforms our EffNetB2 model in terms of performance metrics such as test loss and test accuracy.\nHowever, our EffNetB2 model performs predictions faster and has a much smaller model size.\n\nì°¸ê³ : Performance or inference time is also often referred to as â€œlatencyâ€.\n\nHow about we make this fact visual?\nWe can do so by creating a plot with matplotlib: 1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT time_per_pred_cpu and test_acc values. 2. Add titles and labels respective of the data and customize the fontsize for aesthetics. 3. Annotate the samples on the scatter plot from step 1 with their appropriate labels (the model names). 4. Create a legend based on the model sizes (model_size (MB)).\n\n# 1. Create a plot from model comparison DataFrame\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(data=df, \n                     x=\"time_per_pred_cpu\", \n                     y=\"test_acc\", \n                     c=[\"blue\", \"orange\"], # what colours to use?\n                     s=\"model_size (MB)\") # size the dots by the model sizes\n\n# 2. Add titles, labels and customize fontsize for aesthetics\nax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\nax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\nax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\nax.tick_params(axis='both', labelsize=12)\nax.grid(True)\n\n# 3. Annotate with model names\nfor index, row in df.iterrows():\n    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n                size=12)\n\n# 4. Create a legend based on model sizes\nhandles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\nmodel_size_legend = ax.legend(handles, \n                              labels, \n                              loc=\"lower right\", \n                              title=\"Model size (MB)\",\n                              fontsize=12)\n\n# Save the figure\n!mdkir images/\nplt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n\n# Show the figure\nplt.show()\n\n\n\n\n\n\n\n\nWoah!\nThe plot really visualizes the speed vs.Â performance tradeoff, in other words, when you have a larger, better performing deep model (like our ViT model), it generally takes longer to perform inference (higher latency).\nThere are exceptions to the rule and new research is being published all the time to help make larger models perform faster.\nAnd it can be tempting to just deploy the best performing model but itâ€™s also good to take into consideration where the model is going to be performing.\nIn our case, the differences between our modelâ€™s performance levels (on the test loss and test accuracy) arenâ€™t too extreme.\nBut since weâ€™d like to put an emphasis on speed to begin with, weâ€™re going to stick with deploying EffNetB2 since itâ€™s faster and has a much smaller footprint.\n\nì°¸ê³ : Prediction times will be different across different hardware types (e.g.Â Intel i9 vs Google Colab CPU vs GPU) so itâ€™s important to think about and test where your model is going to end up. Asking questions like â€œwhere is the model going to be run?â€ or â€œwhat is the ideal scenario for running the model?â€ and then running experiments to try and provide answers on your way to deployment is very helpful.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#bringing-foodvision-mini-to-life-by-creating-a-gradio-demo",
    "href": "09_pytorch_model_deployment.html#bringing-foodvision-mini-to-life-by-creating-a-gradio-demo",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.13 7. Bringing FoodVision Mini to life by creating a Gradio demo",
    "text": "11.13 7. Bringing FoodVision Mini to life by creating a Gradio demo\nWeâ€™ve decided weâ€™d like to deploy the EffNetB2 model (to begin with, this could always be changed later).\nSo how can we do that?\nThere are several ways to deploy a machine learning model each with specific use cases (as discussed above).\nWeâ€™re going to be focused on perhaps the quickest and certainly one of the most fun ways to get a model deployed to the internet.\nAnd thatâ€™s by using Gradio.\nWhatâ€™s Gradio?\nThe homepage describes it beautifully:\n\nGradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\n\nWhy create a demo of your models?\nBecause metrics on the test set look nice but you never really know how your model performs until you use it in the wild.\nSo letâ€™s get deploying!\nWeâ€™ll start by importing Gradio with the common alias gr and if itâ€™s not present, weâ€™ll install it.\n\n# Import/install Gradio \ntry:\n    import gradio as gr\nexcept: \n    !pip -q install gradio\n    import gradio as gr\n    \nprint(f\"Gradio version: {gr.__version__}\")\n\nGradio version: 3.1.4\n\n\nGradio ready!\nLetâ€™s turn FoodVision Mini into a demo application.\n\n11.13.1 7.1 Gradio overview\nThe overall premise of Gradio is very similar to what weâ€™ve been repeating throughout the course.\nWhat are our inputs and outputs?\nAnd how should we get there?\nWell thatâ€™s what our machine learning model does.\ninputs -&gt; ML model -&gt; outputs\nIn our case, for FoodVision Mini, our inputs are images of food, our ML model is EffNetB2 and our outputs are classes of food (pizza, steak or sushi).\nimages of food -&gt; EffNetB2 -&gt; outputs\nThough the concepts of inputs and outputs can be bridged to almost any other kind of ML problem.\nYour inputs and outputs might be any combination of the following: * Images * Text * Video * Tabular data * Audio * Numbers * & more\nAnd the ML model you build will depend on your inputs and outputs.\nGradio emulates this paradigm by creating an interface (gradio.Interface()) from inputs to outputs.\ngradio.Interface(fn, inputs, outputs)\nWhere, fn is a Python function to map the inputs to the outputs.\n\nGradio provides a very helpful Interface class to easily create an inputs -&gt; model/function -&gt; outputs workflow where the inputs and outputs could be almost anything you want. For example, you might input Tweets (text) to see if theyâ€™re about machine learning or not or input a text prompt to generate images.\n\nì°¸ê³ : Gradio has a vast number of possible inputs and outputs options known as â€œComponentsâ€ from images to text to numbers to audio to videos and more. You can see all of these in the Gradio Components documentation.\n\n\n\n11.13.2 7.2 Creating a function to map our inputs and outputs\nTo create our FoodVision Mini demo with Gradio, weâ€™ll need a function to map our inputs to our outputs.\nWe created a function earlier called pred_and_store() to make predictions with a given model across a list of target files and store them in a list of dictionaries.\nHow about we create a similar function but this time focusing on making a prediction on a single image with our EffNetB2 model?\nMore specifically, we want a function that takes an image as input, preprocesses (transforms) it, makes a prediction with EffNetB2 and then returns the prediction (pred or pred label for short) as well as the prediction probability (pred prob).\nAnd while weâ€™re here, letâ€™s return the time it took to do so too:\ninput: image -&gt; transform -&gt; predict with EffNetB2 -&gt; output: pred, pred prob, time taken\nThis will be our fn parameter for our Gradio interface.\nFirst, letâ€™s make sure our EffNetB2 model is on the CPU (since weâ€™re sticking with CPU-only predictions, however you could change this if you have access to a GPU).\n\n# Put EffNetB2 on CPU\neffnetb2.to(\"cpu\") \n\n# Check the device\nnext(iter(effnetb2.parameters())).device\n\ndevice(type='cpu')\n\n\nAnd now letâ€™s create a function called predict() to replicate the workflow above.\n\nfrom typing import Tuple, Dict\n\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\nBeautiful!\nNow letâ€™s see our function in action by performing a prediction on a random image from the test dataset.\nWeâ€™ll start by getting a list of all the image paths from the test directory and then randomly selecting one.\nThen weâ€™ll open the randomly selected image with PIL.Image.open().\nFinally, weâ€™ll pass the image to our predict() function.\n\nimport random\nfrom PIL import Image\n\n# Get a list of all test image filepaths\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n\n# Randomly select a test image path\nrandom_image_path = random.sample(test_data_paths, k=1)[0]\n\n# Open the target image\nimage = Image.open(random_image_path)\nprint(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n\n# Predict on the target image and print out the outputs\npred_dict, pred_time = predict(img=image)\nprint(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\nprint(f\"Prediction time: {pred_time} seconds\")\n\n[INFO] Predicting on image at path: data/pizza_steak_sushi_20_percent/test/pizza/3770514.jpg\n\nPrediction label and probability dictionary: \n{'pizza': 0.9785208702087402, 'steak': 0.01169557310640812, 'sushi': 0.009783552028238773}\nPrediction time: 0.027 seconds\n\n\nNice!\nRunning the cell above a few times we can see different prediction probabilities for each label from our EffNetB2 model as well as the time it took per prediction.\n\n\n11.13.3 7.3 Creating a list of example images\nOur predict() function enables us to go from inputs -&gt; transform -&gt; ML model -&gt; outputs.\nWhich is exactly what we need for our Graido demo.\nBut before we create the demo, letâ€™s create one more thing: a list of examples.\nGradioâ€™s Interface class takes a list of examples of as an optional parameter (gradio.Interface(examples=List[Any])).\nAnd the format for the examples parameter is a list of lists.\nSo letâ€™s create a list of lists containing random filepaths to our test images.\nThree examples should be enough.\n\n# Create a list of example inputs to our Gradio demo\nexample_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\nexample_list\n\n[['data/pizza_steak_sushi_20_percent/test/sushi/804460.jpg'],\n ['data/pizza_steak_sushi_20_percent/test/steak/746921.jpg'],\n ['data/pizza_steak_sushi_20_percent/test/steak/2117351.jpg']]\n\n\nPerfect!\nOur Gradio demo will showcase these as example inputs to our demo so people can try it out and see what it does without uploading any of their own data.\n\n\n11.13.4 7.4 Building a Gradio interface\nTime to put everything together and bring our FoodVision Mini demo to life!\nLetâ€™s create a Gradio interface to replicate the workflow:\ninput: image -&gt; transform -&gt; predict with EffNetB2 -&gt; output: pred, pred prob, time taken\nWe can do with the gradio.Interface() class with the following parameters: * fn - a Python function to map inputs to outputs, in our case, weâ€™ll use our predict() function. * inputs - the input to our interface, such as an image using gradio.Image() or \"image\". * outputs - the output of our interface once the inputs have gone through the fn, such as a label using gradio.Label() (for our modelâ€™s predicted labels) or number using gradio.Number() (for our modelâ€™s prediction time). * ì°¸ê³ : Gradio comes with many in-built inputs and outputs options known as â€œComponentsâ€. * examples - a list of examples to showcase for the demo. * title - a string title of the demo. * description - a string description of the demo. * article - a reference note at the bottom of the demo.\nOnce weâ€™ve created our demo instance of gr.Interface(), we can bring it to life using gradio.Interface().launch() or demo.launch() command.\nEasy!\n\nimport gradio as gr\n\n# Create title, description and article strings\ntitle = \"FoodVision Mini ğŸ•ğŸ¥©ğŸ£\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch(debug=False, # print errors locally?\n            share=True) # generate a publically shareable URL?\n\nRunning on local URL:  http://127.0.0.1:7860/\nRunning on public URL: https://27541.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n\n\n\n\n\n(&lt;gradio.routes.App at 0x7f122dd0f0d0&gt;,\n 'http://127.0.0.1:7860/',\n 'https://27541.gradio.app')\n\n\n\nFoodVision Mini Gradio demo running in Google Colab and in the browser (the link when running from Google Colab only lasts for 72 hours). You can see the permanent live demo on Hugging Face Spaces.\nWoohoo!!! What an epic demo!!!\nFoodVision Mini has officially come to life in an interface someone could use and try out.\nIf you set the parameter share=True in the launch() method, Gradio also provides you with a shareable link such as https://123XYZ.gradio.app (this link is an example only and likely expired) which is valid for 72-hours.\nThe link provides a proxy back to the Gradio interface you launched.\nFor more permanent hosting, you can upload your Gradio app to Hugging Face Spaces or anywhere that runs Python code.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#turning-our-foodvision-mini-gradio-demo-into-a-deployable-app",
    "href": "09_pytorch_model_deployment.html#turning-our-foodvision-mini-gradio-demo-into-a-deployable-app",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.14 8. Turning our FoodVision Mini Gradio Demo into a deployable app",
    "text": "11.14 8. Turning our FoodVision Mini Gradio Demo into a deployable app\nWeâ€™ve seen our FoodVision Mini model come to life through a Gradio demo.\nBut what if we wanted to share it with our friends?\nWell, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.\nTo make our FoodVision Mini demo more permanent, we can package it into an app and upload it to Hugging Face Spaces.\n\n11.14.1 8.1 What is Hugging Face Spaces?\nHugging Face Spaces is a resource that allows you to host and share machine learning apps.\nBuilding a demo is one of the best ways to showcase and test what youâ€™ve done.\nAnd Spaces allows you to do just that.\nYou can think of Hugging Face as the GitHub of machine learning.\nIf having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities.\n\nì°¸ê³ : There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors, however, weâ€™re going to use Hugging Face Spaces due to the ease of use and wide adoption by the machine learning community.\n\n\n\n11.14.2 8.2 Deployed Gradio app structure\nTo upload our demo Gradio app, weâ€™ll want to put everything relating to it into a single directory.\nFor example, our demo might live at the path demos/foodvision_mini/ with the file structure:\ndemos/\nâ””â”€â”€ foodvision_mini/\n    â”œâ”€â”€ 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n    â”œâ”€â”€ app.py\n    â”œâ”€â”€ examples/\n    â”‚   â”œâ”€â”€ example_1.jpg\n    â”‚   â”œâ”€â”€ example_2.jpg\n    â”‚   â””â”€â”€ example_3.jpg\n    â”œâ”€â”€ model.py\n    â””â”€â”€ requirements.txt\nWhere: * 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth is our trained PyTorch model file. * app.py contains our Gradio app (similar to the code that launched the app). * ì°¸ê³ : app.py is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called app.py to run. This is changeable in settings. * examples/ contains example images to use with our Gradio app. * model.py contains the model definition as well as any transforms associated with the model. * requirements.txt contains the dependencies to run our app such as torch, torchvision and gradio.\nWhy this way?\nBecause itâ€™s one of the simplest layouts we could begin with.\nOur focus is: experiment, experiment, experiment!\nThe quicker we can run smaller experiments, the better our bigger ones will be.\nWeâ€™re going to work towards recreating the structure above but you can see a live demo app running on Hugging Face Spaces as well as the file structure: * Live Gradio demo of FoodVision Mini ğŸ•ğŸ¥©ğŸ£. * FoodVision Mini file structure on Hugging Face Spaces.\n\n\n11.14.3 8.3 Creating a demos folder to store our FoodVision Mini app files\nTo begin, letâ€™s first create a demos/ directory to store all of our FoodVision Mini app files.\nWe can do with Pythonâ€™s pathlib.Path(\"path_to_dir\") to establish the directory path and pathlib.Path(\"path_to_dir\").mkdir() to create it.\n\nimport shutil\nfrom pathlib import Path\n\n# Create FoodVision mini demo path\nfoodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n\n# Remove files that might already exist there and create new directory\nif foodvision_mini_demo_path.exists():\n    shutil.rmtree(foodvision_mini_demo_path)\n# If the file doesn't exist, create it anyway\nfoodvision_mini_demo_path.mkdir(parents=True, \n                                exist_ok=True)\n    \n# Check what's in the folder\n!ls demos/foodvision_mini/\n\n\n\n11.14.4 8.4 Creating a folder of example images to use with our FoodVision Mini demo\nNow weâ€™ve got a directory to store our FoodVision Mini demo files, letâ€™s add some examples to it.\nThree example images from the test dataset should be enough.\nTo do so weâ€™ll: 1. Create an examples/ directory within the demos/foodvision_mini directory. 2. Choose three random images from the test dataset and collect their filepaths in a list. 3. Copy the three random images from the test dataset to the demos/foodvision_mini/examples/ directory.\n\nimport shutil\nfrom pathlib import Path\n\n# 1. Create an examples directory\nfoodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\nfoodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n\n# 2. Collect three random test dataset image paths\nfoodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n\n# 3. Copy the three random images to the examples directory\nfor example in foodvision_mini_examples:\n    destination = foodvision_mini_examples_path / example.name\n    print(f\"[INFO] Copying {example} to {destination}\")\n    shutil.copy2(src=example, dst=destination)\n\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg to demos/foodvision_mini/examples/592799.jpg\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg to demos/foodvision_mini/examples/3622237.jpg\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg to demos/foodvision_mini/examples/2582289.jpg\n\n\nNow to verify our examples are present, letâ€™s list the contents of our demos/foodvision_mini/examples/ directory with os.listdir() and then format the filepaths into a list of lists (so itâ€™s compatible with Gradioâ€™s gradio.Interface() example parameter).\n\nimport os\n\n# Get example filepaths in a list of lists\nexample_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\nexample_list\n\n[['examples/3622237.jpg'], ['examples/592799.jpg'], ['examples/2582289.jpg']]\n\n\n\n\n11.14.5 8.5 Moving our trained EffNetB2 model to our FoodVision Mini demo directory\nWe previously saved our FoodVision Mini EffNetB2 feature extractor model under models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth.\nAnd rather double up on saved model files, letâ€™s move our model to our demos/foodvision_mini directory.\nWe can do so using Pythonâ€™s shutil.move() method and passing in src (the source path of the target file) and dst (the destination path of the target file to be moved to) parameters.\n\nimport shutil\n\n# Create a source path for our target model\neffnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n\n# Create a destination path for our target model \neffnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n\n# Try to move the file\ntry:\n    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n    \n    # Move the model\n    shutil.move(src=effnetb2_foodvision_mini_model_path, \n                dst=effnetb2_foodvision_mini_model_destination)\n    \n    print(f\"[INFO] Model move complete.\")\n\n# If the model has already been moved, check if it exists\nexcept:\n    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n\n[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n[INFO] Model move complete.\n\n\n\n\n11.14.6 8.6 Turning our EffNetB2 model into a Python script (model.py)\nOur current modelâ€™s state_dict is saved to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth.\nTo load it in we can use model.load_state_dict() along with torch.load().\n\nì°¸ê³ : For a refresh on saving and loading a model (or a modelâ€™s state_dict in PyTorch, see 01. PyTorch Workflow Fundamentals section 5: Saving and loading a PyTorch model or see the PyTorch recipe for What is a state_dict in PyTorch?\n\nBut before we can do this, we first need a way to instantiate a model.\nTo do this in a modular fashion weâ€™ll create a script called model.py which contains our create_effnetb2_model() function we created in section 3.1: Creating a function to make an EffNetB2 feature extractor.\nThat way we can import the function in another script (see app.py below) and then use it to create our EffNetB2 model instance as well as get its appropriate transforms.\nJust like in 05. PyTorch Going Modular, weâ€™ll use the %%writefile path/to/file magic command to turn a cell of code into a file.\n\n%%writefile demos/foodvision_mini/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n\nWriting demos/foodvision_mini/model.py\n\n\n\n\n11.14.7 8.7 Turning our FoodVision Mini Gradio app into a Python script (app.py)\nWeâ€™ve now got a model.py script as well as a path to a saved model state_dict that we can load in.\nTime to construct app.py.\nWe call it app.py because by default when you create a HuggingFace Space, it looks for a file called app.py to run and host (though you can change this in settings).\nOur app.py script will put together all of the pieces of the puzzle to create our Gradio demo and will have four main parts:\n\nImports and class names setup - Here weâ€™ll import the various dependencies for our demo including the create_effnetb2_model() function from model.py as well as setup the different class names for our FoodVision Mini app.\nModel and transforms preparation - Here weâ€™ll create an EffNetB2 model instance along with the transforms to go with it and then weâ€™ll load in the saved model weights/state_dict. When we load the model weâ€™ll also set map_location=torch.device(\"cpu\") in torch.load() so our model gets loaded onto the CPU regardless of the device it trained on (we do this because we wonâ€™t necessarily have a GPU when we deploy and weâ€™ll get an error if our model is trained on GPU but we try to deploy it to CPU without explicitly saying so).\nPredict function - Gradioâ€™s gradio.Interface() takes a fn parameter to map inputs to outputs, our predict() function will be the same as the one we defined above in section 7.2: Creating a function to map our inputs and outputs, it will take in an image and then use the loaded transforms to preprocess it before using the loaded model to make a prediction on it.\n\nì°¸ê³ : Weâ€™ll have to create the example list on the fly via the examples parameter. We can do so by creating a list of the files inside the examples/ directory with: [[\"examples/\" + example] for example in os.listdir(\"examples\")].\n\nGradio app - This is where the main logic of our demo will live, weâ€™ll create a gradio.Interface() instance called demo to put together our inputs, predict() function and outputs. And weâ€™ll finish the script by calling demo.launch() to launch our FoodVision Mini demo!\n\n\n%%writefile demos/foodvision_mini/app.py\n### 1. Imports and class names setup ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Setup class names\nclass_names = [\"pizza\", \"steak\", \"sushi\"]\n\n### 2. Model and transforms preparation ###\n\n# Create EffNetB2 model\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=3, # len(class_names) would also work\n)\n\n# Load saved weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n### 3. Predict function ###\n\n# Create predict function\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article strings\ntitle = \"FoodVision Mini ğŸ•ğŸ¥©ğŸ£\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create examples list from \"examples/\" directory\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    # Create examples list from \"examples/\" directory\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch()\n\nWriting demos/foodvision_mini/app.py\n\n\n\n\n11.14.8 8.8 Creating a requirements file for FoodVision Mini (requirements.txt)\nThe last file we need to create for our FoodVision Mini app is a requirements.txt file.\nThis will be a text file containing all of the required dependencies for our demo.\nWhen we deploy our demo app to Hugging Face Spaces, it will search through this file and install the dependencies we define so our app can run.\nThe good news is, thereâ€™s only three!\n\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n\nThe â€œ==1.12.0â€ states the version number to install.\nDefining the version number is not 100% required but we will for now so if any breaking updates occur in future releases, our app still runs (PS if you find any errors, feel free to post on the course GitHub Issues).\n\n%%writefile demos/foodvision_mini/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n\nWriting demos/foodvision_mini/requirements.txt\n\n\nNice!\nWeâ€™ve officially got all the files we need to deploy our FoodVision Mini demo!",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#deploying-our-foodvision-mini-app-to-huggingface-spaces",
    "href": "09_pytorch_model_deployment.html#deploying-our-foodvision-mini-app-to-huggingface-spaces",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.15 9. Deploying our FoodVision Mini app to HuggingFace Spaces",
    "text": "11.15 9. Deploying our FoodVision Mini app to HuggingFace Spaces\nWeâ€™ve got a file containing our FoodVision Mini demo, now how do we get it to run on Hugging Face Spaces?\nThere are two main options for uploading to a Hugging Face Space (also called a Hugging Face Repository, similar to a git repository): 1. Uploading via the Hugging Face Web interface (easiest). 2. Uploading via the command line or terminal. * Bonus: You can also use the huggingface_hub library to interact with Hugging Face, this would be a good extension to the above two options.\nFeel free to read the documentation on both options but weâ€™re going to go with option two.\n\nì°¸ê³ : To host anything on Hugging Face, you will need to sign up for a free Hugging Face account.\n\n\n11.15.1 9.1 Downloading our FoodVision Mini app files\nLetâ€™s check out the demo files weâ€™ve got inside demos/foodvision_mini.\nTo do so we can use the !ls command followed by the target filepath.\nls stands for â€œlistâ€ and the ! means we want to execute the command at the shell level.\n\n!ls demos/foodvision_mini\n\n09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\napp.py\nexamples\nmodel.py\nrequirements.txt\n\n\nThese are all files that weâ€™ve created!\nTo begin uploading our files to Hugging Face, letâ€™s now download them from Google Colab (or wherever youâ€™re running this notebook).\nTo do so, weâ€™ll first compress the files into a single zip folder via the command:\nzip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\nWhere: * zip stands for â€œzipâ€ as in â€œplease zip together the files in the following directoryâ€. * -r stands for â€œrecursiveâ€ as in, â€œgo through all of the files in the target directoryâ€. * ../foodvision_mini.zip is the target directory weâ€™d like our files to be zipped to. * * stands for â€œall the files in the current directoryâ€. * -x stands for â€œexclude these filesâ€.\nWe can download our zip file from Google Colab using google.colab.files.download(\"demos/foodvision_mini.zip\") (weâ€™ll put this inside a try and except block just in case weâ€™re not running the code inside Google Colab, and if so weâ€™ll print a message saying to manually download the files).\nLetâ€™s try it out!\n\n# Change into and then zip the foodvision_mini folder but exclude certain files\n!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Download the zipped FoodVision Mini app (if running in Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_mini.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")\n\nupdating: 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth (deflated 8%)\nupdating: app.py (deflated 57%)\nupdating: examples/ (stored 0%)\nupdating: examples/3622237.jpg (deflated 0%)\nupdating: examples/592799.jpg (deflated 1%)\nupdating: examples/2582289.jpg (deflated 17%)\nupdating: model.py (deflated 56%)\nupdating: requirements.txt (deflated 4%)\nNot running in Google Colab, can't use google.colab.files.download(), please manually download.\n\n\nWoohoo!\nLooks like our zip command was successful.\nIf youâ€™re running this notebook in Google Colab, you should see a file start to download in your browser.\nOtherwise, you can see the foodvision_mini.zip folder (and more) on the course GitHub under the demos/ directory.\n\n\n11.15.2 9.2 Running our FoodVision Mini demo locally\nIf you download the foodvision_mini.zip file, you can test it locally by: 1. Unzipping the file. 2. Opening terminal or a command line prompt. 3. Changing into the foodvision_mini directory (cd foodvision_mini). 4. Creating an environment (python3 -m venv env). 5. Activating the environment (source env/bin/activate). 5. Installing the requirements (pip install -r requirements.txt, the â€œ-râ€ is for recursive). * ì°¸ê³ : This step may take 5-10 minutes depending on your internet connection. And if youâ€™re facing errors, you may need to upgrade pip first: pip install --upgrade pip. 6. Run the app (python3 app.py).\nThis should result in a Gradio demo just like the one we built above running locally on your machine at a URL such as http://127.0.0.1:7860/.\n\nì°¸ê³ : If you run the app locally and you notice a flagged/ directory appear, it contains samples that have been â€œflaggedâ€.\nFor example, if someone tries the demo and the model produces an incorrect result, the sample can be â€œflaggedâ€ and reviewed for later.\nFor more on flagging in Gradio, see the flagging documentation.\n\n\n\n11.15.3 9.3 Uploading to Hugging Face\nWeâ€™ve verified our FoodVision Mini app works locally, however, the fun of creating a machine learning demo is to show it to other people and allow them to use it.\nTo do so, weâ€™re going to upload our FoodVision Mini demo to Hugging Face.\n\nì°¸ê³ : The following series of steps uses a Git (a file tracking system) workflow. For more on how Git works, Iâ€™d recommend going through the Git and GitHub for Beginners tutorial on freeCodeCamp.\n\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to your profile and then clicking â€œNew Spaceâ€.\n\nì°¸ê³ : A Space in Hugging Face is also known as a â€œcode repositoryâ€ (a place to store your code/files) or â€œrepoâ€ for short.\n\nGive the Space a name, for example, mine is called mrdbourke/foodvision_mini, you can see it here: https://huggingface.co/spaces/mrdbourke/foodvision_mini\nSelect a license (I used MIT).\nSelect Gradio as the Space SDK (software development kit).\n\nì°¸ê³ : You can use other options such as Streamlit but since our app is built with Gradio, weâ€™ll stick with that.\n\nChoose whether your Space is itâ€™s public or private (I selected public since Iâ€™d like my Space to be available to others).\nClick â€œCreate Spaceâ€.\nClone the repo locally by running something like: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in terminal or command prompt.\n\nì°¸ê³ : You can also add files via uploading them under the â€œFiles and versionsâ€ tab.\n\nCopy/move the contents of the downloaded foodvision_mini folder to the cloned repo folder.\nTo upload and track larger files (e.g.Â files over 10MB or in our case, our PyTorch model file) youâ€™ll need to install Git LFS (which stands for â€œgit large file storageâ€).\nAfter youâ€™ve installed Git LFS, you can activate it by running git lfs install.\nIn the foodvision_mini directory, track the files over 10MB with Git LFS with git lfs track \"*.file_extension\".\n\nTrack EffNetB2 PyTorch model file with git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\".\n\nTrack .gitattributes (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example .gitattributes file on the FoodVision Mini Hugging Face Space.\n\ngit add .gitattributes\n\nAdd the rest of the foodvision_mini app files and commit them with:\n\ngit add *\ngit commit -m \"first commit\"\n\nPush (upload) the files to Hugging Face:\n\ngit push\n\nWait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n\nIf everything worked, you should see a live running example of our FoodVision Mini Gradio demo like the one here: https://huggingface.co/spaces/mrdbourke/foodvision_mini\nAnd we can even embed our FoodVision Mini Gradio demo into our notebook as an iframe with IPython.display.IFrame and a link to our space in the format https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+.\n\n# IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#creating-foodvision-big",
    "href": "09_pytorch_model_deployment.html#creating-foodvision-big",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.16 10. Creating FoodVision Big",
    "text": "11.16 10. Creating FoodVision Big\nWeâ€™ve spent the past few sections and chapters working on bringing FoodVision Mini to life.\nAnd now weâ€™ve seen it working in a live demo, how about we step things up a notch?\nHow?\nFoodVision Big!\nSince FoodVision Mini is trained on pizza, steak and sushi images from the Food101 dataset (101 classes of food x 1000 images each), how about we make FoodVision Big by training a model on all 101 classes!\nWeâ€™ll go from three classes to 101!\nFrom pizza, steak, sushi to pizza, steak, sushi, hot dog, apple pie, carrot cake, chocolate cake, french fries, garlic bread, ramen, nachos, tacos and more!\nHow?\nWell, weâ€™ve got all the steps in place, all we have to do is alter our EffNetB2 model slightly as well as prepare a different dataset.\nTo finish Milestone Project 3, letâ€™s recreate a Gradio demo similar to FoodVision Mini (three classes) but for FoodVision Big (101 classes).\n\nFoodVision Mini works with three food classes: pizza, steak and sushi. And FoodVision Big steps it up a notch to work across 101 food classes: all of the classes in the Food101 dataset.\n\n11.16.1 10.1 Creating a model and transforms for FoodVision Big\nWhen creating FoodVision Mini we saw that the EffNetB2 model was a good tradeoff between speed and performance (it performed well with a fast speed).\nSo weâ€™ll continue using the same model for FoodVision Big.\nWe can create an EffNetB2 feature extractor for Food101 by using our create_effnetb2_model() function we created above, in section 3.1, and passing it the parameter num_classes=101 (since Food101 has 101 classes).\n\n# Create EffNetB2 model capable of fitting to 101 classes for Food101\neffnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\nBeautiful!\nLetâ€™s now get a summary of our model.\n\nfrom torchinfo import summary\n\n# # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n# summary(effnetb2_food101, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n\n\nNice!\nSee how just like our EffNetB2 model for FoodVision Mini the base layers are frozen (these are pretrained on ImageNet) and the outer layers (the classifier layers) are trainable with an output shape of [batch_size, 101] (101 for 101 classes in Food101).\nNow since weâ€™re going to be dealing with a fair bit more data than usual, how about we add a little data augmentation to our transforms (effnetb2_transforms) to augment the training data.\n\nì°¸ê³ : Data augmentation is a technique used to alter the appearance of an input training sample (e.g.Â rotating an image or slightly skewing it) to artificially increase the diversity of a training dataset to hopefully prevent overfitting. You can see more on data augmentation in 04. PyTorch Custom Datasets section 6.\n\nLetâ€™s compose a torchvision.transforms pipeline to use torchvision.transforms.TrivialAugmentWide() (the same data augmentation used by the PyTorch team in their computer vision recipes) as well as the effnetb2_transforms to transform our training images.\n\n# Create Food101 training data transforms (only perform data augmentation on the training images)\nfood101_train_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.TrivialAugmentWide(),\n    effnetb2_transforms,\n])\n\nEpic!\nNow letâ€™s compare food101_train_transforms (for the training data) and effnetb2_transforms (for the testing/inference data).\n\nprint(f\"Training transforms:\\n{food101_train_transforms}\\n\") \nprint(f\"Testing transforms:\\n{effnetb2_transforms}\")\n\nTraining transforms:\nCompose(\n    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n    ImageClassification(\n    crop_size=[288]\n    resize_size=[288]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n)\n\nTesting transforms:\nImageClassification(\n    crop_size=[288]\n    resize_size=[288]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n\n\n\n\n11.16.2 10.2 Getting data for FoodVision Big\nFor FoodVision Mini, we made our own custom data splits of the entire Food101 dataset.\nTo get the whole Food101 dataset, we can use torchvision.datasets.Food101().\nWeâ€™ll first setup a path to directory data/ to store the images.\nThen weâ€™ll download and transform the training and testing dataset splits using food101_train_transforms and effnetb2_transforms to transform each dataset respectively.\n\nì°¸ê³ : If youâ€™re using Google Colab, the cell below will take ~3-5 minutes to fully run and download the Food101 images from PyTorch.\nThis is because there is over 100,000 images being downloaded (101 classes x 1000 images per class). If you restart your Google Colab runtime and come back to this cell, the images will have to redownload. Alternatively, if youâ€™re running this notebook locally, the images will be cached and stored in the directory specified by the root parameter of torchvision.datasets.Food101().\n\n\nfrom torchvision import datasets\n\n# Setup data directory\nfrom pathlib import Path\ndata_dir = Path(\"data\")\n\n# Get training data (~750 images x 101 food classes)\ntrain_data = datasets.Food101(root=data_dir, # path to download data to\n                              split=\"train\", # dataset split to get\n                              transform=food101_train_transforms, # perform data augmentation on training data\n                              download=True) # want to download?\n\n# Get testing data (~250 images x 101 food classes)\ntest_data = datasets.Food101(root=data_dir,\n                             split=\"test\",\n                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n                             download=True)\n\nData downloaded!\nNow we can get a list of all the class names using train_data.classes.\n\n# Get Food101 class names\nfood101_class_names = train_data.classes\n\n# View the first 10\nfood101_class_names[:10]\n\n['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']\n\n\nHo ho! Those are some delicious sounding foods (although Iâ€™ve never heard of â€œbeignetsâ€â€¦ update: after a quick Google search, beignets also look delicious).\nYou can see a full list of the Food101 class names on the course GitHub under extras/food101_class_names.txt.\n\n\n11.16.3 10.3 Creating a subset of the Food101 dataset for faster experimenting\nThis is optional.\nWe donâ€™t need to create another subset of the Food101 dataset, we could train and evaluate a model across the whole 101,000 images.\nBut to keep training fast, letâ€™s create a 20% split of the training and test datasets.\nOur goal will be to see if we can beat the original Food101 paperâ€™s best results with only 20% of the data.\nTo breakdown the datasets weâ€™ve used/will use:\n\n\n\n\n\n\n\n\n\n\n\nNotebook(s)\nProject name\nDataset\nNumber of classes\nTraining images\nTesting images\n\n\n\n\n04, 05, 06, 07, 08\nFoodVision Mini (10% data)\nFood101 custom split\n3 (pizza, steak, sushi)\n225\n75\n\n\n07, 08, 09\nFoodVision Mini (20% data)\nFood101 custom split\n3 (pizza, steak, sushi)\n450\n150\n\n\n09 (this one)\nFoodVision Big (20% data)\nFood101 custom split\n101 (all Food101 classes)\n15150\n5050\n\n\nExtension\nFoodVision Big\nFood101 all data\n101\n75750\n25250\n\n\n\nCan you see the trend?\nJust like our model size slowly increased overtime, so has the size of the dataset weâ€™ve been using for experiments.\n\nì°¸ê³ : To truly beat the original Food101 paperâ€™s results with 20% of the data, weâ€™d have to train a model on 20% of the training data and then evaluate our model on the whole test set rather than the split we created. Iâ€™ll leave this as an extension exercise for you to try. Iâ€™d also encourage you to try training a model on the entire Food101 training dataset.\n\nTo make our FoodVision Big (20% data) split, letâ€™s create a function called split_dataset() to split a given dataset into certain proportions.\nWe can use torch.utils.data.random_split() to create splits of given sizes using the lengths parameter.\nThe lengths parameter accepts a list of desired split lengths where the total of the list must equal the overall length of the dataset.\nFor example, with a dataset of size 100, you could pass in lengths=[20, 80] to receive a 20% and 80% split.\nWeâ€™ll want our function to return two splits, one with the target length (e.g.Â 20% of the training data) and the other with the remaining length (e.g.Â the remaining 80% of the training data).\nFinally, weâ€™ll set generator parameter to a torch.manual_seed() value for reproducibility.\n\ndef split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n\n    Args:\n        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n        split_size (float, optional): How much of the dataset should be split? \n            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n        seed (int, optional): Seed for random generator. Defaults to 42.\n\n    Returns:\n        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n            random_split_2 is of size (1-split_size)*len(dataset).\n    \"\"\"\n    # Create split lengths based on original dataset length\n    length_1 = int(len(dataset) * split_size) # desired length\n    length_2 = len(dataset) - length_1 # remaining length\n        \n    # Print out info\n    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n    \n    # Create splits with given random seed\n    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n                                                                   lengths=[length_1, length_2],\n                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n    return random_split_1, random_split_2\n\nDataset split function created!\nNow letâ€™s test it out by creating a 20% training and testing dataset split of Food101.\n\n# Create training 20% split of Food101\ntrain_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n                                                 split_size=0.2)\n\n# Create testing 20% split of Food101\ntest_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n                                                split_size=0.2)\n\nlen(train_data_food101_20_percent), len(test_data_food101_20_percent)\n\n[INFO] Splitting dataset of length 75750 into splits of size: 15150 (20%), 60600 (80%)\n[INFO] Splitting dataset of length 25250 into splits of size: 5050 (20%), 20200 (80%)\n\n\n(15150, 5050)\n\n\nExcellent!\n\n\n11.16.4 10.4 Turning our Food101 datasets into DataLoaders\nNow letâ€™s turn our Food101 20% dataset splits into DataLoaderâ€™s using torch.utils.data.DataLoader().\nWeâ€™ll set shuffle=True for the training data only and the batch size to 32 for both datasets.\nAnd weâ€™ll set num_workers to 4 if the CPU count is available or 2 if itâ€™s not (though the value of num_workers is very experimental and will depend on the hardware youâ€™re using, thereâ€™s an active discussion thread about this on the PyTorch forums).\n\nimport os\nimport torch\n\nBATCH_SIZE = 32\nNUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n\n# Create Food101 20 percent training DataLoader\ntrain_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n                                                                  batch_size=BATCH_SIZE,\n                                                                  shuffle=True,\n                                                                  num_workers=NUM_WORKERS)\n# Create Food101 20 percent testing DataLoader\ntest_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n                                                                 batch_size=BATCH_SIZE,\n                                                                 shuffle=False,\n                                                                 num_workers=NUM_WORKERS)\n\n\n\n11.16.5 10.5 Training FoodVision Big model\nFoodVision Big model and DataLoaders ready!\nTime for training.\nWeâ€™ll create an optimizer using torch.optim.Adam() and a learning rate of 1e-3.\nAnd because weâ€™ve got so many classes, weâ€™ll also setup a loss function using torch.nn.CrossEntropyLoss() with label_smoothing=0.1, inline with torchvisionâ€™s state-of-the-art training recipe.\nWhatâ€™s label smoothing?\nLabel smoothing is a regularization technique (regularization is another word to describe the process of preventing overfitting) that reduces the value a model gives to anyone label and spreads it across the other labels.\nIn essence, rather than a model getting too confident on a single label, label smoothing gives a non-zero value to other labels to help aid in generalization.\nFor example, if a model without label smoothing had the following outputs for 5 classes:\n[0, 0, 0.99, 0.01, 0]\nA model with label smoothing may have the following outputs:\n[0.01, 0.01, 0.96, 0.01, 0.01]\nThe model is still confident on its prediction of class 3 but giving small values to the other labels forces the model to at least consider other options.\nFinally, to keep things quick, weâ€™ll train our model for five epochs using the engine.train() function we created in 05. PyTorch Going Modular section 4 with the goal of beating the original Food101 paperâ€™s result of 56.4% accuracy on the test set.\nLetâ€™s train our biggest model yet!\n\nì°¸ê³ : Running the cell below will take ~15-20 minutes to run on Google Colab. This is because itâ€™s training the biggest model with the largest amount of data weâ€™ve used so far (15,150 training images, 5050 testing images). And itâ€™s a reason we decided to split 20% of the full Food101 dataset off before (so training didnâ€™t take over an hour).\n\n\nfrom going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n                             lr=1e-3)\n\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n\n# Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset\nset_seeds()    \neffnetb2_food101_results = engine.train(model=effnetb2_food101,\n                                        train_dataloader=train_dataloader_food101_20_percent,\n                                        test_dataloader=test_dataloader_food101_20_percent,\n                                        optimizer=optimizer,\n                                        loss_fn=loss_fn,\n                                        epochs=5,\n                                        device=device)\n\n\n\n\nEpoch: 1 | train_loss: 3.6317 | train_acc: 0.2869 | test_loss: 2.7670 | test_acc: 0.4937\nEpoch: 2 | train_loss: 2.8615 | train_acc: 0.4388 | test_loss: 2.4653 | test_acc: 0.5387\nEpoch: 3 | train_loss: 2.6585 | train_acc: 0.4844 | test_loss: 2.3547 | test_acc: 0.5649\nEpoch: 4 | train_loss: 2.5494 | train_acc: 0.5116 | test_loss: 2.3038 | test_acc: 0.5755\nEpoch: 5 | train_loss: 2.5006 | train_acc: 0.5239 | test_loss: 2.2805 | test_acc: 0.5810\n\n\nWoohoo!!!!\nLooks like we beat the original Food101 paperâ€™s results of 56.4% accuracy with only 20% of the training data (though we only evaluated on 20% of the testing data too, to fully replicate the results, we could evaluate on 100% of the testing data).\nThatâ€™s the power of transfer learning!\n\n\n11.16.6 10.6 Inspecting loss curves of FoodVision Big model\nLetâ€™s make our FoodVision Big loss curves visual.\nWe can do so with the plot_loss_curves() function from helper_functions.py.\n\nfrom helper_functions import plot_loss_curves\n\n# Check out the loss curves for FoodVision Big\nplot_loss_curves(effnetb2_food101_results)\n\n\n\n\n\n\n\n\nNice!!!\nIt looks like our regularization techniques (data augmentation and label smoothing) helped prevent our model from overfitting (the training loss is still higher than the test loss) this indicates our model has a bit more capacity to learn and could improve with further training.\n\n\n11.16.7 10.7 Saving and loading FoodVision Big\nNow weâ€™ve trained our biggest model yet, letâ€™s save it so we can load it back in later.\n\nfrom going_modular.going_modular import utils\n\n# Create a model path\neffnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n\n# Save FoodVision Big model\nutils.save_model(model=effnetb2_food101,\n                 target_dir=\"models\",\n                 model_name=effnetb2_food101_model_path)\n\n[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n\n\nModel saved!\nBefore we move on, letâ€™s make sure we can load it back in.\nWeâ€™ll do so by creating a model instance first with create_effnetb2_model(num_classes=101) (101 classes for all Food101 classes).\nAnd then loading the saved state_dict() with torch.nn.Module.load_state_dict() and torch.load().\n\n# Create Food101 compatible EffNetB2 instance\nloaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\n# Load the saved model's state_dict()\nloaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\n\n11.16.8 10.8 Checking FoodVision Big model size\nOur FoodVision Big model is capable of classifying 101 classes versus FoodVision Miniâ€™s 3 classes, a 33.6x increase!\nHow does this affect the model size?\nLetâ€™s find out.\n\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")\n\nPretrained EffNetB2 feature extractor Food101 model size: 30 MB\n\n\nHmm, it looks like the model size stayed largely the same (30 MB for FoodVision Big and 29 MB for FoodVision Mini) despite the large increase in the number of classes.\nThis is because all the extra parameters for FoodVision Big are only in the last layer (the classifier head).\nAll of the base layers are the same between FoodVision Big and FoodVision Mini.\nGoing back up and comparing the model summaries will give more details.\n\n\n\n\n\n\n\n\n\n\nModel\nOutput shape (num classes)\nTrainable parameters\nTotal parameters\nModel size (MB)\n\n\n\n\nFoodVision Mini (EffNetB2 feature extractor)\n3\n4,227\n7,705,221\n29\n\n\nFoodVision Big (EffNetB2 feature extractor)\n101\n142,309\n7,843,303\n30",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#turning-our-foodvision-big-model-into-a-deployable-app",
    "href": "09_pytorch_model_deployment.html#turning-our-foodvision-big-model-into-a-deployable-app",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.17 11. Turning our FoodVision Big model into a deployable app",
    "text": "11.17 11. Turning our FoodVision Big model into a deployable app\nWeâ€™ve got a trained and saved EffNetB2 model on 20% of the Food101 dataset.\nAnd instead of letting our model live in a folder all its life, letâ€™s deploy it!\nWeâ€™ll deploy our FoodVision Big model in the same way we deployed our FoodVision Mini model, as a Gradio demo on Hugging Face Spaces.\nTo begin, letâ€™s create a demos/foodvision_big/ directory to store our FoodVision Big demo files as well as a demos/foodvision_big/examples directory to hold an example image to test the demo with.\nWhen weâ€™re finished weâ€™ll have the following file structure:\ndemos/\n  foodvision_big/\n    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n    app.py\n    class_names.txt\n    examples/\n      example_1.jpg\n    model.py\n    requirements.txt\nWhere: * 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth is our trained PyTorch model file. * app.py contains our FoodVision Big Gradio app. * class_names.txt contains all of the class names for FoodVision Big. * examples/ contains example images to use with our Gradio app. * model.py contains the model definition as well as any transforms associated with the model. * requirements.txt contains the dependencies to run our app such as torch, torchvision and gradio.\n\nfrom pathlib import Path\n\n# Create FoodVision Big demo path\nfoodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n\n# Make FoodVision Big demo directory\nfoodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n\n# Make FoodVision Big demo examples directory\n(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)\n\n\n11.17.1 11.1 Downloading an example image and moving it to the examples directory\nFor our example image, weâ€™re going to use the faithful pizza-dad image (a photo of my dad eating pizza).\nSo letâ€™s download it from the course GitHub via the !wget command and then we can move it to demos/foodvision_big/examples with the !mv command (short for â€œmoveâ€).\nWhile weâ€™re here weâ€™ll move our trained Food101 EffNetB2 model from models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth to demos/foodvision_big as well.\n\n# Download and move an example image\n!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n\n# Move trained model to FoodVision Big demo folder (will error if model is already moved)\n!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big\n\n--2022-08-25 14:24:41--  https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2874848 (2.7M) [image/jpeg]\nSaving to: '04-pizza-dad.jpegâ€™\n\n04-pizza-dad.jpeg   100%[===================&gt;]   2.74M  7.85MB/s    in 0.3s    \n\n2022-08-25 14:24:43 (7.85 MB/s) - '04-pizza-dad.jpegâ€™ saved [2874848/2874848]\n\n\n\n\n\n11.17.2 11.2 Saving Food101 class names to file (class_names.txt)\nBecause there are so many classes in the Food101 dataset, instead of storing them as a list in our app.py file, letâ€™s save them to a .txt file and read them in when necessary instead.\nWeâ€™ll just remind ourselves what they look like first by checking out food101_class_names.\n\n# Check out the first 10 Food101 class names\nfood101_class_names[:10]\n\n['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']\n\n\nWonderful, now we can write these to a text file by first creating a path to demos/foodvision_big/class_names.txt and then opening a file with Pythonâ€™s open() and then writing to it leaving a new line for each class.\nIdeally, we want our class names to be saved like:\napple_pie\nbaby_back_ribs\nbaklava\nbeef_carpaccio\nbeef_tartare\n...\n\n# Create path to Food101 class names\nfoodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n\n# Write Food101 class names list to file\nwith open(foodvision_big_class_names_path, \"w\") as f:\n    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class\n\n[INFO] Saving Food101 class names to demos/foodvision_big/class_names.txt\n\n\nExcellent, now letâ€™s make sure we can read them in.\nTo do so weâ€™ll use Pythonâ€™s open() in read mode (\"r\") and then use the readlines() method to read each line of our class_names.txt file.\nAnd we can save the class names to a list by stripping the newline value of each of them with a list comprehension and strip().\n\n# Open Food101 class names file and read each line into a list\nwith open(foodvision_big_class_names_path, \"r\") as f:\n    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n    \n# View the first 5 class names loaded back in\nfood101_class_names_loaded[:5]\n\n['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']\n\n\n\n\n11.17.3 11.3 Turning our FoodVision Big model into a Python script (model.py)\nJust like the FoodVision Mini demo, letâ€™s create a script thatâ€™s capable of instantiating an EffNetB2 feature extractor model along with its necessary transforms.\n\n%%writefile demos/foodvision_big/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n\nOverwriting demos/foodvision_big/model.py\n\n\n\n\n11.17.4 11.4 Turning our FoodVision Big Gradio app into a Python script (app.py)\nWeâ€™ve got a FoodVision Big model.py script, now letâ€™s create a FoodVision Big app.py script.\nThis will again mostly be the same as the FoodVision Mini app.py script except weâ€™ll change:\n\nImports and class names setup - The class_names variable will be a list for all of the Food101 classes rather than pizza, steak, sushi. We can access these via demos/foodvision_big/class_names.txt.\nModel and transforms preparation - The model will have num_classes=101 rather than num_classes=3. Weâ€™ll also be sure to load the weights from \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" (our FoodVision Big model path).\nPredict function - This will stay the same as FoodVision Miniâ€™s app.py.\nGradio app - The Gradio interface will have different title, description and article parameters to reflect the details of FoodVision Big.\n\nWeâ€™ll also make sure to save it to demos/foodvision_big/app.py using the %%writefile magic command.\n\n%%writefile demos/foodvision_big/app.py\n### 1. Imports and class names setup ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Setup class names\nwith open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n    class_names = [food_name.strip() for food_name in  f.readlines()]\n    \n### 2. Model and transforms preparation ###    \n\n# Create model\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=101, # could also use len(class_names)\n)\n\n# Load saved weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n### 3. Predict function ###\n\n# Create predict function\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article strings\ntitle = \"FoodVision Big ğŸ”ğŸ‘\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create examples list from \"examples/\" directory\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create Gradio interface \ndemo = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=[\n        gr.Label(num_top_classes=5, label=\"Predictions\"),\n        gr.Number(label=\"Prediction time (s)\"),\n    ],\n    examples=example_list,\n    title=title,\n    description=description,\n    article=article,\n)\n\n# Launch the app!\ndemo.launch()\n\nOverwriting demos/foodvision_big/app.py\n\n\n\n\n11.17.5 11.5 Creating a requirements file for FoodVision Big (requirements.txt)\nNow all we need is a requirements.txt file to tell our Hugging Face Space what dependencies our FoodVision Big app requires.\n\n%%writefile demos/foodvision_big/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n\nOverwriting demos/foodvision_big/requirements.txt\n\n\n\n\n11.17.6 11.6 Downloading our FoodVision Big app files\nWeâ€™ve got all the files we need to deploy our FoodVision Big app on Hugging Face, letâ€™s now zip them together and download them.\nWeâ€™ll use the same process we used for the FoodVision Mini app above in section 9.1: Downloading our Foodvision Mini app files.\n\n# Zip foodvision_big folder but exclude certain files\n!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Download the zipped FoodVision Big app (if running in Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_big.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download()\")\n\nupdating: 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth (deflated 8%)\nupdating: app.py (deflated 54%)\nupdating: class_names.txt (deflated 48%)\nupdating: examples/ (stored 0%)\nupdating: flagged/ (stored 0%)\nupdating: model.py (deflated 56%)\nupdating: requirements.txt (deflated 4%)\nupdating: examples/04-pizza-dad.jpg (deflated 0%)\nNot running in Google Colab, can't use google.colab.files.download()\n\n\n\n\n11.17.7 11.7 Deploying our FoodVision Big app to HuggingFace Spaces\nB, E, A, Utiful!\nTime to bring our biggest model of the whole course to life!\nLetâ€™s deploy our FoodVision Big Gradio demo to Hugging Face Spaces so we can test it interactively and let others experience the magic of our machine learning efforts!\n\nì°¸ê³ : There are several ways to upload files to Hugging Face Spaces. The following steps treat Hugging Face as a git repository to track files. However, you can also upload directly to Hugging Face Spaces via the web interface or by the huggingface_hub library.\n\nThe good news is, weâ€™ve already done the steps to do so with FoodVision Mini, so now all we have to do is customize them to suit FoodVision Big:\n\nSign up for a Hugging Face account.\nStart a new Hugging Face Space by going to your profile and then clicking â€œNew Spaceâ€.\n\nì°¸ê³ : A Space in Hugging Face is also known as a â€œcode repositoryâ€ (a place to store your code/files) or â€œrepoâ€ for short.\n\nGive the Space a name, for example, mine is called mrdbourke/foodvision_big, you can see it here: https://huggingface.co/spaces/mrdbourke/foodvision_big\nSelect a license (I used MIT).\nSelect Gradio as the Space SDK (software development kit).\n\nì°¸ê³ : You can use other options such as Streamlit but since our app is built with Gradio, weâ€™ll stick with that.\n\nChoose whether your Space is public or private (I selected public since Iâ€™d like my Space to be available to others).\nClick â€œCreate Spaceâ€.\nClone the repo locally by running: git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME] in terminal or command prompt.\n\nì°¸ê³ : You can also add files via uploading them under the â€œFiles and versionsâ€ tab.\n\nCopy/move the contents of the downloaded foodvision_big folder to the cloned repo folder.\nTo upload and track larger files (e.g.Â files over 10MB or in our case, our PyTorch model file) youâ€™ll need to install Git LFS (which stands for â€œgit large file storageâ€).\nAfter youâ€™ve installed Git LFS, you can activate it by running git lfs install.\nIn the foodvision_big directory, track the files over 10MB with Git LFS with git lfs track \"*.file_extension\".\n\nTrack EffNetB2 PyTorch model file with git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\".\nì°¸ê³ : If you get any errors uploading images, you may have to track them with git lfs too, for example git lfs track \"examples/04-pizza-dad.jpg\"\n\nTrack .gitattributes (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example .gitattributes file on the FoodVision Big Hugging Face Space.\n\ngit add .gitattributes\n\nAdd the rest of the foodvision_big app files and commit them with:\n\ngit add *\ngit commit -m \"first commit\"\n\nPush (upload) the files to Hugging Face:\n\ngit push\n\nWait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n\nIf everything worked correctly, our FoodVision Big Gradio demo should be ready to classify!\nYou can see my version here: https://huggingface.co/spaces/mrdbourke/foodvision_big/\nOr we can even embed our FoodVision Big Gradio demo right within our notebook as an iframe with IPython.display.IFrame and a link to our space in the format https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+.\n\n# IPython is a library to help work with Python interactively\nfrom IPython.display import IFrame\n\n# Embed FoodVision Big Gradio demo as an iFrame\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)\n\n\n        \n        \n\n\nHow cool is that!?!\nWeâ€™ve come a long way from building PyTorch models to predict a straight lineâ€¦ now weâ€™re building computer vision models accessible to people all around the world!",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#main-takeaways",
    "href": "09_pytorch_model_deployment.html#main-takeaways",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.18 Main takeaways",
    "text": "11.18 Main takeaways\n\nDeployment is as important as training. Once youâ€™ve got a good working model, your first question should be: how can I deploy this and make it accessible to others? Deployment allows you to test your model in the real world rather than on private training and test sets.\nThree questions for machine learning model deployment:\n\nWhatâ€™s the most ideal use case for the model (how well and how fast does it perform)?\nWhereâ€™s the model going to go (is it on-device or on the cloud)?\nHowâ€™s the model going to function (are predictions online or offline)?\n\nDeployment options are a plenty. But best to start simple. One of the best current ways (I say current because these things are always changing) is to use Gradio to create a demo and host it on Hugging Face Spaces. Start simple and scale up when needed.\nNever stop experimenting. Your machine learning model needs will likely change overtime so deploying a single model is not the last step. You might find the dataset changes, so youâ€™ll have to update your model. Or new research gets released and thereâ€™s a better architecture to use.\n\nSo deploying one model is an excellent step, but youâ€™ll likely want to update it over time.\n\nMachine learning model deployment is part of the engineering practice of MLOps (machine learning operations). MLOps is an extension of DevOps (development operations) and involves all the engineering parts around training a model: data collection and storage, data preprocessing, model deployment, model monitoring, versioning and more. Itâ€™s a rapidly evolving field but there are some solid resources out there to learn more, many of which are in PyTorch Extra Resources.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#ì—°ìŠµ-ë¬¸ì œ",
    "href": "09_pytorch_model_deployment.html#ì—°ìŠµ-ë¬¸ì œ",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.19 ì—°ìŠµ ë¬¸ì œ",
    "text": "11.19 ì—°ìŠµ ë¬¸ì œ\nAll of the exercises are focused on practicing the code above.\nYou should be able to complete them by referencing each section or by following the resource(s) linked.\nResources:\n\nExercise template notebook for 09.\nExample solutions notebook for 09 try the exercises before looking at this.\n\nSee a live video walkthrough of the solutions on YouTube (errors and all).\n\n\n\nMake and time predictions with both feature extractor models on the test dataset using the GPU (device=\"cuda\"). Compare the modelâ€™s prediction times on GPU vs CPU - does this close the gap between them? As in, does making predictions on the GPU make the ViT feature extractor prediction times closer to the EffNetB2 feature extractor prediction times?\n\nYouâ€™ll find code to do these steps in section 5. Making predictions with our trained models and timing them and section 6. Comparing model results, prediction times and size.\n\nThe ViT feature extractor seems to have more learning capacity (due to more parameters) than EffNetB2, how does it go on the larger 20% split of the entire Food101 dataset?\n\nTrain a ViT feature extractor on the 20% Food101 dataset for 5 epochs, just like we did with EffNetB2 in section 10. Creating FoodVision Big.\n\nMake predictions across the 20% Food101 test dataset with the ViT feature extractor from exercise 2 and find the â€œmost wrongâ€ predictions.\n\nThe predictions will be the ones with the highest prediction probability but with the wrong predicted label.\nWrite a sentence or two about why you think the model got these predictions wrong.\n\nEvaluate the ViT feature extractor across the whole Food101 test dataset rather than just the 20% version, how does it perform?\n\nDoes it beat the original Food101 paperâ€™s best result of 56.4% accuracy?\n\nHead to Paperswithcode.com and find the current best performing model on the Food101 dataset.\n\nWhat model architecture does it use?\n\nWrite down 1-3 potential failure points of our deployed FoodVision models and what some potential solutions might be.\n\nFor example, what happens if someone was to upload a photo that wasnâ€™t of food to our FoodVision Mini model?\n\nPick any dataset from torchvision.datasets and train a feature extractor model on it using a model from torchvision.models (you could use one of the models weâ€™ve already created, e.g.Â EffNetB2 or ViT) for 5 epochs and then deploy your model as a Gradio app to Hugging Face Spaces.\n\nYou may want to pick smaller dataset/make a smaller split of it so training doesnâ€™t take too long.\nIâ€™d love to see your deployed models! So be sure to share them in Discord or on the course GitHub Discussions page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  },
  {
    "objectID": "09_pytorch_model_deployment.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "href": "09_pytorch_model_deployment.html#ì¶”ê°€-í•™ìŠµ-ìë£Œ",
    "title": "11Â  09 - PyTorch ëª¨ë¸ ë°°í¬",
    "section": "11.20 ì¶”ê°€ í•™ìŠµ ìë£Œ",
    "text": "11.20 ì¶”ê°€ í•™ìŠµ ìë£Œ\n\nMachine learning model deployment is generally an engineering challenge rather than a pure machine learning challenge, see the PyTorch Extra Resources machine learning engineering section for resources on learning more.\n\nInside youâ€™ll find recommendations for resources such as Chip Huyenâ€™s book Designing Machine Learning Systems (especially chapter 7 on model deployment) and Goku Mohandasâ€™s Made with ML MLOps course.\n\nAs you start to build more and more of your own projects, youâ€™ll likely start using Git (and potentially GitHub) quite frequently. To learn more about both, Iâ€™d recommend the Git and GitHub for Beginners - Crash Course video on the freeCodeCamp YouTube channel.\nWeâ€™ve only scratched the surface with whatâ€™s possible with Gradio. For more, Iâ€™d recommend checking out the full documentation, especially:\n\nAll of the different kinds of input and output components.\nThe Gradio Blocks API for more advanced workflows.\nThe Hugging Face Course chapter on how to use Gradio with Hugging Face.\n\nEdge devices arenâ€™t limited to mobile phones, they include small computers like the Raspberry Pi and the PyTorch team have a fantastic blog post tutorial on deploying a PyTorch model to one.\nFor a fantastic guide on developing AI and ML-powered applications, see Googleâ€™s People + AI Guidebook. One of my favourites is the section on setting the right expectations.\n\nI covered more of these kinds of resources, including guides from Apple, Microsoft and more in the April 2021 edition of Machine Learning Monthly (a monthly newsletter I send out with the latest and greatest of the ML field).\n\nIf youâ€™d like to speed up your modelâ€™s runtime on CPU, you should be aware of TorchScript, ONNX (Open Neural Network Exchange) and OpenVINO. Going from pure PyTorch to ONNX/OpenVINO models Iâ€™ve seen a ~2x+ increase in performance.\nFor turning models into a deployable and scalable API, see the TorchServe library.\nFor a terrific example and rationale as to why deploying a machine learning model in the browser (a form of edge deployment) offers several benefits (no network transfer latency delay), see Jo Kristian Bergumâ€™s article on Moving ML Inference from the Cloud to the Edge.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>09 - PyTorch ëª¨ë¸ ë°°í¬</span>"
    ]
  }
]